{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    def allowed_moves(self):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def make_move(self, next_state):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def playable(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def predict_winner(self, state):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 939,
     "status": "ok",
     "timestamp": 1594664203723,
     "user": {
      "displayName": "Mikhail Gorshkov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhsKhAWvEyZtHQD5X60iltGBFnzYlE5gMp2b2wv=s64",
      "userId": "12299291148564397237"
     },
     "user_tz": -180
    },
    "id": "a13uhqunn6lI"
   },
   "outputs": [],
   "source": [
    "# Copied from https://github.com/neilslater/game_playing_scripts\n",
    "\n",
    "'''\n",
    "   Copyright 2017 Neil Slater\n",
    "\n",
    "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "   you may not use this file except in compliance with the License.\n",
    "   You may obtain a copy of the License at\n",
    "\n",
    "       http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "   Unless required by applicable law or agreed to in writing, software\n",
    "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "   See the License for the specific language governing permissions and\n",
    "   limitations under the License.\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "import random\n",
    "from itertools import groupby\n",
    "from datetime import datetime\n",
    "\n",
    "class TicTacToeGame(Game):\n",
    "    # 1 is for 'X', -1 is for 'O', 0 is for empty\n",
    "    N_CELLS = 9\n",
    "    \n",
    "    @staticmethod\n",
    "    def output_mark(num):\n",
    "        return 'X' if num == 1 else 'O' if num == -1 else ' '\n",
    "\n",
    "    def __init__(self):\n",
    "        self.state = (0,) * TicTacToeGame.N_CELLS\n",
    "        self.player = 1\n",
    "        self.winner = 0\n",
    "\n",
    "    def allowed_moves(self):\n",
    "        return [i for i in range(TicTacToeGame.N_CELLS) if self.state[i] == 0]\n",
    "\n",
    "    def move_to_state(self, move):\n",
    "        return self.state[:move] + (self.player,) + self.state[move+1:]\n",
    "    \n",
    "    def make_move(self, move):\n",
    "        if self.winner:\n",
    "            raise(Exception(\"Game already completed, cannot make another move!\"))\n",
    "        if not self.__valid_move(move):\n",
    "            raise(Exception(\"Cannot make move from position '{}' to '{}' for player {}\".format(\n",
    "                    self.state, move, self.player)))\n",
    "\n",
    "        next_state = self.move_to_state(move)\n",
    "        self.state = next_state\n",
    "        self.winner = self.predict_winner()\n",
    "        if self.winner:\n",
    "            self.player = 0\n",
    "        else:\n",
    "            self.player = -self.player\n",
    "\n",
    "    def playable(self):\n",
    "        return ( (not self.winner) and any(self.allowed_moves()) )\n",
    "\n",
    "    def predict_winner(self):\n",
    "        lines = [(0,1,2), (3,4,5), (6,7,8), (0,3,6), (1,4,7), (2,5,8), (0,4,8), (2,4,6)]\n",
    "        winner = 0\n",
    "        for line in lines:\n",
    "            line_state = self.state[line[0]] + self.state[line[1]] + self.state[line[2]]\n",
    "            if line_state == 3:\n",
    "                winner = 1\n",
    "            elif line_state == -3:\n",
    "                winner = -1\n",
    "        return winner\n",
    "\n",
    "    def __valid_move(self, move):\n",
    "        return move in self.allowed_moves()\n",
    "\n",
    "    def print_board(self):\n",
    "        s = self.state\n",
    "        def cell(index):\n",
    "            return TicTacToeGame.output_mark(s[index]) if s[index] != 0 else index\n",
    "        print('     {} | {} | {} '.format(cell(0), cell(1), cell(2)))\n",
    "        print('    -----------')\n",
    "        print('     {} | {} | {} '.format(cell(3), cell(4), cell(5)))\n",
    "        print('    -----------')\n",
    "        print('     {} | {} | {} '.format(cell(6), cell(7), cell(8)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, name, epsilon=0.1, player=None):\n",
    "        self.name = name\n",
    "        self.epsilon = epsilon\n",
    "        self.player = player\n",
    "        self.total_games_learned = 0\n",
    "    \n",
    "    def games_learned(self):\n",
    "        return self.total_games_learned\n",
    "    \n",
    "    def get_full_name(self):\n",
    "        return \"{}({})\".format(self.name, TicTacToeGame.output_mark(self.player))\n",
    "        \n",
    "    def learn_game(self, num_episodes=1000):\n",
    "        for episode in range(num_episodes):\n",
    "            train_data = self.learn_from_episode()\n",
    "            self.train(train_data)\n",
    "            \n",
    "        self.total_games_learned += num_episodes\n",
    "            \n",
    "    def learn_from_episode(self):\n",
    "        game = TicTacToeGame()\n",
    "        player = self.player\n",
    "        if not self.player:\n",
    "            if random.random() < 0.5:\n",
    "                self.player = 1\n",
    "            else:\n",
    "                self.player = -1\n",
    "            \n",
    "        opponent = Agent(\"opponent\", player=-self.player)       \n",
    "        train_data = []\n",
    "        \n",
    "        while game.playable():\n",
    "            our_turn = game.player == self.player\n",
    "            # save train data only for current player\n",
    "            if our_turn:\n",
    "                self.learn_from_move(game, train_data)\n",
    "            else:\n",
    "                move = opponent.play_select_move(game)\n",
    "                game.make_move(move)\n",
    "                \n",
    "        train_data.append({'state' : game.state, 'reward' : self.reward(game) })\n",
    "\n",
    "        self.player = player # restore player\n",
    "        return train_data\n",
    "\n",
    "    def learn_from_move(self, game, train_data):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def learn_select_move(self, game):\n",
    "        best_move = self.play_select_move(game)\n",
    "\n",
    "        if random.random() < self.epsilon:\n",
    "            selected_move = random.choice(game.allowed_moves())\n",
    "        else:\n",
    "            selected_move = best_move\n",
    "\n",
    "        return best_move, selected_move\n",
    "\n",
    "    def play_select_move(self, game):\n",
    "        assert game.player == self.player\n",
    "        allowed_state_values = self.predict_state_values(game)\n",
    "        return self.__argmax_V(allowed_state_values)\n",
    "        \n",
    "    def __argmax_V(self, state_values):\n",
    "        max_V = max(state_values.values())\n",
    "        chosen_state = random.choice([move for move, v in state_values.items() if v == max_V])\n",
    "        return chosen_state\n",
    "\n",
    "    def reward(self, game):\n",
    "        return game.winner * self.player\n",
    "    \n",
    "    def train(self, game, train_data):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def predict_state_value(self, state):\n",
    "        return 0.0\n",
    "        \n",
    "    def predict_state_values(self, game):\n",
    "        return dict((move, self.predict_state_value(game.move_to_state(move))) for move in game.allowed_moves())\n",
    "        \n",
    "    def state_after_my_move(self, state):\n",
    "        if self.player == 1:\n",
    "            return state.count(1) == state.count(-1) + 1\n",
    "        elif self.player == -1:\n",
    "            return state.count(1) == state.count(-1)\n",
    "        else:\n",
    "            assert False, \"incorrect player\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_game(agent1, agent2, verbose=False):\n",
    "    t = 0\n",
    "    random_players = False\n",
    "    if not agent1.player and not agent2.player:\n",
    "        random_players = True\n",
    "        if random.random() < 0.5:\n",
    "            agent1.player = 1\n",
    "            agent2.player = -1\n",
    "        else:\n",
    "            agent1.player = -1\n",
    "            agent2.player = 1\n",
    "    assert agent1.player != agent2.player\n",
    "    \n",
    "    game = TicTacToeGame()\n",
    "    agent_to_move = agent1 if agent1.player == 1 else agent2\n",
    "    while game.playable():\n",
    "        if verbose:\n",
    "            print(\" \\nTurn {}\\n\".format(t))\n",
    "            game.print_board()\n",
    "        move = agent_to_move.play_select_move(game)\n",
    "        if agent_to_move == agent1:\n",
    "            agent_to_move = agent2\n",
    "        else:\n",
    "            agent_to_move = agent1\n",
    "        game.make_move(move)\n",
    "        t += 1\n",
    "    if verbose:\n",
    "        print(\" \\nTurn {}\\n\".format(t))\n",
    "        game.print_board()\n",
    "    if random_players:\n",
    "        agent1.player = None\n",
    "        agent2.player = None\n",
    "    if game.winner:\n",
    "        if verbose:\n",
    "            print(\"\\n{} is the winner!\".format(game.winner))\n",
    "        return game.winner\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"\\nIt's a draw!\")\n",
    "        return 0\n",
    "\n",
    "def request_human_move(game):\n",
    "    allowed_moves = game.allowed_moves()\n",
    "    while True:\n",
    "        try:\n",
    "            return int(input(\"Choose move for '{}', from {} : \".format(TicTacToeGame.output_mark(game.player), allowed_moves)))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def interactive_game(agent):\n",
    "    t = 0\n",
    "    game = TicTacToeGame()\n",
    "    while game.playable():\n",
    "        print(\" \\nTurn {}\\n\".format(t))\n",
    "        game.print_board()\n",
    "        if game.player == agent.player:\n",
    "            move = agent.play_select_move(game)\n",
    "        else:\n",
    "            move = request_human_move(game)\n",
    "        game.make_move(move)\n",
    "        t += 1\n",
    "\n",
    "    print(\" \\nTurn {}\\n\".format(t))\n",
    "    game.print_board()\n",
    "\n",
    "    if game.winner:\n",
    "        winner = TicTacToeGame.output_mark(game.winner)\n",
    "        print(\"\\n{} is the winner!\".format(winner))\n",
    "        return winner\n",
    "    print(\"\\nIt's a draw!\")\n",
    "    return '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1311,
     "status": "ok",
     "timestamp": 1594664204114,
     "user": {
      "displayName": "Mikhail Gorshkov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhsKhAWvEyZtHQD5X60iltGBFnzYlE5gMp2b2wv=s64",
      "userId": "12299291148564397237"
     },
     "user_tz": -180
    },
    "id": "8LWBMtion6lO"
   },
   "outputs": [],
   "source": [
    "def demo_game_stats(agent1, agent2, num_games=1000, verbose=False):\n",
    "    results = [demo_game(agent1, agent2, verbose) for i in range(num_games)]\n",
    "    game_stats = {TicTacToeGame.output_mark(k): \"{:.1f}%\".format(0 if num_games == 0 else results.count(k)/num_games*100) for k in [1, -1, 0]}\n",
    "    return game_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent, learn_games=1000):\n",
    "    print ('Training {} by {} games'.format(agent.get_full_name(), learn_games))\n",
    "    agent.learn_game(learn_games)\n",
    "    print (\"{}: {} games learned\".format(agent.get_full_name(), agent.games_learned()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agents(agent1, agent2, demo_games=1000, verbose=False):\n",
    "    game_stats = demo_game_stats(agent1, agent2, num_games=demo_games, verbose=verbose)\n",
    "    print (\"{} vs {}: {}\".format(agent1.get_full_name(), agent2.get_full_name(), game_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(agent, demo_games=1000, verbose=False):\n",
    "    opponent = Agent(\"agent\", player=None if agent.player is None else -agent.player)\n",
    "    test_agents(agent, opponent, demo_games=demo_games, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(agent, rounds=10, demo_games=1000, epsilon_coeff=0.1, verbose=False):\n",
    "    test_agent(agent, demo_games, verbose=verbose)\n",
    "    epsilon = agent.epsilon\n",
    "    for i in range(rounds):\n",
    "        agent.epsilon *= epsilon_coeff\n",
    "        train_agent(agent)\n",
    "        test_agent(agent, demo_games, verbose=verbose)\n",
    "        \n",
    "    agent.epsilon = epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1314,
     "status": "ok",
     "timestamp": 1594664204113,
     "user": {
      "displayName": "Mikhail Gorshkov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhsKhAWvEyZtHQD5X60iltGBFnzYlE5gMp2b2wv=s64",
      "userId": "12299291148564397237"
     },
     "user_tz": -180
    },
    "id": "ScUaE0Zvn6lM"
   },
   "outputs": [],
   "source": [
    "class AgentDict(Agent):\n",
    "    def __init__(self, name, epsilon=0.1, alpha=1.0, player=None):\n",
    "        super(AgentDict, self).__init__(name, epsilon, player)\n",
    "        self.alpha = alpha\n",
    "        self.V = {}\n",
    "\n",
    "    def learn_from_move(self, game, train_data):\n",
    "        best_next_move, selected_next_move = self.learn_select_move(game)\n",
    "        best_next_state_value = self.predict_state_value(game.move_to_state(best_next_move))\n",
    "        \n",
    "        game.make_move(selected_next_move)\n",
    "        \n",
    "        train_data.append({'state' : game.state, 'reward' : best_next_state_value })\n",
    "    \n",
    "        return selected_next_move\n",
    "\n",
    "    def train(self, train_data):\n",
    "        for i in range(len(train_data)-1):\n",
    "            state = train_data[i]['state']\n",
    "            td_target = train_data[i+1]['reward']\n",
    "            current_state_value = self.V.get(state, 0.0)\n",
    "            value = current_state_value + self.alpha * (td_target - current_state_value)\n",
    "            #print (state, value)\n",
    "            self.V[state] = value\n",
    "\n",
    "    def predict_state_value(self, state):\n",
    "        assert self.state_after_my_move(state)\n",
    "        return self.V.get(state, 0.0)\n",
    "    \n",
    "    def round_V(self):\n",
    "        # After training, this makes action selection random from equally-good choices\n",
    "        for k in self.V.keys():\n",
    "            self.V[k] = round(self.V[k],1)\n",
    "\n",
    "    def save_v_table(self, filename):\n",
    "        with open(filename, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['State', 'Value'])\n",
    "            all_states = list(self.V.keys())\n",
    "            all_states.sort()\n",
    "            for state in all_states:\n",
    "                writer.writerow([state, self.V[state]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 517949,
     "status": "ok",
     "timestamp": 1594664898166,
     "user": {
      "displayName": "Mikhail Gorshkov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhsKhAWvEyZtHQD5X60iltGBFnzYlE5gMp2b2wv=s64",
      "userId": "12299291148564397237"
     },
     "user_tz": -180
    },
    "id": "NRvctiyPn6lR",
    "outputId": "79211cbb-f326-4060-c186-75018877b413"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_dict_X(X) vs agent(O): {'X': '55.3%', 'O': '27.7%', ' ': '17.0%'}\n",
      "Training agent_dict_X(X) by 1000 games\n",
      "agent_dict_X(X): 1000 games learned\n",
      "agent_dict_X(X) vs agent(O): {'X': '92.9%', 'O': '4.4%', ' ': '2.7%'}\n",
      "Training agent_dict_X(X) by 1000 games\n",
      "agent_dict_X(X): 2000 games learned\n",
      "agent_dict_X(X) vs agent(O): {'X': '94.5%', 'O': '1.8%', ' ': '3.7%'}\n",
      "Training agent_dict_X(X) by 1000 games\n",
      "agent_dict_X(X): 3000 games learned\n",
      "agent_dict_X(X) vs agent(O): {'X': '95.2%', 'O': '0.0%', ' ': '4.8%'}\n",
      "Training agent_dict_X(X) by 1000 games\n",
      "agent_dict_X(X): 4000 games learned\n",
      "agent_dict_X(X) vs agent(O): {'X': '95.7%', 'O': '0.1%', ' ': '4.2%'}\n",
      "Training agent_dict_X(X) by 1000 games\n",
      "agent_dict_X(X): 5000 games learned\n",
      "agent_dict_X(X) vs agent(O): {'X': '97.0%', 'O': '0.0%', ' ': '3.0%'}\n",
      "Training agent_dict_X(X) by 1000 games\n",
      "agent_dict_X(X): 6000 games learned\n",
      "agent_dict_X(X) vs agent(O): {'X': '96.1%', 'O': '0.0%', ' ': '3.9%'}\n",
      "Training agent_dict_X(X) by 1000 games\n",
      "agent_dict_X(X): 7000 games learned\n",
      "agent_dict_X(X) vs agent(O): {'X': '96.2%', 'O': '0.0%', ' ': '3.8%'}\n",
      "Training agent_dict_X(X) by 1000 games\n",
      "agent_dict_X(X): 8000 games learned\n",
      "agent_dict_X(X) vs agent(O): {'X': '95.4%', 'O': '0.0%', ' ': '4.6%'}\n",
      "Training agent_dict_X(X) by 1000 games\n",
      "agent_dict_X(X): 9000 games learned\n",
      "agent_dict_X(X) vs agent(O): {'X': '95.4%', 'O': '0.0%', ' ': '4.6%'}\n",
      "Training agent_dict_X(X) by 1000 games\n",
      "agent_dict_X(X): 10000 games learned\n",
      "agent_dict_X(X) vs agent(O): {'X': '96.3%', 'O': '0.0%', ' ': '3.7%'}\n"
     ]
    }
   ],
   "source": [
    "# Testing agents with dictionary\n",
    "# Train agent_dict_1 to play for 'X'\n",
    "agent_dict_X = AgentDict(\"agent_dict_X\", epsilon=0.8, alpha=0.8, player=1)\n",
    "train_and_test(agent_dict_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_dict_X.round_V()\n",
    "agent_dict_X.save_v_table(\"state_values.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "{0: 0.0, 1: 1.0, 2: 0.1, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.1, 8: 0.0}\n"
     ]
    }
   ],
   "source": [
    "state = (0,0,0,0,1,0,0,0,0)\n",
    "print (agent_dict_X.predict_state_value(state))\n",
    "g = TicTacToeGame()\n",
    "print (agent_dict_X.predict_state_values(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check agent_dict_X interactively\n",
    "interactive_game(agent_dict_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_dict_O(O) vs agent(X): {'X': '52.7%', 'O': '32.4%', ' ': '14.9%'}\n",
      "Training agent_dict_O(O) by 1000 games\n",
      "agent_dict_O(O): 1000 games learned\n",
      "agent_dict_O(O) vs agent(X): {'X': '27.4%', 'O': '55.8%', ' ': '16.8%'}\n",
      "Training agent_dict_O(O) by 1000 games\n",
      "agent_dict_O(O): 2000 games learned\n",
      "agent_dict_O(O) vs agent(X): {'X': '10.2%', 'O': '73.5%', ' ': '16.3%'}\n",
      "Training agent_dict_O(O) by 1000 games\n",
      "agent_dict_O(O): 3000 games learned\n",
      "agent_dict_O(O) vs agent(X): {'X': '7.9%', 'O': '74.5%', ' ': '17.6%'}\n",
      "Training agent_dict_O(O) by 1000 games\n",
      "agent_dict_O(O): 4000 games learned\n",
      "agent_dict_O(O) vs agent(X): {'X': '4.3%', 'O': '76.0%', ' ': '19.7%'}\n",
      "Training agent_dict_O(O) by 1000 games\n",
      "agent_dict_O(O): 5000 games learned\n",
      "agent_dict_O(O) vs agent(X): {'X': '3.7%', 'O': '75.6%', ' ': '20.7%'}\n",
      "Training agent_dict_O(O) by 1000 games\n",
      "agent_dict_O(O): 6000 games learned\n",
      "agent_dict_O(O) vs agent(X): {'X': '2.1%', 'O': '76.3%', ' ': '21.6%'}\n",
      "Training agent_dict_O(O) by 1000 games\n",
      "agent_dict_O(O): 7000 games learned\n",
      "agent_dict_O(O) vs agent(X): {'X': '0.7%', 'O': '76.2%', ' ': '23.1%'}\n",
      "Training agent_dict_O(O) by 1000 games\n",
      "agent_dict_O(O): 8000 games learned\n",
      "agent_dict_O(O) vs agent(X): {'X': '1.1%', 'O': '77.0%', ' ': '21.9%'}\n",
      "Training agent_dict_O(O) by 1000 games\n",
      "agent_dict_O(O): 9000 games learned\n",
      "agent_dict_O(O) vs agent(X): {'X': '0.2%', 'O': '79.4%', ' ': '20.4%'}\n",
      "Training agent_dict_O(O) by 1000 games\n",
      "agent_dict_O(O): 10000 games learned\n",
      "agent_dict_O(O) vs agent(X): {'X': '0.0%', 'O': '78.8%', ' ': '21.2%'}\n"
     ]
    }
   ],
   "source": [
    "# Train agent_dict_O to play for 'O'\n",
    "agent_dict_O = AgentDict(\"agent_dict_O\", epsilon=0.8, alpha=0.9, player=-1)\n",
    "\n",
    "train_and_test(agent_dict_O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Turn 0\n",
      "\n",
      "     0 | 1 | 2 \n",
      "    -----------\n",
      "     3 | 4 | 5 \n",
      "    -----------\n",
      "     6 | 7 | 8 \n",
      "Choose move for 'X', from [0, 1, 2, 3, 4, 5, 6, 7, 8] : 4\n",
      " \n",
      "Turn 1\n",
      "\n",
      "     0 | 1 | 2 \n",
      "    -----------\n",
      "     3 | X | 5 \n",
      "    -----------\n",
      "     6 | 7 | 8 \n",
      " \n",
      "Turn 2\n",
      "\n",
      "     O | 1 | 2 \n",
      "    -----------\n",
      "     3 | X | 5 \n",
      "    -----------\n",
      "     6 | 7 | 8 \n",
      "Choose move for 'X', from [1, 2, 3, 5, 6, 7, 8] : 2\n",
      " \n",
      "Turn 3\n",
      "\n",
      "     O | 1 | X \n",
      "    -----------\n",
      "     3 | X | 5 \n",
      "    -----------\n",
      "     6 | 7 | 8 \n",
      " \n",
      "Turn 4\n",
      "\n",
      "     O | 1 | X \n",
      "    -----------\n",
      "     3 | X | 5 \n",
      "    -----------\n",
      "     O | 7 | 8 \n",
      "Choose move for 'X', from [1, 3, 5, 7, 8] : 5\n",
      " \n",
      "Turn 5\n",
      "\n",
      "     O | 1 | X \n",
      "    -----------\n",
      "     3 | X | X \n",
      "    -----------\n",
      "     O | 7 | 8 \n",
      " \n",
      "Turn 6\n",
      "\n",
      "     O | 1 | X \n",
      "    -----------\n",
      "     O | X | X \n",
      "    -----------\n",
      "     O | 7 | 8 \n",
      "\n",
      "O is the winner!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'O'"
      ]
     },
     "execution_count": 877,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check agent_dict_O interactively\n",
    "interactive_game(agent_dict_O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_dict_X(X) vs agent_dict_O(O): {'X': '0.0%', 'O': '0.0%', ' ': '100.0%'}\n"
     ]
    }
   ],
   "source": [
    "# Test trained 'agent_dict_X' and 'agent_dict_O' agents together\n",
    "test_agents(agent_dict_X, agent_dict_O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 933,
     "status": "ok",
     "timestamp": 1594664203723,
     "user": {
      "displayName": "Mikhail Gorshkov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhsKhAWvEyZtHQD5X60iltGBFnzYlE5gMp2b2wv=s64",
      "userId": "12299291148564397237"
     },
     "user_tz": -180
    },
    "id": "tt8vRmMVP2z2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 988,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 930,
     "status": "ok",
     "timestamp": 1594664203724,
     "user": {
      "displayName": "Mikhail Gorshkov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhsKhAWvEyZtHQD5X60iltGBFnzYlE5gMp2b2wv=s64",
      "userId": "12299291148564397237"
     },
     "user_tz": -180
    },
    "id": "XPrjAayVPtlU"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(27, 144)\n",
    "        self.fc2 = nn.Linear(144, 288)        \n",
    "        self.fc3 = nn.Linear(288, 144)        \n",
    "        self.fc4 = nn.Linear(144, 36)        \n",
    "        self.fc5 = nn.Linear(36, 9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc5(x)\n",
    "        x = torch.tanh(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1074,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentNN(Agent):\n",
    "    INPUT_SIZE = TicTacToeGame.N_CELLS * 3 # for X, O and empty\n",
    "    \n",
    "    def __init__(self, name, epsilon=0.1, gamma=1.0, lr=0.001, player=1):\n",
    "        super(AgentNN, self).__init__(name, epsilon, player)\n",
    "        self.model = Net()\n",
    "        self.gamma = gamma\n",
    "        import copy\n",
    "        self.model_copy = copy.deepcopy(self.model)\n",
    "\n",
    "        self.optim = torch.optim.SGD(self.model.parameters(), lr=lr)\n",
    "        #self.optim = torch.optim.Adadelta(self.model.parameters())\n",
    "        self.loss = F.torch.nn.MSELoss\n",
    "        \n",
    "        self.batch_data = []\n",
    "\n",
    "        self.update_freq = 10\n",
    "        self.update_counter = self.update_freq\n",
    "        \n",
    "        self.memory = ReplayMemory(capacity=300)\n",
    "\n",
    "    def learn_from_move(self, game, train_data):\n",
    "        _, selected_next_move = self.learn_select_move(game)\n",
    "        state = [s * self.player for s in game.state]\n",
    "        game.make_move(selected_next_move)    \n",
    "        train_data.append({'state' : state, 'move' : selected_next_move, 'reward' : 0})\n",
    "        \n",
    "    def train(self, train_data):\n",
    "        #print (train_data)\n",
    "        states = []\n",
    "        moves = []\n",
    "        new_states = []\n",
    "        rewards = []\n",
    "        moves_made = len(train_data) - 1\n",
    "        for i in range(len(train_data) - 1):\n",
    "            states.append(train_data[i]['state'])\n",
    "            moves.append(train_data[i]['move'])\n",
    "            new_states.append(train_data[i+1]['state'])\n",
    "            rewards.append(train_data[i+1]['reward'])\n",
    "        done = [0] * moves_made\n",
    "        done[-1] = 1\n",
    "        \n",
    "        self.batch_data.append((states, moves, rewards, new_states, done))\n",
    "        if len(self.batch_data) > 16:\n",
    "            self.optimize()\n",
    "            self.batch_data = []\n",
    "\n",
    "        if self.update_counter == 0:\n",
    "            self.model_copy.load_state_dict(self.model.state_dict())\n",
    "            self.update_counter = self.update_freq\n",
    "        else:\n",
    "            self.update_counter -= 1\n",
    "\n",
    "    def optimize(self):\n",
    "        def prepare_batch_data(s):\n",
    "            return self.__tensor_from_states(s).view(-1, AgentNN.INPUT_SIZE)\n",
    "\n",
    "        random.shuffle(self.batch_data)\n",
    "        for batch in self.batch_data:\n",
    "            states, moves, rewards, new_states, done = batch\n",
    "            #print ('states=', states)\n",
    "            #print ('moves=', moves)\n",
    "            #print ('rewards=', rewards)\n",
    "\n",
    "            moves = torch.tensor(moves)\n",
    "            rewards = torch.tensor(rewards).type(torch.float32)\n",
    "            Q_pred = self.model(prepare_batch_data(states))\n",
    "            Q = Q_pred.clone().detach()\n",
    "\n",
    "            new_state_values = self.model_copy(prepare_batch_data(new_states)).detach()\n",
    "\n",
    "            max_Q_new = torch.max(new_state_values, axis=1).values\n",
    "            #print ('max_Q_new=', max_Q_new)\n",
    "            # Q(s, a) = r + gamma * max Q(s', a') \n",
    "            rewards[done == 0] = self.gamma * max_Q_new[done == 0]\n",
    "            Q[range(len(states)), moves] = rewards\n",
    "            Q = Q * (torch.tensor(states) == 0)  # 0 for non-legal moves\n",
    "\n",
    "            loss = self.loss()(Q_pred, Q)\n",
    "            self.optim.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optim.step()\n",
    "\n",
    "    def __tensor_from_states(self, states):    \n",
    "        states_tensor = []\n",
    "        for state in states:\n",
    "            state_tensor = []\n",
    "            assert len(state) == TicTacToeGame.N_CELLS\n",
    "            for sym in state:\n",
    "                state_tensor.append(1 if sym == 1 else 0)\n",
    "            for sym in state:\n",
    "                state_tensor.append(1 if sym == -1 else 0)\n",
    "            for sym in state:\n",
    "                state_tensor.append(1 if sym == 0 else 0)\n",
    "            states_tensor.append(state_tensor)\n",
    "        return torch.tensor(states_tensor).type(torch.float32)\n",
    "\n",
    "    def predict_state_value(self, state, player):\n",
    "        raise NotImplementedError # this method is not used\n",
    "    \n",
    "    def predict_state_values(self, game):\n",
    "        assert self.player == game.player\n",
    "        state = [s * self.player for s in game.state]\n",
    "        x = self.__tensor_from_states([state])\n",
    "        state_values = self.model(x).detach().view(-1).numpy()\n",
    "        predicted_state_values = {}\n",
    "        for i in range(TicTacToeGame.N_CELLS):\n",
    "            if game.state[i] == 0:\n",
    "                predicted_state_values[i] = state_values[i]\n",
    "        return predicted_state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1068,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 176588,
     "status": "aborted",
     "timestamp": 1594664379407,
     "user": {
      "displayName": "Mikhail Gorshkov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhsKhAWvEyZtHQD5X60iltGBFnzYlE5gMp2b2wv=s64",
      "userId": "12299291148564397237"
     },
     "user_tz": -180
    },
    "id": "zpV2rMXCn6lW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_nn_X(X) vs agent(O): {'X': '65.4%', 'O': '16.8%', ' ': '17.8%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 1000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '83.0%', 'O': '11.3%', ' ': '5.7%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 2000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '79.8%', 'O': '14.2%', ' ': '6.0%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 3000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '87.3%', 'O': '11.1%', ' ': '1.6%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 4000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '87.1%', 'O': '10.1%', ' ': '2.8%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 5000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '86.2%', 'O': '13.3%', ' ': '0.5%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 6000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '83.4%', 'O': '14.5%', ' ': '2.1%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 7000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '85.9%', 'O': '13.2%', ' ': '0.9%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 8000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '90.3%', 'O': '9.4%', ' ': '0.3%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 9000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '87.4%', 'O': '9.7%', ' ': '2.9%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 10000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '87.6%', 'O': '9.3%', ' ': '3.1%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 11000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '88.4%', 'O': '7.5%', ' ': '4.1%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 12000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '86.1%', 'O': '13.0%', ' ': '0.9%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 13000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '90.1%', 'O': '8.3%', ' ': '1.6%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 14000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '87.3%', 'O': '11.2%', ' ': '1.5%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 15000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '84.7%', 'O': '11.9%', ' ': '3.4%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 16000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '68.6%', 'O': '20.9%', ' ': '10.5%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 17000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '88.7%', 'O': '9.2%', ' ': '2.1%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 18000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '83.2%', 'O': '12.3%', ' ': '4.5%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 19000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '88.1%', 'O': '9.3%', ' ': '2.6%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 20000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '86.0%', 'O': '11.2%', ' ': '2.8%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 21000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '86.6%', 'O': '11.2%', ' ': '2.2%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 22000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '85.8%', 'O': '11.9%', ' ': '2.3%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 23000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '75.0%', 'O': '20.0%', ' ': '5.0%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 24000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '84.0%', 'O': '11.2%', ' ': '4.8%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 25000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '76.8%', 'O': '15.6%', ' ': '7.6%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 26000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '75.8%', 'O': '16.4%', ' ': '7.8%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 27000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '88.3%', 'O': '8.7%', ' ': '3.0%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 28000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '93.8%', 'O': '3.5%', ' ': '2.7%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 29000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '92.7%', 'O': '3.7%', ' ': '3.6%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 30000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '86.8%', 'O': '7.5%', ' ': '5.7%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 31000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '95.6%', 'O': '2.2%', ' ': '2.2%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 32000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '94.3%', 'O': '4.1%', ' ': '1.6%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 33000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '91.1%', 'O': '3.9%', ' ': '5.0%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 34000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '93.6%', 'O': '2.6%', ' ': '3.8%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 35000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '91.9%', 'O': '2.1%', ' ': '6.0%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 36000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '96.0%', 'O': '1.5%', ' ': '2.5%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 37000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '93.9%', 'O': '2.8%', ' ': '3.3%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 38000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '96.7%', 'O': '2.0%', ' ': '1.3%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 39000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '86.0%', 'O': '8.1%', ' ': '5.9%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 40000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '95.2%', 'O': '2.4%', ' ': '2.4%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 41000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '95.3%', 'O': '1.1%', ' ': '3.6%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 42000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '96.5%', 'O': '1.9%', ' ': '1.6%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 43000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '97.4%', 'O': '0.7%', ' ': '1.9%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 44000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '93.3%', 'O': '0.6%', ' ': '6.1%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 45000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '96.8%', 'O': '0.7%', ' ': '2.5%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 46000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '95.1%', 'O': '0.8%', ' ': '4.1%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 47000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '96.3%', 'O': '1.2%', ' ': '2.5%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 48000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '97.1%', 'O': '0.0%', ' ': '2.9%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 49000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '96.5%', 'O': '0.6%', ' ': '2.9%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 50000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '96.1%', 'O': '0.6%', ' ': '3.3%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 51000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '96.4%', 'O': '0.3%', ' ': '3.3%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 74000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '97.2%', 'O': '0.0%', ' ': '2.8%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 75000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '96.9%', 'O': '0.4%', ' ': '2.7%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 76000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '94.8%', 'O': '0.0%', ' ': '5.2%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 77000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '94.1%', 'O': '0.0%', ' ': '5.9%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 78000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '93.1%', 'O': '0.7%', ' ': '6.2%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 79000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '94.9%', 'O': '0.7%', ' ': '4.4%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 80000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '96.1%', 'O': '0.4%', ' ': '3.5%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 81000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '95.8%', 'O': '1.0%', ' ': '3.2%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 82000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '96.9%', 'O': '0.4%', ' ': '2.7%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 83000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '96.1%', 'O': '0.0%', ' ': '3.9%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 84000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '95.1%', 'O': '0.7%', ' ': '4.2%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 85000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '94.4%', 'O': '0.1%', ' ': '5.5%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 86000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '95.6%', 'O': '0.8%', ' ': '3.6%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 87000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '96.7%', 'O': '0.1%', ' ': '3.2%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 88000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '97.2%', 'O': '0.0%', ' ': '2.8%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 89000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '93.5%', 'O': '0.6%', ' ': '5.9%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 90000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '96.8%', 'O': '0.0%', ' ': '3.2%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 91000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '96.7%', 'O': '0.4%', ' ': '2.9%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 92000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '96.9%', 'O': '0.2%', ' ': '2.9%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 93000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '95.5%', 'O': '0.3%', ' ': '4.2%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 94000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '96.1%', 'O': '0.5%', ' ': '3.4%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 95000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '94.8%', 'O': '1.1%', ' ': '4.1%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 96000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '97.3%', 'O': '0.0%', ' ': '2.7%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 97000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '96.5%', 'O': '0.6%', ' ': '2.9%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 98000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '97.7%', 'O': '0.0%', ' ': '2.3%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 99000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '96.8%', 'O': '0.0%', ' ': '3.2%'}\n",
      "Training agent_nn_X(X) by 1000 games\n",
      "agent_nn_X(X): 100000 games learned\n",
      "agent_nn_X(X) vs agent(O): {'X': '97.0%', 'O': '0.0%', ' ': '3.0%'}\n"
     ]
    }
   ],
   "source": [
    "# Testing agents with NN\n",
    "# Train agent_nn_X to play for 'X'\n",
    "agent_nn_X = AgentNN(\"agent_nn_X\", epsilon=0.7, gamma=0.95, lr=0.1, player=1)\n",
    "train_and_test(agent_nn_X, rounds=100, epsilon_coeff=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1075,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n",
      "{0: -0.062125415, 1: -0.13336438, 2: -1.5240163e-05, 3: -0.1010155, 4: -0.026680224, 5: -0.18843895, 6: -0.08477706, 7: -0.12190332, 8: -0.023210267}\n"
     ]
    }
   ],
   "source": [
    "#state = (0,0,0,0,1,0,0,0,0)\n",
    "#print (agent_nn_X.predict_state_value(state,1))\n",
    "print (agent_nn_X.epsilon)\n",
    "g = TicTacToeGame()\n",
    "print (agent_nn_X.predict_state_values(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1076,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Turn 0\n",
      "\n",
      "     0 | 1 | 2 \n",
      "    -----------\n",
      "     3 | 4 | 5 \n",
      "    -----------\n",
      "     6 | 7 | 8 \n",
      " \n",
      "Turn 1\n",
      "\n",
      "     0 | 1 | X \n",
      "    -----------\n",
      "     3 | 4 | 5 \n",
      "    -----------\n",
      "     6 | 7 | 8 \n",
      "Choose move for 'O', from [0, 1, 3, 4, 5, 6, 7, 8] : 4\n",
      " \n",
      "Turn 2\n",
      "\n",
      "     0 | 1 | X \n",
      "    -----------\n",
      "     3 | O | 5 \n",
      "    -----------\n",
      "     6 | 7 | 8 \n",
      " \n",
      "Turn 3\n",
      "\n",
      "     X | 1 | X \n",
      "    -----------\n",
      "     3 | O | 5 \n",
      "    -----------\n",
      "     6 | 7 | 8 \n",
      "Choose move for 'O', from [1, 3, 5, 6, 7, 8] : 1\n",
      " \n",
      "Turn 4\n",
      "\n",
      "     X | O | X \n",
      "    -----------\n",
      "     3 | O | 5 \n",
      "    -----------\n",
      "     6 | 7 | 8 \n",
      " \n",
      "Turn 5\n",
      "\n",
      "     X | O | X \n",
      "    -----------\n",
      "     3 | O | 5 \n",
      "    -----------\n",
      "     6 | X | 8 \n",
      "Choose move for 'O', from [3, 5, 6, 8] : 3\n",
      " \n",
      "Turn 6\n",
      "\n",
      "     X | O | X \n",
      "    -----------\n",
      "     O | O | 5 \n",
      "    -----------\n",
      "     6 | X | 8 \n",
      " \n",
      "Turn 7\n",
      "\n",
      "     X | O | X \n",
      "    -----------\n",
      "     O | O | X \n",
      "    -----------\n",
      "     6 | X | 8 \n",
      "Choose move for 'O', from [6, 8] : 6\n",
      " \n",
      "Turn 8\n",
      "\n",
      "     X | O | X \n",
      "    -----------\n",
      "     O | O | X \n",
      "    -----------\n",
      "     O | X | 8 \n",
      " \n",
      "Turn 9\n",
      "\n",
      "     X | O | X \n",
      "    -----------\n",
      "     O | O | X \n",
      "    -----------\n",
      "     O | X | X \n",
      "\n",
      "X is the winner!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'X'"
      ]
     },
     "execution_count": 1076,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check agent_nn_X interactively\n",
    "interactive_game(agent_nn_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1069,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_nn_O(O) vs agent(X): {'X': '49.0%', 'O': '41.6%', ' ': '9.4%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 1000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '39.4%', 'O': '52.0%', ' ': '8.6%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 2000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '34.1%', 'O': '59.2%', ' ': '6.7%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 3000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '43.6%', 'O': '50.5%', ' ': '5.9%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 4000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '27.5%', 'O': '68.5%', ' ': '4.0%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 5000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '32.4%', 'O': '63.1%', ' ': '4.5%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 6000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '38.7%', 'O': '56.7%', ' ': '4.6%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 7000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '25.9%', 'O': '69.1%', ' ': '5.0%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 8000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '26.8%', 'O': '68.8%', ' ': '4.4%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 9000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '32.8%', 'O': '63.3%', ' ': '3.9%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 10000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '30.4%', 'O': '68.2%', ' ': '1.4%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 11000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '31.3%', 'O': '63.4%', ' ': '5.3%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 12000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '25.7%', 'O': '70.6%', ' ': '3.7%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 13000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '32.9%', 'O': '65.9%', ' ': '1.2%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 14000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '36.4%', 'O': '62.0%', ' ': '1.6%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 15000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '26.5%', 'O': '69.7%', ' ': '3.8%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 16000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '35.4%', 'O': '62.3%', ' ': '2.3%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 17000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '31.7%', 'O': '67.5%', ' ': '0.8%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 18000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '26.9%', 'O': '69.1%', ' ': '4.0%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 19000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '28.8%', 'O': '66.5%', ' ': '4.7%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 20000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '32.3%', 'O': '64.9%', ' ': '2.8%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 21000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '23.4%', 'O': '70.2%', ' ': '6.4%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 22000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '34.8%', 'O': '63.4%', ' ': '1.8%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 23000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '25.6%', 'O': '73.4%', ' ': '1.0%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 24000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '24.5%', 'O': '71.5%', ' ': '4.0%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 25000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '16.7%', 'O': '80.6%', ' ': '2.7%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 26000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '25.0%', 'O': '74.9%', ' ': '0.1%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 27000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '25.6%', 'O': '67.3%', ' ': '7.1%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 28000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '31.7%', 'O': '61.9%', ' ': '6.4%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 29000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '27.0%', 'O': '67.6%', ' ': '5.4%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 30000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '23.6%', 'O': '72.5%', ' ': '3.9%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 31000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '26.6%', 'O': '65.9%', ' ': '7.5%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 32000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '14.6%', 'O': '82.0%', ' ': '3.4%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 33000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '14.4%', 'O': '82.2%', ' ': '3.4%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 34000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '16.6%', 'O': '79.7%', ' ': '3.7%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 35000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '24.1%', 'O': '69.2%', ' ': '6.7%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 36000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '13.4%', 'O': '76.5%', ' ': '10.1%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 37000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '22.2%', 'O': '66.3%', ' ': '11.5%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 38000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '18.6%', 'O': '75.7%', ' ': '5.7%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 39000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '24.1%', 'O': '68.4%', ' ': '7.5%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 40000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '23.5%', 'O': '72.9%', ' ': '3.6%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 41000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '22.9%', 'O': '66.5%', ' ': '10.6%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 42000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '15.4%', 'O': '77.9%', ' ': '6.7%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 43000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '22.3%', 'O': '64.9%', ' ': '12.8%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 44000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '18.3%', 'O': '70.2%', ' ': '11.5%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 45000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '12.7%', 'O': '81.6%', ' ': '5.7%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 46000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '14.8%', 'O': '71.9%', ' ': '13.3%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 47000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '17.3%', 'O': '73.3%', ' ': '9.4%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 48000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '17.4%', 'O': '70.0%', ' ': '12.6%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 49000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '18.6%', 'O': '69.9%', ' ': '11.5%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 50000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '19.8%', 'O': '71.5%', ' ': '8.7%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 51000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '16.8%', 'O': '74.5%', ' ': '8.7%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 52000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '15.9%', 'O': '73.1%', ' ': '11.0%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 53000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '19.2%', 'O': '59.5%', ' ': '21.3%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 54000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '17.6%', 'O': '62.6%', ' ': '19.8%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 55000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '11.6%', 'O': '79.6%', ' ': '8.8%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 56000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '14.6%', 'O': '70.1%', ' ': '15.3%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 57000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '11.7%', 'O': '80.2%', ' ': '8.1%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 58000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '10.1%', 'O': '70.1%', ' ': '19.8%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 59000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '24.3%', 'O': '49.0%', ' ': '26.7%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 60000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '18.1%', 'O': '60.6%', ' ': '21.3%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 61000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '9.8%', 'O': '80.1%', ' ': '10.1%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 62000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '20.5%', 'O': '64.0%', ' ': '15.5%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 63000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '15.4%', 'O': '65.1%', ' ': '19.5%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 64000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '16.2%', 'O': '56.6%', ' ': '27.2%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 65000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '13.2%', 'O': '74.0%', ' ': '12.8%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 66000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '12.5%', 'O': '71.8%', ' ': '15.7%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 67000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '8.5%', 'O': '76.5%', ' ': '15.0%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 68000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '12.0%', 'O': '73.0%', ' ': '15.0%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 69000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '13.3%', 'O': '70.9%', ' ': '15.8%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 70000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '12.5%', 'O': '68.7%', ' ': '18.8%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 71000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '7.5%', 'O': '72.8%', ' ': '19.7%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 72000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '10.6%', 'O': '68.4%', ' ': '21.0%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 73000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '15.4%', 'O': '59.9%', ' ': '24.7%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 74000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '4.7%', 'O': '72.9%', ' ': '22.4%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 75000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '12.1%', 'O': '62.3%', ' ': '25.6%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 76000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '7.8%', 'O': '81.7%', ' ': '10.5%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 77000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '8.7%', 'O': '69.5%', ' ': '21.8%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 78000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '9.6%', 'O': '66.5%', ' ': '23.9%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 79000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '10.3%', 'O': '74.8%', ' ': '14.9%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 80000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '7.2%', 'O': '71.2%', ' ': '21.6%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 81000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '20.1%', 'O': '51.1%', ' ': '28.8%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 82000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '10.7%', 'O': '67.4%', ' ': '21.9%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 83000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '8.0%', 'O': '68.2%', ' ': '23.8%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 84000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '7.8%', 'O': '68.2%', ' ': '24.0%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 85000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '12.3%', 'O': '57.8%', ' ': '29.9%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 86000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '9.6%', 'O': '72.1%', ' ': '18.3%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 87000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '5.6%', 'O': '72.9%', ' ': '21.5%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 88000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '9.2%', 'O': '64.1%', ' ': '26.7%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 89000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '5.9%', 'O': '72.7%', ' ': '21.4%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 90000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '10.6%', 'O': '60.4%', ' ': '29.0%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 91000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '9.8%', 'O': '55.3%', ' ': '34.9%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 92000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '4.6%', 'O': '78.6%', ' ': '16.8%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 93000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '8.0%', 'O': '58.3%', ' ': '33.7%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 94000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '6.8%', 'O': '62.1%', ' ': '31.1%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 95000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '12.5%', 'O': '61.0%', ' ': '26.5%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 96000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '12.6%', 'O': '62.9%', ' ': '24.5%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 97000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '7.9%', 'O': '67.6%', ' ': '24.5%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 98000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '6.4%', 'O': '61.4%', ' ': '32.2%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 99000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '10.5%', 'O': '61.6%', ' ': '27.9%'}\n",
      "Training agent_nn_O(O) by 1000 games\n",
      "agent_nn_O(O): 100000 games learned\n",
      "agent_nn_O(O) vs agent(X): {'X': '11.1%', 'O': '59.9%', ' ': '29.0%'}\n"
     ]
    }
   ],
   "source": [
    "# Train agent_nn_O to play for 'O'\n",
    "agent_nn_O = AgentNN(\"agent_nn_O\", epsilon=0.7, gamma=0.95, lr=0.1, player=-1)\n",
    "train_and_test(agent_nn_O, rounds=100, epsilon_coeff=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1073,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Turn 0\n",
      "\n",
      "     0 | 1 | 2 \n",
      "    -----------\n",
      "     3 | 4 | 5 \n",
      "    -----------\n",
      "     6 | 7 | 8 \n",
      "Choose move for 'X', from [0, 1, 2, 3, 4, 5, 6, 7, 8] : 4\n",
      " \n",
      "Turn 1\n",
      "\n",
      "     0 | 1 | 2 \n",
      "    -----------\n",
      "     3 | X | 5 \n",
      "    -----------\n",
      "     6 | 7 | 8 \n",
      " \n",
      "Turn 2\n",
      "\n",
      "     0 | 1 | 2 \n",
      "    -----------\n",
      "     3 | X | 5 \n",
      "    -----------\n",
      "     6 | 7 | O \n",
      "Choose move for 'X', from [0, 1, 2, 3, 5, 6, 7] : 5\n",
      " \n",
      "Turn 3\n",
      "\n",
      "     0 | 1 | 2 \n",
      "    -----------\n",
      "     3 | X | X \n",
      "    -----------\n",
      "     6 | 7 | O \n",
      " \n",
      "Turn 4\n",
      "\n",
      "     0 | 1 | 2 \n",
      "    -----------\n",
      "     O | X | X \n",
      "    -----------\n",
      "     6 | 7 | O \n",
      "Choose move for 'X', from [0, 1, 2, 6, 7] : 7\n",
      " \n",
      "Turn 5\n",
      "\n",
      "     0 | 1 | 2 \n",
      "    -----------\n",
      "     O | X | X \n",
      "    -----------\n",
      "     6 | X | O \n",
      " \n",
      "Turn 6\n",
      "\n",
      "     0 | 1 | 2 \n",
      "    -----------\n",
      "     O | X | X \n",
      "    -----------\n",
      "     O | X | O \n",
      "Choose move for 'X', from [0, 1, 2] : 0\n",
      " \n",
      "Turn 7\n",
      "\n",
      "     X | 1 | 2 \n",
      "    -----------\n",
      "     O | X | X \n",
      "    -----------\n",
      "     O | X | O \n",
      " \n",
      "Turn 8\n",
      "\n",
      "     X | O | 2 \n",
      "    -----------\n",
      "     O | X | X \n",
      "    -----------\n",
      "     O | X | O \n",
      "Choose move for 'X', from [2] : 2\n",
      " \n",
      "Turn 9\n",
      "\n",
      "     X | O | X \n",
      "    -----------\n",
      "     O | X | X \n",
      "    -----------\n",
      "     O | X | O \n",
      "\n",
      "It's a draw!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'-'"
      ]
     },
     "execution_count": 1073,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check agent_nn_O interactively\n",
    "interactive_game(agent_nn_O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1070,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_nn_X(X) vs agent_nn_O(O): {'X': '0.0%', 'O': '0.0%', ' ': '100.0%'}\n"
     ]
    }
   ],
   "source": [
    "# Test trained agent_nn_X and agent_nn_O agents\n",
    "test_agents(agent_nn_X, agent_nn_O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1071,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_dict_X(X) vs agent_nn_O(O): {'X': '47.9%', 'O': '52.1%', ' ': '0.0%'}\n"
     ]
    }
   ],
   "source": [
    "# Test trained agent_dict_X and agent_nn_O agents\n",
    "test_agents(agent_dict_X, agent_nn_O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1072,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_nn_X(X) vs agent_dict_O(O): {'X': '0.0%', 'O': '0.0%', ' ': '100.0%'}\n"
     ]
    }
   ],
   "source": [
    "# Test trained agent_nn_X and agent_dict_O agents\n",
    "test_agents(agent_nn_X, agent_dict_O)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "tictactoe.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
