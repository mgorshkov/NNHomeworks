{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    def allowed_moves(self):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def make_move(self, next_state):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def playable(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def predict_winner(self, state):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 939,
     "status": "ok",
     "timestamp": 1594664203723,
     "user": {
      "displayName": "Mikhail Gorshkov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhsKhAWvEyZtHQD5X60iltGBFnzYlE5gMp2b2wv=s64",
      "userId": "12299291148564397237"
     },
     "user_tz": -180
    },
    "id": "a13uhqunn6lI"
   },
   "outputs": [],
   "source": [
    "# Copied from https://github.com/neilslater/game_playing_scripts\n",
    "\n",
    "'''\n",
    "   Copyright 2017 Neil Slater\n",
    "\n",
    "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "   you may not use this file except in compliance with the License.\n",
    "   You may obtain a copy of the License at\n",
    "\n",
    "       http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "   Unless required by applicable law or agreed to in writing, software\n",
    "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "   See the License for the specific language governing permissions and\n",
    "   limitations under the License.\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "import random\n",
    "from itertools import groupby\n",
    "\n",
    "class TicTacToeGame(Game):\n",
    "    # 1 is for 'X', -1 is for 'O', 0 is for empty\n",
    "    @staticmethod\n",
    "    def output_mark(num):\n",
    "        return 'X' if num == 1 else 'O' if num == -1 else ' '\n",
    "\n",
    "    def __init__(self):\n",
    "        self.state = (0,) * 9\n",
    "        self.player = 1\n",
    "        self.winner = 0\n",
    "\n",
    "    def allowed_moves(self):\n",
    "        moves = []\n",
    "        for i in range(len(self.state)):\n",
    "            if self.state[i] == 0:\n",
    "                moves.append(i)\n",
    "        return moves\n",
    "\n",
    "    def move_to_state(self, move):\n",
    "        return self.state[:move] + (self.player,) + self.state[move+1:]\n",
    "    \n",
    "    def make_move(self, move):\n",
    "        if self.winner:\n",
    "            raise(Exception(\"Game already completed, cannot make another move!\"))\n",
    "        if not self.__valid_move(move):\n",
    "            raise(Exception(\"Cannot make move from position '{}' to '{}' for player {}\".format(\n",
    "                    self.state, move, self.player)))\n",
    "\n",
    "        next_state = self.move_to_state(move)\n",
    "        self.state = next_state\n",
    "        self.winner = self.predict_winner()\n",
    "        if self.winner:\n",
    "            self.player = 0\n",
    "        else:\n",
    "            self.player = -self.player\n",
    "\n",
    "    def playable(self):\n",
    "        return ( (not self.winner) and any(self.allowed_moves()) )\n",
    "\n",
    "    def predict_winner(self):\n",
    "        lines = [(0,1,2), (3,4,5), (6,7,8), (0,3,6), (1,4,7), (2,5,8), (0,4,8), (2,4,6)]\n",
    "        winner = 0\n",
    "        for line in lines:\n",
    "            line_state = self.state[line[0]] + self.state[line[1]] + self.state[line[2]]\n",
    "            if line_state == 3:\n",
    "                winner = 1\n",
    "            elif line_state == -3:\n",
    "                winner = -1\n",
    "        return winner\n",
    "\n",
    "    def __valid_move(self, move):\n",
    "        return move in self.allowed_moves()\n",
    "\n",
    "    def print_board(self):\n",
    "        s = self.state\n",
    "        print('     {} | {} | {} '.format(TicTacToeGame.output_mark(s[0]), TicTacToeGame.output_mark(s[1]), TicTacToeGame.output_mark(s[2])))\n",
    "        print('    -----------')\n",
    "        print('     {} | {} | {} '.format(TicTacToeGame.output_mark(s[3]), TicTacToeGame.output_mark(s[4]), TicTacToeGame.output_mark(s[5])))\n",
    "        print('    -----------')\n",
    "        print('     {} | {} | {} '.format(TicTacToeGame.output_mark(s[6]), TicTacToeGame.output_mark(s[7]), TicTacToeGame.output_mark(s[8])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, name, epsilon=0.1, alpha=1.0, player_mark=None):\n",
    "        self.name = name\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.player_mark = player_mark\n",
    "        self.total_games_learned = 0\n",
    "    \n",
    "    def games_learned(self):\n",
    "        return self.total_games_learned\n",
    "    \n",
    "    def get_full_name(self):\n",
    "        return \"{}({})\".format(self.name, TicTacToeGame.output_mark(self.player_mark))\n",
    "        \n",
    "    def learn_game(self, num_episodes=1000):\n",
    "        for episode in range(num_episodes):\n",
    "            train_data = self.learn_from_episode()\n",
    "            self.train(train_data)\n",
    "            \n",
    "        self.total_games_learned += num_episodes\n",
    "            \n",
    "    def learn_from_episode(self):\n",
    "        game = TicTacToeGame()\n",
    "        random_player_mark = False\n",
    "        if not self.player_mark:\n",
    "            random_player_mark = True\n",
    "            if random.random() < 0.5:\n",
    "                self.player_mark = 1\n",
    "            else:\n",
    "                self.player_mark = -1\n",
    "            \n",
    "        opponent = AgentDict(\"opponent\", player_mark=-self.player_mark)       \n",
    "        train_data = []\n",
    "        \n",
    "        while game.playable():\n",
    "            our_turn = game.player == self.player_mark\n",
    "            # save train data only for current player\n",
    "            if our_turn:\n",
    "                self.learn_from_move(game, train_data)\n",
    "            else:\n",
    "                move = opponent.play_select_move(game)\n",
    "                game.make_move(move)\n",
    "\n",
    "        train_data.append({'state' : None, 'move' : None, 'new_state' : None, 'reward' : self.__reward(game)})\n",
    "        if random_player_mark:\n",
    "            self.player_mark = 0\n",
    "        return train_data\n",
    "\n",
    "    def learn_from_move(self, game, train_data):\n",
    "        best_next_move, selected_next_move = self.learn_select_move(game)\n",
    "        \n",
    "        next_state_value = self.predict_state_value(game.move_to_state(best_next_move), game.player)\n",
    "        state = game.state\n",
    "        \n",
    "        game.make_move(selected_next_move)\n",
    "        \n",
    "        train_data.append({'state' : state, 'move' : selected_next_move, 'new_state' : game.state,\n",
    "                           'reward' : next_state_value})\n",
    "        \n",
    "        return selected_next_move\n",
    "\n",
    "    def learn_select_move(self, game):\n",
    "        assert game.player == self.player_mark\n",
    "        \n",
    "        allowed_state_values = self.predict_state_values(game)\n",
    "        best_move = self.__argmax_V(allowed_state_values)\n",
    "\n",
    "        selected_move = best_move\n",
    "        if random.random() < self.epsilon:\n",
    "            selected_move = random.choice(game.allowed_moves())\n",
    "\n",
    "        return best_move, selected_move\n",
    "\n",
    "    def play_select_move(self, game):\n",
    "        assert game.player == self.player_mark\n",
    "        allowed_state_values = self.predict_state_values(game)\n",
    "        return self.__argmax_V(allowed_state_values)\n",
    "        \n",
    "    def __argmax_V(self, state_values):\n",
    "        max_V = max(state_values.values())\n",
    "        chosen_state = random.choice([move for move, v in state_values.items() if v == max_V])\n",
    "        return chosen_state\n",
    "\n",
    "    def __reward(self, game):\n",
    "        return game.winner * self.player_mark\n",
    "\n",
    "    def __request_human_move(self, game):\n",
    "        allowed_moves = [i for i in range(9) if game.state[i] == 0]\n",
    "        human_move = None\n",
    "        while not human_move:\n",
    "            human_move = int(input('Choose move for {}, from {} : '.format(game.player, allowed_moves)))\n",
    "        return human_move\n",
    "    \n",
    "    def train(self, game, train_data):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def predict_state_value(self, state, player_mark):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def predict_state_values(self, game):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_game(agent1, agent2, verbose=False):\n",
    "    t = 0\n",
    "    random_player_marks = False\n",
    "    if not agent1.player_mark and not agent2.player_mark:\n",
    "        random_player_marks = True\n",
    "        if random.random() < 0.5:\n",
    "            agent1.player_mark = 1\n",
    "            agent2.player_mark = -1\n",
    "        else:\n",
    "            agent1.player_mark = -1\n",
    "            agent2.player_mark = 1\n",
    "    assert agent1.player_mark != agent2.player_mark\n",
    "    \n",
    "    game = TicTacToeGame()\n",
    "    agent_to_move = agent1 if agent1.player_mark == 1 else agent2\n",
    "    while game.playable():\n",
    "        if verbose:\n",
    "            print(\" \\nTurn {}\\n\".format(t))\n",
    "            game.print_board()\n",
    "        move = agent_to_move.play_select_move(game)\n",
    "        if agent_to_move == agent1:\n",
    "            agent_to_move = agent2\n",
    "        else:\n",
    "            agent_to_move = agent1\n",
    "        game.make_move(move)\n",
    "        t += 1\n",
    "    if verbose:\n",
    "        print(\" \\nTurn {}\\n\".format(t))\n",
    "        game.print_board()\n",
    "    if random_player_marks:\n",
    "        agent1.player_mark = 0\n",
    "        agent2.player_mark = 0\n",
    "    if game.winner:\n",
    "        if verbose:\n",
    "            print(\"\\n{} is the winner!\".format(game.winner))\n",
    "        return game.winner\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"\\nIt's a draw!\")\n",
    "        return 0\n",
    "\n",
    "def interactive_game(game, agent):\n",
    "    t = 0\n",
    "    while game.playable():\n",
    "        print(\" \\nTurn {}\\n\".format(t))\n",
    "        game.print_board()\n",
    "        if game.player == agent.agent_player:\n",
    "            move = agent.play_select_move(game)\n",
    "        else:\n",
    "            move = agent.__request_human_move()\n",
    "        game.make_move(move)\n",
    "        t += 1\n",
    "\n",
    "    print(\" \\nTurn {}\\n\".format(t))\n",
    "    game.print_board()\n",
    "\n",
    "    if game.winner:\n",
    "        winner = TicTacToeGame.output_mark(game.winner)\n",
    "        print(\"\\n{} is the winner!\".format(winner))\n",
    "        return winner\n",
    "    print(\"\\nIt's a draw!\")\n",
    "    return '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1311,
     "status": "ok",
     "timestamp": 1594664204114,
     "user": {
      "displayName": "Mikhail Gorshkov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhsKhAWvEyZtHQD5X60iltGBFnzYlE5gMp2b2wv=s64",
      "userId": "12299291148564397237"
     },
     "user_tz": -180
    },
    "id": "8LWBMtion6lO"
   },
   "outputs": [],
   "source": [
    "def demo_game_stats(agent1, agent2, num_games=1000):\n",
    "    results = [demo_game(agent1, agent2) for i in range(num_games)]\n",
    "    game_stats = {TicTacToeGame.output_mark(k): \"{:.1f}%\".format(0 if num_games == 0 else results.count(k)/num_games*100) for k in [1, -1, 0]}\n",
    "    return game_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent, learn_games=10000):\n",
    "    print ('Training {} by {} games'.format(agent.get_full_name(), learn_games))\n",
    "    agent.learn_game(learn_games)\n",
    "    print (\"{}: {} games learned\".format(agent.get_full_name(), agent.games_learned()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agents(agent1, agent2, demo_games=1000):\n",
    "    game_stats = demo_game_stats(agent1, agent2, demo_games)\n",
    "    print (\"{} vs {}: {}\".format(agent1.get_full_name(), agent2.get_full_name(), game_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1314,
     "status": "ok",
     "timestamp": 1594664204113,
     "user": {
      "displayName": "Mikhail Gorshkov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhsKhAWvEyZtHQD5X60iltGBFnzYlE5gMp2b2wv=s64",
      "userId": "12299291148564397237"
     },
     "user_tz": -180
    },
    "id": "ScUaE0Zvn6lM"
   },
   "outputs": [],
   "source": [
    "class AgentDict(Agent):\n",
    "    def __init__(self, name, epsilon=0.1, alpha=1.0, player_mark=None):\n",
    "        super(AgentDict, self).__init__(name, epsilon, alpha, player_mark)\n",
    "        self.V = {}\n",
    "\n",
    "    def train(self, train_data):\n",
    "        for i in range(len(train_data)-1):\n",
    "            new_state = train_data[i]['new_state']\n",
    "            td_target = train_data[i+1]['reward']\n",
    "            #print (new_state, td_target)\n",
    "            current_state_value = self.predict_state_value(new_state, self.player_mark)\n",
    "            value = current_state_value + self.alpha * (td_target - current_state_value)\n",
    "            self.V[new_state] = value\n",
    "\n",
    "    def predict_state_value(self, state, player_mark):\n",
    "        return self.V.get(state, 0.0)\n",
    "    \n",
    "    def predict_state_values(self, game):\n",
    "        return dict((move, self.predict_state_value(game.move_to_state(move), game.player)) for move in game.allowed_moves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 517949,
     "status": "ok",
     "timestamp": 1594664898166,
     "user": {
      "displayName": "Mikhail Gorshkov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhsKhAWvEyZtHQD5X60iltGBFnzYlE5gMp2b2wv=s64",
      "userId": "12299291148564397237"
     },
     "user_tz": -180
    },
    "id": "NRvctiyPn6lR",
    "outputId": "79211cbb-f326-4060-c186-75018877b413"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_dict_1(X) vs agent_dict_2(O): {'X': '54.7%', 'O': '30.1%', ' ': '15.2%'}\n",
      "Training agent_dict_1(X) by 10000 games\n",
      "agent_dict_1(X): 10000 games learned\n",
      "agent_dict_1(X) vs agent_dict_2(O): {'X': '98.4%', 'O': '0.5%', ' ': '1.1%'}\n",
      "Training agent_dict_1(X) by 10000 games\n",
      "agent_dict_1(X): 20000 games learned\n",
      "agent_dict_1(X) vs agent_dict_2(O): {'X': '99.0%', 'O': '0.9%', ' ': '0.1%'}\n",
      "Training agent_dict_1(X) by 10000 games\n",
      "agent_dict_1(X): 30000 games learned\n",
      "agent_dict_1(X) vs agent_dict_2(O): {'X': '99.2%', 'O': '0.8%', ' ': '0.0%'}\n",
      "Training agent_dict_1(X) by 10000 games\n",
      "agent_dict_1(X): 40000 games learned\n",
      "agent_dict_1(X) vs agent_dict_2(O): {'X': '97.5%', 'O': '1.9%', ' ': '0.6%'}\n",
      "Training agent_dict_1(X) by 10000 games\n",
      "agent_dict_1(X): 50000 games learned\n",
      "agent_dict_1(X) vs agent_dict_2(O): {'X': '98.4%', 'O': '0.0%', ' ': '1.6%'}\n",
      "Training agent_dict_1(X) by 10000 games\n",
      "agent_dict_1(X): 60000 games learned\n",
      "agent_dict_1(X) vs agent_dict_2(O): {'X': '98.5%', 'O': '0.8%', ' ': '0.7%'}\n",
      "Training agent_dict_1(X) by 10000 games\n",
      "agent_dict_1(X): 70000 games learned\n",
      "agent_dict_1(X) vs agent_dict_2(O): {'X': '98.0%', 'O': '1.5%', ' ': '0.5%'}\n",
      "Training agent_dict_1(X) by 10000 games\n",
      "agent_dict_1(X): 80000 games learned\n",
      "agent_dict_1(X) vs agent_dict_2(O): {'X': '99.3%', 'O': '0.0%', ' ': '0.7%'}\n",
      "Training agent_dict_1(X) by 10000 games\n",
      "agent_dict_1(X): 90000 games learned\n",
      "agent_dict_1(X) vs agent_dict_2(O): {'X': '99.4%', 'O': '0.3%', ' ': '0.3%'}\n",
      "Training agent_dict_1(X) by 10000 games\n",
      "agent_dict_1(X): 100000 games learned\n",
      "agent_dict_1(X) vs agent_dict_2(O): {'X': '97.8%', 'O': '0.7%', ' ': '1.5%'}\n"
     ]
    }
   ],
   "source": [
    "# Testing agents with dictionary\n",
    "# Train agent_dict_1 to play for 'X'\n",
    "agent_dict_1 = AgentDict(\"agent_dict_1\", epsilon=0.8, alpha=0.9, player_mark=1)\n",
    "agent_dict_2 = AgentDict(\"agent_dict_2\", epsilon=0.8, alpha=0.9, player_mark=-1)\n",
    "\n",
    "rounds = 10\n",
    "test_agents(agent_dict_1, agent_dict_2)\n",
    "for round in range(rounds):\n",
    "    train_agent(agent_dict_1)\n",
    "    test_agents(agent_dict_1, agent_dict_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_dict_3(O) vs agent_dict_4(X): {'X': '54.7%', 'O': '27.7%', ' ': '17.6%'}\n",
      "Training agent_dict_3(O) by 10000 games\n",
      "agent_dict_3(O): 10000 games learned\n",
      "agent_dict_3(O) vs agent_dict_4(X): {'X': '9.2%', 'O': '88.2%', ' ': '2.6%'}\n",
      "Training agent_dict_3(O) by 10000 games\n",
      "agent_dict_3(O): 20000 games learned\n",
      "agent_dict_3(O) vs agent_dict_4(X): {'X': '9.9%', 'O': '85.5%', ' ': '4.6%'}\n",
      "Training agent_dict_3(O) by 10000 games\n",
      "agent_dict_3(O): 30000 games learned\n",
      "agent_dict_3(O) vs agent_dict_4(X): {'X': '8.2%', 'O': '87.0%', ' ': '4.8%'}\n",
      "Training agent_dict_3(O) by 10000 games\n",
      "agent_dict_3(O): 40000 games learned\n",
      "agent_dict_3(O) vs agent_dict_4(X): {'X': '13.4%', 'O': '81.2%', ' ': '5.4%'}\n",
      "Training agent_dict_3(O) by 10000 games\n",
      "agent_dict_3(O): 50000 games learned\n",
      "agent_dict_3(O) vs agent_dict_4(X): {'X': '8.5%', 'O': '88.7%', ' ': '2.8%'}\n",
      "Training agent_dict_3(O) by 10000 games\n",
      "agent_dict_3(O): 60000 games learned\n",
      "agent_dict_3(O) vs agent_dict_4(X): {'X': '9.4%', 'O': '86.1%', ' ': '4.5%'}\n",
      "Training agent_dict_3(O) by 10000 games\n",
      "agent_dict_3(O): 70000 games learned\n",
      "agent_dict_3(O) vs agent_dict_4(X): {'X': '7.4%', 'O': '88.2%', ' ': '4.4%'}\n",
      "Training agent_dict_3(O) by 10000 games\n",
      "agent_dict_3(O): 80000 games learned\n",
      "agent_dict_3(O) vs agent_dict_4(X): {'X': '8.5%', 'O': '88.0%', ' ': '3.5%'}\n",
      "Training agent_dict_3(O) by 10000 games\n",
      "agent_dict_3(O): 90000 games learned\n",
      "agent_dict_3(O) vs agent_dict_4(X): {'X': '8.9%', 'O': '85.8%', ' ': '5.3%'}\n",
      "Training agent_dict_3(O) by 10000 games\n",
      "agent_dict_3(O): 100000 games learned\n",
      "agent_dict_3(O) vs agent_dict_4(X): {'X': '8.6%', 'O': '86.6%', ' ': '4.8%'}\n"
     ]
    }
   ],
   "source": [
    "# Train agent_dict_3 to play for 'O'\n",
    "agent_dict_3 = AgentDict(\"agent_dict_3\", epsilon=0.8, alpha=0.9, player_mark=-1)\n",
    "agent_dict_4 = AgentDict(\"agent_dict_4\", epsilon=0.8, alpha=0.9, player_mark=1)\n",
    "\n",
    "rounds = 10\n",
    "test_agents(agent_dict_3, agent_dict_4)\n",
    "for round in range(rounds):\n",
    "    train_agent(agent_dict_3)\n",
    "    test_agents(agent_dict_3, agent_dict_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 933,
     "status": "ok",
     "timestamp": 1594664203723,
     "user": {
      "displayName": "Mikhail Gorshkov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhsKhAWvEyZtHQD5X60iltGBFnzYlE5gMp2b2wv=s64",
      "userId": "12299291148564397237"
     },
     "user_tz": -180
    },
    "id": "tt8vRmMVP2z2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 930,
     "status": "ok",
     "timestamp": 1594664203724,
     "user": {
      "displayName": "Mikhail Gorshkov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhsKhAWvEyZtHQD5X60iltGBFnzYlE5gMp2b2wv=s64",
      "userId": "12299291148564397237"
     },
     "user_tz": -180
    },
    "id": "XPrjAayVPtlU"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, lr=0.0001):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(27, 54)\n",
    "        self.fc2 = nn.Linear(54, 36)        \n",
    "        self.fc3 = nn.Linear(36, 9)\n",
    "\n",
    "        self.optim = torch.optim.SGD(self.parameters(), lr=lr)\n",
    "        self.loss = F.torch.nn.MSELoss()        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = torch.tanh(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentNN(Agent):\n",
    "    def __init__(self, name, epsilon=0.1, alpha=1.0, player_mark=1):\n",
    "        super(AgentNN, self).__init__(name, epsilon, alpha, player_mark)\n",
    "        self.model = Net(lr=0.05)\n",
    "        import copy\n",
    "        self.model_copy = copy.deepcopy(self.model)\n",
    "    \n",
    "    def train(self, train_data):\n",
    "        #print (train_data)\n",
    "        states = []\n",
    "        new_states = []\n",
    "        rewards = []\n",
    "        moves = []\n",
    "        for i in range(len(train_data) - 1):\n",
    "            new_states.append(train_data[i]['new_state'])\n",
    "            moves.append([train_data[i]['move']])\n",
    "            states.append(train_data[i]['state'])\n",
    "            rewards.append(train_data[i+1]['reward'])\n",
    "        moves = torch.tensor(moves)\n",
    "        rewards = torch.tensor(rewards).type(torch.float32)\n",
    "        y_pred = self.model(self.__tensor_from_states(states).view(-1, 27))\n",
    "        y = y_pred.clone().detach()\n",
    "        new_state_values = self.model_copy(self.__tensor_from_states(new_states).view(-1, 27)).detach()\n",
    "\n",
    "        value = torch.max(new_state_values, axis=1).values\n",
    "        rewards[:-1] = self.alpha * value[:-1] # propagate rewards to all moves\n",
    "        y[range(len(train_data) - 1), moves] = rewards\n",
    "\n",
    "        loss = self.model.loss(y_pred, y)\n",
    "        self.model.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.model.optim.step()\n",
    "        self.model_copy.load_state_dict(self.model.state_dict())\n",
    "        \n",
    "    def __tensor_from_states(self, states):    \n",
    "        states_tensor = []\n",
    "        for state in states:\n",
    "            assert len(state) == 9\n",
    "            state_tensor = []\n",
    "            if state is None:\n",
    "                for i in range(27):\n",
    "                    state_tensor += 0.0\n",
    "            else:\n",
    "                for sym in state:\n",
    "                    state_tensor.append(1 if sym == 1 else 0)\n",
    "                for sym in state:\n",
    "                    state_tensor.append(1 if sym == -1 else 0)\n",
    "                for sym in state:\n",
    "                    state_tensor.append(1 if sym == 0 else 0)\n",
    "            states_tensor.append(state_tensor)\n",
    "        return torch.tensor(states_tensor).type(torch.float32)\n",
    "\n",
    "    def learn_from_move(self, game, train_data):\n",
    "        state = game.state\n",
    "        best_next_move, selected_next_move = self.learn_select_move(game)\n",
    "        game.make_move(selected_next_move)\n",
    "        train_data.append({'state' : state, 'move' : selected_next_move, 'new_state' : game.state, 'reward' : 0})\n",
    "\n",
    "    def predict_state_value(self, state, player):\n",
    "        raise NotImplementedError # this method is not used\n",
    "    \n",
    "    def predict_state_values(self, game):\n",
    "        x = game.player * self.__tensor_from_states([game.state])\n",
    "        state_values = self.model(x).detach().view(-1).numpy()\n",
    "        predicted_state_values = {}\n",
    "        for i in range(9):\n",
    "            if game.state[i] == 0:\n",
    "                predicted_state_values[i] = state_values[i]\n",
    "        return predicted_state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 176588,
     "status": "aborted",
     "timestamp": 1594664379407,
     "user": {
      "displayName": "Mikhail Gorshkov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhsKhAWvEyZtHQD5X60iltGBFnzYlE5gMp2b2wv=s64",
      "userId": "12299291148564397237"
     },
     "user_tz": -180
    },
    "id": "zpV2rMXCn6lW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_nn_1(X) vs agent_dict(O): {'X': '35.8%', 'O': '40.3%', ' ': '23.9%'}\n",
      "Training agent_nn_1(X) by 10000 games\n",
      "agent_nn_1(X): 10000 games learned\n",
      "agent_nn_1(X) vs agent_dict(O): {'X': '72.1%', 'O': '18.6%', ' ': '9.3%'}\n",
      "Training agent_nn_1(X) by 10000 games\n",
      "agent_nn_1(X): 20000 games learned\n",
      "agent_nn_1(X) vs agent_dict(O): {'X': '87.6%', 'O': '11.0%', ' ': '1.4%'}\n",
      "Training agent_nn_1(X) by 10000 games\n",
      "agent_nn_1(X): 30000 games learned\n",
      "agent_nn_1(X) vs agent_dict(O): {'X': '94.9%', 'O': '3.4%', ' ': '1.7%'}\n",
      "Training agent_nn_1(X) by 10000 games\n",
      "agent_nn_1(X): 40000 games learned\n",
      "agent_nn_1(X) vs agent_dict(O): {'X': '84.6%', 'O': '5.8%', ' ': '9.6%'}\n",
      "Training agent_nn_1(X) by 10000 games\n",
      "agent_nn_1(X): 50000 games learned\n",
      "agent_nn_1(X) vs agent_dict(O): {'X': '96.0%', 'O': '2.0%', ' ': '2.0%'}\n",
      "Training agent_nn_1(X) by 10000 games\n",
      "agent_nn_1(X): 60000 games learned\n",
      "agent_nn_1(X) vs agent_dict(O): {'X': '99.1%', 'O': '0.0%', ' ': '0.9%'}\n",
      "Training agent_nn_1(X) by 10000 games\n",
      "agent_nn_1(X): 70000 games learned\n",
      "agent_nn_1(X) vs agent_dict(O): {'X': '96.6%', 'O': '0.9%', ' ': '2.5%'}\n",
      "Training agent_nn_1(X) by 10000 games\n",
      "agent_nn_1(X): 80000 games learned\n",
      "agent_nn_1(X) vs agent_dict(O): {'X': '96.7%', 'O': '0.4%', ' ': '2.9%'}\n",
      "Training agent_nn_1(X) by 10000 games\n",
      "agent_nn_1(X): 90000 games learned\n",
      "agent_nn_1(X) vs agent_dict(O): {'X': '91.6%', 'O': '3.7%', ' ': '4.7%'}\n",
      "Training agent_nn_1(X) by 10000 games\n",
      "agent_nn_1(X): 100000 games learned\n",
      "agent_nn_1(X) vs agent_dict(O): {'X': '97.2%', 'O': '0.0%', ' ': '2.8%'}\n"
     ]
    }
   ],
   "source": [
    "# Testing agents with NN\n",
    "# Train agent_nn_1 to play for 'X'\n",
    "agent_nn_1 = AgentNN(\"agent_nn_1\", epsilon=0.7, alpha=0.9, player_mark=1)\n",
    "agent_dict = AgentDict(\"agent_dict\", epsilon=0.7, alpha=0.9, player_mark=-1)\n",
    "rounds = 10\n",
    "test_agents(agent_nn_1, agent_dict)\n",
    "for round in range(rounds):\n",
    "    train_agent(agent_nn_1)\n",
    "    test_agents(agent_nn_1, agent_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_nn_2(O) vs agent_dict(X): {'X': '53.3%', 'O': '29.0%', ' ': '17.7%'}\n",
      "Training agent_nn_2(O) by 10000 games\n",
      "agent_nn_2(O): 10000 games learned\n",
      "agent_nn_2(O) vs agent_dict(X): {'X': '47.5%', 'O': '25.1%', ' ': '27.4%'}\n",
      "Training agent_nn_2(O) by 10000 games\n",
      "agent_nn_2(O): 20000 games learned\n",
      "agent_nn_2(O) vs agent_dict(X): {'X': '38.5%', 'O': '38.4%', ' ': '23.1%'}\n",
      "Training agent_nn_2(O) by 10000 games\n",
      "agent_nn_2(O): 30000 games learned\n",
      "agent_nn_2(O) vs agent_dict(X): {'X': '32.4%', 'O': '36.1%', ' ': '31.5%'}\n",
      "Training agent_nn_2(O) by 10000 games\n",
      "agent_nn_2(O): 40000 games learned\n",
      "agent_nn_2(O) vs agent_dict(X): {'X': '28.1%', 'O': '45.4%', ' ': '26.5%'}\n",
      "Training agent_nn_2(O) by 10000 games\n",
      "agent_nn_2(O): 50000 games learned\n",
      "agent_nn_2(O) vs agent_dict(X): {'X': '26.9%', 'O': '44.4%', ' ': '28.7%'}\n",
      "Training agent_nn_2(O) by 10000 games\n",
      "agent_nn_2(O): 60000 games learned\n",
      "agent_nn_2(O) vs agent_dict(X): {'X': '27.4%', 'O': '45.7%', ' ': '26.9%'}\n",
      "Training agent_nn_2(O) by 10000 games\n",
      "agent_nn_2(O): 70000 games learned\n",
      "agent_nn_2(O) vs agent_dict(X): {'X': '27.4%', 'O': '47.1%', ' ': '25.5%'}\n",
      "Training agent_nn_2(O) by 10000 games\n",
      "agent_nn_2(O): 80000 games learned\n",
      "agent_nn_2(O) vs agent_dict(X): {'X': '20.1%', 'O': '49.6%', ' ': '30.3%'}\n",
      "Training agent_nn_2(O) by 10000 games\n",
      "agent_nn_2(O): 90000 games learned\n",
      "agent_nn_2(O) vs agent_dict(X): {'X': '28.7%', 'O': '41.7%', ' ': '29.6%'}\n",
      "Training agent_nn_2(O) by 10000 games\n",
      "agent_nn_2(O): 100000 games learned\n",
      "agent_nn_2(O) vs agent_dict(X): {'X': '25.9%', 'O': '47.3%', ' ': '26.8%'}\n"
     ]
    }
   ],
   "source": [
    "# Train agent_nn_2 to play for 'O'\n",
    "agent_nn_2 = AgentNN(\"agent_nn_2\", epsilon=0.7, alpha=0.9, player_mark=-1)\n",
    "agent_dict = AgentDict(\"agent_dict\", epsilon=0.7, alpha=0.9, player_mark=1)\n",
    "rounds = 10\n",
    "test_agents(agent_nn_2, agent_dict)\n",
    "for round in range(rounds):\n",
    "    train_agent(agent_nn_2)\n",
    "    test_agents(agent_nn_2, agent_dict)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "tictactoe.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
