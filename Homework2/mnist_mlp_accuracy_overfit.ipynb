{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "mnist_mlp.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "03df5bb909b443acaa9ee9e864c63c94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b41191ca561846bb89b066706b5898e3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ae8e56e0f29c4c1eab0af4e58a5a3858",
              "IPY_MODEL_9634b1bddb484f0d91733c1f7088ca8c"
            ]
          }
        },
        "b41191ca561846bb89b066706b5898e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ae8e56e0f29c4c1eab0af4e58a5a3858": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_753140a8106c4cf3aad2007ede661208",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1c6b5a75ab3f4c87afc5b548c8e168bb"
          }
        },
        "9634b1bddb484f0d91733c1f7088ca8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5f895587da77438ba01d78249a1c33bd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 9920512/? [00:20&lt;00:00, 745868.48it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9e5b92da8cc8412b99f5466acfe006b0"
          }
        },
        "753140a8106c4cf3aad2007ede661208": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1c6b5a75ab3f4c87afc5b548c8e168bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5f895587da77438ba01d78249a1c33bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9e5b92da8cc8412b99f5466acfe006b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2b4062f6083e4d14838a410e2717c8b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c09993c42a934100978b124aa4873aad",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7f58293c9cb8479a9ad31c3d7c5caa5e",
              "IPY_MODEL_53f2c511f7e44fe6b4aa4e1877cee4d8"
            ]
          }
        },
        "c09993c42a934100978b124aa4873aad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7f58293c9cb8479a9ad31c3d7c5caa5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6dcc962162204cd59ae45ab7d3c5887a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4738a380ff8c4c7bba5c33bc9c5ac831"
          }
        },
        "53f2c511f7e44fe6b4aa4e1877cee4d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_008f5032deba4a929f41e5e2c1cb9fad",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 32768/? [00:01&lt;00:00, 22637.87it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2dac79af464b4c59b99adef7b438b981"
          }
        },
        "6dcc962162204cd59ae45ab7d3c5887a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4738a380ff8c4c7bba5c33bc9c5ac831": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "008f5032deba4a929f41e5e2c1cb9fad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2dac79af464b4c59b99adef7b438b981": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "338270aa43004643b5de30f4ff4bbe3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ba27c57f7dca46b5bf1c6b7581efe5b8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0504500098dd48389d8744900b3d0180",
              "IPY_MODEL_28a4c7600b7644acb1d65e39a286a433"
            ]
          }
        },
        "ba27c57f7dca46b5bf1c6b7581efe5b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0504500098dd48389d8744900b3d0180": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bd3dedd1ed2a45a5822e55c2578aec85",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1d53f8552ba5486b9a4c68ae5af09883"
          }
        },
        "28a4c7600b7644acb1d65e39a286a433": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1e7c4b2e99d044b292dc3d6d9975a5f4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1654784/? [00:01&lt;00:00, 1477377.05it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ec52e235beb24e14845e130f7080125a"
          }
        },
        "bd3dedd1ed2a45a5822e55c2578aec85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1d53f8552ba5486b9a4c68ae5af09883": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1e7c4b2e99d044b292dc3d6d9975a5f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ec52e235beb24e14845e130f7080125a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "91c30aa3ac944f21bd7be02e04ae6a43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_654fce3e83c64f3898897ae973069aa6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c0d1ecb085b34238be25e34b4a1a672a",
              "IPY_MODEL_afb162bfed1d469290ce3d8036ea3b42"
            ]
          }
        },
        "654fce3e83c64f3898897ae973069aa6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c0d1ecb085b34238be25e34b4a1a672a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2679517ddee64561832ccc5827c44d46",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_618428aee36548b0a65dee4fd07cfba9"
          }
        },
        "afb162bfed1d469290ce3d8036ea3b42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bb19dc45e90440b0be3f0ffaee97db10",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 8192/? [00:00&lt;00:00, 32043.51it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_72f37dba287746dbab438431b1aed4c8"
          }
        },
        "2679517ddee64561832ccc5827c44d46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "618428aee36548b0a65dee4fd07cfba9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bb19dc45e90440b0be3f0ffaee97db10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "72f37dba287746dbab438431b1aed4c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw7SPqRCbLXf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhACzNHxBoWV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.1307,), (0.3081,)),\n",
        "           ])\n",
        "\n",
        "def mnist(batch_size=50, valid=0, shuffle=True, transform=mnist_transform, path='./MNIST_data'):\n",
        "    test_data = datasets.MNIST(path, train=False, download=True, transform=transform)\n",
        "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "    \n",
        "    train_data = datasets.MNIST(path, train=True, download=True, transform=transform)\n",
        "    train_data.data = train_data.data[:1000,:,:]\n",
        "    if valid > 0:\n",
        "        num_train = len(train_data)\n",
        "        indices = list(range(num_train))\n",
        "        split = num_train-valid\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        train_idx, valid_idx = indices[:split], indices[split:]\n",
        "        train_sampler = SubsetRandomSampler(train_idx)\n",
        "        valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "        train_loader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n",
        "        valid_loader = DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler)\n",
        "    \n",
        "        return train_loader, valid_loader, test_loader\n",
        "    else:\n",
        "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=shuffle)\n",
        "        return train_loader, test_loader\n",
        "\n",
        "\n",
        "def plot_mnist(images, shape):\n",
        "    fig = plt.figure(figsize=shape[::-1], dpi=80)\n",
        "    for j in range(1, len(images) + 1):\n",
        "        ax = fig.add_subplot(shape[0], shape[1], j)\n",
        "        ax.matshow(images[j - 1, 0, :, :], cmap = matplotlib.cm.binary)\n",
        "        plt.xticks(np.array([]))\n",
        "        plt.yticks(np.array([]))\n",
        "    plt.show()\n",
        "    \n",
        "def plot_graphs(log, tpe='loss'):\n",
        "    keys = log.keys()\n",
        "    logs = {k:[z for z in zip(*log[k])] for k in keys}\n",
        "    epochs = {k:range(len(log[k])) for k in keys}\n",
        "    \n",
        "    if tpe == 'loss':\n",
        "        handlers, = zip(*[plt.plot(epochs[k], logs[k][0], label=k) for k in keys])\n",
        "        plt.title('errors')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.ylabel('error')\n",
        "        plt.legend(handles=handlers)\n",
        "        plt.show()\n",
        "    elif tpe == 'accuracy':\n",
        "        handlers, = zip(*[plt.plot(epochs[k], logs[k][1], label=k) for k in log.keys()])\n",
        "        plt.title('accuracy')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.ylabel('accuracy')\n",
        "        plt.legend(handles=handlers)\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-O8JXT6Bwh9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403,
          "referenced_widgets": [
            "03df5bb909b443acaa9ee9e864c63c94",
            "b41191ca561846bb89b066706b5898e3",
            "ae8e56e0f29c4c1eab0af4e58a5a3858",
            "9634b1bddb484f0d91733c1f7088ca8c",
            "753140a8106c4cf3aad2007ede661208",
            "1c6b5a75ab3f4c87afc5b548c8e168bb",
            "5f895587da77438ba01d78249a1c33bd",
            "9e5b92da8cc8412b99f5466acfe006b0",
            "2b4062f6083e4d14838a410e2717c8b7",
            "c09993c42a934100978b124aa4873aad",
            "7f58293c9cb8479a9ad31c3d7c5caa5e",
            "53f2c511f7e44fe6b4aa4e1877cee4d8",
            "6dcc962162204cd59ae45ab7d3c5887a",
            "4738a380ff8c4c7bba5c33bc9c5ac831",
            "008f5032deba4a929f41e5e2c1cb9fad",
            "2dac79af464b4c59b99adef7b438b981",
            "338270aa43004643b5de30f4ff4bbe3a",
            "ba27c57f7dca46b5bf1c6b7581efe5b8",
            "0504500098dd48389d8744900b3d0180",
            "28a4c7600b7644acb1d65e39a286a433",
            "bd3dedd1ed2a45a5822e55c2578aec85",
            "1d53f8552ba5486b9a4c68ae5af09883",
            "1e7c4b2e99d044b292dc3d6d9975a5f4",
            "ec52e235beb24e14845e130f7080125a",
            "91c30aa3ac944f21bd7be02e04ae6a43",
            "654fce3e83c64f3898897ae973069aa6",
            "c0d1ecb085b34238be25e34b4a1a672a",
            "afb162bfed1d469290ce3d8036ea3b42",
            "2679517ddee64561832ccc5827c44d46",
            "618428aee36548b0a65dee4fd07cfba9",
            "bb19dc45e90440b0be3f0ffaee97db10",
            "72f37dba287746dbab438431b1aed4c8"
          ]
        },
        "outputId": "43cf8b1a-6678-4545-fc3b-d996ffd39227"
      },
      "source": [
        "train_loader, test_loader = mnist(5)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "03df5bb909b443acaa9ee9e864c63c94",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST_data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b4062f6083e4d14838a410e2717c8b7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST_data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "338270aa43004643b5de30f4ff4bbe3a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST_data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "91c30aa3ac944f21bd7be02e04ae6a43",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST_data/MNIST/raw\n",
            "Processing...\n",
            "Done!\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0jvvDrjbLXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, log_softmax=False):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 128)\n",
        "        self.fc2 = nn.Linear(128, 256)\n",
        "        self.fc3 = nn.Linear(256, 512)\n",
        "        self.fc4 = nn.Linear(512, 256)\n",
        "        self.fc5 = nn.Linear(256, 10)\n",
        "        self.log_softmax = log_softmax\n",
        "        self.optim = optim.SGD(self.parameters(), lr=1e-4)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = torch.sigmoid(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.tanh(self.fc3(x))\n",
        "        x = torch.sigmoid(self.fc4(x))\n",
        "        x = self.fc5(x)\n",
        "        if self.log_softmax:\n",
        "            x = F.log_softmax(x, dim=1)\n",
        "        else:\n",
        "            x = torch.log(F.softmax(x, dim=1))\n",
        "        return x\n",
        "    \n",
        "    def loss(self, output, target, **kwargs):\n",
        "        self._loss = F.nll_loss(output, target, **kwargs)\n",
        "        return self._loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANxuIpOkbLXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epoch, models):\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        for name, model in models.items():\n",
        "            model.optim.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = model.loss(output, target)\n",
        "            loss.backward()\n",
        "            model.optim.step()\n",
        "            \n",
        "        if batch_idx % 200 == 0:\n",
        "            line = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLosses '.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader))\n",
        "            losses = ' '.join(['{}: {:.6f}'.format(k, m._loss.item()) for k, m in models.items()])\n",
        "            print(line + losses)\n",
        "            \n",
        "    else:\n",
        "        batch_idx += 1\n",
        "        line = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLosses '.format(\n",
        "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "            100. * batch_idx / len(train_loader))\n",
        "        losses = ' '.join(['{}: {:.6f}'.format(k, m._loss.item()) for k, m in models.items()])\n",
        "        print(line + losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbrbWpJNbLXy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models = {'F.softmax': Net(), 'log_softmax': Net(True)}\n",
        "test_log = {k: [] for k in models}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-2EpiW9bLX0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "avg_lambda = lambda l: 'Loss: {:.4f}'.format(l)\n",
        "acc_lambda = lambda c, p: 'Accuracy: {}/{} ({:.0f}%)'.format(c, len(test_loader.dataset), p)\n",
        "line = lambda i, l, c, p: '{}: '.format(i) + avg_lambda(l) + '\\t' + acc_lambda(c, p)\n",
        "\n",
        "def test(models, log=None):\n",
        "    test_size = len(test_loader.sampler)\n",
        "    test_loss = {k: 0. for k in models}\n",
        "    correct = {k: 0. for k in models}\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            for k, m in models.items():\n",
        "                output = m(data)\n",
        "                test_loss[k] += m.loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "                pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "                correct[k] += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "    \n",
        "    for k in models.keys():\n",
        "        test_loss[k] /= test_size\n",
        "    correct_pct = {k: 100. * correct[k] / test_size for k in correct}\n",
        "    lines = '\\n'.join([line(k, test_loss[k], correct[k], correct_pct[k]) for k in models]) + '\\n'\n",
        "    report = 'Test set:\\n' + lines\n",
        "    if log is not None:\n",
        "        for k in models.keys():\n",
        "            log[k].append((test_loss[k], correct_pct[k]))\n",
        "    print(report)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hp2nSaCjbLX2",
        "colab_type": "code",
        "outputId": "9816b29d-0818-41f9-c9eb-a1bc3214187c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(1, 501):\n",
        "    train(epoch, models)\n",
        "    test(models, test_log)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/1000 (0%)]\tLosses F.softmax: 2.352321 log_softmax: 2.470314\n",
            "Train Epoch: 1 [1000/1000 (100%)]\tLosses F.softmax: 2.151545 log_softmax: 2.320926\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3466\tAccuracy: 1032.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3143\tAccuracy: 1135.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 2 [0/1000 (0%)]\tLosses F.softmax: 2.386211 log_softmax: 2.351224\n",
            "Train Epoch: 2 [1000/1000 (100%)]\tLosses F.softmax: 2.386390 log_softmax: 2.389172\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3341\tAccuracy: 1032.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3111\tAccuracy: 1135.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 3 [0/1000 (0%)]\tLosses F.softmax: 2.222358 log_softmax: 2.370589\n",
            "Train Epoch: 3 [1000/1000 (100%)]\tLosses F.softmax: 2.374702 log_softmax: 2.297430\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3255\tAccuracy: 1032.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3087\tAccuracy: 1135.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 4 [0/1000 (0%)]\tLosses F.softmax: 2.184098 log_softmax: 2.253591\n",
            "Train Epoch: 4 [1000/1000 (100%)]\tLosses F.softmax: 2.454169 log_softmax: 2.373380\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3194\tAccuracy: 1032.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3070\tAccuracy: 1135.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 5 [0/1000 (0%)]\tLosses F.softmax: 2.397429 log_softmax: 2.363633\n",
            "Train Epoch: 5 [1000/1000 (100%)]\tLosses F.softmax: 2.410285 log_softmax: 2.248937\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3152\tAccuracy: 1032.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3058\tAccuracy: 1135.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 6 [0/1000 (0%)]\tLosses F.softmax: 2.146523 log_softmax: 2.347190\n",
            "Train Epoch: 6 [1000/1000 (100%)]\tLosses F.softmax: 2.337784 log_softmax: 2.324033\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3120\tAccuracy: 1032.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3049\tAccuracy: 1135.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 7 [0/1000 (0%)]\tLosses F.softmax: 2.308574 log_softmax: 2.224018\n",
            "Train Epoch: 7 [1000/1000 (100%)]\tLosses F.softmax: 2.347685 log_softmax: 2.359643\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3097\tAccuracy: 1032.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3043\tAccuracy: 1135.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 8 [0/1000 (0%)]\tLosses F.softmax: 2.332764 log_softmax: 2.343039\n",
            "Train Epoch: 8 [1000/1000 (100%)]\tLosses F.softmax: 2.314723 log_softmax: 2.261061\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3080\tAccuracy: 1032.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3038\tAccuracy: 1135.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 9 [0/1000 (0%)]\tLosses F.softmax: 2.291776 log_softmax: 2.279729\n",
            "Train Epoch: 9 [1000/1000 (100%)]\tLosses F.softmax: 2.243669 log_softmax: 2.243264\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3068\tAccuracy: 1032.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3034\tAccuracy: 1135.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 10 [0/1000 (0%)]\tLosses F.softmax: 2.199699 log_softmax: 2.295573\n",
            "Train Epoch: 10 [1000/1000 (100%)]\tLosses F.softmax: 2.329829 log_softmax: 2.340415\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3059\tAccuracy: 1273.0/10000 (13%)\n",
            "log_softmax: Loss: 2.3032\tAccuracy: 1135.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 11 [0/1000 (0%)]\tLosses F.softmax: 2.335118 log_softmax: 2.371299\n",
            "Train Epoch: 11 [1000/1000 (100%)]\tLosses F.softmax: 2.337286 log_softmax: 2.335106\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3052\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3030\tAccuracy: 1135.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 12 [0/1000 (0%)]\tLosses F.softmax: 2.304606 log_softmax: 2.375372\n",
            "Train Epoch: 12 [1000/1000 (100%)]\tLosses F.softmax: 2.325977 log_softmax: 2.327571\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3046\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3029\tAccuracy: 1135.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 13 [0/1000 (0%)]\tLosses F.softmax: 2.241289 log_softmax: 2.196786\n",
            "Train Epoch: 13 [1000/1000 (100%)]\tLosses F.softmax: 2.329389 log_softmax: 2.350593\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3042\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3028\tAccuracy: 1135.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 14 [0/1000 (0%)]\tLosses F.softmax: 2.404920 log_softmax: 2.357440\n",
            "Train Epoch: 14 [1000/1000 (100%)]\tLosses F.softmax: 2.270815 log_softmax: 2.288121\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3039\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3028\tAccuracy: 1135.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 15 [0/1000 (0%)]\tLosses F.softmax: 2.269927 log_softmax: 2.268238\n",
            "Train Epoch: 15 [1000/1000 (100%)]\tLosses F.softmax: 2.313781 log_softmax: 2.356576\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3037\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3028\tAccuracy: 1135.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 16 [0/1000 (0%)]\tLosses F.softmax: 2.307345 log_softmax: 2.317657\n",
            "Train Epoch: 16 [1000/1000 (100%)]\tLosses F.softmax: 2.366034 log_softmax: 2.324591\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3035\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3028\tAccuracy: 1135.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 17 [0/1000 (0%)]\tLosses F.softmax: 2.262146 log_softmax: 2.266246\n",
            "Train Epoch: 17 [1000/1000 (100%)]\tLosses F.softmax: 2.266673 log_softmax: 2.263176\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3034\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3028\tAccuracy: 1135.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 18 [0/1000 (0%)]\tLosses F.softmax: 2.251388 log_softmax: 2.256382\n",
            "Train Epoch: 18 [1000/1000 (100%)]\tLosses F.softmax: 2.351820 log_softmax: 2.354010\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3033\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3028\tAccuracy: 1136.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 19 [0/1000 (0%)]\tLosses F.softmax: 2.292848 log_softmax: 2.295301\n",
            "Train Epoch: 19 [1000/1000 (100%)]\tLosses F.softmax: 2.313476 log_softmax: 2.319945\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3032\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3028\tAccuracy: 1137.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 20 [0/1000 (0%)]\tLosses F.softmax: 2.255884 log_softmax: 2.254177\n",
            "Train Epoch: 20 [1000/1000 (100%)]\tLosses F.softmax: 2.320978 log_softmax: 2.352890\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3031\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3028\tAccuracy: 1086.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 21 [0/1000 (0%)]\tLosses F.softmax: 2.373470 log_softmax: 2.368973\n",
            "Train Epoch: 21 [1000/1000 (100%)]\tLosses F.softmax: 2.302241 log_softmax: 2.318292\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3031\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3028\tAccuracy: 699.0/10000 (7%)\n",
            "\n",
            "Train Epoch: 22 [0/1000 (0%)]\tLosses F.softmax: 2.301839 log_softmax: 2.299860\n",
            "Train Epoch: 22 [1000/1000 (100%)]\tLosses F.softmax: 2.297384 log_softmax: 2.288396\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3030\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3028\tAccuracy: 702.0/10000 (7%)\n",
            "\n",
            "Train Epoch: 23 [0/1000 (0%)]\tLosses F.softmax: 2.298146 log_softmax: 2.295740\n",
            "Train Epoch: 23 [1000/1000 (100%)]\tLosses F.softmax: 2.368220 log_softmax: 2.355459\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3030\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3028\tAccuracy: 834.0/10000 (8%)\n",
            "\n",
            "Train Epoch: 24 [0/1000 (0%)]\tLosses F.softmax: 2.334649 log_softmax: 2.321904\n",
            "Train Epoch: 24 [1000/1000 (100%)]\tLosses F.softmax: 2.301259 log_softmax: 2.307932\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3030\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3029\tAccuracy: 884.0/10000 (9%)\n",
            "\n",
            "Train Epoch: 25 [0/1000 (0%)]\tLosses F.softmax: 2.267512 log_softmax: 2.274827\n",
            "Train Epoch: 25 [1000/1000 (100%)]\tLosses F.softmax: 2.298566 log_softmax: 2.294655\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3030\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3029\tAccuracy: 837.0/10000 (8%)\n",
            "\n",
            "Train Epoch: 26 [0/1000 (0%)]\tLosses F.softmax: 2.201072 log_softmax: 2.202930\n",
            "Train Epoch: 26 [1000/1000 (100%)]\tLosses F.softmax: 2.227252 log_softmax: 2.230883\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3029\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3029\tAccuracy: 816.0/10000 (8%)\n",
            "\n",
            "Train Epoch: 27 [0/1000 (0%)]\tLosses F.softmax: 2.256329 log_softmax: 2.257483\n",
            "Train Epoch: 27 [1000/1000 (100%)]\tLosses F.softmax: 2.313797 log_softmax: 2.310361\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3029\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3029\tAccuracy: 890.0/10000 (9%)\n",
            "\n",
            "Train Epoch: 28 [0/1000 (0%)]\tLosses F.softmax: 2.309950 log_softmax: 2.306631\n",
            "Train Epoch: 28 [1000/1000 (100%)]\tLosses F.softmax: 2.351284 log_softmax: 2.342089\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3029\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3029\tAccuracy: 918.0/10000 (9%)\n",
            "\n",
            "Train Epoch: 29 [0/1000 (0%)]\tLosses F.softmax: 2.245823 log_softmax: 2.245458\n",
            "Train Epoch: 29 [1000/1000 (100%)]\tLosses F.softmax: 2.350604 log_softmax: 2.354584\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3029\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3029\tAccuracy: 982.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 30 [0/1000 (0%)]\tLosses F.softmax: 2.337875 log_softmax: 2.344008\n",
            "Train Epoch: 30 [1000/1000 (100%)]\tLosses F.softmax: 2.351592 log_softmax: 2.356059\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3028\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3029\tAccuracy: 1011.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 31 [0/1000 (0%)]\tLosses F.softmax: 2.274910 log_softmax: 2.270825\n",
            "Train Epoch: 31 [1000/1000 (100%)]\tLosses F.softmax: 2.159190 log_softmax: 2.156165\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3028\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3029\tAccuracy: 997.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 32 [0/1000 (0%)]\tLosses F.softmax: 2.353863 log_softmax: 2.360230\n",
            "Train Epoch: 32 [1000/1000 (100%)]\tLosses F.softmax: 2.396925 log_softmax: 2.392559\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3028\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3029\tAccuracy: 1023.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 33 [0/1000 (0%)]\tLosses F.softmax: 2.265427 log_softmax: 2.268638\n",
            "Train Epoch: 33 [1000/1000 (100%)]\tLosses F.softmax: 2.329290 log_softmax: 2.326818\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3028\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3029\tAccuracy: 1024.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 34 [0/1000 (0%)]\tLosses F.softmax: 2.314141 log_softmax: 2.311192\n",
            "Train Epoch: 34 [1000/1000 (100%)]\tLosses F.softmax: 2.260994 log_softmax: 2.259820\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3028\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3029\tAccuracy: 1024.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 35 [0/1000 (0%)]\tLosses F.softmax: 2.299474 log_softmax: 2.302572\n",
            "Train Epoch: 35 [1000/1000 (100%)]\tLosses F.softmax: 2.290721 log_softmax: 2.289522\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3028\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3029\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 36 [0/1000 (0%)]\tLosses F.softmax: 2.339904 log_softmax: 2.338147\n",
            "Train Epoch: 36 [1000/1000 (100%)]\tLosses F.softmax: 2.300926 log_softmax: 2.304793\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3028\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3029\tAccuracy: 1026.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 37 [0/1000 (0%)]\tLosses F.softmax: 2.277225 log_softmax: 2.276393\n",
            "Train Epoch: 37 [1000/1000 (100%)]\tLosses F.softmax: 2.268322 log_softmax: 2.270970\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3028\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3029\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 38 [0/1000 (0%)]\tLosses F.softmax: 2.311406 log_softmax: 2.312333\n",
            "Train Epoch: 38 [1000/1000 (100%)]\tLosses F.softmax: 2.331468 log_softmax: 2.333072\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3028\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3029\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 39 [0/1000 (0%)]\tLosses F.softmax: 2.278912 log_softmax: 2.279388\n",
            "Train Epoch: 39 [1000/1000 (100%)]\tLosses F.softmax: 2.297005 log_softmax: 2.297908\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3028\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3029\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 40 [0/1000 (0%)]\tLosses F.softmax: 2.281044 log_softmax: 2.282042\n",
            "Train Epoch: 40 [1000/1000 (100%)]\tLosses F.softmax: 2.283154 log_softmax: 2.284189\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3028\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3029\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 41 [0/1000 (0%)]\tLosses F.softmax: 2.371388 log_softmax: 2.369871\n",
            "Train Epoch: 41 [1000/1000 (100%)]\tLosses F.softmax: 2.313936 log_softmax: 2.314667\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3028\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3029\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 42 [0/1000 (0%)]\tLosses F.softmax: 2.284749 log_softmax: 2.281707\n",
            "Train Epoch: 42 [1000/1000 (100%)]\tLosses F.softmax: 2.316377 log_softmax: 2.316236\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3028\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3029\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 43 [0/1000 (0%)]\tLosses F.softmax: 2.265153 log_softmax: 2.263020\n",
            "Train Epoch: 43 [1000/1000 (100%)]\tLosses F.softmax: 2.272971 log_softmax: 2.273027\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3027\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3029\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 44 [0/1000 (0%)]\tLosses F.softmax: 2.264945 log_softmax: 2.265753\n",
            "Train Epoch: 44 [1000/1000 (100%)]\tLosses F.softmax: 2.278333 log_softmax: 2.279398\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3027\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3029\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 45 [0/1000 (0%)]\tLosses F.softmax: 2.336100 log_softmax: 2.337967\n",
            "Train Epoch: 45 [1000/1000 (100%)]\tLosses F.softmax: 2.300976 log_softmax: 2.303031\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3027\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3028\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 46 [0/1000 (0%)]\tLosses F.softmax: 2.310965 log_softmax: 2.313669\n",
            "Train Epoch: 46 [1000/1000 (100%)]\tLosses F.softmax: 2.265402 log_softmax: 2.264722\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3027\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3028\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 47 [0/1000 (0%)]\tLosses F.softmax: 2.285241 log_softmax: 2.283813\n",
            "Train Epoch: 47 [1000/1000 (100%)]\tLosses F.softmax: 2.252605 log_softmax: 2.252022\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3027\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3028\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 48 [0/1000 (0%)]\tLosses F.softmax: 2.266119 log_softmax: 2.265226\n",
            "Train Epoch: 48 [1000/1000 (100%)]\tLosses F.softmax: 2.380785 log_softmax: 2.382905\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3027\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3028\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 49 [0/1000 (0%)]\tLosses F.softmax: 2.334037 log_softmax: 2.333642\n",
            "Train Epoch: 49 [1000/1000 (100%)]\tLosses F.softmax: 2.347757 log_softmax: 2.347602\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3026\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3028\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 50 [0/1000 (0%)]\tLosses F.softmax: 2.337663 log_softmax: 2.339372\n",
            "Train Epoch: 50 [1000/1000 (100%)]\tLosses F.softmax: 2.342819 log_softmax: 2.342038\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3026\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3028\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 51 [0/1000 (0%)]\tLosses F.softmax: 2.364423 log_softmax: 2.362451\n",
            "Train Epoch: 51 [1000/1000 (100%)]\tLosses F.softmax: 2.304977 log_softmax: 2.304916\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3026\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3028\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 52 [0/1000 (0%)]\tLosses F.softmax: 2.298674 log_softmax: 2.298504\n",
            "Train Epoch: 52 [1000/1000 (100%)]\tLosses F.softmax: 2.253030 log_softmax: 2.252713\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3026\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3028\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 53 [0/1000 (0%)]\tLosses F.softmax: 2.246616 log_softmax: 2.246816\n",
            "Train Epoch: 53 [1000/1000 (100%)]\tLosses F.softmax: 2.245715 log_softmax: 2.245411\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3026\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3028\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 54 [0/1000 (0%)]\tLosses F.softmax: 2.312994 log_softmax: 2.313542\n",
            "Train Epoch: 54 [1000/1000 (100%)]\tLosses F.softmax: 2.231654 log_softmax: 2.232237\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3026\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3028\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 55 [0/1000 (0%)]\tLosses F.softmax: 2.305760 log_softmax: 2.307111\n",
            "Train Epoch: 55 [1000/1000 (100%)]\tLosses F.softmax: 2.340522 log_softmax: 2.336618\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3026\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3028\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 56 [0/1000 (0%)]\tLosses F.softmax: 2.344398 log_softmax: 2.344242\n",
            "Train Epoch: 56 [1000/1000 (100%)]\tLosses F.softmax: 2.336556 log_softmax: 2.337498\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3026\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3028\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 57 [0/1000 (0%)]\tLosses F.softmax: 2.312918 log_softmax: 2.311620\n",
            "Train Epoch: 57 [1000/1000 (100%)]\tLosses F.softmax: 2.361032 log_softmax: 2.362144\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3026\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3028\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 58 [0/1000 (0%)]\tLosses F.softmax: 2.265073 log_softmax: 2.267641\n",
            "Train Epoch: 58 [1000/1000 (100%)]\tLosses F.softmax: 2.273054 log_softmax: 2.275187\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3026\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3027\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 59 [0/1000 (0%)]\tLosses F.softmax: 2.290246 log_softmax: 2.289407\n",
            "Train Epoch: 59 [1000/1000 (100%)]\tLosses F.softmax: 2.255190 log_softmax: 2.254615\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3025\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3027\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 60 [0/1000 (0%)]\tLosses F.softmax: 2.258739 log_softmax: 2.259722\n",
            "Train Epoch: 60 [1000/1000 (100%)]\tLosses F.softmax: 2.295944 log_softmax: 2.296204\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3025\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3027\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 61 [0/1000 (0%)]\tLosses F.softmax: 2.275357 log_softmax: 2.277619\n",
            "Train Epoch: 61 [1000/1000 (100%)]\tLosses F.softmax: 2.327217 log_softmax: 2.327471\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3025\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3027\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 62 [0/1000 (0%)]\tLosses F.softmax: 2.336305 log_softmax: 2.335392\n",
            "Train Epoch: 62 [1000/1000 (100%)]\tLosses F.softmax: 2.318051 log_softmax: 2.316983\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3025\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3027\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 63 [0/1000 (0%)]\tLosses F.softmax: 2.301651 log_softmax: 2.299506\n",
            "Train Epoch: 63 [1000/1000 (100%)]\tLosses F.softmax: 2.324646 log_softmax: 2.324255\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3025\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3027\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 64 [0/1000 (0%)]\tLosses F.softmax: 2.243273 log_softmax: 2.244197\n",
            "Train Epoch: 64 [1000/1000 (100%)]\tLosses F.softmax: 2.269708 log_softmax: 2.270669\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3025\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3027\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 65 [0/1000 (0%)]\tLosses F.softmax: 2.286153 log_softmax: 2.286247\n",
            "Train Epoch: 65 [1000/1000 (100%)]\tLosses F.softmax: 2.253674 log_softmax: 2.253764\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3025\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3027\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 66 [0/1000 (0%)]\tLosses F.softmax: 2.316352 log_softmax: 2.315792\n",
            "Train Epoch: 66 [1000/1000 (100%)]\tLosses F.softmax: 2.306068 log_softmax: 2.305312\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3025\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3027\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 67 [0/1000 (0%)]\tLosses F.softmax: 2.303024 log_softmax: 2.303460\n",
            "Train Epoch: 67 [1000/1000 (100%)]\tLosses F.softmax: 2.205956 log_softmax: 2.207112\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3024\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3026\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 68 [0/1000 (0%)]\tLosses F.softmax: 2.285014 log_softmax: 2.284751\n",
            "Train Epoch: 68 [1000/1000 (100%)]\tLosses F.softmax: 2.340294 log_softmax: 2.340563\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3024\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3026\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 69 [0/1000 (0%)]\tLosses F.softmax: 2.305207 log_softmax: 2.305773\n",
            "Train Epoch: 69 [1000/1000 (100%)]\tLosses F.softmax: 2.309528 log_softmax: 2.310420\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3024\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3026\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 70 [0/1000 (0%)]\tLosses F.softmax: 2.379351 log_softmax: 2.378793\n",
            "Train Epoch: 70 [1000/1000 (100%)]\tLosses F.softmax: 2.320540 log_softmax: 2.321629\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3024\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3026\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 71 [0/1000 (0%)]\tLosses F.softmax: 2.258685 log_softmax: 2.259594\n",
            "Train Epoch: 71 [1000/1000 (100%)]\tLosses F.softmax: 2.296140 log_softmax: 2.295638\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3024\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3026\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 72 [0/1000 (0%)]\tLosses F.softmax: 2.384670 log_softmax: 2.387053\n",
            "Train Epoch: 72 [1000/1000 (100%)]\tLosses F.softmax: 2.311098 log_softmax: 2.308905\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3024\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3026\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 73 [0/1000 (0%)]\tLosses F.softmax: 2.384002 log_softmax: 2.385179\n",
            "Train Epoch: 73 [1000/1000 (100%)]\tLosses F.softmax: 2.320792 log_softmax: 2.322082\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3023\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3026\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 74 [0/1000 (0%)]\tLosses F.softmax: 2.348854 log_softmax: 2.348561\n",
            "Train Epoch: 74 [1000/1000 (100%)]\tLosses F.softmax: 2.345503 log_softmax: 2.345148\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3023\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3025\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 75 [0/1000 (0%)]\tLosses F.softmax: 2.327811 log_softmax: 2.329406\n",
            "Train Epoch: 75 [1000/1000 (100%)]\tLosses F.softmax: 2.331546 log_softmax: 2.331243\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3023\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3025\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 76 [0/1000 (0%)]\tLosses F.softmax: 2.328407 log_softmax: 2.329442\n",
            "Train Epoch: 76 [1000/1000 (100%)]\tLosses F.softmax: 2.246866 log_softmax: 2.246367\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3023\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3025\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 77 [0/1000 (0%)]\tLosses F.softmax: 2.330710 log_softmax: 2.330343\n",
            "Train Epoch: 77 [1000/1000 (100%)]\tLosses F.softmax: 2.291502 log_softmax: 2.291594\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3023\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3025\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 78 [0/1000 (0%)]\tLosses F.softmax: 2.308943 log_softmax: 2.308274\n",
            "Train Epoch: 78 [1000/1000 (100%)]\tLosses F.softmax: 2.295872 log_softmax: 2.295660\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3023\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3025\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 79 [0/1000 (0%)]\tLosses F.softmax: 2.299448 log_softmax: 2.298960\n",
            "Train Epoch: 79 [1000/1000 (100%)]\tLosses F.softmax: 2.229478 log_softmax: 2.229126\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3023\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3025\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 80 [0/1000 (0%)]\tLosses F.softmax: 2.306408 log_softmax: 2.305781\n",
            "Train Epoch: 80 [1000/1000 (100%)]\tLosses F.softmax: 2.294140 log_softmax: 2.295086\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3023\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3025\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 81 [0/1000 (0%)]\tLosses F.softmax: 2.319541 log_softmax: 2.320985\n",
            "Train Epoch: 81 [1000/1000 (100%)]\tLosses F.softmax: 2.351870 log_softmax: 2.353465\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3023\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3025\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 82 [0/1000 (0%)]\tLosses F.softmax: 2.239136 log_softmax: 2.239712\n",
            "Train Epoch: 82 [1000/1000 (100%)]\tLosses F.softmax: 2.316002 log_softmax: 2.313159\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3022\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3025\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 83 [0/1000 (0%)]\tLosses F.softmax: 2.266553 log_softmax: 2.266968\n",
            "Train Epoch: 83 [1000/1000 (100%)]\tLosses F.softmax: 2.318877 log_softmax: 2.317299\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3022\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3025\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 84 [0/1000 (0%)]\tLosses F.softmax: 2.355873 log_softmax: 2.355596\n",
            "Train Epoch: 84 [1000/1000 (100%)]\tLosses F.softmax: 2.383940 log_softmax: 2.383766\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3022\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3024\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 85 [0/1000 (0%)]\tLosses F.softmax: 2.282753 log_softmax: 2.283078\n",
            "Train Epoch: 85 [1000/1000 (100%)]\tLosses F.softmax: 2.271420 log_softmax: 2.269955\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3022\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3024\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 86 [0/1000 (0%)]\tLosses F.softmax: 2.327086 log_softmax: 2.324596\n",
            "Train Epoch: 86 [1000/1000 (100%)]\tLosses F.softmax: 2.295466 log_softmax: 2.292873\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3022\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3024\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 87 [0/1000 (0%)]\tLosses F.softmax: 2.277016 log_softmax: 2.277269\n",
            "Train Epoch: 87 [1000/1000 (100%)]\tLosses F.softmax: 2.314551 log_softmax: 2.313723\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3022\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3024\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 88 [0/1000 (0%)]\tLosses F.softmax: 2.340139 log_softmax: 2.343462\n",
            "Train Epoch: 88 [1000/1000 (100%)]\tLosses F.softmax: 2.350696 log_softmax: 2.351782\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3022\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3024\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 89 [0/1000 (0%)]\tLosses F.softmax: 2.253749 log_softmax: 2.254611\n",
            "Train Epoch: 89 [1000/1000 (100%)]\tLosses F.softmax: 2.335204 log_softmax: 2.336486\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3021\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3024\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 90 [0/1000 (0%)]\tLosses F.softmax: 2.327435 log_softmax: 2.327023\n",
            "Train Epoch: 90 [1000/1000 (100%)]\tLosses F.softmax: 2.266927 log_softmax: 2.265881\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3021\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3024\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 91 [0/1000 (0%)]\tLosses F.softmax: 2.204904 log_softmax: 2.206052\n",
            "Train Epoch: 91 [1000/1000 (100%)]\tLosses F.softmax: 2.216576 log_softmax: 2.217649\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3021\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3024\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 92 [0/1000 (0%)]\tLosses F.softmax: 2.266874 log_softmax: 2.266834\n",
            "Train Epoch: 92 [1000/1000 (100%)]\tLosses F.softmax: 2.298986 log_softmax: 2.300828\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3021\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3024\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 93 [0/1000 (0%)]\tLosses F.softmax: 2.281359 log_softmax: 2.282831\n",
            "Train Epoch: 93 [1000/1000 (100%)]\tLosses F.softmax: 2.288180 log_softmax: 2.285421\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3021\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3023\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 94 [0/1000 (0%)]\tLosses F.softmax: 2.303622 log_softmax: 2.302935\n",
            "Train Epoch: 94 [1000/1000 (100%)]\tLosses F.softmax: 2.322488 log_softmax: 2.324346\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3021\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3023\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 95 [0/1000 (0%)]\tLosses F.softmax: 2.352846 log_softmax: 2.355425\n",
            "Train Epoch: 95 [1000/1000 (100%)]\tLosses F.softmax: 2.289832 log_softmax: 2.290337\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3021\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3023\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 96 [0/1000 (0%)]\tLosses F.softmax: 2.346228 log_softmax: 2.344805\n",
            "Train Epoch: 96 [1000/1000 (100%)]\tLosses F.softmax: 2.298232 log_softmax: 2.297553\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3021\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3023\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 97 [0/1000 (0%)]\tLosses F.softmax: 2.291887 log_softmax: 2.291841\n",
            "Train Epoch: 97 [1000/1000 (100%)]\tLosses F.softmax: 2.327720 log_softmax: 2.328857\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3021\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3023\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 98 [0/1000 (0%)]\tLosses F.softmax: 2.350780 log_softmax: 2.349348\n",
            "Train Epoch: 98 [1000/1000 (100%)]\tLosses F.softmax: 2.302519 log_softmax: 2.302235\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3021\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3023\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 99 [0/1000 (0%)]\tLosses F.softmax: 2.221181 log_softmax: 2.223460\n",
            "Train Epoch: 99 [1000/1000 (100%)]\tLosses F.softmax: 2.337584 log_softmax: 2.336792\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3020\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3023\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 100 [0/1000 (0%)]\tLosses F.softmax: 2.306097 log_softmax: 2.306630\n",
            "Train Epoch: 100 [1000/1000 (100%)]\tLosses F.softmax: 2.277735 log_softmax: 2.278882\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3020\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3023\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 101 [0/1000 (0%)]\tLosses F.softmax: 2.285219 log_softmax: 2.285889\n",
            "Train Epoch: 101 [1000/1000 (100%)]\tLosses F.softmax: 2.269996 log_softmax: 2.270190\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3020\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3023\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 102 [0/1000 (0%)]\tLosses F.softmax: 2.282764 log_softmax: 2.283400\n",
            "Train Epoch: 102 [1000/1000 (100%)]\tLosses F.softmax: 2.318778 log_softmax: 2.320955\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3020\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3023\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 103 [0/1000 (0%)]\tLosses F.softmax: 2.382171 log_softmax: 2.379172\n",
            "Train Epoch: 103 [1000/1000 (100%)]\tLosses F.softmax: 2.202255 log_softmax: 2.203845\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3020\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3023\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 104 [0/1000 (0%)]\tLosses F.softmax: 2.283434 log_softmax: 2.284094\n",
            "Train Epoch: 104 [1000/1000 (100%)]\tLosses F.softmax: 2.262747 log_softmax: 2.262447\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3020\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3022\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 105 [0/1000 (0%)]\tLosses F.softmax: 2.354062 log_softmax: 2.357203\n",
            "Train Epoch: 105 [1000/1000 (100%)]\tLosses F.softmax: 2.361748 log_softmax: 2.363115\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3020\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3022\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 106 [0/1000 (0%)]\tLosses F.softmax: 2.320613 log_softmax: 2.321173\n",
            "Train Epoch: 106 [1000/1000 (100%)]\tLosses F.softmax: 2.303705 log_softmax: 2.304646\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3020\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3022\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 107 [0/1000 (0%)]\tLosses F.softmax: 2.303479 log_softmax: 2.305707\n",
            "Train Epoch: 107 [1000/1000 (100%)]\tLosses F.softmax: 2.345838 log_softmax: 2.347457\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3020\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3022\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 108 [0/1000 (0%)]\tLosses F.softmax: 2.360887 log_softmax: 2.359957\n",
            "Train Epoch: 108 [1000/1000 (100%)]\tLosses F.softmax: 2.325079 log_softmax: 2.325955\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3019\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3022\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 109 [0/1000 (0%)]\tLosses F.softmax: 2.279737 log_softmax: 2.279709\n",
            "Train Epoch: 109 [1000/1000 (100%)]\tLosses F.softmax: 2.301920 log_softmax: 2.301029\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3019\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3022\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 110 [0/1000 (0%)]\tLosses F.softmax: 2.275161 log_softmax: 2.276021\n",
            "Train Epoch: 110 [1000/1000 (100%)]\tLosses F.softmax: 2.261937 log_softmax: 2.262288\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3019\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3022\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 111 [0/1000 (0%)]\tLosses F.softmax: 2.333070 log_softmax: 2.333316\n",
            "Train Epoch: 111 [1000/1000 (100%)]\tLosses F.softmax: 2.337336 log_softmax: 2.335847\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3019\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3022\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 112 [0/1000 (0%)]\tLosses F.softmax: 2.283832 log_softmax: 2.284722\n",
            "Train Epoch: 112 [1000/1000 (100%)]\tLosses F.softmax: 2.283289 log_softmax: 2.282816\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3019\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3022\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 113 [0/1000 (0%)]\tLosses F.softmax: 2.261299 log_softmax: 2.260848\n",
            "Train Epoch: 113 [1000/1000 (100%)]\tLosses F.softmax: 2.246955 log_softmax: 2.248011\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3019\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3021\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 114 [0/1000 (0%)]\tLosses F.softmax: 2.221885 log_softmax: 2.221575\n",
            "Train Epoch: 114 [1000/1000 (100%)]\tLosses F.softmax: 2.283417 log_softmax: 2.285787\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3018\tAccuracy: 1029.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3021\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 115 [0/1000 (0%)]\tLosses F.softmax: 2.353772 log_softmax: 2.352747\n",
            "Train Epoch: 115 [1000/1000 (100%)]\tLosses F.softmax: 2.364999 log_softmax: 2.362460\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3018\tAccuracy: 1029.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3021\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 116 [0/1000 (0%)]\tLosses F.softmax: 2.221315 log_softmax: 2.221785\n",
            "Train Epoch: 116 [1000/1000 (100%)]\tLosses F.softmax: 2.284952 log_softmax: 2.285493\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3018\tAccuracy: 1031.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3021\tAccuracy: 1032.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 117 [0/1000 (0%)]\tLosses F.softmax: 2.291501 log_softmax: 2.292052\n",
            "Train Epoch: 117 [1000/1000 (100%)]\tLosses F.softmax: 2.293947 log_softmax: 2.293819\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3018\tAccuracy: 1029.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3021\tAccuracy: 1029.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 118 [0/1000 (0%)]\tLosses F.softmax: 2.310545 log_softmax: 2.311828\n",
            "Train Epoch: 118 [1000/1000 (100%)]\tLosses F.softmax: 2.357865 log_softmax: 2.359654\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3018\tAccuracy: 1029.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3021\tAccuracy: 1029.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 119 [0/1000 (0%)]\tLosses F.softmax: 2.305632 log_softmax: 2.307274\n",
            "Train Epoch: 119 [1000/1000 (100%)]\tLosses F.softmax: 2.317758 log_softmax: 2.321023\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3018\tAccuracy: 1029.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3021\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 120 [0/1000 (0%)]\tLosses F.softmax: 2.265273 log_softmax: 2.264616\n",
            "Train Epoch: 120 [1000/1000 (100%)]\tLosses F.softmax: 2.245652 log_softmax: 2.245130\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3018\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3021\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 121 [0/1000 (0%)]\tLosses F.softmax: 2.272427 log_softmax: 2.271581\n",
            "Train Epoch: 121 [1000/1000 (100%)]\tLosses F.softmax: 2.262976 log_softmax: 2.264128\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3018\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3021\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 122 [0/1000 (0%)]\tLosses F.softmax: 2.354971 log_softmax: 2.354482\n",
            "Train Epoch: 122 [1000/1000 (100%)]\tLosses F.softmax: 2.327089 log_softmax: 2.325989\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3018\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3021\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 123 [0/1000 (0%)]\tLosses F.softmax: 2.266613 log_softmax: 2.268609\n",
            "Train Epoch: 123 [1000/1000 (100%)]\tLosses F.softmax: 2.305232 log_softmax: 2.306111\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3017\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3020\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 124 [0/1000 (0%)]\tLosses F.softmax: 2.226392 log_softmax: 2.228325\n",
            "Train Epoch: 124 [1000/1000 (100%)]\tLosses F.softmax: 2.299378 log_softmax: 2.299117\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3017\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3020\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 125 [0/1000 (0%)]\tLosses F.softmax: 2.323357 log_softmax: 2.322297\n",
            "Train Epoch: 125 [1000/1000 (100%)]\tLosses F.softmax: 2.356684 log_softmax: 2.357703\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3017\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3020\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 126 [0/1000 (0%)]\tLosses F.softmax: 2.312603 log_softmax: 2.312004\n",
            "Train Epoch: 126 [1000/1000 (100%)]\tLosses F.softmax: 2.358740 log_softmax: 2.358696\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3017\tAccuracy: 1029.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3020\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 127 [0/1000 (0%)]\tLosses F.softmax: 2.288900 log_softmax: 2.289719\n",
            "Train Epoch: 127 [1000/1000 (100%)]\tLosses F.softmax: 2.231361 log_softmax: 2.231713\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3017\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3020\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 128 [0/1000 (0%)]\tLosses F.softmax: 2.267846 log_softmax: 2.268438\n",
            "Train Epoch: 128 [1000/1000 (100%)]\tLosses F.softmax: 2.275880 log_softmax: 2.273816\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3017\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3020\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 129 [0/1000 (0%)]\tLosses F.softmax: 2.279252 log_softmax: 2.279011\n",
            "Train Epoch: 129 [1000/1000 (100%)]\tLosses F.softmax: 2.310707 log_softmax: 2.311832\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3017\tAccuracy: 1029.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3020\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 130 [0/1000 (0%)]\tLosses F.softmax: 2.244923 log_softmax: 2.245160\n",
            "Train Epoch: 130 [1000/1000 (100%)]\tLosses F.softmax: 2.281184 log_softmax: 2.282823\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3017\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3019\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 131 [0/1000 (0%)]\tLosses F.softmax: 2.261406 log_softmax: 2.261413\n",
            "Train Epoch: 131 [1000/1000 (100%)]\tLosses F.softmax: 2.239146 log_softmax: 2.240483\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3016\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3019\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 132 [0/1000 (0%)]\tLosses F.softmax: 2.315280 log_softmax: 2.315303\n",
            "Train Epoch: 132 [1000/1000 (100%)]\tLosses F.softmax: 2.279913 log_softmax: 2.281128\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3016\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3019\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 133 [0/1000 (0%)]\tLosses F.softmax: 2.342606 log_softmax: 2.344510\n",
            "Train Epoch: 133 [1000/1000 (100%)]\tLosses F.softmax: 2.315145 log_softmax: 2.316594\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3016\tAccuracy: 1029.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3019\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 134 [0/1000 (0%)]\tLosses F.softmax: 2.331681 log_softmax: 2.331812\n",
            "Train Epoch: 134 [1000/1000 (100%)]\tLosses F.softmax: 2.361821 log_softmax: 2.364321\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3016\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3019\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 135 [0/1000 (0%)]\tLosses F.softmax: 2.358260 log_softmax: 2.359796\n",
            "Train Epoch: 135 [1000/1000 (100%)]\tLosses F.softmax: 2.358964 log_softmax: 2.357880\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3016\tAccuracy: 1029.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3019\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 136 [0/1000 (0%)]\tLosses F.softmax: 2.260138 log_softmax: 2.262140\n",
            "Train Epoch: 136 [1000/1000 (100%)]\tLosses F.softmax: 2.354698 log_softmax: 2.354120\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3016\tAccuracy: 1029.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3019\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 137 [0/1000 (0%)]\tLosses F.softmax: 2.279654 log_softmax: 2.279109\n",
            "Train Epoch: 137 [1000/1000 (100%)]\tLosses F.softmax: 2.311086 log_softmax: 2.312815\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3016\tAccuracy: 1029.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3019\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 138 [0/1000 (0%)]\tLosses F.softmax: 2.294023 log_softmax: 2.295962\n",
            "Train Epoch: 138 [1000/1000 (100%)]\tLosses F.softmax: 2.307608 log_softmax: 2.309596\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3015\tAccuracy: 1029.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3018\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 139 [0/1000 (0%)]\tLosses F.softmax: 2.296957 log_softmax: 2.297014\n",
            "Train Epoch: 139 [1000/1000 (100%)]\tLosses F.softmax: 2.351529 log_softmax: 2.351435\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3015\tAccuracy: 1033.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3018\tAccuracy: 1031.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 140 [0/1000 (0%)]\tLosses F.softmax: 2.283810 log_softmax: 2.285144\n",
            "Train Epoch: 140 [1000/1000 (100%)]\tLosses F.softmax: 2.298425 log_softmax: 2.298119\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3015\tAccuracy: 1029.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3018\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 141 [0/1000 (0%)]\tLosses F.softmax: 2.237460 log_softmax: 2.239514\n",
            "Train Epoch: 141 [1000/1000 (100%)]\tLosses F.softmax: 2.267853 log_softmax: 2.268528\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3015\tAccuracy: 1029.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3018\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 142 [0/1000 (0%)]\tLosses F.softmax: 2.263455 log_softmax: 2.264194\n",
            "Train Epoch: 142 [1000/1000 (100%)]\tLosses F.softmax: 2.355948 log_softmax: 2.354278\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3015\tAccuracy: 1029.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3018\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 143 [0/1000 (0%)]\tLosses F.softmax: 2.313506 log_softmax: 2.313736\n",
            "Train Epoch: 143 [1000/1000 (100%)]\tLosses F.softmax: 2.346303 log_softmax: 2.348236\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3015\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3018\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 144 [0/1000 (0%)]\tLosses F.softmax: 2.314449 log_softmax: 2.315503\n",
            "Train Epoch: 144 [1000/1000 (100%)]\tLosses F.softmax: 2.292865 log_softmax: 2.293370\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3015\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3018\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 145 [0/1000 (0%)]\tLosses F.softmax: 2.378987 log_softmax: 2.381895\n",
            "Train Epoch: 145 [1000/1000 (100%)]\tLosses F.softmax: 2.238458 log_softmax: 2.238917\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3015\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3018\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 146 [0/1000 (0%)]\tLosses F.softmax: 2.285939 log_softmax: 2.285147\n",
            "Train Epoch: 146 [1000/1000 (100%)]\tLosses F.softmax: 2.344842 log_softmax: 2.344903\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3015\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3018\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 147 [0/1000 (0%)]\tLosses F.softmax: 2.308059 log_softmax: 2.307437\n",
            "Train Epoch: 147 [1000/1000 (100%)]\tLosses F.softmax: 2.263315 log_softmax: 2.262825\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3015\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3018\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 148 [0/1000 (0%)]\tLosses F.softmax: 2.282139 log_softmax: 2.282604\n",
            "Train Epoch: 148 [1000/1000 (100%)]\tLosses F.softmax: 2.276377 log_softmax: 2.276091\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3015\tAccuracy: 1029.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3018\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 149 [0/1000 (0%)]\tLosses F.softmax: 2.257315 log_softmax: 2.256399\n",
            "Train Epoch: 149 [1000/1000 (100%)]\tLosses F.softmax: 2.312474 log_softmax: 2.311476\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3015\tAccuracy: 1029.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3018\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 150 [0/1000 (0%)]\tLosses F.softmax: 2.304690 log_softmax: 2.305786\n",
            "Train Epoch: 150 [1000/1000 (100%)]\tLosses F.softmax: 2.235492 log_softmax: 2.235645\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3015\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3018\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 151 [0/1000 (0%)]\tLosses F.softmax: 2.263242 log_softmax: 2.263922\n",
            "Train Epoch: 151 [1000/1000 (100%)]\tLosses F.softmax: 2.326954 log_softmax: 2.327271\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3014\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3018\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 152 [0/1000 (0%)]\tLosses F.softmax: 2.377881 log_softmax: 2.380178\n",
            "Train Epoch: 152 [1000/1000 (100%)]\tLosses F.softmax: 2.285912 log_softmax: 2.286623\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3014\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3018\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 153 [0/1000 (0%)]\tLosses F.softmax: 2.269614 log_softmax: 2.269496\n",
            "Train Epoch: 153 [1000/1000 (100%)]\tLosses F.softmax: 2.286787 log_softmax: 2.287523\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3014\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3018\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 154 [0/1000 (0%)]\tLosses F.softmax: 2.359494 log_softmax: 2.358154\n",
            "Train Epoch: 154 [1000/1000 (100%)]\tLosses F.softmax: 2.328283 log_softmax: 2.329574\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3014\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3017\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 155 [0/1000 (0%)]\tLosses F.softmax: 2.224904 log_softmax: 2.225834\n",
            "Train Epoch: 155 [1000/1000 (100%)]\tLosses F.softmax: 2.244633 log_softmax: 2.245715\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3014\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3017\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 156 [0/1000 (0%)]\tLosses F.softmax: 2.278487 log_softmax: 2.277940\n",
            "Train Epoch: 156 [1000/1000 (100%)]\tLosses F.softmax: 2.404699 log_softmax: 2.403887\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3014\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3017\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 157 [0/1000 (0%)]\tLosses F.softmax: 2.254664 log_softmax: 2.256215\n",
            "Train Epoch: 157 [1000/1000 (100%)]\tLosses F.softmax: 2.279302 log_softmax: 2.280443\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3014\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3017\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 158 [0/1000 (0%)]\tLosses F.softmax: 2.309885 log_softmax: 2.310108\n",
            "Train Epoch: 158 [1000/1000 (100%)]\tLosses F.softmax: 2.270245 log_softmax: 2.272550\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3014\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3017\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 159 [0/1000 (0%)]\tLosses F.softmax: 2.267607 log_softmax: 2.268412\n",
            "Train Epoch: 159 [1000/1000 (100%)]\tLosses F.softmax: 2.285997 log_softmax: 2.284929\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3013\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3017\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 160 [0/1000 (0%)]\tLosses F.softmax: 2.279430 log_softmax: 2.281237\n",
            "Train Epoch: 160 [1000/1000 (100%)]\tLosses F.softmax: 2.263159 log_softmax: 2.264187\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3013\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3017\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 161 [0/1000 (0%)]\tLosses F.softmax: 2.244143 log_softmax: 2.243680\n",
            "Train Epoch: 161 [1000/1000 (100%)]\tLosses F.softmax: 2.281307 log_softmax: 2.282221\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3013\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3017\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 162 [0/1000 (0%)]\tLosses F.softmax: 2.312326 log_softmax: 2.313820\n",
            "Train Epoch: 162 [1000/1000 (100%)]\tLosses F.softmax: 2.301339 log_softmax: 2.301008\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3013\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3016\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 163 [0/1000 (0%)]\tLosses F.softmax: 2.323774 log_softmax: 2.322172\n",
            "Train Epoch: 163 [1000/1000 (100%)]\tLosses F.softmax: 2.325991 log_softmax: 2.324994\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3013\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3016\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 164 [0/1000 (0%)]\tLosses F.softmax: 2.345131 log_softmax: 2.345861\n",
            "Train Epoch: 164 [1000/1000 (100%)]\tLosses F.softmax: 2.309120 log_softmax: 2.310574\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3013\tAccuracy: 1029.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3016\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 165 [0/1000 (0%)]\tLosses F.softmax: 2.313255 log_softmax: 2.311941\n",
            "Train Epoch: 165 [1000/1000 (100%)]\tLosses F.softmax: 2.364682 log_softmax: 2.365741\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3013\tAccuracy: 1029.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3016\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 166 [0/1000 (0%)]\tLosses F.softmax: 2.316444 log_softmax: 2.317357\n",
            "Train Epoch: 166 [1000/1000 (100%)]\tLosses F.softmax: 2.313525 log_softmax: 2.314574\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3012\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3016\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 167 [0/1000 (0%)]\tLosses F.softmax: 2.308498 log_softmax: 2.309386\n",
            "Train Epoch: 167 [1000/1000 (100%)]\tLosses F.softmax: 2.330056 log_softmax: 2.327912\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3012\tAccuracy: 1029.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3016\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 168 [0/1000 (0%)]\tLosses F.softmax: 2.285946 log_softmax: 2.285875\n",
            "Train Epoch: 168 [1000/1000 (100%)]\tLosses F.softmax: 2.280296 log_softmax: 2.281620\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3012\tAccuracy: 1029.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3015\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 169 [0/1000 (0%)]\tLosses F.softmax: 2.307077 log_softmax: 2.305768\n",
            "Train Epoch: 169 [1000/1000 (100%)]\tLosses F.softmax: 2.293858 log_softmax: 2.293495\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3012\tAccuracy: 1029.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3015\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 170 [0/1000 (0%)]\tLosses F.softmax: 2.315129 log_softmax: 2.315971\n",
            "Train Epoch: 170 [1000/1000 (100%)]\tLosses F.softmax: 2.325668 log_softmax: 2.327602\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3012\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3015\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 171 [0/1000 (0%)]\tLosses F.softmax: 2.312242 log_softmax: 2.311685\n",
            "Train Epoch: 171 [1000/1000 (100%)]\tLosses F.softmax: 2.301863 log_softmax: 2.302119\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3012\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3015\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 172 [0/1000 (0%)]\tLosses F.softmax: 2.290938 log_softmax: 2.289300\n",
            "Train Epoch: 172 [1000/1000 (100%)]\tLosses F.softmax: 2.354193 log_softmax: 2.355600\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3012\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3015\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 173 [0/1000 (0%)]\tLosses F.softmax: 2.274046 log_softmax: 2.275675\n",
            "Train Epoch: 173 [1000/1000 (100%)]\tLosses F.softmax: 2.242900 log_softmax: 2.244526\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3011\tAccuracy: 1031.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3015\tAccuracy: 1030.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 174 [0/1000 (0%)]\tLosses F.softmax: 2.317402 log_softmax: 2.317760\n",
            "Train Epoch: 174 [1000/1000 (100%)]\tLosses F.softmax: 2.290726 log_softmax: 2.291815\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3012\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3015\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 175 [0/1000 (0%)]\tLosses F.softmax: 2.279325 log_softmax: 2.281062\n",
            "Train Epoch: 175 [1000/1000 (100%)]\tLosses F.softmax: 2.265218 log_softmax: 2.264323\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3012\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3015\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 176 [0/1000 (0%)]\tLosses F.softmax: 2.278491 log_softmax: 2.277229\n",
            "Train Epoch: 176 [1000/1000 (100%)]\tLosses F.softmax: 2.322753 log_softmax: 2.321708\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3011\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3015\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 177 [0/1000 (0%)]\tLosses F.softmax: 2.248181 log_softmax: 2.249543\n",
            "Train Epoch: 177 [1000/1000 (100%)]\tLosses F.softmax: 2.309987 log_softmax: 2.310242\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3011\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3015\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 178 [0/1000 (0%)]\tLosses F.softmax: 2.286356 log_softmax: 2.286198\n",
            "Train Epoch: 178 [1000/1000 (100%)]\tLosses F.softmax: 2.278685 log_softmax: 2.278381\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3011\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3015\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 179 [0/1000 (0%)]\tLosses F.softmax: 2.269852 log_softmax: 2.272622\n",
            "Train Epoch: 179 [1000/1000 (100%)]\tLosses F.softmax: 2.276085 log_softmax: 2.277497\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3011\tAccuracy: 1031.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3014\tAccuracy: 1030.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 180 [0/1000 (0%)]\tLosses F.softmax: 2.268188 log_softmax: 2.268336\n",
            "Train Epoch: 180 [1000/1000 (100%)]\tLosses F.softmax: 2.334595 log_softmax: 2.334586\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3010\tAccuracy: 1035.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3014\tAccuracy: 1033.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 181 [0/1000 (0%)]\tLosses F.softmax: 2.314851 log_softmax: 2.315765\n",
            "Train Epoch: 181 [1000/1000 (100%)]\tLosses F.softmax: 2.305138 log_softmax: 2.307014\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3010\tAccuracy: 1033.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3014\tAccuracy: 1031.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 182 [0/1000 (0%)]\tLosses F.softmax: 2.290429 log_softmax: 2.290287\n",
            "Train Epoch: 182 [1000/1000 (100%)]\tLosses F.softmax: 2.259028 log_softmax: 2.259095\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3010\tAccuracy: 1035.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3014\tAccuracy: 1031.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 183 [0/1000 (0%)]\tLosses F.softmax: 2.297125 log_softmax: 2.296691\n",
            "Train Epoch: 183 [1000/1000 (100%)]\tLosses F.softmax: 2.268709 log_softmax: 2.269511\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3010\tAccuracy: 1033.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3014\tAccuracy: 1030.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 184 [0/1000 (0%)]\tLosses F.softmax: 2.318123 log_softmax: 2.317464\n",
            "Train Epoch: 184 [1000/1000 (100%)]\tLosses F.softmax: 2.318016 log_softmax: 2.318230\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3010\tAccuracy: 1029.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3014\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 185 [0/1000 (0%)]\tLosses F.softmax: 2.344472 log_softmax: 2.345435\n",
            "Train Epoch: 185 [1000/1000 (100%)]\tLosses F.softmax: 2.354697 log_softmax: 2.355028\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3010\tAccuracy: 1029.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3014\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 186 [0/1000 (0%)]\tLosses F.softmax: 2.302511 log_softmax: 2.303213\n",
            "Train Epoch: 186 [1000/1000 (100%)]\tLosses F.softmax: 2.315467 log_softmax: 2.316659\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3010\tAccuracy: 1033.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3014\tAccuracy: 1031.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 187 [0/1000 (0%)]\tLosses F.softmax: 2.313115 log_softmax: 2.314761\n",
            "Train Epoch: 187 [1000/1000 (100%)]\tLosses F.softmax: 2.344299 log_softmax: 2.343577\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3010\tAccuracy: 1035.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3014\tAccuracy: 1031.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 188 [0/1000 (0%)]\tLosses F.softmax: 2.283435 log_softmax: 2.282280\n",
            "Train Epoch: 188 [1000/1000 (100%)]\tLosses F.softmax: 2.232777 log_softmax: 2.232431\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3010\tAccuracy: 1042.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3013\tAccuracy: 1037.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 189 [0/1000 (0%)]\tLosses F.softmax: 2.349190 log_softmax: 2.349111\n",
            "Train Epoch: 189 [1000/1000 (100%)]\tLosses F.softmax: 2.374941 log_softmax: 2.374879\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3009\tAccuracy: 1070.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3013\tAccuracy: 1051.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 190 [0/1000 (0%)]\tLosses F.softmax: 2.339771 log_softmax: 2.338782\n",
            "Train Epoch: 190 [1000/1000 (100%)]\tLosses F.softmax: 2.306515 log_softmax: 2.305438\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3010\tAccuracy: 1039.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3013\tAccuracy: 1034.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 191 [0/1000 (0%)]\tLosses F.softmax: 2.318907 log_softmax: 2.321441\n",
            "Train Epoch: 191 [1000/1000 (100%)]\tLosses F.softmax: 2.242133 log_softmax: 2.242993\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3009\tAccuracy: 1035.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3013\tAccuracy: 1031.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 192 [0/1000 (0%)]\tLosses F.softmax: 2.315225 log_softmax: 2.316943\n",
            "Train Epoch: 192 [1000/1000 (100%)]\tLosses F.softmax: 2.269666 log_softmax: 2.269390\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3009\tAccuracy: 1029.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3013\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 193 [0/1000 (0%)]\tLosses F.softmax: 2.293689 log_softmax: 2.296676\n",
            "Train Epoch: 193 [1000/1000 (100%)]\tLosses F.softmax: 2.282726 log_softmax: 2.283175\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3009\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3013\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 194 [0/1000 (0%)]\tLosses F.softmax: 2.320040 log_softmax: 2.320064\n",
            "Train Epoch: 194 [1000/1000 (100%)]\tLosses F.softmax: 2.287968 log_softmax: 2.288494\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3009\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3013\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 195 [0/1000 (0%)]\tLosses F.softmax: 2.278971 log_softmax: 2.281904\n",
            "Train Epoch: 195 [1000/1000 (100%)]\tLosses F.softmax: 2.311615 log_softmax: 2.310907\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3009\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3013\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 196 [0/1000 (0%)]\tLosses F.softmax: 2.347564 log_softmax: 2.348376\n",
            "Train Epoch: 196 [1000/1000 (100%)]\tLosses F.softmax: 2.307196 log_softmax: 2.308912\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3009\tAccuracy: 1029.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3013\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 197 [0/1000 (0%)]\tLosses F.softmax: 2.286392 log_softmax: 2.287445\n",
            "Train Epoch: 197 [1000/1000 (100%)]\tLosses F.softmax: 2.225212 log_softmax: 2.226031\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3008\tAccuracy: 1029.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3012\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 198 [0/1000 (0%)]\tLosses F.softmax: 2.281187 log_softmax: 2.281394\n",
            "Train Epoch: 198 [1000/1000 (100%)]\tLosses F.softmax: 2.269112 log_softmax: 2.269184\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3008\tAccuracy: 1029.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3012\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 199 [0/1000 (0%)]\tLosses F.softmax: 2.296761 log_softmax: 2.296558\n",
            "Train Epoch: 199 [1000/1000 (100%)]\tLosses F.softmax: 2.287037 log_softmax: 2.288951\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3008\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3012\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 200 [0/1000 (0%)]\tLosses F.softmax: 2.374753 log_softmax: 2.376742\n",
            "Train Epoch: 200 [1000/1000 (100%)]\tLosses F.softmax: 2.344466 log_softmax: 2.345024\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3008\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3012\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 201 [0/1000 (0%)]\tLosses F.softmax: 2.271782 log_softmax: 2.271952\n",
            "Train Epoch: 201 [1000/1000 (100%)]\tLosses F.softmax: 2.269460 log_softmax: 2.272246\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3008\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3012\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 202 [0/1000 (0%)]\tLosses F.softmax: 2.315414 log_softmax: 2.314964\n",
            "Train Epoch: 202 [1000/1000 (100%)]\tLosses F.softmax: 2.220301 log_softmax: 2.221901\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3008\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3012\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 203 [0/1000 (0%)]\tLosses F.softmax: 2.326585 log_softmax: 2.326881\n",
            "Train Epoch: 203 [1000/1000 (100%)]\tLosses F.softmax: 2.194586 log_softmax: 2.194958\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3008\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3012\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 204 [0/1000 (0%)]\tLosses F.softmax: 2.249583 log_softmax: 2.251251\n",
            "Train Epoch: 204 [1000/1000 (100%)]\tLosses F.softmax: 2.264249 log_softmax: 2.264495\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3007\tAccuracy: 1033.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3012\tAccuracy: 1029.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 205 [0/1000 (0%)]\tLosses F.softmax: 2.274382 log_softmax: 2.274135\n",
            "Train Epoch: 205 [1000/1000 (100%)]\tLosses F.softmax: 2.199033 log_softmax: 2.198682\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3007\tAccuracy: 1039.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3011\tAccuracy: 1034.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 206 [0/1000 (0%)]\tLosses F.softmax: 2.186550 log_softmax: 2.188251\n",
            "Train Epoch: 206 [1000/1000 (100%)]\tLosses F.softmax: 2.295294 log_softmax: 2.296820\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3007\tAccuracy: 1037.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3011\tAccuracy: 1032.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 207 [0/1000 (0%)]\tLosses F.softmax: 2.307794 log_softmax: 2.308174\n",
            "Train Epoch: 207 [1000/1000 (100%)]\tLosses F.softmax: 2.214172 log_softmax: 2.213072\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3007\tAccuracy: 1065.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3011\tAccuracy: 1047.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 208 [0/1000 (0%)]\tLosses F.softmax: 2.304261 log_softmax: 2.306721\n",
            "Train Epoch: 208 [1000/1000 (100%)]\tLosses F.softmax: 2.295696 log_softmax: 2.298017\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3007\tAccuracy: 1042.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3011\tAccuracy: 1037.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 209 [0/1000 (0%)]\tLosses F.softmax: 2.257143 log_softmax: 2.257855\n",
            "Train Epoch: 209 [1000/1000 (100%)]\tLosses F.softmax: 2.296261 log_softmax: 2.298004\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3007\tAccuracy: 1050.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3011\tAccuracy: 1040.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 210 [0/1000 (0%)]\tLosses F.softmax: 2.315902 log_softmax: 2.316309\n",
            "Train Epoch: 210 [1000/1000 (100%)]\tLosses F.softmax: 2.282611 log_softmax: 2.282265\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3007\tAccuracy: 1059.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3011\tAccuracy: 1043.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 211 [0/1000 (0%)]\tLosses F.softmax: 2.355368 log_softmax: 2.357191\n",
            "Train Epoch: 211 [1000/1000 (100%)]\tLosses F.softmax: 2.284044 log_softmax: 2.285067\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3006\tAccuracy: 1156.0/10000 (12%)\n",
            "log_softmax: Loss: 2.3010\tAccuracy: 1089.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 212 [0/1000 (0%)]\tLosses F.softmax: 2.319037 log_softmax: 2.319285\n",
            "Train Epoch: 212 [1000/1000 (100%)]\tLosses F.softmax: 2.248918 log_softmax: 2.249755\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3006\tAccuracy: 1160.0/10000 (12%)\n",
            "log_softmax: Loss: 2.3010\tAccuracy: 1089.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 213 [0/1000 (0%)]\tLosses F.softmax: 2.297240 log_softmax: 2.297931\n",
            "Train Epoch: 213 [1000/1000 (100%)]\tLosses F.softmax: 2.284635 log_softmax: 2.283297\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3006\tAccuracy: 1058.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3010\tAccuracy: 1041.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 214 [0/1000 (0%)]\tLosses F.softmax: 2.244778 log_softmax: 2.245637\n",
            "Train Epoch: 214 [1000/1000 (100%)]\tLosses F.softmax: 2.316466 log_softmax: 2.316828\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3006\tAccuracy: 1056.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3010\tAccuracy: 1040.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 215 [0/1000 (0%)]\tLosses F.softmax: 2.344571 log_softmax: 2.345631\n",
            "Train Epoch: 215 [1000/1000 (100%)]\tLosses F.softmax: 2.277777 log_softmax: 2.278747\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3006\tAccuracy: 1078.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3010\tAccuracy: 1054.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 216 [0/1000 (0%)]\tLosses F.softmax: 2.257276 log_softmax: 2.256719\n",
            "Train Epoch: 216 [1000/1000 (100%)]\tLosses F.softmax: 2.257411 log_softmax: 2.257707\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3006\tAccuracy: 1059.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3010\tAccuracy: 1041.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 217 [0/1000 (0%)]\tLosses F.softmax: 2.316096 log_softmax: 2.319086\n",
            "Train Epoch: 217 [1000/1000 (100%)]\tLosses F.softmax: 2.305746 log_softmax: 2.305883\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3006\tAccuracy: 1062.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3010\tAccuracy: 1045.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 218 [0/1000 (0%)]\tLosses F.softmax: 2.300099 log_softmax: 2.299876\n",
            "Train Epoch: 218 [1000/1000 (100%)]\tLosses F.softmax: 2.304177 log_softmax: 2.302318\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3006\tAccuracy: 1033.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3010\tAccuracy: 1030.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 219 [0/1000 (0%)]\tLosses F.softmax: 2.262807 log_softmax: 2.261357\n",
            "Train Epoch: 219 [1000/1000 (100%)]\tLosses F.softmax: 2.327989 log_softmax: 2.328187\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3006\tAccuracy: 1034.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3010\tAccuracy: 1031.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 220 [0/1000 (0%)]\tLosses F.softmax: 2.312200 log_softmax: 2.313684\n",
            "Train Epoch: 220 [1000/1000 (100%)]\tLosses F.softmax: 2.312950 log_softmax: 2.314163\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3005\tAccuracy: 1041.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3010\tAccuracy: 1034.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 221 [0/1000 (0%)]\tLosses F.softmax: 2.293675 log_softmax: 2.293096\n",
            "Train Epoch: 221 [1000/1000 (100%)]\tLosses F.softmax: 2.304484 log_softmax: 2.302778\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3005\tAccuracy: 1030.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3010\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 222 [0/1000 (0%)]\tLosses F.softmax: 2.311107 log_softmax: 2.310210\n",
            "Train Epoch: 222 [1000/1000 (100%)]\tLosses F.softmax: 2.216085 log_softmax: 2.215586\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3005\tAccuracy: 1033.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3010\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 223 [0/1000 (0%)]\tLosses F.softmax: 2.256158 log_softmax: 2.257453\n",
            "Train Epoch: 223 [1000/1000 (100%)]\tLosses F.softmax: 2.331935 log_softmax: 2.332476\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3005\tAccuracy: 1036.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3009\tAccuracy: 1031.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 224 [0/1000 (0%)]\tLosses F.softmax: 2.273463 log_softmax: 2.273088\n",
            "Train Epoch: 224 [1000/1000 (100%)]\tLosses F.softmax: 2.280189 log_softmax: 2.282093\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3005\tAccuracy: 1043.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3009\tAccuracy: 1036.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 225 [0/1000 (0%)]\tLosses F.softmax: 2.315493 log_softmax: 2.316719\n",
            "Train Epoch: 225 [1000/1000 (100%)]\tLosses F.softmax: 2.305373 log_softmax: 2.305926\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3004\tAccuracy: 1039.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3009\tAccuracy: 1034.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 226 [0/1000 (0%)]\tLosses F.softmax: 2.344039 log_softmax: 2.345671\n",
            "Train Epoch: 226 [1000/1000 (100%)]\tLosses F.softmax: 2.287948 log_softmax: 2.288845\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3005\tAccuracy: 1041.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3009\tAccuracy: 1034.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 227 [0/1000 (0%)]\tLosses F.softmax: 2.363511 log_softmax: 2.362845\n",
            "Train Epoch: 227 [1000/1000 (100%)]\tLosses F.softmax: 2.307824 log_softmax: 2.309395\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3004\tAccuracy: 1033.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3009\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 228 [0/1000 (0%)]\tLosses F.softmax: 2.348210 log_softmax: 2.346219\n",
            "Train Epoch: 228 [1000/1000 (100%)]\tLosses F.softmax: 2.295833 log_softmax: 2.296077\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3004\tAccuracy: 1033.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3009\tAccuracy: 1029.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 229 [0/1000 (0%)]\tLosses F.softmax: 2.232913 log_softmax: 2.232580\n",
            "Train Epoch: 229 [1000/1000 (100%)]\tLosses F.softmax: 2.284006 log_softmax: 2.285779\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3004\tAccuracy: 1042.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3009\tAccuracy: 1036.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 230 [0/1000 (0%)]\tLosses F.softmax: 2.278011 log_softmax: 2.278098\n",
            "Train Epoch: 230 [1000/1000 (100%)]\tLosses F.softmax: 2.222955 log_softmax: 2.222543\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3004\tAccuracy: 1058.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3009\tAccuracy: 1041.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 231 [0/1000 (0%)]\tLosses F.softmax: 2.318234 log_softmax: 2.320256\n",
            "Train Epoch: 231 [1000/1000 (100%)]\tLosses F.softmax: 2.321805 log_softmax: 2.322307\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3004\tAccuracy: 1055.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3008\tAccuracy: 1039.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 232 [0/1000 (0%)]\tLosses F.softmax: 2.314135 log_softmax: 2.312990\n",
            "Train Epoch: 232 [1000/1000 (100%)]\tLosses F.softmax: 2.346647 log_softmax: 2.349070\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3004\tAccuracy: 1054.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3008\tAccuracy: 1039.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 233 [0/1000 (0%)]\tLosses F.softmax: 2.258520 log_softmax: 2.259493\n",
            "Train Epoch: 233 [1000/1000 (100%)]\tLosses F.softmax: 2.380674 log_softmax: 2.383479\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3003\tAccuracy: 1076.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3008\tAccuracy: 1050.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 234 [0/1000 (0%)]\tLosses F.softmax: 2.277596 log_softmax: 2.277710\n",
            "Train Epoch: 234 [1000/1000 (100%)]\tLosses F.softmax: 2.289665 log_softmax: 2.291664\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3003\tAccuracy: 1110.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3008\tAccuracy: 1066.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 235 [0/1000 (0%)]\tLosses F.softmax: 2.296383 log_softmax: 2.298135\n",
            "Train Epoch: 235 [1000/1000 (100%)]\tLosses F.softmax: 2.322177 log_softmax: 2.324060\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3003\tAccuracy: 1047.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3008\tAccuracy: 1036.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 236 [0/1000 (0%)]\tLosses F.softmax: 2.282543 log_softmax: 2.282923\n",
            "Train Epoch: 236 [1000/1000 (100%)]\tLosses F.softmax: 2.253841 log_softmax: 2.253446\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3003\tAccuracy: 1067.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3008\tAccuracy: 1045.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 237 [0/1000 (0%)]\tLosses F.softmax: 2.394282 log_softmax: 2.393629\n",
            "Train Epoch: 237 [1000/1000 (100%)]\tLosses F.softmax: 2.335506 log_softmax: 2.336095\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3003\tAccuracy: 1087.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3007\tAccuracy: 1055.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 238 [0/1000 (0%)]\tLosses F.softmax: 2.315354 log_softmax: 2.315578\n",
            "Train Epoch: 238 [1000/1000 (100%)]\tLosses F.softmax: 2.366620 log_softmax: 2.366650\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3003\tAccuracy: 1068.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3007\tAccuracy: 1046.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 239 [0/1000 (0%)]\tLosses F.softmax: 2.307229 log_softmax: 2.307512\n",
            "Train Epoch: 239 [1000/1000 (100%)]\tLosses F.softmax: 2.284103 log_softmax: 2.286008\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3003\tAccuracy: 1041.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3007\tAccuracy: 1034.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 240 [0/1000 (0%)]\tLosses F.softmax: 2.339366 log_softmax: 2.338860\n",
            "Train Epoch: 240 [1000/1000 (100%)]\tLosses F.softmax: 2.335712 log_softmax: 2.335234\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3003\tAccuracy: 1034.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3007\tAccuracy: 1029.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 241 [0/1000 (0%)]\tLosses F.softmax: 2.333593 log_softmax: 2.333521\n",
            "Train Epoch: 241 [1000/1000 (100%)]\tLosses F.softmax: 2.286549 log_softmax: 2.288791\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3003\tAccuracy: 1029.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3007\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 242 [0/1000 (0%)]\tLosses F.softmax: 2.335768 log_softmax: 2.336745\n",
            "Train Epoch: 242 [1000/1000 (100%)]\tLosses F.softmax: 2.273177 log_softmax: 2.274806\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3003\tAccuracy: 1029.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3007\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 243 [0/1000 (0%)]\tLosses F.softmax: 2.313520 log_softmax: 2.313987\n",
            "Train Epoch: 243 [1000/1000 (100%)]\tLosses F.softmax: 2.302554 log_softmax: 2.303060\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3003\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3007\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 244 [0/1000 (0%)]\tLosses F.softmax: 2.310823 log_softmax: 2.312209\n",
            "Train Epoch: 244 [1000/1000 (100%)]\tLosses F.softmax: 2.253335 log_softmax: 2.254293\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3002\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3007\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 245 [0/1000 (0%)]\tLosses F.softmax: 2.340404 log_softmax: 2.340112\n",
            "Train Epoch: 245 [1000/1000 (100%)]\tLosses F.softmax: 2.332490 log_softmax: 2.330579\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3002\tAccuracy: 1028.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3007\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 246 [0/1000 (0%)]\tLosses F.softmax: 2.323771 log_softmax: 2.324544\n",
            "Train Epoch: 246 [1000/1000 (100%)]\tLosses F.softmax: 2.340383 log_softmax: 2.339822\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3002\tAccuracy: 1041.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3007\tAccuracy: 1036.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 247 [0/1000 (0%)]\tLosses F.softmax: 2.296589 log_softmax: 2.298381\n",
            "Train Epoch: 247 [1000/1000 (100%)]\tLosses F.softmax: 2.278766 log_softmax: 2.281322\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3002\tAccuracy: 1033.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3007\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 248 [0/1000 (0%)]\tLosses F.softmax: 2.326965 log_softmax: 2.327876\n",
            "Train Epoch: 248 [1000/1000 (100%)]\tLosses F.softmax: 2.356471 log_softmax: 2.357935\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3002\tAccuracy: 1037.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3007\tAccuracy: 1031.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 249 [0/1000 (0%)]\tLosses F.softmax: 2.298398 log_softmax: 2.298789\n",
            "Train Epoch: 249 [1000/1000 (100%)]\tLosses F.softmax: 2.350495 log_softmax: 2.352134\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3002\tAccuracy: 1034.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3006\tAccuracy: 1030.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 250 [0/1000 (0%)]\tLosses F.softmax: 2.246394 log_softmax: 2.247744\n",
            "Train Epoch: 250 [1000/1000 (100%)]\tLosses F.softmax: 2.258742 log_softmax: 2.257060\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3001\tAccuracy: 1041.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3006\tAccuracy: 1034.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 251 [0/1000 (0%)]\tLosses F.softmax: 2.281404 log_softmax: 2.281354\n",
            "Train Epoch: 251 [1000/1000 (100%)]\tLosses F.softmax: 2.351202 log_softmax: 2.352309\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3001\tAccuracy: 1041.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3006\tAccuracy: 1035.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 252 [0/1000 (0%)]\tLosses F.softmax: 2.253915 log_softmax: 2.255834\n",
            "Train Epoch: 252 [1000/1000 (100%)]\tLosses F.softmax: 2.320563 log_softmax: 2.321205\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3001\tAccuracy: 1057.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3006\tAccuracy: 1037.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 253 [0/1000 (0%)]\tLosses F.softmax: 2.269351 log_softmax: 2.269127\n",
            "Train Epoch: 253 [1000/1000 (100%)]\tLosses F.softmax: 2.301575 log_softmax: 2.300883\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3001\tAccuracy: 1071.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3006\tAccuracy: 1046.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 254 [0/1000 (0%)]\tLosses F.softmax: 2.274447 log_softmax: 2.272566\n",
            "Train Epoch: 254 [1000/1000 (100%)]\tLosses F.softmax: 2.332828 log_softmax: 2.331850\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3001\tAccuracy: 1070.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3006\tAccuracy: 1044.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 255 [0/1000 (0%)]\tLosses F.softmax: 2.272291 log_softmax: 2.273069\n",
            "Train Epoch: 255 [1000/1000 (100%)]\tLosses F.softmax: 2.257869 log_softmax: 2.258882\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3000\tAccuracy: 1082.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3005\tAccuracy: 1051.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 256 [0/1000 (0%)]\tLosses F.softmax: 2.329107 log_softmax: 2.327742\n",
            "Train Epoch: 256 [1000/1000 (100%)]\tLosses F.softmax: 2.325353 log_softmax: 2.325486\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3000\tAccuracy: 1201.0/10000 (12%)\n",
            "log_softmax: Loss: 2.3005\tAccuracy: 1122.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 257 [0/1000 (0%)]\tLosses F.softmax: 2.296945 log_softmax: 2.297574\n",
            "Train Epoch: 257 [1000/1000 (100%)]\tLosses F.softmax: 2.182341 log_softmax: 2.184620\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3000\tAccuracy: 1198.0/10000 (12%)\n",
            "log_softmax: Loss: 2.3005\tAccuracy: 1115.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 258 [0/1000 (0%)]\tLosses F.softmax: 2.327322 log_softmax: 2.326673\n",
            "Train Epoch: 258 [1000/1000 (100%)]\tLosses F.softmax: 2.262953 log_softmax: 2.264898\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3000\tAccuracy: 1070.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3005\tAccuracy: 1043.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 259 [0/1000 (0%)]\tLosses F.softmax: 2.300491 log_softmax: 2.302090\n",
            "Train Epoch: 259 [1000/1000 (100%)]\tLosses F.softmax: 2.329529 log_softmax: 2.331522\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3000\tAccuracy: 1129.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3005\tAccuracy: 1072.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 260 [0/1000 (0%)]\tLosses F.softmax: 2.269769 log_softmax: 2.271445\n",
            "Train Epoch: 260 [1000/1000 (100%)]\tLosses F.softmax: 2.332322 log_softmax: 2.333200\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3000\tAccuracy: 1080.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3005\tAccuracy: 1051.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 261 [0/1000 (0%)]\tLosses F.softmax: 2.241286 log_softmax: 2.242353\n",
            "Train Epoch: 261 [1000/1000 (100%)]\tLosses F.softmax: 2.313454 log_softmax: 2.316725\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3000\tAccuracy: 1086.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3005\tAccuracy: 1051.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 262 [0/1000 (0%)]\tLosses F.softmax: 2.304022 log_softmax: 2.305158\n",
            "Train Epoch: 262 [1000/1000 (100%)]\tLosses F.softmax: 2.333140 log_softmax: 2.334931\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3000\tAccuracy: 1134.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3005\tAccuracy: 1074.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 263 [0/1000 (0%)]\tLosses F.softmax: 2.295163 log_softmax: 2.293604\n",
            "Train Epoch: 263 [1000/1000 (100%)]\tLosses F.softmax: 2.320670 log_softmax: 2.322460\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3000\tAccuracy: 1085.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3005\tAccuracy: 1051.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 264 [0/1000 (0%)]\tLosses F.softmax: 2.295631 log_softmax: 2.296537\n",
            "Train Epoch: 264 [1000/1000 (100%)]\tLosses F.softmax: 2.330373 log_softmax: 2.331654\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2999\tAccuracy: 1071.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3005\tAccuracy: 1044.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 265 [0/1000 (0%)]\tLosses F.softmax: 2.327119 log_softmax: 2.328554\n",
            "Train Epoch: 265 [1000/1000 (100%)]\tLosses F.softmax: 2.358639 log_softmax: 2.358675\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2999\tAccuracy: 1170.0/10000 (12%)\n",
            "log_softmax: Loss: 2.3005\tAccuracy: 1098.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 266 [0/1000 (0%)]\tLosses F.softmax: 2.330424 log_softmax: 2.329974\n",
            "Train Epoch: 266 [1000/1000 (100%)]\tLosses F.softmax: 2.222717 log_softmax: 2.222829\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2999\tAccuracy: 1168.0/10000 (12%)\n",
            "log_softmax: Loss: 2.3004\tAccuracy: 1095.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 267 [0/1000 (0%)]\tLosses F.softmax: 2.380702 log_softmax: 2.380796\n",
            "Train Epoch: 267 [1000/1000 (100%)]\tLosses F.softmax: 2.331470 log_softmax: 2.332024\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2999\tAccuracy: 1172.0/10000 (12%)\n",
            "log_softmax: Loss: 2.3004\tAccuracy: 1098.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 268 [0/1000 (0%)]\tLosses F.softmax: 2.327770 log_softmax: 2.327177\n",
            "Train Epoch: 268 [1000/1000 (100%)]\tLosses F.softmax: 2.230794 log_softmax: 2.232698\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2999\tAccuracy: 1296.0/10000 (13%)\n",
            "log_softmax: Loss: 2.3004\tAccuracy: 1189.0/10000 (12%)\n",
            "\n",
            "Train Epoch: 269 [0/1000 (0%)]\tLosses F.softmax: 2.375118 log_softmax: 2.376337\n",
            "Train Epoch: 269 [1000/1000 (100%)]\tLosses F.softmax: 2.246804 log_softmax: 2.248891\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2999\tAccuracy: 1116.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3004\tAccuracy: 1062.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 270 [0/1000 (0%)]\tLosses F.softmax: 2.291970 log_softmax: 2.292795\n",
            "Train Epoch: 270 [1000/1000 (100%)]\tLosses F.softmax: 2.269346 log_softmax: 2.268700\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2999\tAccuracy: 1087.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3004\tAccuracy: 1051.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 271 [0/1000 (0%)]\tLosses F.softmax: 2.295649 log_softmax: 2.297663\n",
            "Train Epoch: 271 [1000/1000 (100%)]\tLosses F.softmax: 2.273718 log_softmax: 2.274940\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2998\tAccuracy: 1276.0/10000 (13%)\n",
            "log_softmax: Loss: 2.3004\tAccuracy: 1167.0/10000 (12%)\n",
            "\n",
            "Train Epoch: 272 [0/1000 (0%)]\tLosses F.softmax: 2.331320 log_softmax: 2.332726\n",
            "Train Epoch: 272 [1000/1000 (100%)]\tLosses F.softmax: 2.238845 log_softmax: 2.239247\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2998\tAccuracy: 1337.0/10000 (13%)\n",
            "log_softmax: Loss: 2.3004\tAccuracy: 1219.0/10000 (12%)\n",
            "\n",
            "Train Epoch: 273 [0/1000 (0%)]\tLosses F.softmax: 2.307728 log_softmax: 2.309775\n",
            "Train Epoch: 273 [1000/1000 (100%)]\tLosses F.softmax: 2.312564 log_softmax: 2.312850\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2998\tAccuracy: 1350.0/10000 (14%)\n",
            "log_softmax: Loss: 2.3004\tAccuracy: 1241.0/10000 (12%)\n",
            "\n",
            "Train Epoch: 274 [0/1000 (0%)]\tLosses F.softmax: 2.243661 log_softmax: 2.244373\n",
            "Train Epoch: 274 [1000/1000 (100%)]\tLosses F.softmax: 2.280619 log_softmax: 2.280795\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2998\tAccuracy: 1266.0/10000 (13%)\n",
            "log_softmax: Loss: 2.3003\tAccuracy: 1162.0/10000 (12%)\n",
            "\n",
            "Train Epoch: 275 [0/1000 (0%)]\tLosses F.softmax: 2.344176 log_softmax: 2.344954\n",
            "Train Epoch: 275 [1000/1000 (100%)]\tLosses F.softmax: 2.248990 log_softmax: 2.248293\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2997\tAccuracy: 1293.0/10000 (13%)\n",
            "log_softmax: Loss: 2.3003\tAccuracy: 1184.0/10000 (12%)\n",
            "\n",
            "Train Epoch: 276 [0/1000 (0%)]\tLosses F.softmax: 2.268280 log_softmax: 2.269103\n",
            "Train Epoch: 276 [1000/1000 (100%)]\tLosses F.softmax: 2.302687 log_softmax: 2.302524\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2998\tAccuracy: 1130.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3003\tAccuracy: 1069.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 277 [0/1000 (0%)]\tLosses F.softmax: 2.266907 log_softmax: 2.267960\n",
            "Train Epoch: 277 [1000/1000 (100%)]\tLosses F.softmax: 2.250073 log_softmax: 2.250439\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2997\tAccuracy: 1219.0/10000 (12%)\n",
            "log_softmax: Loss: 2.3003\tAccuracy: 1131.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 278 [0/1000 (0%)]\tLosses F.softmax: 2.332009 log_softmax: 2.328768\n",
            "Train Epoch: 278 [1000/1000 (100%)]\tLosses F.softmax: 2.299574 log_softmax: 2.299524\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2997\tAccuracy: 1388.0/10000 (14%)\n",
            "log_softmax: Loss: 2.3003\tAccuracy: 1273.0/10000 (13%)\n",
            "\n",
            "Train Epoch: 279 [0/1000 (0%)]\tLosses F.softmax: 2.330866 log_softmax: 2.330912\n",
            "Train Epoch: 279 [1000/1000 (100%)]\tLosses F.softmax: 2.245141 log_softmax: 2.245178\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2997\tAccuracy: 1214.0/10000 (12%)\n",
            "log_softmax: Loss: 2.3003\tAccuracy: 1126.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 280 [0/1000 (0%)]\tLosses F.softmax: 2.307150 log_softmax: 2.307321\n",
            "Train Epoch: 280 [1000/1000 (100%)]\tLosses F.softmax: 2.414037 log_softmax: 2.416292\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2997\tAccuracy: 1200.0/10000 (12%)\n",
            "log_softmax: Loss: 2.3003\tAccuracy: 1114.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 281 [0/1000 (0%)]\tLosses F.softmax: 2.266389 log_softmax: 2.268296\n",
            "Train Epoch: 281 [1000/1000 (100%)]\tLosses F.softmax: 2.219337 log_softmax: 2.220587\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2997\tAccuracy: 1143.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3003\tAccuracy: 1073.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 282 [0/1000 (0%)]\tLosses F.softmax: 2.212823 log_softmax: 2.214082\n",
            "Train Epoch: 282 [1000/1000 (100%)]\tLosses F.softmax: 2.355437 log_softmax: 2.357086\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2997\tAccuracy: 1214.0/10000 (12%)\n",
            "log_softmax: Loss: 2.3002\tAccuracy: 1125.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 283 [0/1000 (0%)]\tLosses F.softmax: 2.377578 log_softmax: 2.377702\n",
            "Train Epoch: 283 [1000/1000 (100%)]\tLosses F.softmax: 2.307818 log_softmax: 2.308825\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2997\tAccuracy: 1195.0/10000 (12%)\n",
            "log_softmax: Loss: 2.3002\tAccuracy: 1113.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 284 [0/1000 (0%)]\tLosses F.softmax: 2.367550 log_softmax: 2.367703\n",
            "Train Epoch: 284 [1000/1000 (100%)]\tLosses F.softmax: 2.315308 log_softmax: 2.314234\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2996\tAccuracy: 1165.0/10000 (12%)\n",
            "log_softmax: Loss: 2.3002\tAccuracy: 1087.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 285 [0/1000 (0%)]\tLosses F.softmax: 2.297778 log_softmax: 2.298053\n",
            "Train Epoch: 285 [1000/1000 (100%)]\tLosses F.softmax: 2.289773 log_softmax: 2.291186\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2996\tAccuracy: 1173.0/10000 (12%)\n",
            "log_softmax: Loss: 2.3002\tAccuracy: 1095.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 286 [0/1000 (0%)]\tLosses F.softmax: 2.290294 log_softmax: 2.291483\n",
            "Train Epoch: 286 [1000/1000 (100%)]\tLosses F.softmax: 2.323128 log_softmax: 2.325390\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2996\tAccuracy: 1198.0/10000 (12%)\n",
            "log_softmax: Loss: 2.3002\tAccuracy: 1114.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 287 [0/1000 (0%)]\tLosses F.softmax: 2.330979 log_softmax: 2.330812\n",
            "Train Epoch: 287 [1000/1000 (100%)]\tLosses F.softmax: 2.324961 log_softmax: 2.326153\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2996\tAccuracy: 1239.0/10000 (12%)\n",
            "log_softmax: Loss: 2.3002\tAccuracy: 1138.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 288 [0/1000 (0%)]\tLosses F.softmax: 2.332081 log_softmax: 2.329056\n",
            "Train Epoch: 288 [1000/1000 (100%)]\tLosses F.softmax: 2.247721 log_softmax: 2.251001\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2996\tAccuracy: 1243.0/10000 (12%)\n",
            "log_softmax: Loss: 2.3002\tAccuracy: 1138.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 289 [0/1000 (0%)]\tLosses F.softmax: 2.299005 log_softmax: 2.300291\n",
            "Train Epoch: 289 [1000/1000 (100%)]\tLosses F.softmax: 2.313589 log_softmax: 2.313661\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2996\tAccuracy: 1089.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3002\tAccuracy: 1050.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 290 [0/1000 (0%)]\tLosses F.softmax: 2.295080 log_softmax: 2.295362\n",
            "Train Epoch: 290 [1000/1000 (100%)]\tLosses F.softmax: 2.368090 log_softmax: 2.367966\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2995\tAccuracy: 1111.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3001\tAccuracy: 1056.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 291 [0/1000 (0%)]\tLosses F.softmax: 2.270753 log_softmax: 2.271552\n",
            "Train Epoch: 291 [1000/1000 (100%)]\tLosses F.softmax: 2.313874 log_softmax: 2.315006\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2995\tAccuracy: 1085.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3001\tAccuracy: 1049.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 292 [0/1000 (0%)]\tLosses F.softmax: 2.306411 log_softmax: 2.308813\n",
            "Train Epoch: 292 [1000/1000 (100%)]\tLosses F.softmax: 2.242006 log_softmax: 2.242098\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2995\tAccuracy: 1068.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3001\tAccuracy: 1038.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 293 [0/1000 (0%)]\tLosses F.softmax: 2.335890 log_softmax: 2.334924\n",
            "Train Epoch: 293 [1000/1000 (100%)]\tLosses F.softmax: 2.282597 log_softmax: 2.282644\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2995\tAccuracy: 1058.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3001\tAccuracy: 1036.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 294 [0/1000 (0%)]\tLosses F.softmax: 2.316057 log_softmax: 2.317270\n",
            "Train Epoch: 294 [1000/1000 (100%)]\tLosses F.softmax: 2.409887 log_softmax: 2.410243\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2995\tAccuracy: 1075.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3001\tAccuracy: 1040.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 295 [0/1000 (0%)]\tLosses F.softmax: 2.372829 log_softmax: 2.371972\n",
            "Train Epoch: 295 [1000/1000 (100%)]\tLosses F.softmax: 2.291162 log_softmax: 2.290672\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2995\tAccuracy: 1117.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3001\tAccuracy: 1058.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 296 [0/1000 (0%)]\tLosses F.softmax: 2.316008 log_softmax: 2.316194\n",
            "Train Epoch: 296 [1000/1000 (100%)]\tLosses F.softmax: 2.361443 log_softmax: 2.359996\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2995\tAccuracy: 1129.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3001\tAccuracy: 1062.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 297 [0/1000 (0%)]\tLosses F.softmax: 2.343972 log_softmax: 2.346162\n",
            "Train Epoch: 297 [1000/1000 (100%)]\tLosses F.softmax: 2.198898 log_softmax: 2.200118\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2995\tAccuracy: 1159.0/10000 (12%)\n",
            "log_softmax: Loss: 2.3001\tAccuracy: 1079.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 298 [0/1000 (0%)]\tLosses F.softmax: 2.231140 log_softmax: 2.232002\n",
            "Train Epoch: 298 [1000/1000 (100%)]\tLosses F.softmax: 2.297443 log_softmax: 2.298356\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2994\tAccuracy: 1220.0/10000 (12%)\n",
            "log_softmax: Loss: 2.3001\tAccuracy: 1127.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 299 [0/1000 (0%)]\tLosses F.softmax: 2.285316 log_softmax: 2.285400\n",
            "Train Epoch: 299 [1000/1000 (100%)]\tLosses F.softmax: 2.253558 log_softmax: 2.254300\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2994\tAccuracy: 1294.0/10000 (13%)\n",
            "log_softmax: Loss: 2.3001\tAccuracy: 1179.0/10000 (12%)\n",
            "\n",
            "Train Epoch: 300 [0/1000 (0%)]\tLosses F.softmax: 2.210362 log_softmax: 2.211272\n",
            "Train Epoch: 300 [1000/1000 (100%)]\tLosses F.softmax: 2.314211 log_softmax: 2.313814\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2994\tAccuracy: 1120.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3000\tAccuracy: 1059.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 301 [0/1000 (0%)]\tLosses F.softmax: 2.274082 log_softmax: 2.275792\n",
            "Train Epoch: 301 [1000/1000 (100%)]\tLosses F.softmax: 2.246674 log_softmax: 2.246996\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2994\tAccuracy: 1090.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3000\tAccuracy: 1047.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 302 [0/1000 (0%)]\tLosses F.softmax: 2.327952 log_softmax: 2.329162\n",
            "Train Epoch: 302 [1000/1000 (100%)]\tLosses F.softmax: 2.232908 log_softmax: 2.234219\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2994\tAccuracy: 1096.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3000\tAccuracy: 1051.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 303 [0/1000 (0%)]\tLosses F.softmax: 2.286641 log_softmax: 2.288878\n",
            "Train Epoch: 303 [1000/1000 (100%)]\tLosses F.softmax: 2.265206 log_softmax: 2.265156\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2994\tAccuracy: 1114.0/10000 (11%)\n",
            "log_softmax: Loss: 2.3000\tAccuracy: 1057.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 304 [0/1000 (0%)]\tLosses F.softmax: 2.300873 log_softmax: 2.301356\n",
            "Train Epoch: 304 [1000/1000 (100%)]\tLosses F.softmax: 2.320690 log_softmax: 2.321951\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2994\tAccuracy: 1163.0/10000 (12%)\n",
            "log_softmax: Loss: 2.3000\tAccuracy: 1080.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 305 [0/1000 (0%)]\tLosses F.softmax: 2.260039 log_softmax: 2.260431\n",
            "Train Epoch: 305 [1000/1000 (100%)]\tLosses F.softmax: 2.304404 log_softmax: 2.304961\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2993\tAccuracy: 1159.0/10000 (12%)\n",
            "log_softmax: Loss: 2.3000\tAccuracy: 1076.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 306 [0/1000 (0%)]\tLosses F.softmax: 2.289204 log_softmax: 2.290630\n",
            "Train Epoch: 306 [1000/1000 (100%)]\tLosses F.softmax: 2.265187 log_softmax: 2.267498\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2993\tAccuracy: 1175.0/10000 (12%)\n",
            "log_softmax: Loss: 2.2999\tAccuracy: 1090.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 307 [0/1000 (0%)]\tLosses F.softmax: 2.372444 log_softmax: 2.374614\n",
            "Train Epoch: 307 [1000/1000 (100%)]\tLosses F.softmax: 2.329307 log_softmax: 2.333030\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2993\tAccuracy: 1205.0/10000 (12%)\n",
            "log_softmax: Loss: 2.2999\tAccuracy: 1113.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 308 [0/1000 (0%)]\tLosses F.softmax: 2.228355 log_softmax: 2.230060\n",
            "Train Epoch: 308 [1000/1000 (100%)]\tLosses F.softmax: 2.309849 log_softmax: 2.310240\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2993\tAccuracy: 1167.0/10000 (12%)\n",
            "log_softmax: Loss: 2.2999\tAccuracy: 1082.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 309 [0/1000 (0%)]\tLosses F.softmax: 2.266246 log_softmax: 2.267306\n",
            "Train Epoch: 309 [1000/1000 (100%)]\tLosses F.softmax: 2.314064 log_softmax: 2.314296\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2992\tAccuracy: 1219.0/10000 (12%)\n",
            "log_softmax: Loss: 2.2999\tAccuracy: 1122.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 310 [0/1000 (0%)]\tLosses F.softmax: 2.288717 log_softmax: 2.287721\n",
            "Train Epoch: 310 [1000/1000 (100%)]\tLosses F.softmax: 2.283651 log_softmax: 2.282747\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2992\tAccuracy: 1362.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2999\tAccuracy: 1241.0/10000 (12%)\n",
            "\n",
            "Train Epoch: 311 [0/1000 (0%)]\tLosses F.softmax: 2.279954 log_softmax: 2.283380\n",
            "Train Epoch: 311 [1000/1000 (100%)]\tLosses F.softmax: 2.339976 log_softmax: 2.340605\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2992\tAccuracy: 1467.0/10000 (15%)\n",
            "log_softmax: Loss: 2.2999\tAccuracy: 1350.0/10000 (14%)\n",
            "\n",
            "Train Epoch: 312 [0/1000 (0%)]\tLosses F.softmax: 2.315467 log_softmax: 2.315496\n",
            "Train Epoch: 312 [1000/1000 (100%)]\tLosses F.softmax: 2.281890 log_softmax: 2.282613\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2992\tAccuracy: 1497.0/10000 (15%)\n",
            "log_softmax: Loss: 2.2998\tAccuracy: 1392.0/10000 (14%)\n",
            "\n",
            "Train Epoch: 313 [0/1000 (0%)]\tLosses F.softmax: 2.299753 log_softmax: 2.299457\n",
            "Train Epoch: 313 [1000/1000 (100%)]\tLosses F.softmax: 2.282226 log_softmax: 2.282277\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2992\tAccuracy: 1590.0/10000 (16%)\n",
            "log_softmax: Loss: 2.2998\tAccuracy: 1497.0/10000 (15%)\n",
            "\n",
            "Train Epoch: 314 [0/1000 (0%)]\tLosses F.softmax: 2.280411 log_softmax: 2.281762\n",
            "Train Epoch: 314 [1000/1000 (100%)]\tLosses F.softmax: 2.329702 log_softmax: 2.329679\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2992\tAccuracy: 1632.0/10000 (16%)\n",
            "log_softmax: Loss: 2.2998\tAccuracy: 1553.0/10000 (16%)\n",
            "\n",
            "Train Epoch: 315 [0/1000 (0%)]\tLosses F.softmax: 2.319688 log_softmax: 2.321748\n",
            "Train Epoch: 315 [1000/1000 (100%)]\tLosses F.softmax: 2.318600 log_softmax: 2.316742\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2991\tAccuracy: 1599.0/10000 (16%)\n",
            "log_softmax: Loss: 2.2998\tAccuracy: 1514.0/10000 (15%)\n",
            "\n",
            "Train Epoch: 316 [0/1000 (0%)]\tLosses F.softmax: 2.270344 log_softmax: 2.271019\n",
            "Train Epoch: 316 [1000/1000 (100%)]\tLosses F.softmax: 2.324816 log_softmax: 2.326097\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2991\tAccuracy: 1634.0/10000 (16%)\n",
            "log_softmax: Loss: 2.2998\tAccuracy: 1554.0/10000 (16%)\n",
            "\n",
            "Train Epoch: 317 [0/1000 (0%)]\tLosses F.softmax: 2.321719 log_softmax: 2.322038\n",
            "Train Epoch: 317 [1000/1000 (100%)]\tLosses F.softmax: 2.313554 log_softmax: 2.314858\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2991\tAccuracy: 1556.0/10000 (16%)\n",
            "log_softmax: Loss: 2.2998\tAccuracy: 1453.0/10000 (15%)\n",
            "\n",
            "Train Epoch: 318 [0/1000 (0%)]\tLosses F.softmax: 2.265706 log_softmax: 2.267033\n",
            "Train Epoch: 318 [1000/1000 (100%)]\tLosses F.softmax: 2.310705 log_softmax: 2.312429\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2991\tAccuracy: 1560.0/10000 (16%)\n",
            "log_softmax: Loss: 2.2998\tAccuracy: 1459.0/10000 (15%)\n",
            "\n",
            "Train Epoch: 319 [0/1000 (0%)]\tLosses F.softmax: 2.249847 log_softmax: 2.251159\n",
            "Train Epoch: 319 [1000/1000 (100%)]\tLosses F.softmax: 2.321736 log_softmax: 2.320935\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2991\tAccuracy: 1602.0/10000 (16%)\n",
            "log_softmax: Loss: 2.2998\tAccuracy: 1517.0/10000 (15%)\n",
            "\n",
            "Train Epoch: 320 [0/1000 (0%)]\tLosses F.softmax: 2.336300 log_softmax: 2.337432\n",
            "Train Epoch: 320 [1000/1000 (100%)]\tLosses F.softmax: 2.281554 log_softmax: 2.280972\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2991\tAccuracy: 1453.0/10000 (15%)\n",
            "log_softmax: Loss: 2.2998\tAccuracy: 1343.0/10000 (13%)\n",
            "\n",
            "Train Epoch: 321 [0/1000 (0%)]\tLosses F.softmax: 2.322881 log_softmax: 2.323765\n",
            "Train Epoch: 321 [1000/1000 (100%)]\tLosses F.softmax: 2.367996 log_softmax: 2.366087\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2991\tAccuracy: 1568.0/10000 (16%)\n",
            "log_softmax: Loss: 2.2998\tAccuracy: 1463.0/10000 (15%)\n",
            "\n",
            "Train Epoch: 322 [0/1000 (0%)]\tLosses F.softmax: 2.330808 log_softmax: 2.332427\n",
            "Train Epoch: 322 [1000/1000 (100%)]\tLosses F.softmax: 2.325796 log_softmax: 2.326787\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2991\tAccuracy: 1425.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2998\tAccuracy: 1305.0/10000 (13%)\n",
            "\n",
            "Train Epoch: 323 [0/1000 (0%)]\tLosses F.softmax: 2.251297 log_softmax: 2.252159\n",
            "Train Epoch: 323 [1000/1000 (100%)]\tLosses F.softmax: 2.338151 log_softmax: 2.339852\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2991\tAccuracy: 1401.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2997\tAccuracy: 1270.0/10000 (13%)\n",
            "\n",
            "Train Epoch: 324 [0/1000 (0%)]\tLosses F.softmax: 2.244794 log_softmax: 2.244249\n",
            "Train Epoch: 324 [1000/1000 (100%)]\tLosses F.softmax: 2.274673 log_softmax: 2.275653\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2991\tAccuracy: 1420.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2997\tAccuracy: 1297.0/10000 (13%)\n",
            "\n",
            "Train Epoch: 325 [0/1000 (0%)]\tLosses F.softmax: 2.324418 log_softmax: 2.324014\n",
            "Train Epoch: 325 [1000/1000 (100%)]\tLosses F.softmax: 2.311552 log_softmax: 2.311951\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2990\tAccuracy: 1387.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2997\tAccuracy: 1261.0/10000 (13%)\n",
            "\n",
            "Train Epoch: 326 [0/1000 (0%)]\tLosses F.softmax: 2.352815 log_softmax: 2.352741\n",
            "Train Epoch: 326 [1000/1000 (100%)]\tLosses F.softmax: 2.301813 log_softmax: 2.301131\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2990\tAccuracy: 1326.0/10000 (13%)\n",
            "log_softmax: Loss: 2.2997\tAccuracy: 1197.0/10000 (12%)\n",
            "\n",
            "Train Epoch: 327 [0/1000 (0%)]\tLosses F.softmax: 2.241402 log_softmax: 2.242193\n",
            "Train Epoch: 327 [1000/1000 (100%)]\tLosses F.softmax: 2.317000 log_softmax: 2.318457\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2990\tAccuracy: 1350.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2997\tAccuracy: 1225.0/10000 (12%)\n",
            "\n",
            "Train Epoch: 328 [0/1000 (0%)]\tLosses F.softmax: 2.314537 log_softmax: 2.314463\n",
            "Train Epoch: 328 [1000/1000 (100%)]\tLosses F.softmax: 2.257386 log_softmax: 2.258621\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2990\tAccuracy: 1429.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2997\tAccuracy: 1306.0/10000 (13%)\n",
            "\n",
            "Train Epoch: 329 [0/1000 (0%)]\tLosses F.softmax: 2.271847 log_softmax: 2.271754\n",
            "Train Epoch: 329 [1000/1000 (100%)]\tLosses F.softmax: 2.266186 log_softmax: 2.268307\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2990\tAccuracy: 1532.0/10000 (15%)\n",
            "log_softmax: Loss: 2.2997\tAccuracy: 1430.0/10000 (14%)\n",
            "\n",
            "Train Epoch: 330 [0/1000 (0%)]\tLosses F.softmax: 2.318155 log_softmax: 2.318805\n",
            "Train Epoch: 330 [1000/1000 (100%)]\tLosses F.softmax: 2.283042 log_softmax: 2.283083\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2990\tAccuracy: 1250.0/10000 (12%)\n",
            "log_softmax: Loss: 2.2997\tAccuracy: 1132.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 331 [0/1000 (0%)]\tLosses F.softmax: 2.310882 log_softmax: 2.312593\n",
            "Train Epoch: 331 [1000/1000 (100%)]\tLosses F.softmax: 2.242244 log_softmax: 2.245446\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2990\tAccuracy: 1299.0/10000 (13%)\n",
            "log_softmax: Loss: 2.2997\tAccuracy: 1169.0/10000 (12%)\n",
            "\n",
            "Train Epoch: 332 [0/1000 (0%)]\tLosses F.softmax: 2.354996 log_softmax: 2.356344\n",
            "Train Epoch: 332 [1000/1000 (100%)]\tLosses F.softmax: 2.379529 log_softmax: 2.381819\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2990\tAccuracy: 1229.0/10000 (12%)\n",
            "log_softmax: Loss: 2.2997\tAccuracy: 1119.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 333 [0/1000 (0%)]\tLosses F.softmax: 2.334218 log_softmax: 2.332098\n",
            "Train Epoch: 333 [1000/1000 (100%)]\tLosses F.softmax: 2.290668 log_softmax: 2.290677\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2989\tAccuracy: 1415.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2997\tAccuracy: 1289.0/10000 (13%)\n",
            "\n",
            "Train Epoch: 334 [0/1000 (0%)]\tLosses F.softmax: 2.372494 log_softmax: 2.371754\n",
            "Train Epoch: 334 [1000/1000 (100%)]\tLosses F.softmax: 2.233560 log_softmax: 2.234229\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2989\tAccuracy: 1399.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2996\tAccuracy: 1270.0/10000 (13%)\n",
            "\n",
            "Train Epoch: 335 [0/1000 (0%)]\tLosses F.softmax: 2.303674 log_softmax: 2.304300\n",
            "Train Epoch: 335 [1000/1000 (100%)]\tLosses F.softmax: 2.300260 log_softmax: 2.299313\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2989\tAccuracy: 1371.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2996\tAccuracy: 1244.0/10000 (12%)\n",
            "\n",
            "Train Epoch: 336 [0/1000 (0%)]\tLosses F.softmax: 2.344638 log_softmax: 2.347216\n",
            "Train Epoch: 336 [1000/1000 (100%)]\tLosses F.softmax: 2.284716 log_softmax: 2.285535\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2989\tAccuracy: 1218.0/10000 (12%)\n",
            "log_softmax: Loss: 2.2996\tAccuracy: 1107.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 337 [0/1000 (0%)]\tLosses F.softmax: 2.213704 log_softmax: 2.214412\n",
            "Train Epoch: 337 [1000/1000 (100%)]\tLosses F.softmax: 2.217913 log_softmax: 2.219220\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2989\tAccuracy: 1112.0/10000 (11%)\n",
            "log_softmax: Loss: 2.2996\tAccuracy: 1049.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 338 [0/1000 (0%)]\tLosses F.softmax: 2.330983 log_softmax: 2.332256\n",
            "Train Epoch: 338 [1000/1000 (100%)]\tLosses F.softmax: 2.279150 log_softmax: 2.280075\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2989\tAccuracy: 1178.0/10000 (12%)\n",
            "log_softmax: Loss: 2.2996\tAccuracy: 1079.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 339 [0/1000 (0%)]\tLosses F.softmax: 2.292599 log_softmax: 2.292452\n",
            "Train Epoch: 339 [1000/1000 (100%)]\tLosses F.softmax: 2.327292 log_softmax: 2.328674\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2988\tAccuracy: 1290.0/10000 (13%)\n",
            "log_softmax: Loss: 2.2996\tAccuracy: 1158.0/10000 (12%)\n",
            "\n",
            "Train Epoch: 340 [0/1000 (0%)]\tLosses F.softmax: 2.342297 log_softmax: 2.345198\n",
            "Train Epoch: 340 [1000/1000 (100%)]\tLosses F.softmax: 2.353734 log_softmax: 2.355436\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2988\tAccuracy: 1186.0/10000 (12%)\n",
            "log_softmax: Loss: 2.2996\tAccuracy: 1086.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 341 [0/1000 (0%)]\tLosses F.softmax: 2.336922 log_softmax: 2.341558\n",
            "Train Epoch: 341 [1000/1000 (100%)]\tLosses F.softmax: 2.296816 log_softmax: 2.297557\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2988\tAccuracy: 1169.0/10000 (12%)\n",
            "log_softmax: Loss: 2.2996\tAccuracy: 1077.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 342 [0/1000 (0%)]\tLosses F.softmax: 2.325139 log_softmax: 2.326837\n",
            "Train Epoch: 342 [1000/1000 (100%)]\tLosses F.softmax: 2.274886 log_softmax: 2.273590\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2988\tAccuracy: 1154.0/10000 (12%)\n",
            "log_softmax: Loss: 2.2995\tAccuracy: 1066.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 343 [0/1000 (0%)]\tLosses F.softmax: 2.268596 log_softmax: 2.267126\n",
            "Train Epoch: 343 [1000/1000 (100%)]\tLosses F.softmax: 2.374720 log_softmax: 2.375945\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2988\tAccuracy: 1104.0/10000 (11%)\n",
            "log_softmax: Loss: 2.2995\tAccuracy: 1044.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 344 [0/1000 (0%)]\tLosses F.softmax: 2.297389 log_softmax: 2.298355\n",
            "Train Epoch: 344 [1000/1000 (100%)]\tLosses F.softmax: 2.338335 log_softmax: 2.337537\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2988\tAccuracy: 1082.0/10000 (11%)\n",
            "log_softmax: Loss: 2.2995\tAccuracy: 1040.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 345 [0/1000 (0%)]\tLosses F.softmax: 2.241056 log_softmax: 2.242816\n",
            "Train Epoch: 345 [1000/1000 (100%)]\tLosses F.softmax: 2.334002 log_softmax: 2.333704\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2987\tAccuracy: 1054.0/10000 (11%)\n",
            "log_softmax: Loss: 2.2995\tAccuracy: 1033.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 346 [0/1000 (0%)]\tLosses F.softmax: 2.198214 log_softmax: 2.201031\n",
            "Train Epoch: 346 [1000/1000 (100%)]\tLosses F.softmax: 2.285363 log_softmax: 2.287949\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2987\tAccuracy: 1111.0/10000 (11%)\n",
            "log_softmax: Loss: 2.2995\tAccuracy: 1044.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 347 [0/1000 (0%)]\tLosses F.softmax: 2.300989 log_softmax: 2.302415\n",
            "Train Epoch: 347 [1000/1000 (100%)]\tLosses F.softmax: 2.229589 log_softmax: 2.229667\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2987\tAccuracy: 1056.0/10000 (11%)\n",
            "log_softmax: Loss: 2.2995\tAccuracy: 1033.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 348 [0/1000 (0%)]\tLosses F.softmax: 2.306715 log_softmax: 2.308799\n",
            "Train Epoch: 348 [1000/1000 (100%)]\tLosses F.softmax: 2.377006 log_softmax: 2.376851\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2987\tAccuracy: 1073.0/10000 (11%)\n",
            "log_softmax: Loss: 2.2995\tAccuracy: 1036.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 349 [0/1000 (0%)]\tLosses F.softmax: 2.249587 log_softmax: 2.248868\n",
            "Train Epoch: 349 [1000/1000 (100%)]\tLosses F.softmax: 2.220403 log_softmax: 2.220556\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2987\tAccuracy: 1104.0/10000 (11%)\n",
            "log_softmax: Loss: 2.2994\tAccuracy: 1043.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 350 [0/1000 (0%)]\tLosses F.softmax: 2.344219 log_softmax: 2.343649\n",
            "Train Epoch: 350 [1000/1000 (100%)]\tLosses F.softmax: 2.259239 log_softmax: 2.260720\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2987\tAccuracy: 1082.0/10000 (11%)\n",
            "log_softmax: Loss: 2.2994\tAccuracy: 1040.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 351 [0/1000 (0%)]\tLosses F.softmax: 2.328115 log_softmax: 2.329721\n",
            "Train Epoch: 351 [1000/1000 (100%)]\tLosses F.softmax: 2.271292 log_softmax: 2.273599\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2986\tAccuracy: 1112.0/10000 (11%)\n",
            "log_softmax: Loss: 2.2994\tAccuracy: 1045.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 352 [0/1000 (0%)]\tLosses F.softmax: 2.256307 log_softmax: 2.258047\n",
            "Train Epoch: 352 [1000/1000 (100%)]\tLosses F.softmax: 2.342152 log_softmax: 2.343896\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2986\tAccuracy: 1165.0/10000 (12%)\n",
            "log_softmax: Loss: 2.2994\tAccuracy: 1072.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 353 [0/1000 (0%)]\tLosses F.softmax: 2.308130 log_softmax: 2.310023\n",
            "Train Epoch: 353 [1000/1000 (100%)]\tLosses F.softmax: 2.232899 log_softmax: 2.233365\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2986\tAccuracy: 1229.0/10000 (12%)\n",
            "log_softmax: Loss: 2.2994\tAccuracy: 1108.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 354 [0/1000 (0%)]\tLosses F.softmax: 2.251611 log_softmax: 2.253658\n",
            "Train Epoch: 354 [1000/1000 (100%)]\tLosses F.softmax: 2.341741 log_softmax: 2.343197\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2986\tAccuracy: 1230.0/10000 (12%)\n",
            "log_softmax: Loss: 2.2994\tAccuracy: 1112.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 355 [0/1000 (0%)]\tLosses F.softmax: 2.235899 log_softmax: 2.236093\n",
            "Train Epoch: 355 [1000/1000 (100%)]\tLosses F.softmax: 2.231796 log_softmax: 2.232648\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2986\tAccuracy: 1267.0/10000 (13%)\n",
            "log_softmax: Loss: 2.2993\tAccuracy: 1135.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 356 [0/1000 (0%)]\tLosses F.softmax: 2.241608 log_softmax: 2.242114\n",
            "Train Epoch: 356 [1000/1000 (100%)]\tLosses F.softmax: 2.274453 log_softmax: 2.276961\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2986\tAccuracy: 1242.0/10000 (12%)\n",
            "log_softmax: Loss: 2.2994\tAccuracy: 1117.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 357 [0/1000 (0%)]\tLosses F.softmax: 2.301703 log_softmax: 2.303339\n",
            "Train Epoch: 357 [1000/1000 (100%)]\tLosses F.softmax: 2.268643 log_softmax: 2.269610\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2985\tAccuracy: 1252.0/10000 (13%)\n",
            "log_softmax: Loss: 2.2993\tAccuracy: 1128.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 358 [0/1000 (0%)]\tLosses F.softmax: 2.260851 log_softmax: 2.262572\n",
            "Train Epoch: 358 [1000/1000 (100%)]\tLosses F.softmax: 2.319014 log_softmax: 2.319235\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2985\tAccuracy: 1174.0/10000 (12%)\n",
            "log_softmax: Loss: 2.2993\tAccuracy: 1074.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 359 [0/1000 (0%)]\tLosses F.softmax: 2.340884 log_softmax: 2.342327\n",
            "Train Epoch: 359 [1000/1000 (100%)]\tLosses F.softmax: 2.205589 log_softmax: 2.206424\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2985\tAccuracy: 1209.0/10000 (12%)\n",
            "log_softmax: Loss: 2.2993\tAccuracy: 1090.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 360 [0/1000 (0%)]\tLosses F.softmax: 2.378779 log_softmax: 2.378333\n",
            "Train Epoch: 360 [1000/1000 (100%)]\tLosses F.softmax: 2.303276 log_softmax: 2.305982\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2985\tAccuracy: 1164.0/10000 (12%)\n",
            "log_softmax: Loss: 2.2993\tAccuracy: 1067.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 361 [0/1000 (0%)]\tLosses F.softmax: 2.293369 log_softmax: 2.293619\n",
            "Train Epoch: 361 [1000/1000 (100%)]\tLosses F.softmax: 2.268905 log_softmax: 2.270513\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2985\tAccuracy: 1283.0/10000 (13%)\n",
            "log_softmax: Loss: 2.2993\tAccuracy: 1146.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 362 [0/1000 (0%)]\tLosses F.softmax: 2.295374 log_softmax: 2.294249\n",
            "Train Epoch: 362 [1000/1000 (100%)]\tLosses F.softmax: 2.222618 log_softmax: 2.222560\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2985\tAccuracy: 1302.0/10000 (13%)\n",
            "log_softmax: Loss: 2.2993\tAccuracy: 1157.0/10000 (12%)\n",
            "\n",
            "Train Epoch: 363 [0/1000 (0%)]\tLosses F.softmax: 2.375825 log_softmax: 2.375854\n",
            "Train Epoch: 363 [1000/1000 (100%)]\tLosses F.softmax: 2.256255 log_softmax: 2.255959\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2985\tAccuracy: 1205.0/10000 (12%)\n",
            "log_softmax: Loss: 2.2993\tAccuracy: 1087.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 364 [0/1000 (0%)]\tLosses F.softmax: 2.340269 log_softmax: 2.341851\n",
            "Train Epoch: 364 [1000/1000 (100%)]\tLosses F.softmax: 2.268167 log_softmax: 2.271084\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2984\tAccuracy: 1173.0/10000 (12%)\n",
            "log_softmax: Loss: 2.2993\tAccuracy: 1072.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 365 [0/1000 (0%)]\tLosses F.softmax: 2.232187 log_softmax: 2.233897\n",
            "Train Epoch: 365 [1000/1000 (100%)]\tLosses F.softmax: 2.327466 log_softmax: 2.329906\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2984\tAccuracy: 1302.0/10000 (13%)\n",
            "log_softmax: Loss: 2.2992\tAccuracy: 1154.0/10000 (12%)\n",
            "\n",
            "Train Epoch: 366 [0/1000 (0%)]\tLosses F.softmax: 2.340466 log_softmax: 2.340382\n",
            "Train Epoch: 366 [1000/1000 (100%)]\tLosses F.softmax: 2.296717 log_softmax: 2.296554\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2984\tAccuracy: 1245.0/10000 (12%)\n",
            "log_softmax: Loss: 2.2992\tAccuracy: 1119.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 367 [0/1000 (0%)]\tLosses F.softmax: 2.197886 log_softmax: 2.200299\n",
            "Train Epoch: 367 [1000/1000 (100%)]\tLosses F.softmax: 2.272908 log_softmax: 2.272345\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2984\tAccuracy: 1206.0/10000 (12%)\n",
            "log_softmax: Loss: 2.2992\tAccuracy: 1087.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 368 [0/1000 (0%)]\tLosses F.softmax: 2.347855 log_softmax: 2.349261\n",
            "Train Epoch: 368 [1000/1000 (100%)]\tLosses F.softmax: 2.255024 log_softmax: 2.254963\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2984\tAccuracy: 1147.0/10000 (11%)\n",
            "log_softmax: Loss: 2.2992\tAccuracy: 1055.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 369 [0/1000 (0%)]\tLosses F.softmax: 2.313909 log_softmax: 2.315386\n",
            "Train Epoch: 369 [1000/1000 (100%)]\tLosses F.softmax: 2.307641 log_softmax: 2.307768\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2983\tAccuracy: 1197.0/10000 (12%)\n",
            "log_softmax: Loss: 2.2992\tAccuracy: 1083.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 370 [0/1000 (0%)]\tLosses F.softmax: 2.323977 log_softmax: 2.324697\n",
            "Train Epoch: 370 [1000/1000 (100%)]\tLosses F.softmax: 2.245633 log_softmax: 2.246338\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2983\tAccuracy: 1280.0/10000 (13%)\n",
            "log_softmax: Loss: 2.2992\tAccuracy: 1137.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 371 [0/1000 (0%)]\tLosses F.softmax: 2.276030 log_softmax: 2.276942\n",
            "Train Epoch: 371 [1000/1000 (100%)]\tLosses F.softmax: 2.225459 log_softmax: 2.226363\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2983\tAccuracy: 1341.0/10000 (13%)\n",
            "log_softmax: Loss: 2.2992\tAccuracy: 1188.0/10000 (12%)\n",
            "\n",
            "Train Epoch: 372 [0/1000 (0%)]\tLosses F.softmax: 2.299555 log_softmax: 2.299289\n",
            "Train Epoch: 372 [1000/1000 (100%)]\tLosses F.softmax: 2.303623 log_softmax: 2.303608\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2983\tAccuracy: 1299.0/10000 (13%)\n",
            "log_softmax: Loss: 2.2991\tAccuracy: 1149.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 373 [0/1000 (0%)]\tLosses F.softmax: 2.334979 log_softmax: 2.334831\n",
            "Train Epoch: 373 [1000/1000 (100%)]\tLosses F.softmax: 2.331357 log_softmax: 2.331642\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2983\tAccuracy: 1407.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2991\tAccuracy: 1262.0/10000 (13%)\n",
            "\n",
            "Train Epoch: 374 [0/1000 (0%)]\tLosses F.softmax: 2.291991 log_softmax: 2.291873\n",
            "Train Epoch: 374 [1000/1000 (100%)]\tLosses F.softmax: 2.374811 log_softmax: 2.375936\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2983\tAccuracy: 1479.0/10000 (15%)\n",
            "log_softmax: Loss: 2.2991\tAccuracy: 1358.0/10000 (14%)\n",
            "\n",
            "Train Epoch: 375 [0/1000 (0%)]\tLosses F.softmax: 2.369377 log_softmax: 2.374208\n",
            "Train Epoch: 375 [1000/1000 (100%)]\tLosses F.softmax: 2.340683 log_softmax: 2.339340\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2982\tAccuracy: 1600.0/10000 (16%)\n",
            "log_softmax: Loss: 2.2991\tAccuracy: 1498.0/10000 (15%)\n",
            "\n",
            "Train Epoch: 376 [0/1000 (0%)]\tLosses F.softmax: 2.310405 log_softmax: 2.313601\n",
            "Train Epoch: 376 [1000/1000 (100%)]\tLosses F.softmax: 2.305040 log_softmax: 2.306035\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2982\tAccuracy: 1441.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2991\tAccuracy: 1298.0/10000 (13%)\n",
            "\n",
            "Train Epoch: 377 [0/1000 (0%)]\tLosses F.softmax: 2.205881 log_softmax: 2.208733\n",
            "Train Epoch: 377 [1000/1000 (100%)]\tLosses F.softmax: 2.312542 log_softmax: 2.315768\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2982\tAccuracy: 1380.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2991\tAccuracy: 1232.0/10000 (12%)\n",
            "\n",
            "Train Epoch: 378 [0/1000 (0%)]\tLosses F.softmax: 2.263586 log_softmax: 2.265009\n",
            "Train Epoch: 378 [1000/1000 (100%)]\tLosses F.softmax: 2.279999 log_softmax: 2.279453\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2982\tAccuracy: 1425.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2991\tAccuracy: 1277.0/10000 (13%)\n",
            "\n",
            "Train Epoch: 379 [0/1000 (0%)]\tLosses F.softmax: 2.293563 log_softmax: 2.293794\n",
            "Train Epoch: 379 [1000/1000 (100%)]\tLosses F.softmax: 2.279652 log_softmax: 2.279207\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2982\tAccuracy: 1511.0/10000 (15%)\n",
            "log_softmax: Loss: 2.2990\tAccuracy: 1385.0/10000 (14%)\n",
            "\n",
            "Train Epoch: 380 [0/1000 (0%)]\tLosses F.softmax: 2.306926 log_softmax: 2.307320\n",
            "Train Epoch: 380 [1000/1000 (100%)]\tLosses F.softmax: 2.239554 log_softmax: 2.240004\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2982\tAccuracy: 1589.0/10000 (16%)\n",
            "log_softmax: Loss: 2.2990\tAccuracy: 1481.0/10000 (15%)\n",
            "\n",
            "Train Epoch: 381 [0/1000 (0%)]\tLosses F.softmax: 2.239325 log_softmax: 2.237602\n",
            "Train Epoch: 381 [1000/1000 (100%)]\tLosses F.softmax: 2.328101 log_softmax: 2.328969\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2981\tAccuracy: 1478.0/10000 (15%)\n",
            "log_softmax: Loss: 2.2990\tAccuracy: 1353.0/10000 (14%)\n",
            "\n",
            "Train Epoch: 382 [0/1000 (0%)]\tLosses F.softmax: 2.320238 log_softmax: 2.317098\n",
            "Train Epoch: 382 [1000/1000 (100%)]\tLosses F.softmax: 2.256811 log_softmax: 2.258846\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2981\tAccuracy: 1576.0/10000 (16%)\n",
            "log_softmax: Loss: 2.2990\tAccuracy: 1448.0/10000 (14%)\n",
            "\n",
            "Train Epoch: 383 [0/1000 (0%)]\tLosses F.softmax: 2.319275 log_softmax: 2.319009\n",
            "Train Epoch: 383 [1000/1000 (100%)]\tLosses F.softmax: 2.224520 log_softmax: 2.226657\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2981\tAccuracy: 1568.0/10000 (16%)\n",
            "log_softmax: Loss: 2.2990\tAccuracy: 1440.0/10000 (14%)\n",
            "\n",
            "Train Epoch: 384 [0/1000 (0%)]\tLosses F.softmax: 2.251447 log_softmax: 2.252776\n",
            "Train Epoch: 384 [1000/1000 (100%)]\tLosses F.softmax: 2.275323 log_softmax: 2.276366\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2981\tAccuracy: 1405.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2990\tAccuracy: 1254.0/10000 (13%)\n",
            "\n",
            "Train Epoch: 385 [0/1000 (0%)]\tLosses F.softmax: 2.321209 log_softmax: 2.319990\n",
            "Train Epoch: 385 [1000/1000 (100%)]\tLosses F.softmax: 2.257351 log_softmax: 2.258162\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2981\tAccuracy: 1334.0/10000 (13%)\n",
            "log_softmax: Loss: 2.2990\tAccuracy: 1169.0/10000 (12%)\n",
            "\n",
            "Train Epoch: 386 [0/1000 (0%)]\tLosses F.softmax: 2.310588 log_softmax: 2.310725\n",
            "Train Epoch: 386 [1000/1000 (100%)]\tLosses F.softmax: 2.341052 log_softmax: 2.342938\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2980\tAccuracy: 1319.0/10000 (13%)\n",
            "log_softmax: Loss: 2.2989\tAccuracy: 1156.0/10000 (12%)\n",
            "\n",
            "Train Epoch: 387 [0/1000 (0%)]\tLosses F.softmax: 2.239114 log_softmax: 2.240201\n",
            "Train Epoch: 387 [1000/1000 (100%)]\tLosses F.softmax: 2.289334 log_softmax: 2.290827\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2980\tAccuracy: 1298.0/10000 (13%)\n",
            "log_softmax: Loss: 2.2989\tAccuracy: 1142.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 388 [0/1000 (0%)]\tLosses F.softmax: 2.279643 log_softmax: 2.279725\n",
            "Train Epoch: 388 [1000/1000 (100%)]\tLosses F.softmax: 2.337099 log_softmax: 2.336274\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2980\tAccuracy: 1418.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2989\tAccuracy: 1271.0/10000 (13%)\n",
            "\n",
            "Train Epoch: 389 [0/1000 (0%)]\tLosses F.softmax: 2.232713 log_softmax: 2.234825\n",
            "Train Epoch: 389 [1000/1000 (100%)]\tLosses F.softmax: 2.324143 log_softmax: 2.324796\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2980\tAccuracy: 1360.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2989\tAccuracy: 1194.0/10000 (12%)\n",
            "\n",
            "Train Epoch: 390 [0/1000 (0%)]\tLosses F.softmax: 2.296587 log_softmax: 2.297974\n",
            "Train Epoch: 390 [1000/1000 (100%)]\tLosses F.softmax: 2.243776 log_softmax: 2.244792\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2980\tAccuracy: 1365.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2989\tAccuracy: 1201.0/10000 (12%)\n",
            "\n",
            "Train Epoch: 391 [0/1000 (0%)]\tLosses F.softmax: 2.237990 log_softmax: 2.237911\n",
            "Train Epoch: 391 [1000/1000 (100%)]\tLosses F.softmax: 2.253616 log_softmax: 2.254858\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2980\tAccuracy: 1238.0/10000 (12%)\n",
            "log_softmax: Loss: 2.2989\tAccuracy: 1097.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 392 [0/1000 (0%)]\tLosses F.softmax: 2.274289 log_softmax: 2.274896\n",
            "Train Epoch: 392 [1000/1000 (100%)]\tLosses F.softmax: 2.332745 log_softmax: 2.332762\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2979\tAccuracy: 1349.0/10000 (13%)\n",
            "log_softmax: Loss: 2.2989\tAccuracy: 1180.0/10000 (12%)\n",
            "\n",
            "Train Epoch: 393 [0/1000 (0%)]\tLosses F.softmax: 2.313349 log_softmax: 2.314813\n",
            "Train Epoch: 393 [1000/1000 (100%)]\tLosses F.softmax: 2.285279 log_softmax: 2.284157\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2979\tAccuracy: 1393.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2989\tAccuracy: 1237.0/10000 (12%)\n",
            "\n",
            "Train Epoch: 394 [0/1000 (0%)]\tLosses F.softmax: 2.236535 log_softmax: 2.238492\n",
            "Train Epoch: 394 [1000/1000 (100%)]\tLosses F.softmax: 2.270814 log_softmax: 2.271520\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2979\tAccuracy: 1542.0/10000 (15%)\n",
            "log_softmax: Loss: 2.2988\tAccuracy: 1412.0/10000 (14%)\n",
            "\n",
            "Train Epoch: 395 [0/1000 (0%)]\tLosses F.softmax: 2.349265 log_softmax: 2.350662\n",
            "Train Epoch: 395 [1000/1000 (100%)]\tLosses F.softmax: 2.312559 log_softmax: 2.312767\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2979\tAccuracy: 1432.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2988\tAccuracy: 1278.0/10000 (13%)\n",
            "\n",
            "Train Epoch: 396 [0/1000 (0%)]\tLosses F.softmax: 2.315308 log_softmax: 2.316305\n",
            "Train Epoch: 396 [1000/1000 (100%)]\tLosses F.softmax: 2.283305 log_softmax: 2.285988\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2979\tAccuracy: 1579.0/10000 (16%)\n",
            "log_softmax: Loss: 2.2988\tAccuracy: 1454.0/10000 (15%)\n",
            "\n",
            "Train Epoch: 397 [0/1000 (0%)]\tLosses F.softmax: 2.288136 log_softmax: 2.291709\n",
            "Train Epoch: 397 [1000/1000 (100%)]\tLosses F.softmax: 2.314479 log_softmax: 2.315755\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2979\tAccuracy: 1532.0/10000 (15%)\n",
            "log_softmax: Loss: 2.2988\tAccuracy: 1394.0/10000 (14%)\n",
            "\n",
            "Train Epoch: 398 [0/1000 (0%)]\tLosses F.softmax: 2.287714 log_softmax: 2.290057\n",
            "Train Epoch: 398 [1000/1000 (100%)]\tLosses F.softmax: 2.276683 log_softmax: 2.279635\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2979\tAccuracy: 1549.0/10000 (15%)\n",
            "log_softmax: Loss: 2.2988\tAccuracy: 1425.0/10000 (14%)\n",
            "\n",
            "Train Epoch: 399 [0/1000 (0%)]\tLosses F.softmax: 2.337112 log_softmax: 2.337945\n",
            "Train Epoch: 399 [1000/1000 (100%)]\tLosses F.softmax: 2.270513 log_softmax: 2.271945\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2979\tAccuracy: 1572.0/10000 (16%)\n",
            "log_softmax: Loss: 2.2988\tAccuracy: 1440.0/10000 (14%)\n",
            "\n",
            "Train Epoch: 400 [0/1000 (0%)]\tLosses F.softmax: 2.356486 log_softmax: 2.357929\n",
            "Train Epoch: 400 [1000/1000 (100%)]\tLosses F.softmax: 2.298983 log_softmax: 2.299543\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2978\tAccuracy: 1599.0/10000 (16%)\n",
            "log_softmax: Loss: 2.2988\tAccuracy: 1483.0/10000 (15%)\n",
            "\n",
            "Train Epoch: 401 [0/1000 (0%)]\tLosses F.softmax: 2.319396 log_softmax: 2.318518\n",
            "Train Epoch: 401 [1000/1000 (100%)]\tLosses F.softmax: 2.320941 log_softmax: 2.319254\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2978\tAccuracy: 1517.0/10000 (15%)\n",
            "log_softmax: Loss: 2.2988\tAccuracy: 1379.0/10000 (14%)\n",
            "\n",
            "Train Epoch: 402 [0/1000 (0%)]\tLosses F.softmax: 2.313922 log_softmax: 2.314077\n",
            "Train Epoch: 402 [1000/1000 (100%)]\tLosses F.softmax: 2.272561 log_softmax: 2.273949\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2978\tAccuracy: 1483.0/10000 (15%)\n",
            "log_softmax: Loss: 2.2987\tAccuracy: 1350.0/10000 (14%)\n",
            "\n",
            "Train Epoch: 403 [0/1000 (0%)]\tLosses F.softmax: 2.348574 log_softmax: 2.348787\n",
            "Train Epoch: 403 [1000/1000 (100%)]\tLosses F.softmax: 2.355275 log_softmax: 2.356014\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2978\tAccuracy: 1354.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2987\tAccuracy: 1174.0/10000 (12%)\n",
            "\n",
            "Train Epoch: 404 [0/1000 (0%)]\tLosses F.softmax: 2.301874 log_softmax: 2.303067\n",
            "Train Epoch: 404 [1000/1000 (100%)]\tLosses F.softmax: 2.280370 log_softmax: 2.281264\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2977\tAccuracy: 1571.0/10000 (16%)\n",
            "log_softmax: Loss: 2.2987\tAccuracy: 1434.0/10000 (14%)\n",
            "\n",
            "Train Epoch: 405 [0/1000 (0%)]\tLosses F.softmax: 2.312194 log_softmax: 2.313341\n",
            "Train Epoch: 405 [1000/1000 (100%)]\tLosses F.softmax: 2.325708 log_softmax: 2.325728\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2977\tAccuracy: 1442.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2987\tAccuracy: 1286.0/10000 (13%)\n",
            "\n",
            "Train Epoch: 406 [0/1000 (0%)]\tLosses F.softmax: 2.335919 log_softmax: 2.335402\n",
            "Train Epoch: 406 [1000/1000 (100%)]\tLosses F.softmax: 2.346438 log_softmax: 2.348665\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2977\tAccuracy: 1425.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2987\tAccuracy: 1260.0/10000 (13%)\n",
            "\n",
            "Train Epoch: 407 [0/1000 (0%)]\tLosses F.softmax: 2.314551 log_softmax: 2.315574\n",
            "Train Epoch: 407 [1000/1000 (100%)]\tLosses F.softmax: 2.203351 log_softmax: 2.205071\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2977\tAccuracy: 1429.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2987\tAccuracy: 1265.0/10000 (13%)\n",
            "\n",
            "Train Epoch: 408 [0/1000 (0%)]\tLosses F.softmax: 2.284765 log_softmax: 2.285807\n",
            "Train Epoch: 408 [1000/1000 (100%)]\tLosses F.softmax: 2.255081 log_softmax: 2.255717\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2977\tAccuracy: 1522.0/10000 (15%)\n",
            "log_softmax: Loss: 2.2987\tAccuracy: 1381.0/10000 (14%)\n",
            "\n",
            "Train Epoch: 409 [0/1000 (0%)]\tLosses F.softmax: 2.256008 log_softmax: 2.257055\n",
            "Train Epoch: 409 [1000/1000 (100%)]\tLosses F.softmax: 2.293949 log_softmax: 2.296816\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2976\tAccuracy: 1621.0/10000 (16%)\n",
            "log_softmax: Loss: 2.2986\tAccuracy: 1498.0/10000 (15%)\n",
            "\n",
            "Train Epoch: 410 [0/1000 (0%)]\tLosses F.softmax: 2.329820 log_softmax: 2.331800\n",
            "Train Epoch: 410 [1000/1000 (100%)]\tLosses F.softmax: 2.203011 log_softmax: 2.203927\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2976\tAccuracy: 1669.0/10000 (17%)\n",
            "log_softmax: Loss: 2.2986\tAccuracy: 1581.0/10000 (16%)\n",
            "\n",
            "Train Epoch: 411 [0/1000 (0%)]\tLosses F.softmax: 2.295341 log_softmax: 2.297060\n",
            "Train Epoch: 411 [1000/1000 (100%)]\tLosses F.softmax: 2.277171 log_softmax: 2.279084\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2976\tAccuracy: 1446.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2986\tAccuracy: 1298.0/10000 (13%)\n",
            "\n",
            "Train Epoch: 412 [0/1000 (0%)]\tLosses F.softmax: 2.343339 log_softmax: 2.345701\n",
            "Train Epoch: 412 [1000/1000 (100%)]\tLosses F.softmax: 2.305673 log_softmax: 2.308458\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2976\tAccuracy: 1387.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2986\tAccuracy: 1218.0/10000 (12%)\n",
            "\n",
            "Train Epoch: 413 [0/1000 (0%)]\tLosses F.softmax: 2.243469 log_softmax: 2.246451\n",
            "Train Epoch: 413 [1000/1000 (100%)]\tLosses F.softmax: 2.314202 log_softmax: 2.317650\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2976\tAccuracy: 1416.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2986\tAccuracy: 1241.0/10000 (12%)\n",
            "\n",
            "Train Epoch: 414 [0/1000 (0%)]\tLosses F.softmax: 2.265188 log_softmax: 2.266862\n",
            "Train Epoch: 414 [1000/1000 (100%)]\tLosses F.softmax: 2.282277 log_softmax: 2.283066\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2976\tAccuracy: 1373.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2986\tAccuracy: 1192.0/10000 (12%)\n",
            "\n",
            "Train Epoch: 415 [0/1000 (0%)]\tLosses F.softmax: 2.238905 log_softmax: 2.239668\n",
            "Train Epoch: 415 [1000/1000 (100%)]\tLosses F.softmax: 2.296614 log_softmax: 2.299358\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2976\tAccuracy: 1376.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2986\tAccuracy: 1192.0/10000 (12%)\n",
            "\n",
            "Train Epoch: 416 [0/1000 (0%)]\tLosses F.softmax: 2.143529 log_softmax: 2.146557\n",
            "Train Epoch: 416 [1000/1000 (100%)]\tLosses F.softmax: 2.288515 log_softmax: 2.288924\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2976\tAccuracy: 1416.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2986\tAccuracy: 1239.0/10000 (12%)\n",
            "\n",
            "Train Epoch: 417 [0/1000 (0%)]\tLosses F.softmax: 2.217878 log_softmax: 2.218444\n",
            "Train Epoch: 417 [1000/1000 (100%)]\tLosses F.softmax: 2.390053 log_softmax: 2.393831\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2975\tAccuracy: 1514.0/10000 (15%)\n",
            "log_softmax: Loss: 2.2986\tAccuracy: 1371.0/10000 (14%)\n",
            "\n",
            "Train Epoch: 418 [0/1000 (0%)]\tLosses F.softmax: 2.332220 log_softmax: 2.334282\n",
            "Train Epoch: 418 [1000/1000 (100%)]\tLosses F.softmax: 2.312223 log_softmax: 2.312018\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2975\tAccuracy: 1461.0/10000 (15%)\n",
            "log_softmax: Loss: 2.2986\tAccuracy: 1308.0/10000 (13%)\n",
            "\n",
            "Train Epoch: 419 [0/1000 (0%)]\tLosses F.softmax: 2.278023 log_softmax: 2.277024\n",
            "Train Epoch: 419 [1000/1000 (100%)]\tLosses F.softmax: 2.266334 log_softmax: 2.271713\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2975\tAccuracy: 1491.0/10000 (15%)\n",
            "log_softmax: Loss: 2.2986\tAccuracy: 1349.0/10000 (13%)\n",
            "\n",
            "Train Epoch: 420 [0/1000 (0%)]\tLosses F.softmax: 2.305214 log_softmax: 2.306885\n",
            "Train Epoch: 420 [1000/1000 (100%)]\tLosses F.softmax: 2.205516 log_softmax: 2.207872\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2975\tAccuracy: 1418.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2986\tAccuracy: 1243.0/10000 (12%)\n",
            "\n",
            "Train Epoch: 421 [0/1000 (0%)]\tLosses F.softmax: 2.239542 log_softmax: 2.241634\n",
            "Train Epoch: 421 [1000/1000 (100%)]\tLosses F.softmax: 2.307669 log_softmax: 2.309689\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2975\tAccuracy: 1442.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2985\tAccuracy: 1275.0/10000 (13%)\n",
            "\n",
            "Train Epoch: 422 [0/1000 (0%)]\tLosses F.softmax: 2.326085 log_softmax: 2.324149\n",
            "Train Epoch: 422 [1000/1000 (100%)]\tLosses F.softmax: 2.193867 log_softmax: 2.195022\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2975\tAccuracy: 1326.0/10000 (13%)\n",
            "log_softmax: Loss: 2.2985\tAccuracy: 1142.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 423 [0/1000 (0%)]\tLosses F.softmax: 2.319815 log_softmax: 2.321630\n",
            "Train Epoch: 423 [1000/1000 (100%)]\tLosses F.softmax: 2.299663 log_softmax: 2.300207\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2975\tAccuracy: 1269.0/10000 (13%)\n",
            "log_softmax: Loss: 2.2985\tAccuracy: 1105.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 424 [0/1000 (0%)]\tLosses F.softmax: 2.291834 log_softmax: 2.291827\n",
            "Train Epoch: 424 [1000/1000 (100%)]\tLosses F.softmax: 2.294943 log_softmax: 2.297592\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2974\tAccuracy: 1386.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2985\tAccuracy: 1204.0/10000 (12%)\n",
            "\n",
            "Train Epoch: 425 [0/1000 (0%)]\tLosses F.softmax: 2.311574 log_softmax: 2.310847\n",
            "Train Epoch: 425 [1000/1000 (100%)]\tLosses F.softmax: 2.294747 log_softmax: 2.295777\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2974\tAccuracy: 1252.0/10000 (13%)\n",
            "log_softmax: Loss: 2.2985\tAccuracy: 1094.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 426 [0/1000 (0%)]\tLosses F.softmax: 2.305919 log_softmax: 2.306802\n",
            "Train Epoch: 426 [1000/1000 (100%)]\tLosses F.softmax: 2.285519 log_softmax: 2.285673\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2974\tAccuracy: 1290.0/10000 (13%)\n",
            "log_softmax: Loss: 2.2985\tAccuracy: 1117.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 427 [0/1000 (0%)]\tLosses F.softmax: 2.289690 log_softmax: 2.287041\n",
            "Train Epoch: 427 [1000/1000 (100%)]\tLosses F.softmax: 2.300251 log_softmax: 2.300745\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2974\tAccuracy: 1352.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2985\tAccuracy: 1163.0/10000 (12%)\n",
            "\n",
            "Train Epoch: 428 [0/1000 (0%)]\tLosses F.softmax: 2.311695 log_softmax: 2.313039\n",
            "Train Epoch: 428 [1000/1000 (100%)]\tLosses F.softmax: 2.355530 log_softmax: 2.355291\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2974\tAccuracy: 1448.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2984\tAccuracy: 1288.0/10000 (13%)\n",
            "\n",
            "Train Epoch: 429 [0/1000 (0%)]\tLosses F.softmax: 2.257193 log_softmax: 2.258828\n",
            "Train Epoch: 429 [1000/1000 (100%)]\tLosses F.softmax: 2.194315 log_softmax: 2.196430\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2974\tAccuracy: 1429.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2984\tAccuracy: 1252.0/10000 (13%)\n",
            "\n",
            "Train Epoch: 430 [0/1000 (0%)]\tLosses F.softmax: 2.297850 log_softmax: 2.298632\n",
            "Train Epoch: 430 [1000/1000 (100%)]\tLosses F.softmax: 2.247686 log_softmax: 2.247550\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2973\tAccuracy: 1483.0/10000 (15%)\n",
            "log_softmax: Loss: 2.2984\tAccuracy: 1329.0/10000 (13%)\n",
            "\n",
            "Train Epoch: 431 [0/1000 (0%)]\tLosses F.softmax: 2.319417 log_softmax: 2.319350\n",
            "Train Epoch: 431 [1000/1000 (100%)]\tLosses F.softmax: 2.314729 log_softmax: 2.314248\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2973\tAccuracy: 1611.0/10000 (16%)\n",
            "log_softmax: Loss: 2.2984\tAccuracy: 1482.0/10000 (15%)\n",
            "\n",
            "Train Epoch: 432 [0/1000 (0%)]\tLosses F.softmax: 2.238187 log_softmax: 2.238670\n",
            "Train Epoch: 432 [1000/1000 (100%)]\tLosses F.softmax: 2.288944 log_softmax: 2.291856\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2973\tAccuracy: 1594.0/10000 (16%)\n",
            "log_softmax: Loss: 2.2984\tAccuracy: 1460.0/10000 (15%)\n",
            "\n",
            "Train Epoch: 433 [0/1000 (0%)]\tLosses F.softmax: 2.279594 log_softmax: 2.278919\n",
            "Train Epoch: 433 [1000/1000 (100%)]\tLosses F.softmax: 2.284732 log_softmax: 2.284796\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2973\tAccuracy: 1614.0/10000 (16%)\n",
            "log_softmax: Loss: 2.2984\tAccuracy: 1484.0/10000 (15%)\n",
            "\n",
            "Train Epoch: 434 [0/1000 (0%)]\tLosses F.softmax: 2.285280 log_softmax: 2.287750\n",
            "Train Epoch: 434 [1000/1000 (100%)]\tLosses F.softmax: 2.278273 log_softmax: 2.278727\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2972\tAccuracy: 1639.0/10000 (16%)\n",
            "log_softmax: Loss: 2.2983\tAccuracy: 1522.0/10000 (15%)\n",
            "\n",
            "Train Epoch: 435 [0/1000 (0%)]\tLosses F.softmax: 2.278868 log_softmax: 2.281036\n",
            "Train Epoch: 435 [1000/1000 (100%)]\tLosses F.softmax: 2.256996 log_softmax: 2.258116\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2972\tAccuracy: 1634.0/10000 (16%)\n",
            "log_softmax: Loss: 2.2983\tAccuracy: 1510.0/10000 (15%)\n",
            "\n",
            "Train Epoch: 436 [0/1000 (0%)]\tLosses F.softmax: 2.264856 log_softmax: 2.262391\n",
            "Train Epoch: 436 [1000/1000 (100%)]\tLosses F.softmax: 2.323457 log_softmax: 2.324161\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2972\tAccuracy: 1676.0/10000 (17%)\n",
            "log_softmax: Loss: 2.2983\tAccuracy: 1578.0/10000 (16%)\n",
            "\n",
            "Train Epoch: 437 [0/1000 (0%)]\tLosses F.softmax: 2.299766 log_softmax: 2.301362\n",
            "Train Epoch: 437 [1000/1000 (100%)]\tLosses F.softmax: 2.244973 log_softmax: 2.245998\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2972\tAccuracy: 1591.0/10000 (16%)\n",
            "log_softmax: Loss: 2.2983\tAccuracy: 1447.0/10000 (14%)\n",
            "\n",
            "Train Epoch: 438 [0/1000 (0%)]\tLosses F.softmax: 2.358095 log_softmax: 2.358644\n",
            "Train Epoch: 438 [1000/1000 (100%)]\tLosses F.softmax: 2.347823 log_softmax: 2.348211\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2972\tAccuracy: 1496.0/10000 (15%)\n",
            "log_softmax: Loss: 2.2983\tAccuracy: 1343.0/10000 (13%)\n",
            "\n",
            "Train Epoch: 439 [0/1000 (0%)]\tLosses F.softmax: 2.230400 log_softmax: 2.232695\n",
            "Train Epoch: 439 [1000/1000 (100%)]\tLosses F.softmax: 2.314028 log_softmax: 2.316923\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2972\tAccuracy: 1424.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2983\tAccuracy: 1239.0/10000 (12%)\n",
            "\n",
            "Train Epoch: 440 [0/1000 (0%)]\tLosses F.softmax: 2.270143 log_softmax: 2.271556\n",
            "Train Epoch: 440 [1000/1000 (100%)]\tLosses F.softmax: 2.308296 log_softmax: 2.309337\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2971\tAccuracy: 1519.0/10000 (15%)\n",
            "log_softmax: Loss: 2.2982\tAccuracy: 1363.0/10000 (14%)\n",
            "\n",
            "Train Epoch: 441 [0/1000 (0%)]\tLosses F.softmax: 2.273266 log_softmax: 2.273626\n",
            "Train Epoch: 441 [1000/1000 (100%)]\tLosses F.softmax: 2.340648 log_softmax: 2.340570\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2971\tAccuracy: 1541.0/10000 (15%)\n",
            "log_softmax: Loss: 2.2982\tAccuracy: 1385.0/10000 (14%)\n",
            "\n",
            "Train Epoch: 442 [0/1000 (0%)]\tLosses F.softmax: 2.300519 log_softmax: 2.302051\n",
            "Train Epoch: 442 [1000/1000 (100%)]\tLosses F.softmax: 2.297698 log_softmax: 2.300346\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2971\tAccuracy: 1573.0/10000 (16%)\n",
            "log_softmax: Loss: 2.2982\tAccuracy: 1422.0/10000 (14%)\n",
            "\n",
            "Train Epoch: 443 [0/1000 (0%)]\tLosses F.softmax: 2.287292 log_softmax: 2.290879\n",
            "Train Epoch: 443 [1000/1000 (100%)]\tLosses F.softmax: 2.270718 log_softmax: 2.273452\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2971\tAccuracy: 1521.0/10000 (15%)\n",
            "log_softmax: Loss: 2.2982\tAccuracy: 1365.0/10000 (14%)\n",
            "\n",
            "Train Epoch: 444 [0/1000 (0%)]\tLosses F.softmax: 2.318574 log_softmax: 2.320095\n",
            "Train Epoch: 444 [1000/1000 (100%)]\tLosses F.softmax: 2.267863 log_softmax: 2.269017\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2970\tAccuracy: 1547.0/10000 (15%)\n",
            "log_softmax: Loss: 2.2982\tAccuracy: 1388.0/10000 (14%)\n",
            "\n",
            "Train Epoch: 445 [0/1000 (0%)]\tLosses F.softmax: 2.235763 log_softmax: 2.237972\n",
            "Train Epoch: 445 [1000/1000 (100%)]\tLosses F.softmax: 2.320229 log_softmax: 2.319442\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2970\tAccuracy: 1594.0/10000 (16%)\n",
            "log_softmax: Loss: 2.2981\tAccuracy: 1450.0/10000 (14%)\n",
            "\n",
            "Train Epoch: 446 [0/1000 (0%)]\tLosses F.softmax: 2.280558 log_softmax: 2.281530\n",
            "Train Epoch: 446 [1000/1000 (100%)]\tLosses F.softmax: 2.288768 log_softmax: 2.287338\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2970\tAccuracy: 1477.0/10000 (15%)\n",
            "log_softmax: Loss: 2.2981\tAccuracy: 1311.0/10000 (13%)\n",
            "\n",
            "Train Epoch: 447 [0/1000 (0%)]\tLosses F.softmax: 2.253788 log_softmax: 2.255126\n",
            "Train Epoch: 447 [1000/1000 (100%)]\tLosses F.softmax: 2.310245 log_softmax: 2.312653\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2970\tAccuracy: 1464.0/10000 (15%)\n",
            "log_softmax: Loss: 2.2981\tAccuracy: 1294.0/10000 (13%)\n",
            "\n",
            "Train Epoch: 448 [0/1000 (0%)]\tLosses F.softmax: 2.166302 log_softmax: 2.170003\n",
            "Train Epoch: 448 [1000/1000 (100%)]\tLosses F.softmax: 2.267808 log_softmax: 2.268887\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2969\tAccuracy: 1383.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2981\tAccuracy: 1179.0/10000 (12%)\n",
            "\n",
            "Train Epoch: 449 [0/1000 (0%)]\tLosses F.softmax: 2.264754 log_softmax: 2.266562\n",
            "Train Epoch: 449 [1000/1000 (100%)]\tLosses F.softmax: 2.349657 log_softmax: 2.351242\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2969\tAccuracy: 1412.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2981\tAccuracy: 1207.0/10000 (12%)\n",
            "\n",
            "Train Epoch: 450 [0/1000 (0%)]\tLosses F.softmax: 2.310799 log_softmax: 2.313193\n",
            "Train Epoch: 450 [1000/1000 (100%)]\tLosses F.softmax: 2.259621 log_softmax: 2.257931\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2969\tAccuracy: 1390.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2981\tAccuracy: 1189.0/10000 (12%)\n",
            "\n",
            "Train Epoch: 451 [0/1000 (0%)]\tLosses F.softmax: 2.370161 log_softmax: 2.372239\n",
            "Train Epoch: 451 [1000/1000 (100%)]\tLosses F.softmax: 2.229870 log_softmax: 2.232612\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2969\tAccuracy: 1415.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2981\tAccuracy: 1215.0/10000 (12%)\n",
            "\n",
            "Train Epoch: 452 [0/1000 (0%)]\tLosses F.softmax: 2.253076 log_softmax: 2.253614\n",
            "Train Epoch: 452 [1000/1000 (100%)]\tLosses F.softmax: 2.204266 log_softmax: 2.207237\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2969\tAccuracy: 1277.0/10000 (13%)\n",
            "log_softmax: Loss: 2.2981\tAccuracy: 1102.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 453 [0/1000 (0%)]\tLosses F.softmax: 2.349917 log_softmax: 2.349921\n",
            "Train Epoch: 453 [1000/1000 (100%)]\tLosses F.softmax: 2.236600 log_softmax: 2.239764\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2969\tAccuracy: 1396.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2980\tAccuracy: 1193.0/10000 (12%)\n",
            "\n",
            "Train Epoch: 454 [0/1000 (0%)]\tLosses F.softmax: 2.327565 log_softmax: 2.328483\n",
            "Train Epoch: 454 [1000/1000 (100%)]\tLosses F.softmax: 2.294830 log_softmax: 2.299402\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2968\tAccuracy: 1498.0/10000 (15%)\n",
            "log_softmax: Loss: 2.2980\tAccuracy: 1331.0/10000 (13%)\n",
            "\n",
            "Train Epoch: 455 [0/1000 (0%)]\tLosses F.softmax: 2.288308 log_softmax: 2.286567\n",
            "Train Epoch: 455 [1000/1000 (100%)]\tLosses F.softmax: 2.288339 log_softmax: 2.288316\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2968\tAccuracy: 1444.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2980\tAccuracy: 1256.0/10000 (13%)\n",
            "\n",
            "Train Epoch: 456 [0/1000 (0%)]\tLosses F.softmax: 2.188636 log_softmax: 2.190043\n",
            "Train Epoch: 456 [1000/1000 (100%)]\tLosses F.softmax: 2.330118 log_softmax: 2.328150\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2968\tAccuracy: 1491.0/10000 (15%)\n",
            "log_softmax: Loss: 2.2980\tAccuracy: 1320.0/10000 (13%)\n",
            "\n",
            "Train Epoch: 457 [0/1000 (0%)]\tLosses F.softmax: 2.337447 log_softmax: 2.339296\n",
            "Train Epoch: 457 [1000/1000 (100%)]\tLosses F.softmax: 2.267707 log_softmax: 2.268391\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2968\tAccuracy: 1574.0/10000 (16%)\n",
            "log_softmax: Loss: 2.2980\tAccuracy: 1418.0/10000 (14%)\n",
            "\n",
            "Train Epoch: 458 [0/1000 (0%)]\tLosses F.softmax: 2.345276 log_softmax: 2.345536\n",
            "Train Epoch: 458 [1000/1000 (100%)]\tLosses F.softmax: 2.311683 log_softmax: 2.312227\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2968\tAccuracy: 1575.0/10000 (16%)\n",
            "log_softmax: Loss: 2.2980\tAccuracy: 1419.0/10000 (14%)\n",
            "\n",
            "Train Epoch: 459 [0/1000 (0%)]\tLosses F.softmax: 2.232651 log_softmax: 2.233035\n",
            "Train Epoch: 459 [1000/1000 (100%)]\tLosses F.softmax: 2.286387 log_softmax: 2.288553\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2967\tAccuracy: 1505.0/10000 (15%)\n",
            "log_softmax: Loss: 2.2980\tAccuracy: 1340.0/10000 (13%)\n",
            "\n",
            "Train Epoch: 460 [0/1000 (0%)]\tLosses F.softmax: 2.341271 log_softmax: 2.342738\n",
            "Train Epoch: 460 [1000/1000 (100%)]\tLosses F.softmax: 2.330086 log_softmax: 2.331449\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2967\tAccuracy: 1576.0/10000 (16%)\n",
            "log_softmax: Loss: 2.2979\tAccuracy: 1419.0/10000 (14%)\n",
            "\n",
            "Train Epoch: 461 [0/1000 (0%)]\tLosses F.softmax: 2.327524 log_softmax: 2.329695\n",
            "Train Epoch: 461 [1000/1000 (100%)]\tLosses F.softmax: 2.265575 log_softmax: 2.266456\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2967\tAccuracy: 1484.0/10000 (15%)\n",
            "log_softmax: Loss: 2.2979\tAccuracy: 1310.0/10000 (13%)\n",
            "\n",
            "Train Epoch: 462 [0/1000 (0%)]\tLosses F.softmax: 2.339307 log_softmax: 2.337826\n",
            "Train Epoch: 462 [1000/1000 (100%)]\tLosses F.softmax: 2.257042 log_softmax: 2.259194\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2967\tAccuracy: 1645.0/10000 (16%)\n",
            "log_softmax: Loss: 2.2979\tAccuracy: 1513.0/10000 (15%)\n",
            "\n",
            "Train Epoch: 463 [0/1000 (0%)]\tLosses F.softmax: 2.173317 log_softmax: 2.173219\n",
            "Train Epoch: 463 [1000/1000 (100%)]\tLosses F.softmax: 2.301099 log_softmax: 2.302936\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2967\tAccuracy: 1590.0/10000 (16%)\n",
            "log_softmax: Loss: 2.2979\tAccuracy: 1434.0/10000 (14%)\n",
            "\n",
            "Train Epoch: 464 [0/1000 (0%)]\tLosses F.softmax: 2.317003 log_softmax: 2.319323\n",
            "Train Epoch: 464 [1000/1000 (100%)]\tLosses F.softmax: 2.312681 log_softmax: 2.313339\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2967\tAccuracy: 1616.0/10000 (16%)\n",
            "log_softmax: Loss: 2.2979\tAccuracy: 1476.0/10000 (15%)\n",
            "\n",
            "Train Epoch: 465 [0/1000 (0%)]\tLosses F.softmax: 2.262662 log_softmax: 2.263638\n",
            "Train Epoch: 465 [1000/1000 (100%)]\tLosses F.softmax: 2.272360 log_softmax: 2.273287\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2966\tAccuracy: 1600.0/10000 (16%)\n",
            "log_softmax: Loss: 2.2979\tAccuracy: 1454.0/10000 (15%)\n",
            "\n",
            "Train Epoch: 466 [0/1000 (0%)]\tLosses F.softmax: 2.262083 log_softmax: 2.266186\n",
            "Train Epoch: 466 [1000/1000 (100%)]\tLosses F.softmax: 2.266623 log_softmax: 2.265894\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2966\tAccuracy: 1688.0/10000 (17%)\n",
            "log_softmax: Loss: 2.2978\tAccuracy: 1574.0/10000 (16%)\n",
            "\n",
            "Train Epoch: 467 [0/1000 (0%)]\tLosses F.softmax: 2.289206 log_softmax: 2.286861\n",
            "Train Epoch: 467 [1000/1000 (100%)]\tLosses F.softmax: 2.324317 log_softmax: 2.322227\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2966\tAccuracy: 1736.0/10000 (17%)\n",
            "log_softmax: Loss: 2.2978\tAccuracy: 1642.0/10000 (16%)\n",
            "\n",
            "Train Epoch: 468 [0/1000 (0%)]\tLosses F.softmax: 2.313317 log_softmax: 2.314643\n",
            "Train Epoch: 468 [1000/1000 (100%)]\tLosses F.softmax: 2.322976 log_softmax: 2.322767\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2966\tAccuracy: 1663.0/10000 (17%)\n",
            "log_softmax: Loss: 2.2978\tAccuracy: 1527.0/10000 (15%)\n",
            "\n",
            "Train Epoch: 469 [0/1000 (0%)]\tLosses F.softmax: 2.323313 log_softmax: 2.321515\n",
            "Train Epoch: 469 [1000/1000 (100%)]\tLosses F.softmax: 2.309459 log_softmax: 2.308265\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2965\tAccuracy: 1657.0/10000 (17%)\n",
            "log_softmax: Loss: 2.2978\tAccuracy: 1515.0/10000 (15%)\n",
            "\n",
            "Train Epoch: 470 [0/1000 (0%)]\tLosses F.softmax: 2.358960 log_softmax: 2.358486\n",
            "Train Epoch: 470 [1000/1000 (100%)]\tLosses F.softmax: 2.215161 log_softmax: 2.218084\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2965\tAccuracy: 1566.0/10000 (16%)\n",
            "log_softmax: Loss: 2.2978\tAccuracy: 1396.0/10000 (14%)\n",
            "\n",
            "Train Epoch: 471 [0/1000 (0%)]\tLosses F.softmax: 2.349434 log_softmax: 2.352684\n",
            "Train Epoch: 471 [1000/1000 (100%)]\tLosses F.softmax: 2.240831 log_softmax: 2.240384\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2965\tAccuracy: 1684.0/10000 (17%)\n",
            "log_softmax: Loss: 2.2978\tAccuracy: 1564.0/10000 (16%)\n",
            "\n",
            "Train Epoch: 472 [0/1000 (0%)]\tLosses F.softmax: 2.182427 log_softmax: 2.185193\n",
            "Train Epoch: 472 [1000/1000 (100%)]\tLosses F.softmax: 2.258072 log_softmax: 2.259650\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2965\tAccuracy: 1750.0/10000 (18%)\n",
            "log_softmax: Loss: 2.2978\tAccuracy: 1685.0/10000 (17%)\n",
            "\n",
            "Train Epoch: 473 [0/1000 (0%)]\tLosses F.softmax: 2.158633 log_softmax: 2.161503\n",
            "Train Epoch: 473 [1000/1000 (100%)]\tLosses F.softmax: 2.246569 log_softmax: 2.248886\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2965\tAccuracy: 1707.0/10000 (17%)\n",
            "log_softmax: Loss: 2.2978\tAccuracy: 1604.0/10000 (16%)\n",
            "\n",
            "Train Epoch: 474 [0/1000 (0%)]\tLosses F.softmax: 2.322716 log_softmax: 2.326183\n",
            "Train Epoch: 474 [1000/1000 (100%)]\tLosses F.softmax: 2.242108 log_softmax: 2.244573\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2965\tAccuracy: 1607.0/10000 (16%)\n",
            "log_softmax: Loss: 2.2977\tAccuracy: 1455.0/10000 (15%)\n",
            "\n",
            "Train Epoch: 475 [0/1000 (0%)]\tLosses F.softmax: 2.310249 log_softmax: 2.314670\n",
            "Train Epoch: 475 [1000/1000 (100%)]\tLosses F.softmax: 2.318810 log_softmax: 2.321016\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2965\tAccuracy: 1651.0/10000 (17%)\n",
            "log_softmax: Loss: 2.2977\tAccuracy: 1505.0/10000 (15%)\n",
            "\n",
            "Train Epoch: 476 [0/1000 (0%)]\tLosses F.softmax: 2.280877 log_softmax: 2.287151\n",
            "Train Epoch: 476 [1000/1000 (100%)]\tLosses F.softmax: 2.241602 log_softmax: 2.243708\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2965\tAccuracy: 1577.0/10000 (16%)\n",
            "log_softmax: Loss: 2.2978\tAccuracy: 1408.0/10000 (14%)\n",
            "\n",
            "Train Epoch: 477 [0/1000 (0%)]\tLosses F.softmax: 2.297205 log_softmax: 2.299559\n",
            "Train Epoch: 477 [1000/1000 (100%)]\tLosses F.softmax: 2.289374 log_softmax: 2.290985\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2964\tAccuracy: 1475.0/10000 (15%)\n",
            "log_softmax: Loss: 2.2977\tAccuracy: 1278.0/10000 (13%)\n",
            "\n",
            "Train Epoch: 478 [0/1000 (0%)]\tLosses F.softmax: 2.340282 log_softmax: 2.340545\n",
            "Train Epoch: 478 [1000/1000 (100%)]\tLosses F.softmax: 2.265882 log_softmax: 2.265077\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2964\tAccuracy: 1599.0/10000 (16%)\n",
            "log_softmax: Loss: 2.2977\tAccuracy: 1438.0/10000 (14%)\n",
            "\n",
            "Train Epoch: 479 [0/1000 (0%)]\tLosses F.softmax: 2.208247 log_softmax: 2.211173\n",
            "Train Epoch: 479 [1000/1000 (100%)]\tLosses F.softmax: 2.287176 log_softmax: 2.288883\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2964\tAccuracy: 1617.0/10000 (16%)\n",
            "log_softmax: Loss: 2.2977\tAccuracy: 1464.0/10000 (15%)\n",
            "\n",
            "Train Epoch: 480 [0/1000 (0%)]\tLosses F.softmax: 2.317637 log_softmax: 2.318372\n",
            "Train Epoch: 480 [1000/1000 (100%)]\tLosses F.softmax: 2.297088 log_softmax: 2.296597\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2963\tAccuracy: 1726.0/10000 (17%)\n",
            "log_softmax: Loss: 2.2977\tAccuracy: 1623.0/10000 (16%)\n",
            "\n",
            "Train Epoch: 481 [0/1000 (0%)]\tLosses F.softmax: 2.293344 log_softmax: 2.296109\n",
            "Train Epoch: 481 [1000/1000 (100%)]\tLosses F.softmax: 2.338148 log_softmax: 2.345061\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2963\tAccuracy: 1754.0/10000 (18%)\n",
            "log_softmax: Loss: 2.2976\tAccuracy: 1686.0/10000 (17%)\n",
            "\n",
            "Train Epoch: 482 [0/1000 (0%)]\tLosses F.softmax: 2.270525 log_softmax: 2.272618\n",
            "Train Epoch: 482 [1000/1000 (100%)]\tLosses F.softmax: 2.251098 log_softmax: 2.251694\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2963\tAccuracy: 1795.0/10000 (18%)\n",
            "log_softmax: Loss: 2.2976\tAccuracy: 1744.0/10000 (17%)\n",
            "\n",
            "Train Epoch: 483 [0/1000 (0%)]\tLosses F.softmax: 2.336514 log_softmax: 2.340858\n",
            "Train Epoch: 483 [1000/1000 (100%)]\tLosses F.softmax: 2.241853 log_softmax: 2.244334\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2963\tAccuracy: 1794.0/10000 (18%)\n",
            "log_softmax: Loss: 2.2976\tAccuracy: 1742.0/10000 (17%)\n",
            "\n",
            "Train Epoch: 484 [0/1000 (0%)]\tLosses F.softmax: 2.380517 log_softmax: 2.382251\n",
            "Train Epoch: 484 [1000/1000 (100%)]\tLosses F.softmax: 2.321780 log_softmax: 2.319544\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2962\tAccuracy: 1738.0/10000 (17%)\n",
            "log_softmax: Loss: 2.2976\tAccuracy: 1653.0/10000 (17%)\n",
            "\n",
            "Train Epoch: 485 [0/1000 (0%)]\tLosses F.softmax: 2.372059 log_softmax: 2.374775\n",
            "Train Epoch: 485 [1000/1000 (100%)]\tLosses F.softmax: 2.319274 log_softmax: 2.322139\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2962\tAccuracy: 1738.0/10000 (17%)\n",
            "log_softmax: Loss: 2.2976\tAccuracy: 1641.0/10000 (16%)\n",
            "\n",
            "Train Epoch: 486 [0/1000 (0%)]\tLosses F.softmax: 2.318677 log_softmax: 2.322667\n",
            "Train Epoch: 486 [1000/1000 (100%)]\tLosses F.softmax: 2.291906 log_softmax: 2.294749\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2962\tAccuracy: 1736.0/10000 (17%)\n",
            "log_softmax: Loss: 2.2976\tAccuracy: 1634.0/10000 (16%)\n",
            "\n",
            "Train Epoch: 487 [0/1000 (0%)]\tLosses F.softmax: 2.254874 log_softmax: 2.255197\n",
            "Train Epoch: 487 [1000/1000 (100%)]\tLosses F.softmax: 2.251082 log_softmax: 2.252931\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2962\tAccuracy: 1695.0/10000 (17%)\n",
            "log_softmax: Loss: 2.2976\tAccuracy: 1569.0/10000 (16%)\n",
            "\n",
            "Train Epoch: 488 [0/1000 (0%)]\tLosses F.softmax: 2.351876 log_softmax: 2.352111\n",
            "Train Epoch: 488 [1000/1000 (100%)]\tLosses F.softmax: 2.283464 log_softmax: 2.285794\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2962\tAccuracy: 1717.0/10000 (17%)\n",
            "log_softmax: Loss: 2.2975\tAccuracy: 1611.0/10000 (16%)\n",
            "\n",
            "Train Epoch: 489 [0/1000 (0%)]\tLosses F.softmax: 2.308498 log_softmax: 2.310533\n",
            "Train Epoch: 489 [1000/1000 (100%)]\tLosses F.softmax: 2.287245 log_softmax: 2.288593\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2961\tAccuracy: 1710.0/10000 (17%)\n",
            "log_softmax: Loss: 2.2975\tAccuracy: 1596.0/10000 (16%)\n",
            "\n",
            "Train Epoch: 490 [0/1000 (0%)]\tLosses F.softmax: 2.322834 log_softmax: 2.321536\n",
            "Train Epoch: 490 [1000/1000 (100%)]\tLosses F.softmax: 2.375073 log_softmax: 2.375928\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2961\tAccuracy: 1635.0/10000 (16%)\n",
            "log_softmax: Loss: 2.2975\tAccuracy: 1482.0/10000 (15%)\n",
            "\n",
            "Train Epoch: 491 [0/1000 (0%)]\tLosses F.softmax: 2.353014 log_softmax: 2.354837\n",
            "Train Epoch: 491 [1000/1000 (100%)]\tLosses F.softmax: 2.316854 log_softmax: 2.318261\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2961\tAccuracy: 1710.0/10000 (17%)\n",
            "log_softmax: Loss: 2.2975\tAccuracy: 1596.0/10000 (16%)\n",
            "\n",
            "Train Epoch: 492 [0/1000 (0%)]\tLosses F.softmax: 2.264227 log_softmax: 2.264675\n",
            "Train Epoch: 492 [1000/1000 (100%)]\tLosses F.softmax: 2.334576 log_softmax: 2.336143\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2961\tAccuracy: 1738.0/10000 (17%)\n",
            "log_softmax: Loss: 2.2975\tAccuracy: 1635.0/10000 (16%)\n",
            "\n",
            "Train Epoch: 493 [0/1000 (0%)]\tLosses F.softmax: 2.369406 log_softmax: 2.369939\n",
            "Train Epoch: 493 [1000/1000 (100%)]\tLosses F.softmax: 2.258765 log_softmax: 2.260698\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2961\tAccuracy: 1739.0/10000 (17%)\n",
            "log_softmax: Loss: 2.2974\tAccuracy: 1642.0/10000 (16%)\n",
            "\n",
            "Train Epoch: 494 [0/1000 (0%)]\tLosses F.softmax: 2.340434 log_softmax: 2.337985\n",
            "Train Epoch: 494 [1000/1000 (100%)]\tLosses F.softmax: 2.288518 log_softmax: 2.292134\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2960\tAccuracy: 1712.0/10000 (17%)\n",
            "log_softmax: Loss: 2.2974\tAccuracy: 1607.0/10000 (16%)\n",
            "\n",
            "Train Epoch: 495 [0/1000 (0%)]\tLosses F.softmax: 2.310188 log_softmax: 2.311699\n",
            "Train Epoch: 495 [1000/1000 (100%)]\tLosses F.softmax: 2.336251 log_softmax: 2.337189\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2960\tAccuracy: 1739.0/10000 (17%)\n",
            "log_softmax: Loss: 2.2974\tAccuracy: 1643.0/10000 (16%)\n",
            "\n",
            "Train Epoch: 496 [0/1000 (0%)]\tLosses F.softmax: 2.290154 log_softmax: 2.290046\n",
            "Train Epoch: 496 [1000/1000 (100%)]\tLosses F.softmax: 2.232292 log_softmax: 2.232900\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2960\tAccuracy: 1736.0/10000 (17%)\n",
            "log_softmax: Loss: 2.2974\tAccuracy: 1631.0/10000 (16%)\n",
            "\n",
            "Train Epoch: 497 [0/1000 (0%)]\tLosses F.softmax: 2.290973 log_softmax: 2.292270\n",
            "Train Epoch: 497 [1000/1000 (100%)]\tLosses F.softmax: 2.282963 log_softmax: 2.287560\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2960\tAccuracy: 1705.0/10000 (17%)\n",
            "log_softmax: Loss: 2.2974\tAccuracy: 1576.0/10000 (16%)\n",
            "\n",
            "Train Epoch: 498 [0/1000 (0%)]\tLosses F.softmax: 2.294336 log_softmax: 2.294094\n",
            "Train Epoch: 498 [1000/1000 (100%)]\tLosses F.softmax: 2.349447 log_softmax: 2.348232\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2960\tAccuracy: 1714.0/10000 (17%)\n",
            "log_softmax: Loss: 2.2974\tAccuracy: 1606.0/10000 (16%)\n",
            "\n",
            "Train Epoch: 499 [0/1000 (0%)]\tLosses F.softmax: 2.295667 log_softmax: 2.296108\n",
            "Train Epoch: 499 [1000/1000 (100%)]\tLosses F.softmax: 2.244971 log_softmax: 2.246403\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2960\tAccuracy: 1565.0/10000 (16%)\n",
            "log_softmax: Loss: 2.2974\tAccuracy: 1382.0/10000 (14%)\n",
            "\n",
            "Train Epoch: 500 [0/1000 (0%)]\tLosses F.softmax: 2.282094 log_softmax: 2.284835\n",
            "Train Epoch: 500 [1000/1000 (100%)]\tLosses F.softmax: 2.332039 log_softmax: 2.333212\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2959\tAccuracy: 1534.0/10000 (15%)\n",
            "log_softmax: Loss: 2.2974\tAccuracy: 1337.0/10000 (13%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jv5dQ_scbLX4",
        "colab_type": "code",
        "outputId": "1e5e86d5-a910-4328-c50f-050773a19aff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plot_graphs(test_log, 'loss')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZJYmqLhmuAf",
        "colab_type": "code",
        "outputId": "947ad9a9-7901-43d3-c566-533c17e78467",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plot_graphs(test_log, 'accuracy')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYi8f4pRmw73",
        "colab_type": "code",
        "outputId": "5dc0de1d-ebc0-4b79-fc6a-0f9606fdd723",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "test_log"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'F.softmax': [(2.346628357028961, tensor(10.3200)),\n",
              "  (2.3340728853225707, tensor(10.3200)),\n",
              "  (2.3254540694236754, tensor(10.3200)),\n",
              "  (2.319427324485779, tensor(10.3200)),\n",
              "  (2.3151577219963073, tensor(10.3200)),\n",
              "  (2.311979053783417, tensor(10.3200)),\n",
              "  (2.309715236854553, tensor(10.3200)),\n",
              "  (2.3080407976150514, tensor(10.3200)),\n",
              "  (2.306786625480652, tensor(10.3200)),\n",
              "  (2.3058515606880188, tensor(12.7300)),\n",
              "  (2.3051628129959107, tensor(10.2800)),\n",
              "  (2.3046465989112854, tensor(10.2800)),\n",
              "  (2.304237362766266, tensor(10.2800)),\n",
              "  (2.303939515686035, tensor(10.2800)),\n",
              "  (2.3037228024482728, tensor(10.2800)),\n",
              "  (2.303542326831818, tensor(10.2800)),\n",
              "  (2.303411889839172, tensor(10.2800)),\n",
              "  (2.303296516895294, tensor(10.2800)),\n",
              "  (2.3031931060791018, tensor(10.2800)),\n",
              "  (2.3031310984611513, tensor(10.2800)),\n",
              "  (2.3030977776527406, tensor(10.2800)),\n",
              "  (2.3030432970046997, tensor(10.2800)),\n",
              "  (2.3030055475234987, tensor(10.2800)),\n",
              "  (2.3029924315452575, tensor(10.2800)),\n",
              "  (2.302953277206421, tensor(10.2800)),\n",
              "  (2.3029318323135377, tensor(10.2800)),\n",
              "  (2.302913365840912, tensor(10.2800)),\n",
              "  (2.302877439022064, tensor(10.2800)),\n",
              "  (2.3028534843444826, tensor(10.2800)),\n",
              "  (2.3028413380622865, tensor(10.2800)),\n",
              "  (2.3028224912643434, tensor(10.2800)),\n",
              "  (2.302809533691406, tensor(10.2800)),\n",
              "  (2.3027931621551514, tensor(10.2800)),\n",
              "  (2.3027928524971006, tensor(10.2800)),\n",
              "  (2.30279289932251, tensor(10.2800)),\n",
              "  (2.302780293083191, tensor(10.2800)),\n",
              "  (2.3027880185127256, tensor(10.2800)),\n",
              "  (2.3027893355369566, tensor(10.2800)),\n",
              "  (2.302776241016388, tensor(10.2800)),\n",
              "  (2.3027605120658876, tensor(10.2800)),\n",
              "  (2.30276510181427, tensor(10.2800)),\n",
              "  (2.3027514523506163, tensor(10.2800)),\n",
              "  (2.302734213924408, tensor(10.2800)),\n",
              "  (2.302716717529297, tensor(10.2800)),\n",
              "  (2.302695546245575, tensor(10.2800)),\n",
              "  (2.302674779319763, tensor(10.2800)),\n",
              "  (2.30268085899353, tensor(10.2800)),\n",
              "  (2.3026580753326416, tensor(10.2800)),\n",
              "  (2.302637250709534, tensor(10.2800)),\n",
              "  (2.302630645275116, tensor(10.2800)),\n",
              "  (2.302631795883179, tensor(10.2800)),\n",
              "  (2.3026230276107786, tensor(10.2800)),\n",
              "  (2.3026012687683104, tensor(10.2800)),\n",
              "  (2.3025970520973207, tensor(10.2800)),\n",
              "  (2.3026021550178526, tensor(10.2800)),\n",
              "  (2.3026058348655702, tensor(10.2800)),\n",
              "  (2.3025778854370116, tensor(10.2800)),\n",
              "  (2.302552628135681, tensor(10.2800)),\n",
              "  (2.302545721721649, tensor(10.2800)),\n",
              "  (2.302537987232208, tensor(10.2800)),\n",
              "  (2.3025372036933898, tensor(10.2800)),\n",
              "  (2.3025030965805056, tensor(10.2800)),\n",
              "  (2.3024855134010314, tensor(10.2800)),\n",
              "  (2.302489752197266, tensor(10.2800)),\n",
              "  (2.302461468601227, tensor(10.2800)),\n",
              "  (2.3024529863357546, tensor(10.2800)),\n",
              "  (2.3024368492126466, tensor(10.2800)),\n",
              "  (2.302419795131683, tensor(10.2800)),\n",
              "  (2.3024021465301514, tensor(10.2800)),\n",
              "  (2.302379137325287, tensor(10.2800)),\n",
              "  (2.3023568761825564, tensor(10.2800)),\n",
              "  (2.3023529116630552, tensor(10.2800)),\n",
              "  (2.302341299247742, tensor(10.2800)),\n",
              "  (2.30232941570282, tensor(10.2800)),\n",
              "  (2.3023097763061524, tensor(10.2800)),\n",
              "  (2.302293752193451, tensor(10.2800)),\n",
              "  (2.3022846709251406, tensor(10.2800)),\n",
              "  (2.302290000247955, tensor(10.2800)),\n",
              "  (2.302294696521759, tensor(10.2800)),\n",
              "  (2.302272016811371, tensor(10.2800)),\n",
              "  (2.3022696241378786, tensor(10.2800)),\n",
              "  (2.3022407322883605, tensor(10.2800)),\n",
              "  (2.3022410104751585, tensor(10.2800)),\n",
              "  (2.3022208590507507, tensor(10.2800)),\n",
              "  (2.3021950558662416, tensor(10.2800)),\n",
              "  (2.3021613856315613, tensor(10.2800)),\n",
              "  (2.3021648901939393, tensor(10.2800)),\n",
              "  (2.302173755264282, tensor(10.2800)),\n",
              "  (2.3021462602615355, tensor(10.2800)),\n",
              "  (2.3021388826370237, tensor(10.2800)),\n",
              "  (2.30212893409729, tensor(10.2800)),\n",
              "  (2.3021180196762083, tensor(10.2800)),\n",
              "  (2.302100211906433, tensor(10.2800)),\n",
              "  (2.302086992454529, tensor(10.2800)),\n",
              "  (2.3020902130126952, tensor(10.2800)),\n",
              "  (2.3020905672073364, tensor(10.2800)),\n",
              "  (2.302056153678894, tensor(10.2800)),\n",
              "  (2.3020508069992065, tensor(10.2800)),\n",
              "  (2.3020431350708006, tensor(10.2800)),\n",
              "  (2.302028837108612, tensor(10.2800)),\n",
              "  (2.302034618282318, tensor(10.2800)),\n",
              "  (2.302019291782379, tensor(10.2800)),\n",
              "  (2.302004037666321, tensor(10.2800)),\n",
              "  (2.301989622783661, tensor(10.2800)),\n",
              "  (2.3019917093276976, tensor(10.2800)),\n",
              "  (2.301980742263794, tensor(10.2800)),\n",
              "  (2.3019560700416566, tensor(10.2800)),\n",
              "  (2.301931645107269, tensor(10.2800)),\n",
              "  (2.301916482925415, tensor(10.2800)),\n",
              "  (2.301927149963379, tensor(10.2800)),\n",
              "  (2.301900257587433, tensor(10.2800)),\n",
              "  (2.3018865198135376, tensor(10.2800)),\n",
              "  (2.301869228744507, tensor(10.2800)),\n",
              "  (2.301839250564575, tensor(10.2900)),\n",
              "  (2.301833751773834, tensor(10.2900)),\n",
              "  (2.3018255703926087, tensor(10.3100)),\n",
              "  (2.30181601266861, tensor(10.2900)),\n",
              "  (2.3018116930007935, tensor(10.2900)),\n",
              "  (2.301801877498627, tensor(10.2900)),\n",
              "  (2.301792042541504, tensor(10.2800)),\n",
              "  (2.3017791887283323, tensor(10.2800)),\n",
              "  (2.301776731491089, tensor(10.2800)),\n",
              "  (2.301740959358215, tensor(10.2800)),\n",
              "  (2.3017241097450256, tensor(10.2800)),\n",
              "  (2.301711351585388, tensor(10.2800)),\n",
              "  (2.3016923988342284, tensor(10.2900)),\n",
              "  (2.301693192100525, tensor(10.2800)),\n",
              "  (2.3016943873405458, tensor(10.2800)),\n",
              "  (2.301670650100708, tensor(10.2900)),\n",
              "  (2.301658519744873, tensor(10.2800)),\n",
              "  (2.3016355073928834, tensor(10.2800)),\n",
              "  (2.3016373399734498, tensor(10.2800)),\n",
              "  (2.3016116276741028, tensor(10.2900)),\n",
              "  (2.301600584697723, tensor(10.2800)),\n",
              "  (2.3015914467811585, tensor(10.2900)),\n",
              "  (2.301576953125, tensor(10.2900)),\n",
              "  (2.301579301071167, tensor(10.2900)),\n",
              "  (2.301540786743164, tensor(10.2900)),\n",
              "  (2.301524492740631, tensor(10.3300)),\n",
              "  (2.3015217883110046, tensor(10.2900)),\n",
              "  (2.3015116371154787, tensor(10.2900)),\n",
              "  (2.301496421909332, tensor(10.2900)),\n",
              "  (2.301489987182617, tensor(10.2800)),\n",
              "  (2.3014691358566286, tensor(10.2800)),\n",
              "  (2.3014703607559204, tensor(10.2800)),\n",
              "  (2.3014932864189146, tensor(10.2800)),\n",
              "  (2.301484167766571, tensor(10.2800)),\n",
              "  (2.301480223655701, tensor(10.2900)),\n",
              "  (2.3014724339485166, tensor(10.2900)),\n",
              "  (2.3014571558952333, tensor(10.2800)),\n",
              "  (2.301449171257019, tensor(10.2800)),\n",
              "  (2.301437206172943, tensor(10.2800)),\n",
              "  (2.301427816295624, tensor(10.2800)),\n",
              "  (2.301418420886993, tensor(10.2800)),\n",
              "  (2.301388208389282, tensor(10.2800)),\n",
              "  (2.301375921058655, tensor(10.2800)),\n",
              "  (2.301368579006195, tensor(10.2800)),\n",
              "  (2.301354639053345, tensor(10.2800)),\n",
              "  (2.30134156703949, tensor(10.2800)),\n",
              "  (2.3013468616485597, tensor(10.2800)),\n",
              "  (2.3013343068122865, tensor(10.2800)),\n",
              "  (2.3013118521690368, tensor(10.2800)),\n",
              "  (2.30127159948349, tensor(10.2800)),\n",
              "  (2.3012645098686217, tensor(10.2900)),\n",
              "  (2.3012519011497496, tensor(10.2900)),\n",
              "  (2.301233620262146, tensor(10.2800)),\n",
              "  (2.3012098019599914, tensor(10.2900)),\n",
              "  (2.301197835159302, tensor(10.2900)),\n",
              "  (2.301187815761566, tensor(10.2900)),\n",
              "  (2.3011840757369995, tensor(10.2800)),\n",
              "  (2.3011793460845946, tensor(10.2800)),\n",
              "  (2.3011670409202574, tensor(10.2800)),\n",
              "  (2.3011466086387635, tensor(10.3100)),\n",
              "  (2.3011519733428956, tensor(10.2800)),\n",
              "  (2.3011526951789856, tensor(10.2800)),\n",
              "  (2.3011275598526, tensor(10.2800)),\n",
              "  (2.301106306648254, tensor(10.2800)),\n",
              "  (2.3010984498023985, tensor(10.2800)),\n",
              "  (2.3010525342941284, tensor(10.3100)),\n",
              "  (2.3010460084915163, tensor(10.3500)),\n",
              "  (2.3010440591812134, tensor(10.3300)),\n",
              "  (2.3010438955307007, tensor(10.3500)),\n",
              "  (2.3010248563766478, tensor(10.3300)),\n",
              "  (2.301025684738159, tensor(10.2900)),\n",
              "  (2.301015817451477, tensor(10.2900)),\n",
              "  (2.3009939233779906, tensor(10.3300)),\n",
              "  (2.3009807956695556, tensor(10.3500)),\n",
              "  (2.300971540737152, tensor(10.4200)),\n",
              "  (2.3009489802360537, tensor(10.7000)),\n",
              "  (2.300951649570465, tensor(10.3900)),\n",
              "  (2.3009439260482787, tensor(10.3500)),\n",
              "  (2.3009252306938173, tensor(10.2900)),\n",
              "  (2.3009060133934023, tensor(10.2800)),\n",
              "  (2.3008993726730345, tensor(10.2800)),\n",
              "  (2.300883086681366, tensor(10.2800)),\n",
              "  (2.3008658953666687, tensor(10.2900)),\n",
              "  (2.300847692680359, tensor(10.2900)),\n",
              "  (2.3008359835624694, tensor(10.2900)),\n",
              "  (2.3008302227020265, tensor(10.2800)),\n",
              "  (2.3008321895599364, tensor(10.2800)),\n",
              "  (2.3007995735168456, tensor(10.2800)),\n",
              "  (2.3007862340927123, tensor(10.2800)),\n",
              "  (2.3007711141586302, tensor(10.2800)),\n",
              "  (2.300744871330261, tensor(10.3300)),\n",
              "  (2.3007188548088076, tensor(10.3900)),\n",
              "  (2.3007075843811036, tensor(10.3700)),\n",
              "  (2.3006982957839965, tensor(10.6500)),\n",
              "  (2.3006842365264895, tensor(10.4200)),\n",
              "  (2.300668512535095, tensor(10.5000)),\n",
              "  (2.300650789928436, tensor(10.5900)),\n",
              "  (2.30062096452713, tensor(11.5600)),\n",
              "  (2.300619434261322, tensor(11.6000)),\n",
              "  (2.3006140309333802, tensor(10.5800)),\n",
              "  (2.300602766895294, tensor(10.5600)),\n",
              "  (2.300586334705353, tensor(10.7800)),\n",
              "  (2.300598812007904, tensor(10.5900)),\n",
              "  (2.300587202358246, tensor(10.6200)),\n",
              "  (2.3005905374526976, tensor(10.3300)),\n",
              "  (2.3005608132362365, tensor(10.3400)),\n",
              "  (2.300538015651703, tensor(10.4100)),\n",
              "  (2.3005264298439028, tensor(10.3000)),\n",
              "  (2.3005109882354735, tensor(10.3300)),\n",
              "  (2.3004917300224306, tensor(10.3600)),\n",
              "  (2.3004596530914307, tensor(10.4300)),\n",
              "  (2.300449754047394, tensor(10.3900)),\n",
              "  (2.3004505316734316, tensor(10.4100)),\n",
              "  (2.300442657661438, tensor(10.3300)),\n",
              "  (2.300439392375946, tensor(10.3300)),\n",
              "  (2.3004267488479613, tensor(10.4200)),\n",
              "  (2.300419239330292, tensor(10.5800)),\n",
              "  (2.3003808846473692, tensor(10.5500)),\n",
              "  (2.300365771865845, tensor(10.5400)),\n",
              "  (2.300333980464935, tensor(10.7600)),\n",
              "  (2.3003203165054322, tensor(11.1000)),\n",
              "  (2.300308499622345, tensor(10.4700)),\n",
              "  (2.300298836708069, tensor(10.6700)),\n",
              "  (2.3002786799430845, tensor(10.8700)),\n",
              "  (2.3002523463249207, tensor(10.6800)),\n",
              "  (2.300267863368988, tensor(10.4100)),\n",
              "  (2.30026885137558, tensor(10.3400)),\n",
              "  (2.300256317615509, tensor(10.2900)),\n",
              "  (2.300253217792511, tensor(10.2900)),\n",
              "  (2.3002502126693725, tensor(10.2800)),\n",
              "  (2.3002346991539, tensor(10.2800)),\n",
              "  (2.300206304073334, tensor(10.2800)),\n",
              "  (2.3001849933624268, tensor(10.4100)),\n",
              "  (2.300177532672882, tensor(10.3300)),\n",
              "  (2.3001710474014283, tensor(10.3700)),\n",
              "  (2.3001507397651673, tensor(10.3400)),\n",
              "  (2.3001255717277527, tensor(10.4100)),\n",
              "  (2.3001038024902343, tensor(10.4100)),\n",
              "  (2.3000897673606873, tensor(10.5700)),\n",
              "  (2.300081900024414, tensor(10.7100)),\n",
              "  (2.3000583030700685, tensor(10.7000)),\n",
              "  (2.3000416049003602, tensor(10.8200)),\n",
              "  (2.3000199992179873, tensor(12.0100)),\n",
              "  (2.2999994276046754, tensor(11.9800)),\n",
              "  (2.3000011084556578, tensor(10.7000)),\n",
              "  (2.2999766191482545, tensor(11.2900)),\n",
              "  (2.2999770087242126, tensor(10.8000)),\n",
              "  (2.299964858531952, tensor(10.8600)),\n",
              "  (2.299954207897186, tensor(11.3400)),\n",
              "  (2.2999517175674438, tensor(10.8500)),\n",
              "  (2.2999447491645815, tensor(10.7100)),\n",
              "  (2.2999229432106016, tensor(11.7000)),\n",
              "  (2.2999013289451598, tensor(11.6800)),\n",
              "  (2.299874389266968, tensor(11.7200)),\n",
              "  (2.2998506547927855, tensor(12.9600)),\n",
              "  (2.2998586913108827, tensor(11.1600)),\n",
              "  (2.2998618401527406, tensor(10.8700)),\n",
              "  (2.2998434366226195, tensor(12.7600)),\n",
              "  (2.299823612308502, tensor(13.3700)),\n",
              "  (2.2998048733711243, tensor(13.5000)),\n",
              "  (2.2997794143676757, tensor(12.6600)),\n",
              "  (2.299747445678711, tensor(12.9300)),\n",
              "  (2.299759537124634, tensor(11.3000)),\n",
              "  (2.2997109607696533, tensor(12.1900)),\n",
              "  (2.29970695438385, tensor(13.8800)),\n",
              "  (2.299713988304138, tensor(12.1400)),\n",
              "  (2.299707466316223, tensor(12.)),\n",
              "  (2.299693354511261, tensor(11.4300)),\n",
              "  (2.29967132396698, tensor(12.1400)),\n",
              "  (2.2996586679458617, tensor(11.9500)),\n",
              "  (2.299640939426422, tensor(11.6500)),\n",
              "  (2.299623692512512, tensor(11.7300)),\n",
              "  (2.299603494644165, tensor(11.9800)),\n",
              "  (2.2995819604873655, tensor(12.3900)),\n",
              "  (2.299577640914917, tensor(12.4300)),\n",
              "  (2.2995632928848266, tensor(10.8900)),\n",
              "  (2.2995464096069336, tensor(11.1100)),\n",
              "  (2.299529538536072, tensor(10.8500)),\n",
              "  (2.299525951766968, tensor(10.6800)),\n",
              "  (2.2995069858551025, tensor(10.5800)),\n",
              "  (2.2995038432121278, tensor(10.7500)),\n",
              "  (2.299501894569397, tensor(11.1700)),\n",
              "  (2.2994795649528506, tensor(11.2900)),\n",
              "  (2.299459456539154, tensor(11.5900)),\n",
              "  (2.2994486787796022, tensor(12.2000)),\n",
              "  (2.2994368052482606, tensor(12.9400)),\n",
              "  (2.299407255268097, tensor(11.2000)),\n",
              "  (2.299392489719391, tensor(10.9000)),\n",
              "  (2.2993832794189455, tensor(10.9600)),\n",
              "  (2.299384980201721, tensor(11.1400)),\n",
              "  (2.2993572304725647, tensor(11.6300)),\n",
              "  (2.299335374355316, tensor(11.5900)),\n",
              "  (2.2993032105445863, tensor(11.7500)),\n",
              "  (2.299283012390137, tensor(12.0500)),\n",
              "  (2.29926230134964, tensor(11.6700)),\n",
              "  (2.299245022583008, tensor(12.1900)),\n",
              "  (2.2992256016731263, tensor(13.6200)),\n",
              "  (2.299209524154663, tensor(14.6700)),\n",
              "  (2.299185973072052, tensor(14.9700)),\n",
              "  (2.299188976097107, tensor(15.9000)),\n",
              "  (2.2991633143424988, tensor(16.3200)),\n",
              "  (2.299136264419556, tensor(15.9900)),\n",
              "  (2.299125189399719, tensor(16.3400)),\n",
              "  (2.299116018295288, tensor(15.5600)),\n",
              "  (2.2991148404121398, tensor(15.6000)),\n",
              "  (2.2990970591545103, tensor(16.0200)),\n",
              "  (2.2990928964614867, tensor(14.5300)),\n",
              "  (2.299085582256317, tensor(15.6800)),\n",
              "  (2.299080606174469, tensor(14.2500)),\n",
              "  (2.299059590148926, tensor(14.0100)),\n",
              "  (2.2990637630462647, tensor(14.2000)),\n",
              "  (2.299045306682587, tensor(13.8700)),\n",
              "  (2.2990239523887634, tensor(13.2600)),\n",
              "  (2.2990129007339477, tensor(13.5000)),\n",
              "  (2.299000619125366, tensor(14.2900)),\n",
              "  (2.2989834759712218, tensor(15.3200)),\n",
              "  (2.2989816464424133, tensor(12.5000)),\n",
              "  (2.2989637340545652, tensor(12.9900)),\n",
              "  (2.2989562004089357, tensor(12.2900)),\n",
              "  (2.29893806848526, tensor(14.1500)),\n",
              "  (2.2989153239250184, tensor(13.9900)),\n",
              "  (2.2989037563323973, tensor(13.7100)),\n",
              "  (2.298885294818878, tensor(12.1800)),\n",
              "  (2.2988896615028382, tensor(11.1200)),\n",
              "  (2.2988538821220397, tensor(11.7800)),\n",
              "  (2.2988272362709044, tensor(12.9000)),\n",
              "  (2.2988232089042664, tensor(11.8600)),\n",
              "  (2.2988200871467592, tensor(11.6900)),\n",
              "  (2.298785059928894, tensor(11.5400)),\n",
              "  (2.298776282405853, tensor(11.0400)),\n",
              "  (2.298761453914642, tensor(10.8200)),\n",
              "  (2.2987390785217285, tensor(10.5400)),\n",
              "  (2.298721739387512, tensor(11.1100)),\n",
              "  (2.2987219268798826, tensor(10.5600)),\n",
              "  (2.298692339229584, tensor(10.7300)),\n",
              "  (2.2986548011779786, tensor(11.0400)),\n",
              "  (2.2986631901741026, tensor(10.8200)),\n",
              "  (2.29864140291214, tensor(11.1200)),\n",
              "  (2.2986257402420045, tensor(11.6500)),\n",
              "  (2.2986107619285585, tensor(12.2900)),\n",
              "  (2.298575537586212, tensor(12.3000)),\n",
              "  (2.2985604132652284, tensor(12.6700)),\n",
              "  (2.29856390914917, tensor(12.4200)),\n",
              "  (2.2985411402702334, tensor(12.5200)),\n",
              "  (2.2985193285942076, tensor(11.7400)),\n",
              "  (2.2985198826789857, tensor(12.0900)),\n",
              "  (2.2985156606674195, tensor(11.6400)),\n",
              "  (2.2984914454460146, tensor(12.8300)),\n",
              "  (2.2984675137519837, tensor(13.0200)),\n",
              "  (2.2984559565544127, tensor(12.0500)),\n",
              "  (2.298440166568756, tensor(11.7300)),\n",
              "  (2.2984071600914002, tensor(13.0200)),\n",
              "  (2.298421584701538, tensor(12.4500)),\n",
              "  (2.2984046606063844, tensor(12.0600)),\n",
              "  (2.2983894406318663, tensor(11.4700)),\n",
              "  (2.2983489645957946, tensor(11.9700)),\n",
              "  (2.2983462336540224, tensor(12.8000)),\n",
              "  (2.298314921760559, tensor(13.4100)),\n",
              "  (2.2983038286209108, tensor(12.9900)),\n",
              "  (2.2982800577163696, tensor(14.0700)),\n",
              "  (2.298268551635742, tensor(14.7900)),\n",
              "  (2.298238614273071, tensor(16.)),\n",
              "  (2.2982219264030457, tensor(14.4100)),\n",
              "  (2.298213072681427, tensor(13.8000)),\n",
              "  (2.298190302658081, tensor(14.2500)),\n",
              "  (2.298177272415161, tensor(15.1100)),\n",
              "  (2.2981666239738465, tensor(15.8900)),\n",
              "  (2.2981433973312377, tensor(14.7800)),\n",
              "  (2.298131061935425, tensor(15.7600)),\n",
              "  (2.2980904866218568, tensor(15.6800)),\n",
              "  (2.298086170959473, tensor(14.0500)),\n",
              "  (2.29806789522171, tensor(13.3400)),\n",
              "  (2.298036321926117, tensor(13.1900)),\n",
              "  (2.298023421764374, tensor(12.9800)),\n",
              "  (2.2980006979942322, tensor(14.1800)),\n",
              "  (2.297988493347168, tensor(13.6000)),\n",
              "  (2.2979763602256775, tensor(13.6500)),\n",
              "  (2.2979630776405333, tensor(12.3800)),\n",
              "  (2.297947830867767, tensor(13.4900)),\n",
              "  (2.2979374647140505, tensor(13.9300)),\n",
              "  (2.2979023427963257, tensor(15.4200)),\n",
              "  (2.297899158668518, tensor(14.3200)),\n",
              "  (2.297880277824402, tensor(15.7900)),\n",
              "  (2.2978652975082396, tensor(15.3200)),\n",
              "  (2.2978617899894713, tensor(15.4900)),\n",
              "  (2.297855412864685, tensor(15.7200)),\n",
              "  (2.2978174472808837, tensor(15.9900)),\n",
              "  (2.2978042722702026, tensor(15.1700)),\n",
              "  (2.2977814610481264, tensor(14.8300)),\n",
              "  (2.297770413780212, tensor(13.5400)),\n",
              "  (2.2977399483680725, tensor(15.7100)),\n",
              "  (2.29774246339798, tensor(14.4200)),\n",
              "  (2.2977185277938843, tensor(14.2500)),\n",
              "  (2.2977051403045654, tensor(14.2900)),\n",
              "  (2.297672857761383, tensor(15.2200)),\n",
              "  (2.297641838932037, tensor(16.2100)),\n",
              "  (2.2976337467193604, tensor(16.6900)),\n",
              "  (2.297645212841034, tensor(14.4600)),\n",
              "  (2.2976357767105102, tensor(13.8700)),\n",
              "  (2.297623822784424, tensor(14.1600)),\n",
              "  (2.2976054266929626, tensor(13.7300)),\n",
              "  (2.297583263587952, tensor(13.7600)),\n",
              "  (2.2975668966293337, tensor(14.1600)),\n",
              "  (2.297532745361328, tensor(15.1400)),\n",
              "  (2.297536302280426, tensor(14.6100)),\n",
              "  (2.297527037525177, tensor(14.9100)),\n",
              "  (2.2975287048339843, tensor(14.1800)),\n",
              "  (2.2974960310935972, tensor(14.4200)),\n",
              "  (2.297497898864746, tensor(13.2600)),\n",
              "  (2.29748554983139, tensor(12.6900)),\n",
              "  (2.2974478791236876, tensor(13.8600)),\n",
              "  (2.2974336587905886, tensor(12.5200)),\n",
              "  (2.297424744796753, tensor(12.9000)),\n",
              "  (2.297408410453796, tensor(13.5200)),\n",
              "  (2.29738209028244, tensor(14.4800)),\n",
              "  (2.2973748630523683, tensor(14.2900)),\n",
              "  (2.297347689437866, tensor(14.8300)),\n",
              "  (2.2973254873275755, tensor(16.1100)),\n",
              "  (2.2973016504287718, tensor(15.9400)),\n",
              "  (2.2972789825439452, tensor(16.1400)),\n",
              "  (2.2972398944854735, tensor(16.3900)),\n",
              "  (2.2972110023498535, tensor(16.3400)),\n",
              "  (2.2971960180282593, tensor(16.7600)),\n",
              "  (2.2971827558517455, tensor(15.9100)),\n",
              "  (2.297160619163513, tensor(14.9600)),\n",
              "  (2.297151116275787, tensor(14.2400)),\n",
              "  (2.2971222180366517, tensor(15.1900)),\n",
              "  (2.297101721096039, tensor(15.4100)),\n",
              "  (2.29706860704422, tensor(15.7300)),\n",
              "  (2.297051368331909, tensor(15.2100)),\n",
              "  (2.2970206939697264, tensor(15.4700)),\n",
              "  (2.2969916563034056, tensor(15.9400)),\n",
              "  (2.2969759031295776, tensor(14.7700)),\n",
              "  (2.296952993774414, tensor(14.6400)),\n",
              "  (2.2969435240745546, tensor(13.8300)),\n",
              "  (2.29694389629364, tensor(14.1200)),\n",
              "  (2.296927648448944, tensor(13.9000)),\n",
              "  (2.296910123157501, tensor(14.1500)),\n",
              "  (2.296894019126892, tensor(12.7700)),\n",
              "  (2.2968622612953187, tensor(13.9600)),\n",
              "  (2.2968367455482483, tensor(14.9800)),\n",
              "  (2.296837409591675, tensor(14.4400)),\n",
              "  (2.2968175422668455, tensor(14.9100)),\n",
              "  (2.296804199886322, tensor(15.7400)),\n",
              "  (2.2967849662780764, tensor(15.7500)),\n",
              "  (2.296745978832245, tensor(15.0500)),\n",
              "  (2.2967270524978636, tensor(15.7600)),\n",
              "  (2.296717081737518, tensor(14.8400)),\n",
              "  (2.2967055888175962, tensor(16.4500)),\n",
              "  (2.2966959000587464, tensor(15.9000)),\n",
              "  (2.296667236804962, tensor(16.1600)),\n",
              "  (2.2966345989227297, tensor(16.)),\n",
              "  (2.2965934056282045, tensor(16.8800)),\n",
              "  (2.2965931624412534, tensor(17.3600)),\n",
              "  (2.2965512598991396, tensor(16.6300)),\n",
              "  (2.2965294025421144, tensor(16.5700)),\n",
              "  (2.296507341003418, tensor(15.6600)),\n",
              "  (2.2964939264297484, tensor(16.8400)),\n",
              "  (2.2964870810508726, tensor(17.5000)),\n",
              "  (2.296469221019745, tensor(17.0700)),\n",
              "  (2.2964605115890504, tensor(16.0700)),\n",
              "  (2.296453782749176, tensor(16.5100)),\n",
              "  (2.2964515953063964, tensor(15.7700)),\n",
              "  (2.2964238889694215, tensor(14.7500)),\n",
              "  (2.2963883158683775, tensor(15.9900)),\n",
              "  (2.296374333000183, tensor(16.1700)),\n",
              "  (2.296338233089447, tensor(17.2600)),\n",
              "  (2.296308152580261, tensor(17.5400)),\n",
              "  (2.2962715505599975, tensor(17.9500)),\n",
              "  (2.2962520312309267, tensor(17.9400)),\n",
              "  (2.296246161556244, tensor(17.3800)),\n",
              "  (2.296229570674896, tensor(17.3800)),\n",
              "  (2.296207840633392, tensor(17.3600)),\n",
              "  (2.2961951355934143, tensor(16.9500)),\n",
              "  (2.2961804829597474, tensor(17.1700)),\n",
              "  (2.296149891757965, tensor(17.1000)),\n",
              "  (2.296144030761719, tensor(16.3500)),\n",
              "  (2.296098534297943, tensor(17.1000)),\n",
              "  (2.2960690762519835, tensor(17.3800)),\n",
              "  (2.296059536266327, tensor(17.3900)),\n",
              "  (2.296042657375336, tensor(17.1200)),\n",
              "  (2.296007532310486, tensor(17.3900)),\n",
              "  (2.295981716442108, tensor(17.3600)),\n",
              "  (2.2959691617012026, tensor(17.0500)),\n",
              "  (2.2959581124305726, tensor(17.1400)),\n",
              "  (2.2959502774238585, tensor(15.6500)),\n",
              "  (2.2959274689674376, tensor(15.3400))],\n",
              " 'log_softmax': [(2.314306327152252, tensor(11.3500)),\n",
              "  (2.3111071219444277, tensor(11.3500)),\n",
              "  (2.3087360458374024, tensor(11.3500)),\n",
              "  (2.3070159461021422, tensor(11.3500)),\n",
              "  (2.3057869119644163, tensor(11.3500)),\n",
              "  (2.3048982970237732, tensor(11.3500)),\n",
              "  (2.304258832740784, tensor(11.3500)),\n",
              "  (2.303796380805969, tensor(11.3500)),\n",
              "  (2.3034365085601807, tensor(11.3500)),\n",
              "  (2.3032027292251587, tensor(11.3500)),\n",
              "  (2.3030443954467774, tensor(11.3500)),\n",
              "  (2.3029292938232424, tensor(11.3500)),\n",
              "  (2.3028456554412844, tensor(11.3500)),\n",
              "  (2.3028070382118226, tensor(11.3500)),\n",
              "  (2.3027906465530394, tensor(11.3500)),\n",
              "  (2.302785775375366, tensor(11.3500)),\n",
              "  (2.302781760025024, tensor(11.3500)),\n",
              "  (2.302780673980713, tensor(11.3600)),\n",
              "  (2.3027748052597046, tensor(11.3700)),\n",
              "  (2.302786572551727, tensor(10.8600)),\n",
              "  (2.3028104421615603, tensor(6.9900)),\n",
              "  (2.3028179889678957, tensor(7.0200)),\n",
              "  (2.3028287476539613, tensor(8.3400)),\n",
              "  (2.302856859111786, tensor(8.8400)),\n",
              "  (2.302858866214752, tensor(8.3700)),\n",
              "  (2.302867481422424, tensor(8.1600)),\n",
              "  (2.302877156543732, tensor(8.9000)),\n",
              "  (2.302864134120941, tensor(9.1800)),\n",
              "  (2.302859018135071, tensor(9.8200)),\n",
              "  (2.302862475967407, tensor(10.1100)),\n",
              "  (2.3028603255271913, tensor(9.9700)),\n",
              "  (2.3028615436553954, tensor(10.2300)),\n",
              "  (2.3028580554008484, tensor(10.2400)),\n",
              "  (2.302868622779846, tensor(10.2400)),\n",
              "  (2.302879689693451, tensor(10.2800)),\n",
              "  (2.302876999568939, tensor(10.2600)),\n",
              "  (2.302893625926971, tensor(10.2800)),\n",
              "  (2.3029026926040648, tensor(10.2800)),\n",
              "  (2.302897361469269, tensor(10.2800)),\n",
              "  (2.3028878924369813, tensor(10.2800)),\n",
              "  (2.302898468208313, tensor(10.2800)),\n",
              "  (2.3028902300834657, tensor(10.2800)),\n",
              "  (2.3028778895378115, tensor(10.2800)),\n",
              "  (2.3028651579856874, tensor(10.2800)),\n",
              "  (2.3028485652923583, tensor(10.2800)),\n",
              "  (2.3028318205833433, tensor(10.2800)),\n",
              "  (2.3028419489860537, tensor(10.2800)),\n",
              "  (2.302822704792023, tensor(10.2800)),\n",
              "  (2.302805032444, tensor(10.2800)),\n",
              "  (2.3028015900611876, tensor(10.2800)),\n",
              "  (2.3028057737350465, tensor(10.2800)),\n",
              "  (2.302799634552002, tensor(10.2800)),\n",
              "  (2.302780226612091, tensor(10.2800)),\n",
              "  (2.302778517150879, tensor(10.2800)),\n",
              "  (2.3027861810684205, tensor(10.2800)),\n",
              "  (2.3027921486854552, tensor(10.2800)),\n",
              "  (2.302765997982025, tensor(10.2800)),\n",
              "  (2.3027425525665284, tensor(10.2800)),\n",
              "  (2.3027377046585085, tensor(10.2800)),\n",
              "  (2.302731876659393, tensor(10.2800)),\n",
              "  (2.302733072280884, tensor(10.2800)),\n",
              "  (2.30270049533844, tensor(10.2800)),\n",
              "  (2.3026844636917114, tensor(10.2800)),\n",
              "  (2.3026904829978943, tensor(10.2800)),\n",
              "  (2.302663593387604, tensor(10.2800)),\n",
              "  (2.3026567527770996, tensor(10.2800)),\n",
              "  (2.3026421184539796, tensor(10.2800)),\n",
              "  (2.3026265171051024, tensor(10.2800)),\n",
              "  (2.3026103964805604, tensor(10.2800)),\n",
              "  (2.3025887291908265, tensor(10.2800)),\n",
              "  (2.302567771434784, tensor(10.2800)),\n",
              "  (2.302565331840515, tensor(10.2800)),\n",
              "  (2.3025551410675047, tensor(10.2800)),\n",
              "  (2.302544701385498, tensor(10.2800)),\n",
              "  (2.302526330947876, tensor(10.2800)),\n",
              "  (2.3025115767478943, tensor(10.2800)),\n",
              "  (2.3025038737297057, tensor(10.2800)),\n",
              "  (2.302510745811462, tensor(10.2800)),\n",
              "  (2.302516931438446, tensor(10.2800)),\n",
              "  (2.302495484828949, tensor(10.2800)),\n",
              "  (2.3024944709777833, tensor(10.2800)),\n",
              "  (2.302466709518433, tensor(10.2800)),\n",
              "  (2.3024685346603393, tensor(10.2800)),\n",
              "  (2.302449595069885, tensor(10.2800)),\n",
              "  (2.3024249151229856, tensor(10.2800)),\n",
              "  (2.3023923175811767, tensor(10.2800)),\n",
              "  (2.3023973915100098, tensor(10.2800)),\n",
              "  (2.302407763385773, tensor(10.2800)),\n",
              "  (2.3023815448760985, tensor(10.2800)),\n",
              "  (2.302375553035736, tensor(10.2800)),\n",
              "  (2.3023669305801393, tensor(10.2800)),\n",
              "  (2.3023573128700257, tensor(10.2800)),\n",
              "  (2.3023407024383546, tensor(10.2800)),\n",
              "  (2.302328778934479, tensor(10.2800)),\n",
              "  (2.302333477115631, tensor(10.2800)),\n",
              "  (2.302335258293152, tensor(10.2800)),\n",
              "  (2.3023019220352174, tensor(10.2800)),\n",
              "  (2.3022979578971863, tensor(10.2800)),\n",
              "  (2.302291576957703, tensor(10.2800)),\n",
              "  (2.30227850894928, tensor(10.2800)),\n",
              "  (2.3022857804298402, tensor(10.2800)),\n",
              "  (2.3022716569900514, tensor(10.2800)),\n",
              "  (2.302257654762268, tensor(10.2800)),\n",
              "  (2.3022445444107054, tensor(10.2800)),\n",
              "  (2.302248208141327, tensor(10.2800)),\n",
              "  (2.3022385378837584, tensor(10.2800)),\n",
              "  (2.3022148566246035, tensor(10.2800)),\n",
              "  (2.302191671848297, tensor(10.2800)),\n",
              "  (2.302177728176117, tensor(10.2800)),\n",
              "  (2.3021899904251097, tensor(10.2800)),\n",
              "  (2.302164287185669, tensor(10.2800)),\n",
              "  (2.3021518666267395, tensor(10.2800)),\n",
              "  (2.302135868740082, tensor(10.2800)),\n",
              "  (2.302106966495514, tensor(10.2800)),\n",
              "  (2.3021029666900636, tensor(10.2800)),\n",
              "  (2.3020961634635926, tensor(10.3200)),\n",
              "  (2.3020879187583922, tensor(10.2900)),\n",
              "  (2.3020849435806277, tensor(10.2900)),\n",
              "  (2.3020765434265136, tensor(10.2800)),\n",
              "  (2.302068022441864, tensor(10.2800)),\n",
              "  (2.3020564570426942, tensor(10.2800)),\n",
              "  (2.302055466747284, tensor(10.2800)),\n",
              "  (2.3020207204818726, tensor(10.2800)),\n",
              "  (2.3020050703048707, tensor(10.2800)),\n",
              "  (2.3019937806129454, tensor(10.2800)),\n",
              "  (2.301976303577423, tensor(10.2800)),\n",
              "  (2.3019786299705505, tensor(10.2800)),\n",
              "  (2.301981346988678, tensor(10.2800)),\n",
              "  (2.301958735656738, tensor(10.2800)),\n",
              "  (2.301947917461395, tensor(10.2800)),\n",
              "  (2.3019260789871216, tensor(10.2800)),\n",
              "  (2.301929382324219, tensor(10.2800)),\n",
              "  (2.3019049437522887, tensor(10.2800)),\n",
              "  (2.3018952290534975, tensor(10.2800)),\n",
              "  (2.3018874753952026, tensor(10.2800)),\n",
              "  (2.3018742835998536, tensor(10.2800)),\n",
              "  (2.3018782742500306, tensor(10.2800)),\n",
              "  (2.3018408222198485, tensor(10.2800)),\n",
              "  (2.3018258522987365, tensor(10.3100)),\n",
              "  (2.301824666118622, tensor(10.2800)),\n",
              "  (2.301815901374817, tensor(10.2800)),\n",
              "  (2.301802112865448, tensor(10.2800)),\n",
              "  (2.301797096538544, tensor(10.2800)),\n",
              "  (2.3017775844573976, tensor(10.2800)),\n",
              "  (2.3017804591178894, tensor(10.2800)),\n",
              "  (2.3018052210807802, tensor(10.2800)),\n",
              "  (2.3017975878715515, tensor(10.2800)),\n",
              "  (2.30179520406723, tensor(10.2800)),\n",
              "  (2.301788819026947, tensor(10.2800)),\n",
              "  (2.301775005245209, tensor(10.2800)),\n",
              "  (2.3017685344696046, tensor(10.2800)),\n",
              "  (2.301757995223999, tensor(10.2800)),\n",
              "  (2.3017501185417175, tensor(10.2800)),\n",
              "  (2.3017422494888304, tensor(10.2800)),\n",
              "  (2.3017133245468138, tensor(10.2800)),\n",
              "  (2.3017024011611937, tensor(10.2800)),\n",
              "  (2.3016966733932493, tensor(10.2800)),\n",
              "  (2.301684266281128, tensor(10.2800)),\n",
              "  (2.3016727564811705, tensor(10.2800)),\n",
              "  (2.30167974023819, tensor(10.2800)),\n",
              "  (2.3016687314987183, tensor(10.2800)),\n",
              "  (2.3016477464675904, tensor(10.2800)),\n",
              "  (2.301608800315857, tensor(10.2800)),\n",
              "  (2.30160338640213, tensor(10.2800)),\n",
              "  (2.3015924145698547, tensor(10.2800)),\n",
              "  (2.3015756567001344, tensor(10.2800)),\n",
              "  (2.30155319480896, tensor(10.2800)),\n",
              "  (2.301542872810364, tensor(10.2800)),\n",
              "  (2.3015345074653624, tensor(10.2800)),\n",
              "  (2.3015325110435487, tensor(10.2800)),\n",
              "  (2.3015294240951536, tensor(10.2800)),\n",
              "  (2.3015185991287233, tensor(10.2800)),\n",
              "  (2.301499717712402, tensor(10.3000)),\n",
              "  (2.3015068923950195, tensor(10.2800)),\n",
              "  (2.3015094079017637, tensor(10.2800)),\n",
              "  (2.3014857385635374, tensor(10.2800)),\n",
              "  (2.301466004371643, tensor(10.2800)),\n",
              "  (2.3014598753929136, tensor(10.2800)),\n",
              "  (2.3014152325630186, tensor(10.3000)),\n",
              "  (2.3014103318214416, tensor(10.3300)),\n",
              "  (2.3014101105690004, tensor(10.3100)),\n",
              "  (2.3014116403579714, tensor(10.3100)),\n",
              "  (2.301394189453125, tensor(10.3000)),\n",
              "  (2.301396800994873, tensor(10.2800)),\n",
              "  (2.301388693332672, tensor(10.2800)),\n",
              "  (2.3013683693885802, tensor(10.3100)),\n",
              "  (2.3013569056510925, tensor(10.3100)),\n",
              "  (2.3013493378639223, tensor(10.3700)),\n",
              "  (2.301328360652924, tensor(10.5100)),\n",
              "  (2.3013328043937684, tensor(10.3400)),\n",
              "  (2.301326819896698, tensor(10.3100)),\n",
              "  (2.301309726524353, tensor(10.2800)),\n",
              "  (2.3012921822547914, tensor(10.2800)),\n",
              "  (2.3012873454093934, tensor(10.2800)),\n",
              "  (2.301272597408295, tensor(10.2800)),\n",
              "  (2.301257128238678, tensor(10.2800)),\n",
              "  (2.3012406143188477, tensor(10.2800)),\n",
              "  (2.3012307189941406, tensor(10.2800)),\n",
              "  (2.3012267426490784, tensor(10.2800)),\n",
              "  (2.301230662345886, tensor(10.2800)),\n",
              "  (2.3011995295524597, tensor(10.2800)),\n",
              "  (2.3011879163742064, tensor(10.2800)),\n",
              "  (2.3011745900154112, tensor(10.2800)),\n",
              "  (2.3011500255584716, tensor(10.2900)),\n",
              "  (2.3011256660461425, tensor(10.3400)),\n",
              "  (2.3011162539482117, tensor(10.3200)),\n",
              "  (2.301108916187286, tensor(10.4700)),\n",
              "  (2.3010967542648317, tensor(10.3700)),\n",
              "  (2.3010828439712525, tensor(10.4000)),\n",
              "  (2.301067017173767, tensor(10.4300)),\n",
              "  (2.3010389167785643, tensor(10.8900)),\n",
              "  (2.301039434719086, tensor(10.8900)),\n",
              "  (2.301035927772522, tensor(10.4100)),\n",
              "  (2.3010266921043394, tensor(10.4000)),\n",
              "  (2.3010121181488037, tensor(10.5400)),\n",
              "  (2.301026833152771, tensor(10.4100)),\n",
              "  (2.301017166900635, tensor(10.4500)),\n",
              "  (2.30102251663208, tensor(10.3000)),\n",
              "  (2.300994619846344, tensor(10.3100)),\n",
              "  (2.300973610019684, tensor(10.3400)),\n",
              "  (2.3009639209747315, tensor(10.2800)),\n",
              "  (2.3009504388809203, tensor(10.2800)),\n",
              "  (2.3009330137252806, tensor(10.3100)),\n",
              "  (2.30090267496109, tensor(10.3600)),\n",
              "  (2.300894762802124, tensor(10.3400)),\n",
              "  (2.300897751903534, tensor(10.3400)),\n",
              "  (2.300891913318634, tensor(10.2800)),\n",
              "  (2.30089074010849, tensor(10.2900)),\n",
              "  (2.3008801637649534, tensor(10.3600)),\n",
              "  (2.3008747282981874, tensor(10.4100)),\n",
              "  (2.3008380908966064, tensor(10.3900)),\n",
              "  (2.300824868583679, tensor(10.3900)),\n",
              "  (2.300794861602783, tensor(10.5000)),\n",
              "  (2.300783190155029, tensor(10.6600)),\n",
              "  (2.3007735760688783, tensor(10.3600)),\n",
              "  (2.300765944957733, tensor(10.4500)),\n",
              "  (2.3007477485656738, tensor(10.5500)),\n",
              "  (2.3007233946800234, tensor(10.4600)),\n",
              "  (2.3007413409233095, tensor(10.3400)),\n",
              "  (2.300744529247284, tensor(10.2900)),\n",
              "  (2.3007340792655944, tensor(10.2800)),\n",
              "  (2.300733117389679, tensor(10.2800)),\n",
              "  (2.300732334327698, tensor(10.2800)),\n",
              "  (2.300718870830536, tensor(10.2800)),\n",
              "  (2.300692574596405, tensor(10.2800)),\n",
              "  (2.3006733169555664, tensor(10.3600)),\n",
              "  (2.300668005466461, tensor(10.2800)),\n",
              "  (2.3006636894226076, tensor(10.3100)),\n",
              "  (2.300645405673981, tensor(10.3000)),\n",
              "  (2.30062228269577, tensor(10.3400)),\n",
              "  (2.300602606201172, tensor(10.3500)),\n",
              "  (2.3005907971382142, tensor(10.3700)),\n",
              "  (2.300585217761993, tensor(10.4600)),\n",
              "  (2.300563810825348, tensor(10.4400)),\n",
              "  (2.30054937210083, tensor(10.5100)),\n",
              "  (2.3005299194335938, tensor(11.2200)),\n",
              "  (2.3005115837097168, tensor(11.1500)),\n",
              "  (2.300515688800812, tensor(10.4300)),\n",
              "  (2.300493418216705, tensor(10.7200)),\n",
              "  (2.300496227169037, tensor(10.5100)),\n",
              "  (2.300486361312866, tensor(10.5100)),\n",
              "  (2.3004780535697935, tensor(10.7400)),\n",
              "  (2.300477879333496, tensor(10.5100)),\n",
              "  (2.3004733508110045, tensor(10.4400)),\n",
              "  (2.3004538040161133, tensor(10.9800)),\n",
              "  (2.300434405994415, tensor(10.9500)),\n",
              "  (2.300409624290466, tensor(10.9800)),\n",
              "  (2.3003880719184875, tensor(11.8900)),\n",
              "  (2.3003987303733826, tensor(10.6200)),\n",
              "  (2.300404470825195, tensor(10.5100)),\n",
              "  (2.300388514995575, tensor(11.6700)),\n",
              "  (2.300371024131775, tensor(12.1900)),\n",
              "  (2.300354707622528, tensor(12.4100)),\n",
              "  (2.300331531715393, tensor(11.6200)),\n",
              "  (2.300301673698425, tensor(11.8400)),\n",
              "  (2.3003164180755613, tensor(10.6900)),\n",
              "  (2.3002699271202087, tensor(11.3100)),\n",
              "  (2.3002685168266295, tensor(12.7300)),\n",
              "  (2.3002781963348387, tensor(11.2600)),\n",
              "  (2.3002742389678956, tensor(11.1400)),\n",
              "  (2.3002625400543213, tensor(10.7300)),\n",
              "  (2.300242937088013, tensor(11.2500)),\n",
              "  (2.3002327878952027, tensor(11.1300)),\n",
              "  (2.3002174965858457, tensor(10.8700)),\n",
              "  (2.3002027521133424, tensor(10.9500)),\n",
              "  (2.300185020160675, tensor(11.1400)),\n",
              "  (2.300165960597992, tensor(11.3800)),\n",
              "  (2.3001642740249633, tensor(11.3800)),\n",
              "  (2.3001524074554442, tensor(10.5000)),\n",
              "  (2.300138095188141, tensor(10.5600)),\n",
              "  (2.300123810386658, tensor(10.4900)),\n",
              "  (2.300122921562195, tensor(10.3800)),\n",
              "  (2.300106461429596, tensor(10.3600)),\n",
              "  (2.3001062012672424, tensor(10.4000)),\n",
              "  (2.3001069766044617, tensor(10.5800)),\n",
              "  (2.30008716173172, tensor(10.6200)),\n",
              "  (2.300069606208801, tensor(10.7900)),\n",
              "  (2.3000615839004515, tensor(11.2700)),\n",
              "  (2.300052414035797, tensor(11.7900)),\n",
              "  (2.300025212955475, tensor(10.5900)),\n",
              "  (2.300013125705719, tensor(10.4700)),\n",
              "  (2.3000066640853882, tensor(10.5100)),\n",
              "  (2.3000112340927124, tensor(10.5700)),\n",
              "  (2.299986111831665, tensor(10.8000)),\n",
              "  (2.2999668751716613, tensor(10.7600)),\n",
              "  (2.299937329864502, tensor(10.9000)),\n",
              "  (2.2999197722435, tensor(11.1300)),\n",
              "  (2.299901722049713, tensor(10.8200)),\n",
              "  (2.2998872352600097, tensor(11.2200)),\n",
              "  (2.2998706073760986, tensor(12.4100)),\n",
              "  (2.2998574337005615, tensor(13.5000)),\n",
              "  (2.29983656167984, tensor(13.9200)),\n",
              "  (2.2998426535606384, tensor(14.9700)),\n",
              "  (2.299819701385498, tensor(15.5300)),\n",
              "  (2.29979536819458, tensor(15.1400)),\n",
              "  (2.299787155246735, tensor(15.5400)),\n",
              "  (2.299780927371979, tensor(14.5300)),\n",
              "  (2.299782829093933, tensor(14.5900)),\n",
              "  (2.299767948913574, tensor(15.1700)),\n",
              "  (2.299766732120514, tensor(13.4300)),\n",
              "  (2.2997623693466185, tensor(14.6300)),\n",
              "  (2.29976047410965, tensor(13.0500)),\n",
              "  (2.2997423535346986, tensor(12.7000)),\n",
              "  (2.2997496914863587, tensor(12.9700)),\n",
              "  (2.299734159851074, tensor(12.6100)),\n",
              "  (2.299715597820282, tensor(11.9700)),\n",
              "  (2.299707633495331, tensor(12.2500)),\n",
              "  (2.2996983395576476, tensor(13.0600)),\n",
              "  (2.2996842741012573, tensor(14.3000)),\n",
              "  (2.2996856183052063, tensor(11.3200)),\n",
              "  (2.2996706856727602, tensor(11.6900)),\n",
              "  (2.2996661972999575, tensor(11.1900)),\n",
              "  (2.2996511317253114, tensor(12.8900)),\n",
              "  (2.2996313000679014, tensor(12.7000)),\n",
              "  (2.2996229139328004, tensor(12.4400)),\n",
              "  (2.299607443714142, tensor(11.0700)),\n",
              "  (2.299615130138397, tensor(10.4900)),\n",
              "  (2.299582343864441, tensor(10.7900)),\n",
              "  (2.2995586908340453, tensor(11.5800)),\n",
              "  (2.2995579250335694, tensor(10.8600)),\n",
              "  (2.299558077335358, tensor(10.7700)),\n",
              "  (2.2995260150909425, tensor(10.6600)),\n",
              "  (2.299520589637756, tensor(10.4400)),\n",
              "  (2.299508946990967, tensor(10.4000)),\n",
              "  (2.2994898016929626, tensor(10.3300)),\n",
              "  (2.299475756263733, tensor(10.4400)),\n",
              "  (2.2994793704032896, tensor(10.3300)),\n",
              "  (2.2994528467178346, tensor(10.3600)),\n",
              "  (2.29941843585968, tensor(10.4300)),\n",
              "  (2.2994303149223327, tensor(10.4000)),\n",
              "  (2.2994116893768313, tensor(10.4500)),\n",
              "  (2.299399351024628, tensor(10.7200)),\n",
              "  (2.2993876987457273, tensor(11.0800)),\n",
              "  (2.2993556280136107, tensor(11.1200)),\n",
              "  (2.299343865966797, tensor(11.3500)),\n",
              "  (2.299350963973999, tensor(11.1700)),\n",
              "  (2.2993314982414246, tensor(11.2800)),\n",
              "  (2.2993129643440247, tensor(10.7400)),\n",
              "  (2.2993172063827516, tensor(10.9000)),\n",
              "  (2.299316537475586, tensor(10.6700)),\n",
              "  (2.2992956824302673, tensor(11.4600)),\n",
              "  (2.299275204372406, tensor(11.5700)),\n",
              "  (2.2992672132492067, tensor(10.8700)),\n",
              "  (2.2992548318862913, tensor(10.7200)),\n",
              "  (2.29922513256073, tensor(11.5400)),\n",
              "  (2.29924335193634, tensor(11.1900)),\n",
              "  (2.299229995250702, tensor(10.8700)),\n",
              "  (2.299218206119537, tensor(10.5500)),\n",
              "  (2.2991810732841493, tensor(10.8300)),\n",
              "  (2.299182224082947, tensor(11.3700)),\n",
              "  (2.2991542951583863, tensor(11.8800)),\n",
              "  (2.299146908378601, tensor(11.4900)),\n",
              "  (2.299126655769348, tensor(12.6200)),\n",
              "  (2.2991188656806947, tensor(13.5800)),\n",
              "  (2.299092559051514, tensor(14.9800)),\n",
              "  (2.2990794463157656, tensor(12.9800)),\n",
              "  (2.299074317550659, tensor(12.3200)),\n",
              "  (2.2990551461219786, tensor(12.7700)),\n",
              "  (2.299045865726471, tensor(13.8500)),\n",
              "  (2.2990391107559205, tensor(14.8100)),\n",
              "  (2.2990194994926454, tensor(13.5300)),\n",
              "  (2.2990110200881957, tensor(14.4800)),\n",
              "  (2.2989738646507263, tensor(14.4000)),\n",
              "  (2.298973307800293, tensor(12.5400)),\n",
              "  (2.298958744907379, tensor(11.6900)),\n",
              "  (2.2989307302474975, tensor(11.5600)),\n",
              "  (2.2989216388702394, tensor(11.4200)),\n",
              "  (2.298902702331543, tensor(12.7100)),\n",
              "  (2.298894301700592, tensor(11.9400)),\n",
              "  (2.2988861535072327, tensor(12.0100)),\n",
              "  (2.2988766777038574, tensor(10.9700)),\n",
              "  (2.2988654306411744, tensor(11.8000)),\n",
              "  (2.2988590668678284, tensor(12.3700)),\n",
              "  (2.2988276514053343, tensor(14.1200)),\n",
              "  (2.298828514480591, tensor(12.7800)),\n",
              "  (2.2988135683059694, tensor(14.5400)),\n",
              "  (2.298802540397644, tensor(13.9400)),\n",
              "  (2.298803097820282, tensor(14.2500)),\n",
              "  (2.2988007613182067, tensor(14.4000)),\n",
              "  (2.2987665671348574, tensor(14.8300)),\n",
              "  (2.298757398033142, tensor(13.7900)),\n",
              "  (2.298738480949402, tensor(13.5000)),\n",
              "  (2.2987315177917482, tensor(11.7400)),\n",
              "  (2.2987049607276915, tensor(14.3400)),\n",
              "  (2.2987116654396056, tensor(12.8600)),\n",
              "  (2.298691727733612, tensor(12.6000)),\n",
              "  (2.298682458972931, tensor(12.6500)),\n",
              "  (2.2986541843414305, tensor(13.8100)),\n",
              "  (2.298627210330963, tensor(14.9800)),\n",
              "  (2.298623451423645, tensor(15.8100)),\n",
              "  (2.298639158344269, tensor(12.9800)),\n",
              "  (2.2986339853286744, tensor(12.1800)),\n",
              "  (2.298626174449921, tensor(12.4100)),\n",
              "  (2.2986118927001953, tensor(11.9200)),\n",
              "  (2.2985938214302064, tensor(11.9200)),\n",
              "  (2.2985815780639647, tensor(12.3900)),\n",
              "  (2.298551563453674, tensor(13.7100)),\n",
              "  (2.298559639072418, tensor(13.0800)),\n",
              "  (2.298554696750641, tensor(13.4900)),\n",
              "  (2.2985610767364504, tensor(12.4300)),\n",
              "  (2.2985325149536133, tensor(12.7500)),\n",
              "  (2.2985389016151427, tensor(11.4200)),\n",
              "  (2.2985308906555177, tensor(11.0500)),\n",
              "  (2.298497357940674, tensor(12.0400)),\n",
              "  (2.2984873428344725, tensor(10.9400)),\n",
              "  (2.298482968902588, tensor(11.1700)),\n",
              "  (2.298470956802368, tensor(11.6300)),\n",
              "  (2.298449073600769, tensor(12.8800)),\n",
              "  (2.298446329498291, tensor(12.5200)),\n",
              "  (2.298423602294922, tensor(13.2900)),\n",
              "  (2.2984058434486387, tensor(14.8200)),\n",
              "  (2.2983864453315737, tensor(14.6000)),\n",
              "  (2.2983682173728943, tensor(14.8400)),\n",
              "  (2.2983334672927858, tensor(15.2200)),\n",
              "  (2.2983088854789733, tensor(15.1000)),\n",
              "  (2.2982985873222352, tensor(15.7800)),\n",
              "  (2.298289880561829, tensor(14.4700)),\n",
              "  (2.298272254180908, tensor(13.4300)),\n",
              "  (2.298267387294769, tensor(12.3900)),\n",
              "  (2.29824309463501, tensor(13.6300)),\n",
              "  (2.298227210235596, tensor(13.8500)),\n",
              "  (2.298198640060425, tensor(14.2200)),\n",
              "  (2.2981860294342042, tensor(13.6500)),\n",
              "  (2.298159924507141, tensor(13.8800)),\n",
              "  (2.298135499191284, tensor(14.5000)),\n",
              "  (2.2981243633270263, tensor(13.1100)),\n",
              "  (2.2981060688972472, tensor(12.9400)),\n",
              "  (2.298101395988464, tensor(11.7900)),\n",
              "  (2.2981067519187928, tensor(12.0700)),\n",
              "  (2.298095390701294, tensor(11.8900)),\n",
              "  (2.298082712364197, tensor(12.1500)),\n",
              "  (2.2980713871955873, tensor(11.0200)),\n",
              "  (2.2980443995475768, tensor(11.9300)),\n",
              "  (2.2980237131118773, tensor(13.3100)),\n",
              "  (2.298029331970215, tensor(12.5600)),\n",
              "  (2.2980143411636353, tensor(13.2000)),\n",
              "  (2.2980059401512145, tensor(14.1800)),\n",
              "  (2.2979916133880613, tensor(14.1900)),\n",
              "  (2.297957216835022, tensor(13.4000)),\n",
              "  (2.297943338394165, tensor(14.1900)),\n",
              "  (2.297938449859619, tensor(13.1000)),\n",
              "  (2.2979321298599245, tensor(15.1300)),\n",
              "  (2.2979274495124815, tensor(14.3400)),\n",
              "  (2.2979036774635313, tensor(14.7600)),\n",
              "  (2.29787593832016, tensor(14.5400)),\n",
              "  (2.297839643573761, tensor(15.7400)),\n",
              "  (2.2978447403907776, tensor(16.4200)),\n",
              "  (2.297807747364044, tensor(15.2700)),\n",
              "  (2.297790982532501, tensor(15.1500)),\n",
              "  (2.2977740015029906, tensor(13.9600)),\n",
              "  (2.297765876865387, tensor(15.6400)),\n",
              "  (2.297764369392395, tensor(16.8500)),\n",
              "  (2.297751578712463, tensor(16.0400)),\n",
              "  (2.2977481956481935, tensor(14.5500)),\n",
              "  (2.297746876335144, tensor(15.0500)),\n",
              "  (2.297750102043152, tensor(14.0800)),\n",
              "  (2.2977275215148927, tensor(12.7800)),\n",
              "  (2.2976972619056704, tensor(14.3800)),\n",
              "  (2.297688676261902, tensor(14.6400)),\n",
              "  (2.2976579107284545, tensor(16.2300)),\n",
              "  (2.297633054256439, tensor(16.8600)),\n",
              "  (2.2976016282081604, tensor(17.4400)),\n",
              "  (2.2975874552726747, tensor(17.4200)),\n",
              "  (2.297587034225464, tensor(16.5300)),\n",
              "  (2.2975759516716003, tensor(16.4100)),\n",
              "  (2.2975595336914063, tensor(16.3400)),\n",
              "  (2.2975523074150086, tensor(15.6900)),\n",
              "  (2.297543105220795, tensor(16.1100)),\n",
              "  (2.2975178589820864, tensor(15.9600)),\n",
              "  (2.2975177147865296, tensor(14.8200)),\n",
              "  (2.297477626609802, tensor(15.9600)),\n",
              "  (2.2974536890029906, tensor(16.3500)),\n",
              "  (2.2974498439788817, tensor(16.4200)),\n",
              "  (2.2974385094642638, tensor(16.0700)),\n",
              "  (2.2974088311195375, tensor(16.4300)),\n",
              "  (2.297388625049591, tensor(16.3100)),\n",
              "  (2.2973818342208863, tensor(15.7600)),\n",
              "  (2.2973765891075133, tensor(16.0600)),\n",
              "  (2.2973744866371155, tensor(13.8200)),\n",
              "  (2.29735748462677, tensor(13.3700))]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    }
  ]
}