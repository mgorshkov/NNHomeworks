{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "mnist_mlp.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "12d955c72ab0446fb3d9ed6c23095cfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4e1bdc451cf04ba2906520850496ac31",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7dd872c094d84fe9a6464eab9cec84ca",
              "IPY_MODEL_49fb7fa74fe8455aad7ed19573b0403e"
            ]
          }
        },
        "4e1bdc451cf04ba2906520850496ac31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7dd872c094d84fe9a6464eab9cec84ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a027dff7cb3e4a1ca0d9d84c52db3e97",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_749aa13a19fb45d2bb43fc51fbe003c9"
          }
        },
        "49fb7fa74fe8455aad7ed19573b0403e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0b6b33edd132427eb45f836086808be8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 9920512/? [00:20&lt;00:00, 1154046.23it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f35242bb51b445ac9c464cef822c6436"
          }
        },
        "a027dff7cb3e4a1ca0d9d84c52db3e97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "749aa13a19fb45d2bb43fc51fbe003c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0b6b33edd132427eb45f836086808be8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f35242bb51b445ac9c464cef822c6436": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "777bebec95624f62ac2c575f191cd293": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_372504627a16414782a329011f0cf2cb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b9fc8f6b0cbf4a3b84420415b06d0ca8",
              "IPY_MODEL_86957f5f9c684fada8057b666f7f3d04"
            ]
          }
        },
        "372504627a16414782a329011f0cf2cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b9fc8f6b0cbf4a3b84420415b06d0ca8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d8e9f9247e4f471cbb8a1ce4c728a032",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_18f7b11c7ffe495ca8eb36ce95176c6c"
          }
        },
        "86957f5f9c684fada8057b666f7f3d04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_dff22b5a71d34b7ca981b360048b24cb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 32768/? [00:01&lt;00:00, 27742.80it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0d45989f94e545cf93c0cb24fee8de6f"
          }
        },
        "d8e9f9247e4f471cbb8a1ce4c728a032": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "18f7b11c7ffe495ca8eb36ce95176c6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dff22b5a71d34b7ca981b360048b24cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0d45989f94e545cf93c0cb24fee8de6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d189a545fc16473dbd50ce7c38f0cafa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_04e891f7af284441b5a77b2348c82c8f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bb219c74ec884628a1ed82f4eba69e71",
              "IPY_MODEL_883d7c344b794438ad5f843c7156bce2"
            ]
          }
        },
        "04e891f7af284441b5a77b2348c82c8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bb219c74ec884628a1ed82f4eba69e71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9f016105b0dc485b977bf64f0b593782",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cc51b2b01cc34fa59e32d01781e3f415"
          }
        },
        "883d7c344b794438ad5f843c7156bce2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_298a0f0ce63a472cb5eb239ac0b048e2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1654784/? [00:00&lt;00:00, 1796617.39it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_345dbf4949a34d9b8e9a68a1261abe54"
          }
        },
        "9f016105b0dc485b977bf64f0b593782": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cc51b2b01cc34fa59e32d01781e3f415": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "298a0f0ce63a472cb5eb239ac0b048e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "345dbf4949a34d9b8e9a68a1261abe54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6cac0ecaca134f0bba51113261c5b58a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_51a9370aa95249c5b9cafaafb6ddf7ba",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_879a7d7fd1cf487d9898cf128bc2f493",
              "IPY_MODEL_2c93b1f83577404891c2cd8286ecf200"
            ]
          }
        },
        "51a9370aa95249c5b9cafaafb6ddf7ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "879a7d7fd1cf487d9898cf128bc2f493": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_58827964d9064efda6b67a6cac4cb8c7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e1858f7292af414ba179aa3a86a57afb"
          }
        },
        "2c93b1f83577404891c2cd8286ecf200": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cdfaacdc03044cc58a82d32314c0b552",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 8192/? [00:00&lt;00:00, 29494.98it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d7a395d94ab84b8b8eb533b9f59bdd71"
          }
        },
        "58827964d9064efda6b67a6cac4cb8c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e1858f7292af414ba179aa3a86a57afb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cdfaacdc03044cc58a82d32314c0b552": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d7a395d94ab84b8b8eb533b9f59bdd71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw7SPqRCbLXf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qz6iMBeybLXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.1307,), (0.3081,)),\n",
        "           ])\n",
        "\n",
        "def mnist(batch_size=50, valid=0, shuffle=True, transform=mnist_transform, path='./MNIST_data'):\n",
        "    test_data = datasets.MNIST(path, train=False, download=True, transform=transform)\n",
        "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "    \n",
        "    train_data = datasets.MNIST(path, train=True, download=True, transform=transform)\n",
        "    train_data.data = train_data.data[:1000,:,:]\n",
        "    if valid > 0:\n",
        "        num_train = len(train_data)\n",
        "        indices = list(range(num_train))\n",
        "        split = num_train-valid\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        train_idx, valid_idx = indices[:split], indices[split:]\n",
        "        train_sampler = SubsetRandomSampler(train_idx)\n",
        "        valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "        train_loader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n",
        "        valid_loader = DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler)\n",
        "    \n",
        "        return train_loader, valid_loader, test_loader\n",
        "    else:\n",
        "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=shuffle)\n",
        "        return train_loader, test_loader\n",
        "\n",
        "\n",
        "def plot_mnist(images, shape):\n",
        "    fig = plt.figure(figsize=shape[::-1], dpi=80)\n",
        "    for j in range(1, len(images) + 1):\n",
        "        ax = fig.add_subplot(shape[0], shape[1], j)\n",
        "        ax.matshow(images[j - 1, 0, :, :], cmap = matplotlib.cm.binary)\n",
        "        plt.xticks(np.array([]))\n",
        "        plt.yticks(np.array([]))\n",
        "    plt.show()\n",
        "    \n",
        "def plot_graphs(log, tpe='loss'):\n",
        "    keys = log.keys()\n",
        "    logs = {k:[z for z in zip(*log[k])] for k in keys}\n",
        "    epochs = {k:range(len(log[k])) for k in keys}\n",
        "    \n",
        "    if tpe == 'loss':\n",
        "        handlers, = zip(*[plt.plot(epochs[k], logs[k][0], label=k) for k in keys])\n",
        "        plt.title('errors')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.ylabel('error')\n",
        "        plt.legend(handles=handlers)\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "    elif tpe == 'accuracy':\n",
        "        handlers, = zip(*[plt.plot(epochs[k], logs[k][1], label=k) for k in log.keys()])\n",
        "        plt.title('accuracy')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.ylabel('accuracy')\n",
        "        plt.legend(handles=handlers)\n",
        "        plt.grid()\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DhDm90RNw8_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403,
          "referenced_widgets": [
            "12d955c72ab0446fb3d9ed6c23095cfc",
            "4e1bdc451cf04ba2906520850496ac31",
            "7dd872c094d84fe9a6464eab9cec84ca",
            "49fb7fa74fe8455aad7ed19573b0403e",
            "a027dff7cb3e4a1ca0d9d84c52db3e97",
            "749aa13a19fb45d2bb43fc51fbe003c9",
            "0b6b33edd132427eb45f836086808be8",
            "f35242bb51b445ac9c464cef822c6436",
            "777bebec95624f62ac2c575f191cd293",
            "372504627a16414782a329011f0cf2cb",
            "b9fc8f6b0cbf4a3b84420415b06d0ca8",
            "86957f5f9c684fada8057b666f7f3d04",
            "d8e9f9247e4f471cbb8a1ce4c728a032",
            "18f7b11c7ffe495ca8eb36ce95176c6c",
            "dff22b5a71d34b7ca981b360048b24cb",
            "0d45989f94e545cf93c0cb24fee8de6f",
            "d189a545fc16473dbd50ce7c38f0cafa",
            "04e891f7af284441b5a77b2348c82c8f",
            "bb219c74ec884628a1ed82f4eba69e71",
            "883d7c344b794438ad5f843c7156bce2",
            "9f016105b0dc485b977bf64f0b593782",
            "cc51b2b01cc34fa59e32d01781e3f415",
            "298a0f0ce63a472cb5eb239ac0b048e2",
            "345dbf4949a34d9b8e9a68a1261abe54",
            "6cac0ecaca134f0bba51113261c5b58a",
            "51a9370aa95249c5b9cafaafb6ddf7ba",
            "879a7d7fd1cf487d9898cf128bc2f493",
            "2c93b1f83577404891c2cd8286ecf200",
            "58827964d9064efda6b67a6cac4cb8c7",
            "e1858f7292af414ba179aa3a86a57afb",
            "cdfaacdc03044cc58a82d32314c0b552",
            "d7a395d94ab84b8b8eb533b9f59bdd71"
          ]
        },
        "outputId": "15c96f54-b5f7-4697-8d79-c125a3f3c3e3"
      },
      "source": [
        "train_loader, test_loader = mnist(1)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12d955c72ab0446fb3d9ed6c23095cfc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST_data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "777bebec95624f62ac2c575f191cd293",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST_data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d189a545fc16473dbd50ce7c38f0cafa",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST_data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6cac0ecaca134f0bba51113261c5b58a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST_data/MNIST/raw\n",
            "Processing...\n",
            "\n",
            "\n",
            "\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0jvvDrjbLXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, log_softmax=False):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 128)\n",
        "        self.fc2 = nn.Linear(128, 256)\n",
        "        self.fc3 = nn.Linear(256, 512)\n",
        "        self.fc4 = nn.Linear(512, 1024)\n",
        "        self.fc5 = nn.Linear(1024, 10)\n",
        "        self.log_softmax = log_softmax\n",
        "        self.optim = optim.SGD(self.parameters(), lr=1e-4)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = torch.sigmoid(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = torch.relu(self.fc4(x))\n",
        "        x = self.fc5(x)\n",
        "        if self.log_softmax:\n",
        "            x = F.log_softmax(x, dim=1)\n",
        "        else:\n",
        "            x = torch.log(F.softmax(x, dim=1))\n",
        "        return x\n",
        "    \n",
        "    def loss(self, output, target, **kwargs):\n",
        "        self._loss = F.nll_loss(output, target, **kwargs)\n",
        "        return self._loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANxuIpOkbLXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epoch, models):\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        for name, model in models.items():\n",
        "            model.optim.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = model.loss(output, target)\n",
        "            loss.backward()\n",
        "            model.optim.step()\n",
        "            \n",
        "        if batch_idx % 200 == 0:\n",
        "            line = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLosses '.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader))\n",
        "            losses = ' '.join(['{}: {:.6f}'.format(k, m._loss.item()) for k, m in models.items()])\n",
        "            print(line + losses)\n",
        "            \n",
        "    else:\n",
        "        batch_idx += 1\n",
        "        line = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLosses '.format(\n",
        "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "            100. * batch_idx / len(train_loader))\n",
        "        losses = ' '.join(['{}: {:.6f}'.format(k, m._loss.item()) for k, m in models.items()])\n",
        "        print(line + losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbrbWpJNbLXy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models = {'F.softmax': Net(), 'log_softmax': Net(True)}\n",
        "test_log = {k: [] for k in models}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-2EpiW9bLX0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "avg_lambda = lambda l: 'Loss: {:.4f}'.format(l)\n",
        "acc_lambda = lambda c, p: 'Accuracy: {}/{} ({:.0f}%)'.format(c, len(test_loader.dataset), p)\n",
        "line = lambda i, l, c, p: '{}: '.format(i) + avg_lambda(l) + '\\t' + acc_lambda(c, p)\n",
        "\n",
        "def test(models, log=None):\n",
        "    test_size = len(test_loader.sampler)\n",
        "    test_loss = {k: 0. for k in models}\n",
        "    correct = {k: 0. for k in models}\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            for k, m in models.items():\n",
        "                output = m(data)\n",
        "                test_loss[k] += m.loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "                pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "                correct[k] += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "    \n",
        "    for k in models.keys():\n",
        "        test_loss[k] /= test_size\n",
        "    correct_pct = {k: 100. * correct[k] / test_size for k in correct}\n",
        "    lines = '\\n'.join([line(k, test_loss[k], correct[k], correct_pct[k]) for k in models]) + '\\n'\n",
        "    report = 'Test set:\\n' + lines\n",
        "    if log is not None:\n",
        "        for k in models.keys():\n",
        "            log[k].append((test_loss[k], correct_pct[k]))\n",
        "    print(report)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hp2nSaCjbLX2",
        "colab_type": "code",
        "outputId": "0a70ae00-161f-4174-c186-212094940805",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(1, 501):\n",
        "    train(epoch, models)\n",
        "    test(models, test_log)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Train Epoch: 1 [200/1000 (20%)]\tLosses F.softmax: 0.099362 log_softmax: 0.102697\n",
            "Train Epoch: 1 [400/1000 (40%)]\tLosses F.softmax: 0.004183 log_softmax: 0.013048\n",
            "Train Epoch: 1 [600/1000 (60%)]\tLosses F.softmax: 0.011280 log_softmax: 0.017038\n",
            "Train Epoch: 1 [800/1000 (80%)]\tLosses F.softmax: 0.006026 log_softmax: 0.000144\n",
            "Train Epoch: 1 [1000/1000 (100%)]\tLosses F.softmax: 0.193056 log_softmax: 0.096496\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8906\tAccuracy: 8076.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9018\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 2 [0/1000 (0%)]\tLosses F.softmax: 0.000044 log_softmax: 0.000087\n",
            "Train Epoch: 2 [200/1000 (20%)]\tLosses F.softmax: 0.026522 log_softmax: 0.009843\n",
            "Train Epoch: 2 [400/1000 (40%)]\tLosses F.softmax: 0.000680 log_softmax: 0.001617\n",
            "Train Epoch: 2 [600/1000 (60%)]\tLosses F.softmax: 0.000694 log_softmax: 0.000109\n",
            "Train Epoch: 2 [800/1000 (80%)]\tLosses F.softmax: 0.004629 log_softmax: 0.008259\n",
            "Train Epoch: 2 [1000/1000 (100%)]\tLosses F.softmax: 0.123746 log_softmax: 0.094733\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8932\tAccuracy: 8077.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9037\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 3 [0/1000 (0%)]\tLosses F.softmax: 0.000506 log_softmax: 0.000080\n",
            "Train Epoch: 3 [200/1000 (20%)]\tLosses F.softmax: 0.070195 log_softmax: 0.049372\n",
            "Train Epoch: 3 [400/1000 (40%)]\tLosses F.softmax: 0.004707 log_softmax: 0.004388\n",
            "Train Epoch: 3 [600/1000 (60%)]\tLosses F.softmax: 0.086689 log_softmax: 0.046175\n",
            "Train Epoch: 3 [800/1000 (80%)]\tLosses F.softmax: 0.001179 log_softmax: 0.003598\n",
            "Train Epoch: 3 [1000/1000 (100%)]\tLosses F.softmax: 0.000024 log_softmax: 0.000475\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8958\tAccuracy: 8059.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9065\tAccuracy: 8029.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 4 [0/1000 (0%)]\tLosses F.softmax: 0.006383 log_softmax: 0.005679\n",
            "Train Epoch: 4 [200/1000 (20%)]\tLosses F.softmax: 0.000160 log_softmax: 0.000109\n",
            "Train Epoch: 4 [400/1000 (40%)]\tLosses F.softmax: 0.001952 log_softmax: 0.000103\n",
            "Train Epoch: 4 [600/1000 (60%)]\tLosses F.softmax: 0.005112 log_softmax: 0.019831\n",
            "Train Epoch: 4 [800/1000 (80%)]\tLosses F.softmax: 0.031320 log_softmax: 0.058850\n",
            "Train Epoch: 4 [1000/1000 (100%)]\tLosses F.softmax: 0.470724 log_softmax: 0.269970\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8956\tAccuracy: 8070.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9061\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 5 [0/1000 (0%)]\tLosses F.softmax: 0.198946 log_softmax: 0.056885\n",
            "Train Epoch: 5 [200/1000 (20%)]\tLosses F.softmax: 0.079490 log_softmax: 0.111122\n",
            "Train Epoch: 5 [400/1000 (40%)]\tLosses F.softmax: 0.009068 log_softmax: 0.124946\n",
            "Train Epoch: 5 [600/1000 (60%)]\tLosses F.softmax: 0.003887 log_softmax: 0.001738\n",
            "Train Epoch: 5 [800/1000 (80%)]\tLosses F.softmax: 0.141017 log_softmax: 0.054669\n",
            "Train Epoch: 5 [1000/1000 (100%)]\tLosses F.softmax: 0.029298 log_softmax: 0.000642\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8965\tAccuracy: 8066.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9085\tAccuracy: 8046.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 6 [0/1000 (0%)]\tLosses F.softmax: 0.001350 log_softmax: 0.002398\n",
            "Train Epoch: 6 [200/1000 (20%)]\tLosses F.softmax: 0.111423 log_softmax: 0.096602\n",
            "Train Epoch: 6 [400/1000 (40%)]\tLosses F.softmax: 0.023341 log_softmax: 0.024489\n",
            "Train Epoch: 6 [600/1000 (60%)]\tLosses F.softmax: 0.010891 log_softmax: 0.027843\n",
            "Train Epoch: 6 [800/1000 (80%)]\tLosses F.softmax: 0.011242 log_softmax: 0.040606\n",
            "Train Epoch: 6 [1000/1000 (100%)]\tLosses F.softmax: 0.000213 log_softmax: 0.000073\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8969\tAccuracy: 8073.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9079\tAccuracy: 8050.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 7 [0/1000 (0%)]\tLosses F.softmax: 0.008853 log_softmax: 0.002258\n",
            "Train Epoch: 7 [200/1000 (20%)]\tLosses F.softmax: 0.001419 log_softmax: 0.002764\n",
            "Train Epoch: 7 [400/1000 (40%)]\tLosses F.softmax: 0.001613 log_softmax: 0.000747\n",
            "Train Epoch: 7 [600/1000 (60%)]\tLosses F.softmax: 0.000119 log_softmax: 0.000162\n",
            "Train Epoch: 7 [800/1000 (80%)]\tLosses F.softmax: 0.007141 log_softmax: 0.004627\n",
            "Train Epoch: 7 [1000/1000 (100%)]\tLosses F.softmax: 0.037130 log_softmax: 0.067704\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8970\tAccuracy: 8073.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9092\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 8 [0/1000 (0%)]\tLosses F.softmax: 0.000130 log_softmax: 0.000058\n",
            "Train Epoch: 8 [200/1000 (20%)]\tLosses F.softmax: 0.000226 log_softmax: 0.000146\n",
            "Train Epoch: 8 [400/1000 (40%)]\tLosses F.softmax: 0.070246 log_softmax: 0.037175\n",
            "Train Epoch: 8 [600/1000 (60%)]\tLosses F.softmax: 0.004507 log_softmax: 0.010301\n",
            "Train Epoch: 8 [800/1000 (80%)]\tLosses F.softmax: 0.310296 log_softmax: 0.282298\n",
            "Train Epoch: 8 [1000/1000 (100%)]\tLosses F.softmax: 0.002769 log_softmax: 0.010368\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8987\tAccuracy: 8078.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9109\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 9 [0/1000 (0%)]\tLosses F.softmax: 0.020524 log_softmax: 0.039203\n",
            "Train Epoch: 9 [200/1000 (20%)]\tLosses F.softmax: 0.007712 log_softmax: 0.042081\n",
            "Train Epoch: 9 [400/1000 (40%)]\tLosses F.softmax: 0.054856 log_softmax: 0.066055\n",
            "Train Epoch: 9 [600/1000 (60%)]\tLosses F.softmax: 0.074594 log_softmax: 0.090893\n",
            "Train Epoch: 9 [800/1000 (80%)]\tLosses F.softmax: 0.207205 log_softmax: 0.385925\n",
            "Train Epoch: 9 [1000/1000 (100%)]\tLosses F.softmax: 0.000027 log_softmax: 0.000009\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8988\tAccuracy: 8073.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9107\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 10 [0/1000 (0%)]\tLosses F.softmax: 0.000958 log_softmax: 0.000946\n",
            "Train Epoch: 10 [200/1000 (20%)]\tLosses F.softmax: 0.017488 log_softmax: 0.017416\n",
            "Train Epoch: 10 [400/1000 (40%)]\tLosses F.softmax: 0.000161 log_softmax: 0.000023\n",
            "Train Epoch: 10 [600/1000 (60%)]\tLosses F.softmax: 0.054307 log_softmax: 0.028769\n",
            "Train Epoch: 10 [800/1000 (80%)]\tLosses F.softmax: 0.001757 log_softmax: 0.000422\n",
            "Train Epoch: 10 [1000/1000 (100%)]\tLosses F.softmax: 0.011516 log_softmax: 0.001123\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9025\tAccuracy: 8064.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9137\tAccuracy: 8040.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 11 [0/1000 (0%)]\tLosses F.softmax: 0.005100 log_softmax: 0.009932\n",
            "Train Epoch: 11 [200/1000 (20%)]\tLosses F.softmax: 0.008381 log_softmax: 0.008778\n",
            "Train Epoch: 11 [400/1000 (40%)]\tLosses F.softmax: 0.013412 log_softmax: 0.031926\n",
            "Train Epoch: 11 [600/1000 (60%)]\tLosses F.softmax: 0.000298 log_softmax: 0.000693\n",
            "Train Epoch: 11 [800/1000 (80%)]\tLosses F.softmax: 0.030937 log_softmax: 0.072317\n",
            "Train Epoch: 11 [1000/1000 (100%)]\tLosses F.softmax: 0.001338 log_softmax: 0.002018\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9016\tAccuracy: 8074.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9136\tAccuracy: 8065.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 12 [0/1000 (0%)]\tLosses F.softmax: 0.010458 log_softmax: 0.007269\n",
            "Train Epoch: 12 [200/1000 (20%)]\tLosses F.softmax: 0.000736 log_softmax: 0.000930\n",
            "Train Epoch: 12 [400/1000 (40%)]\tLosses F.softmax: 0.000097 log_softmax: 0.000015\n",
            "Train Epoch: 12 [600/1000 (60%)]\tLosses F.softmax: 0.000935 log_softmax: 0.003321\n",
            "Train Epoch: 12 [800/1000 (80%)]\tLosses F.softmax: 0.000074 log_softmax: 0.000027\n",
            "Train Epoch: 12 [1000/1000 (100%)]\tLosses F.softmax: 0.000020 log_softmax: 0.000836\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9039\tAccuracy: 8079.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9150\tAccuracy: 8057.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 13 [0/1000 (0%)]\tLosses F.softmax: 0.092586 log_softmax: 0.076867\n",
            "Train Epoch: 13 [200/1000 (20%)]\tLosses F.softmax: 0.010828 log_softmax: 0.009181\n",
            "Train Epoch: 13 [400/1000 (40%)]\tLosses F.softmax: 0.008598 log_softmax: 0.012882\n",
            "Train Epoch: 13 [600/1000 (60%)]\tLosses F.softmax: 0.003782 log_softmax: 0.007319\n",
            "Train Epoch: 13 [800/1000 (80%)]\tLosses F.softmax: 0.008471 log_softmax: 0.011108\n",
            "Train Epoch: 13 [1000/1000 (100%)]\tLosses F.softmax: 0.000079 log_softmax: 0.000158\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9038\tAccuracy: 8072.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9175\tAccuracy: 8040.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 14 [0/1000 (0%)]\tLosses F.softmax: 0.001082 log_softmax: 0.000216\n",
            "Train Epoch: 14 [200/1000 (20%)]\tLosses F.softmax: 0.109279 log_softmax: 0.050861\n",
            "Train Epoch: 14 [400/1000 (40%)]\tLosses F.softmax: 0.007292 log_softmax: 0.002185\n",
            "Train Epoch: 14 [600/1000 (60%)]\tLosses F.softmax: 0.000174 log_softmax: 0.000134\n",
            "Train Epoch: 14 [800/1000 (80%)]\tLosses F.softmax: 0.000316 log_softmax: 0.000306\n",
            "Train Epoch: 14 [1000/1000 (100%)]\tLosses F.softmax: 0.160190 log_softmax: 0.079403\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9051\tAccuracy: 8078.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9171\tAccuracy: 8060.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 15 [0/1000 (0%)]\tLosses F.softmax: 0.000580 log_softmax: 0.001836\n",
            "Train Epoch: 15 [200/1000 (20%)]\tLosses F.softmax: 0.022607 log_softmax: 0.013712\n",
            "Train Epoch: 15 [400/1000 (40%)]\tLosses F.softmax: 0.008379 log_softmax: 0.011761\n",
            "Train Epoch: 15 [600/1000 (60%)]\tLosses F.softmax: 0.001965 log_softmax: 0.000429\n",
            "Train Epoch: 15 [800/1000 (80%)]\tLosses F.softmax: 0.019569 log_softmax: 0.000793\n",
            "Train Epoch: 15 [1000/1000 (100%)]\tLosses F.softmax: 0.000046 log_softmax: 0.000410\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9069\tAccuracy: 8071.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9196\tAccuracy: 8048.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 16 [0/1000 (0%)]\tLosses F.softmax: 0.000588 log_softmax: 0.000447\n",
            "Train Epoch: 16 [200/1000 (20%)]\tLosses F.softmax: 0.035515 log_softmax: 0.062629\n",
            "Train Epoch: 16 [400/1000 (40%)]\tLosses F.softmax: 0.006329 log_softmax: 0.011162\n",
            "Train Epoch: 16 [600/1000 (60%)]\tLosses F.softmax: 0.330925 log_softmax: 0.120952\n",
            "Train Epoch: 16 [800/1000 (80%)]\tLosses F.softmax: 0.000072 log_softmax: 0.000812\n",
            "Train Epoch: 16 [1000/1000 (100%)]\tLosses F.softmax: 0.003868 log_softmax: 0.001228\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9069\tAccuracy: 8083.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9190\tAccuracy: 8068.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 17 [0/1000 (0%)]\tLosses F.softmax: 0.059102 log_softmax: 0.028590\n",
            "Train Epoch: 17 [200/1000 (20%)]\tLosses F.softmax: 0.000310 log_softmax: 0.000313\n",
            "Train Epoch: 17 [400/1000 (40%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000002\n",
            "Train Epoch: 17 [600/1000 (60%)]\tLosses F.softmax: 0.007688 log_softmax: 0.023418\n",
            "Train Epoch: 17 [800/1000 (80%)]\tLosses F.softmax: 0.027644 log_softmax: 0.006250\n",
            "Train Epoch: 17 [1000/1000 (100%)]\tLosses F.softmax: 0.003541 log_softmax: 0.000501\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9094\tAccuracy: 8071.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9211\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 18 [0/1000 (0%)]\tLosses F.softmax: 0.000008 log_softmax: 0.000008\n",
            "Train Epoch: 18 [200/1000 (20%)]\tLosses F.softmax: 0.117782 log_softmax: 0.074062\n",
            "Train Epoch: 18 [400/1000 (40%)]\tLosses F.softmax: 0.051534 log_softmax: 0.038627\n",
            "Train Epoch: 18 [600/1000 (60%)]\tLosses F.softmax: 0.001143 log_softmax: 0.005694\n",
            "Train Epoch: 18 [800/1000 (80%)]\tLosses F.softmax: 0.095886 log_softmax: 0.081999\n",
            "Train Epoch: 18 [1000/1000 (100%)]\tLosses F.softmax: 0.003252 log_softmax: 0.004681\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9088\tAccuracy: 8079.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9216\tAccuracy: 8070.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 19 [0/1000 (0%)]\tLosses F.softmax: 0.134211 log_softmax: 0.158382\n",
            "Train Epoch: 19 [200/1000 (20%)]\tLosses F.softmax: 0.039795 log_softmax: 0.020566\n",
            "Train Epoch: 19 [400/1000 (40%)]\tLosses F.softmax: 0.001202 log_softmax: 0.000135\n",
            "Train Epoch: 19 [600/1000 (60%)]\tLosses F.softmax: 0.005024 log_softmax: 0.017194\n",
            "Train Epoch: 19 [800/1000 (80%)]\tLosses F.softmax: 0.000505 log_softmax: 0.001973\n",
            "Train Epoch: 19 [1000/1000 (100%)]\tLosses F.softmax: 0.036660 log_softmax: 0.050970\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9128\tAccuracy: 8067.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9236\tAccuracy: 8051.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 20 [0/1000 (0%)]\tLosses F.softmax: 0.000432 log_softmax: 0.000763\n",
            "Train Epoch: 20 [200/1000 (20%)]\tLosses F.softmax: 0.002464 log_softmax: 0.001188\n",
            "Train Epoch: 20 [400/1000 (40%)]\tLosses F.softmax: 0.000687 log_softmax: 0.000419\n",
            "Train Epoch: 20 [600/1000 (60%)]\tLosses F.softmax: 0.010070 log_softmax: 0.000639\n",
            "Train Epoch: 20 [800/1000 (80%)]\tLosses F.softmax: 0.000644 log_softmax: 0.004302\n",
            "Train Epoch: 20 [1000/1000 (100%)]\tLosses F.softmax: 0.000786 log_softmax: 0.002984\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9108\tAccuracy: 8075.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9239\tAccuracy: 8057.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 21 [0/1000 (0%)]\tLosses F.softmax: 0.001933 log_softmax: 0.000261\n",
            "Train Epoch: 21 [200/1000 (20%)]\tLosses F.softmax: 0.000084 log_softmax: 0.000017\n",
            "Train Epoch: 21 [400/1000 (40%)]\tLosses F.softmax: 0.019397 log_softmax: 0.006609\n",
            "Train Epoch: 21 [600/1000 (60%)]\tLosses F.softmax: 0.000136 log_softmax: 0.000069\n",
            "Train Epoch: 21 [800/1000 (80%)]\tLosses F.softmax: 0.023678 log_softmax: 0.014607\n",
            "Train Epoch: 21 [1000/1000 (100%)]\tLosses F.softmax: 0.035304 log_softmax: 0.059239\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9135\tAccuracy: 8072.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9264\tAccuracy: 8037.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 22 [0/1000 (0%)]\tLosses F.softmax: 0.003506 log_softmax: 0.000582\n",
            "Train Epoch: 22 [200/1000 (20%)]\tLosses F.softmax: 0.056045 log_softmax: 0.039846\n",
            "Train Epoch: 22 [400/1000 (40%)]\tLosses F.softmax: 0.008707 log_softmax: 0.019372\n",
            "Train Epoch: 22 [600/1000 (60%)]\tLosses F.softmax: 0.113956 log_softmax: 0.101734\n",
            "Train Epoch: 22 [800/1000 (80%)]\tLosses F.softmax: 0.001386 log_softmax: 0.000072\n",
            "Train Epoch: 22 [1000/1000 (100%)]\tLosses F.softmax: 0.029971 log_softmax: 0.034725\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9147\tAccuracy: 8073.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9268\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 23 [0/1000 (0%)]\tLosses F.softmax: 0.020334 log_softmax: 0.053043\n",
            "Train Epoch: 23 [200/1000 (20%)]\tLosses F.softmax: 0.000147 log_softmax: 0.000019\n",
            "Train Epoch: 23 [400/1000 (40%)]\tLosses F.softmax: 0.000727 log_softmax: 0.000898\n",
            "Train Epoch: 23 [600/1000 (60%)]\tLosses F.softmax: 0.006815 log_softmax: 0.005333\n",
            "Train Epoch: 23 [800/1000 (80%)]\tLosses F.softmax: 0.093430 log_softmax: 0.063359\n",
            "Train Epoch: 23 [1000/1000 (100%)]\tLosses F.softmax: 0.001916 log_softmax: 0.002875\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9164\tAccuracy: 8069.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9291\tAccuracy: 8036.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 24 [0/1000 (0%)]\tLosses F.softmax: 0.003842 log_softmax: 0.001253\n",
            "Train Epoch: 24 [200/1000 (20%)]\tLosses F.softmax: 0.013560 log_softmax: 0.007074\n",
            "Train Epoch: 24 [400/1000 (40%)]\tLosses F.softmax: 0.003999 log_softmax: 0.010240\n",
            "Train Epoch: 24 [600/1000 (60%)]\tLosses F.softmax: 0.061742 log_softmax: 0.113449\n",
            "Train Epoch: 24 [800/1000 (80%)]\tLosses F.softmax: 0.007834 log_softmax: 0.009890\n",
            "Train Epoch: 24 [1000/1000 (100%)]\tLosses F.softmax: 0.003287 log_softmax: 0.002850\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9143\tAccuracy: 8079.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9271\tAccuracy: 8067.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 25 [0/1000 (0%)]\tLosses F.softmax: 0.000098 log_softmax: 0.016896\n",
            "Train Epoch: 25 [200/1000 (20%)]\tLosses F.softmax: 0.001358 log_softmax: 0.001347\n",
            "Train Epoch: 25 [400/1000 (40%)]\tLosses F.softmax: 0.000018 log_softmax: 0.000431\n",
            "Train Epoch: 25 [600/1000 (60%)]\tLosses F.softmax: 0.196946 log_softmax: 0.228713\n",
            "Train Epoch: 25 [800/1000 (80%)]\tLosses F.softmax: 0.001737 log_softmax: 0.002935\n",
            "Train Epoch: 25 [1000/1000 (100%)]\tLosses F.softmax: 0.009896 log_softmax: 0.007113\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9165\tAccuracy: 8078.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9298\tAccuracy: 8064.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 26 [0/1000 (0%)]\tLosses F.softmax: 0.200390 log_softmax: 0.240117\n",
            "Train Epoch: 26 [200/1000 (20%)]\tLosses F.softmax: 0.002894 log_softmax: 0.000265\n",
            "Train Epoch: 26 [400/1000 (40%)]\tLosses F.softmax: 0.047296 log_softmax: 0.039615\n",
            "Train Epoch: 26 [600/1000 (60%)]\tLosses F.softmax: 0.023035 log_softmax: 0.041352\n",
            "Train Epoch: 26 [800/1000 (80%)]\tLosses F.softmax: 0.052237 log_softmax: 0.048284\n",
            "Train Epoch: 26 [1000/1000 (100%)]\tLosses F.softmax: 0.001437 log_softmax: 0.002181\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9165\tAccuracy: 8077.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9294\tAccuracy: 8062.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 27 [0/1000 (0%)]\tLosses F.softmax: 0.005586 log_softmax: 0.001525\n",
            "Train Epoch: 27 [200/1000 (20%)]\tLosses F.softmax: 0.000016 log_softmax: 0.000002\n",
            "Train Epoch: 27 [400/1000 (40%)]\tLosses F.softmax: 0.090913 log_softmax: 0.040865\n",
            "Train Epoch: 27 [600/1000 (60%)]\tLosses F.softmax: 0.001472 log_softmax: 0.001585\n",
            "Train Epoch: 27 [800/1000 (80%)]\tLosses F.softmax: 0.002749 log_softmax: 0.004825\n",
            "Train Epoch: 27 [1000/1000 (100%)]\tLosses F.softmax: 0.002527 log_softmax: 0.003100\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9170\tAccuracy: 8080.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9299\tAccuracy: 8063.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 28 [0/1000 (0%)]\tLosses F.softmax: 0.042995 log_softmax: 0.046589\n",
            "Train Epoch: 28 [200/1000 (20%)]\tLosses F.softmax: 0.000557 log_softmax: 0.000726\n",
            "Train Epoch: 28 [400/1000 (40%)]\tLosses F.softmax: 0.040649 log_softmax: 0.015908\n",
            "Train Epoch: 28 [600/1000 (60%)]\tLosses F.softmax: 0.000549 log_softmax: 0.000155\n",
            "Train Epoch: 28 [800/1000 (80%)]\tLosses F.softmax: 0.053107 log_softmax: 0.020812\n",
            "Train Epoch: 28 [1000/1000 (100%)]\tLosses F.softmax: 0.000366 log_softmax: 0.000219\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9196\tAccuracy: 8080.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9321\tAccuracy: 8062.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 29 [0/1000 (0%)]\tLosses F.softmax: 0.003638 log_softmax: 0.000155\n",
            "Train Epoch: 29 [200/1000 (20%)]\tLosses F.softmax: 0.000439 log_softmax: 0.000068\n",
            "Train Epoch: 29 [400/1000 (40%)]\tLosses F.softmax: 0.018001 log_softmax: 0.027569\n",
            "Train Epoch: 29 [600/1000 (60%)]\tLosses F.softmax: 0.039974 log_softmax: 0.025483\n",
            "Train Epoch: 29 [800/1000 (80%)]\tLosses F.softmax: 0.001673 log_softmax: 0.001470\n",
            "Train Epoch: 29 [1000/1000 (100%)]\tLosses F.softmax: 0.000305 log_softmax: 0.005360\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9204\tAccuracy: 8077.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9331\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 30 [0/1000 (0%)]\tLosses F.softmax: 0.000030 log_softmax: 0.000044\n",
            "Train Epoch: 30 [200/1000 (20%)]\tLosses F.softmax: 0.018905 log_softmax: 0.005120\n",
            "Train Epoch: 30 [400/1000 (40%)]\tLosses F.softmax: 0.029014 log_softmax: 0.053690\n",
            "Train Epoch: 30 [600/1000 (60%)]\tLosses F.softmax: 0.047093 log_softmax: 0.030524\n",
            "Train Epoch: 30 [800/1000 (80%)]\tLosses F.softmax: 0.008501 log_softmax: 0.010171\n",
            "Train Epoch: 30 [1000/1000 (100%)]\tLosses F.softmax: 0.000890 log_softmax: 0.003059\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9205\tAccuracy: 8082.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9341\tAccuracy: 8051.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 31 [0/1000 (0%)]\tLosses F.softmax: 0.018984 log_softmax: 0.014608\n",
            "Train Epoch: 31 [200/1000 (20%)]\tLosses F.softmax: 0.010035 log_softmax: 0.009580\n",
            "Train Epoch: 31 [400/1000 (40%)]\tLosses F.softmax: 0.005525 log_softmax: 0.007048\n",
            "Train Epoch: 31 [600/1000 (60%)]\tLosses F.softmax: 0.000070 log_softmax: 0.000023\n",
            "Train Epoch: 31 [800/1000 (80%)]\tLosses F.softmax: 0.164544 log_softmax: 0.189131\n",
            "Train Epoch: 31 [1000/1000 (100%)]\tLosses F.softmax: 0.000093 log_softmax: 0.000040\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9223\tAccuracy: 8075.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9352\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 32 [0/1000 (0%)]\tLosses F.softmax: 0.015591 log_softmax: 0.033472\n",
            "Train Epoch: 32 [200/1000 (20%)]\tLosses F.softmax: 0.010694 log_softmax: 0.007441\n",
            "Train Epoch: 32 [400/1000 (40%)]\tLosses F.softmax: 0.001691 log_softmax: 0.000744\n",
            "Train Epoch: 32 [600/1000 (60%)]\tLosses F.softmax: 0.000258 log_softmax: 0.001583\n",
            "Train Epoch: 32 [800/1000 (80%)]\tLosses F.softmax: 0.000138 log_softmax: 0.000014\n",
            "Train Epoch: 32 [1000/1000 (100%)]\tLosses F.softmax: 0.008013 log_softmax: 0.009893\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9228\tAccuracy: 8082.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9360\tAccuracy: 8062.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 33 [0/1000 (0%)]\tLosses F.softmax: 0.007198 log_softmax: 0.006390\n",
            "Train Epoch: 33 [200/1000 (20%)]\tLosses F.softmax: 0.000600 log_softmax: 0.000169\n",
            "Train Epoch: 33 [400/1000 (40%)]\tLosses F.softmax: 0.020625 log_softmax: 0.038887\n",
            "Train Epoch: 33 [600/1000 (60%)]\tLosses F.softmax: 0.008026 log_softmax: 0.029660\n",
            "Train Epoch: 33 [800/1000 (80%)]\tLosses F.softmax: 0.134403 log_softmax: 0.073062\n",
            "Train Epoch: 33 [1000/1000 (100%)]\tLosses F.softmax: 0.000086 log_softmax: 0.017257\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9248\tAccuracy: 8077.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9376\tAccuracy: 8058.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 34 [0/1000 (0%)]\tLosses F.softmax: 0.011924 log_softmax: 0.000818\n",
            "Train Epoch: 34 [200/1000 (20%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000014\n",
            "Train Epoch: 34 [400/1000 (40%)]\tLosses F.softmax: 0.002925 log_softmax: 0.003272\n",
            "Train Epoch: 34 [600/1000 (60%)]\tLosses F.softmax: 0.010725 log_softmax: 0.012602\n",
            "Train Epoch: 34 [800/1000 (80%)]\tLosses F.softmax: 0.000465 log_softmax: 0.001627\n",
            "Train Epoch: 34 [1000/1000 (100%)]\tLosses F.softmax: 0.007728 log_softmax: 0.005043\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9234\tAccuracy: 8083.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9372\tAccuracy: 8060.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 35 [0/1000 (0%)]\tLosses F.softmax: 0.019730 log_softmax: 0.023332\n",
            "Train Epoch: 35 [200/1000 (20%)]\tLosses F.softmax: 0.026632 log_softmax: 0.023289\n",
            "Train Epoch: 35 [400/1000 (40%)]\tLosses F.softmax: 0.000056 log_softmax: 0.000187\n",
            "Train Epoch: 35 [600/1000 (60%)]\tLosses F.softmax: 0.000027 log_softmax: 0.000017\n",
            "Train Epoch: 35 [800/1000 (80%)]\tLosses F.softmax: 0.000807 log_softmax: 0.001080\n",
            "Train Epoch: 35 [1000/1000 (100%)]\tLosses F.softmax: 0.037442 log_softmax: 0.069636\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9247\tAccuracy: 8080.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9392\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 36 [0/1000 (0%)]\tLosses F.softmax: 0.001087 log_softmax: 0.000316\n",
            "Train Epoch: 36 [200/1000 (20%)]\tLosses F.softmax: 0.005019 log_softmax: 0.013321\n",
            "Train Epoch: 36 [400/1000 (40%)]\tLosses F.softmax: 0.002592 log_softmax: 0.002144\n",
            "Train Epoch: 36 [600/1000 (60%)]\tLosses F.softmax: 0.009323 log_softmax: 0.018054\n",
            "Train Epoch: 36 [800/1000 (80%)]\tLosses F.softmax: 0.000687 log_softmax: 0.001471\n",
            "Train Epoch: 36 [1000/1000 (100%)]\tLosses F.softmax: 0.057377 log_softmax: 0.066562\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9262\tAccuracy: 8082.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9403\tAccuracy: 8065.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 37 [0/1000 (0%)]\tLosses F.softmax: 0.000144 log_softmax: 0.000048\n",
            "Train Epoch: 37 [200/1000 (20%)]\tLosses F.softmax: 0.001211 log_softmax: 0.001967\n",
            "Train Epoch: 37 [400/1000 (40%)]\tLosses F.softmax: 0.001991 log_softmax: 0.000069\n",
            "Train Epoch: 37 [600/1000 (60%)]\tLosses F.softmax: 0.000356 log_softmax: 0.000173\n",
            "Train Epoch: 37 [800/1000 (80%)]\tLosses F.softmax: 0.000997 log_softmax: 0.002859\n",
            "Train Epoch: 37 [1000/1000 (100%)]\tLosses F.softmax: 0.007645 log_softmax: 0.016828\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9283\tAccuracy: 8078.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9423\tAccuracy: 8059.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 38 [0/1000 (0%)]\tLosses F.softmax: 0.000152 log_softmax: 0.000054\n",
            "Train Epoch: 38 [200/1000 (20%)]\tLosses F.softmax: 0.006533 log_softmax: 0.010196\n",
            "Train Epoch: 38 [400/1000 (40%)]\tLosses F.softmax: 0.014457 log_softmax: 0.019624\n",
            "Train Epoch: 38 [600/1000 (60%)]\tLosses F.softmax: 0.062843 log_softmax: 0.080253\n",
            "Train Epoch: 38 [800/1000 (80%)]\tLosses F.softmax: 0.056479 log_softmax: 0.039309\n",
            "Train Epoch: 38 [1000/1000 (100%)]\tLosses F.softmax: 0.003005 log_softmax: 0.000136\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9303\tAccuracy: 8071.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9439\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 39 [0/1000 (0%)]\tLosses F.softmax: 0.000886 log_softmax: 0.001425\n",
            "Train Epoch: 39 [200/1000 (20%)]\tLosses F.softmax: 0.000566 log_softmax: 0.000331\n",
            "Train Epoch: 39 [400/1000 (40%)]\tLosses F.softmax: 0.005857 log_softmax: 0.001260\n",
            "Train Epoch: 39 [600/1000 (60%)]\tLosses F.softmax: 0.015700 log_softmax: 0.007001\n",
            "Train Epoch: 39 [800/1000 (80%)]\tLosses F.softmax: 0.000517 log_softmax: 0.000116\n",
            "Train Epoch: 39 [1000/1000 (100%)]\tLosses F.softmax: 0.015746 log_softmax: 0.004398\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9309\tAccuracy: 8070.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9442\tAccuracy: 8058.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 40 [0/1000 (0%)]\tLosses F.softmax: 0.008602 log_softmax: 0.001417\n",
            "Train Epoch: 40 [200/1000 (20%)]\tLosses F.softmax: 0.002400 log_softmax: 0.006346\n",
            "Train Epoch: 40 [400/1000 (40%)]\tLosses F.softmax: 0.000176 log_softmax: 0.000056\n",
            "Train Epoch: 40 [600/1000 (60%)]\tLosses F.softmax: 0.001663 log_softmax: 0.001448\n",
            "Train Epoch: 40 [800/1000 (80%)]\tLosses F.softmax: 0.000023 log_softmax: 0.000030\n",
            "Train Epoch: 40 [1000/1000 (100%)]\tLosses F.softmax: 0.000151 log_softmax: 0.000047\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9306\tAccuracy: 8079.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9448\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 41 [0/1000 (0%)]\tLosses F.softmax: 0.226583 log_softmax: 0.147159\n",
            "Train Epoch: 41 [200/1000 (20%)]\tLosses F.softmax: 0.000781 log_softmax: 0.001465\n",
            "Train Epoch: 41 [400/1000 (40%)]\tLosses F.softmax: 0.004097 log_softmax: 0.002557\n",
            "Train Epoch: 41 [600/1000 (60%)]\tLosses F.softmax: 0.002652 log_softmax: 0.000336\n",
            "Train Epoch: 41 [800/1000 (80%)]\tLosses F.softmax: 0.000008 log_softmax: 0.000016\n",
            "Train Epoch: 41 [1000/1000 (100%)]\tLosses F.softmax: 0.031314 log_softmax: 0.036169\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9324\tAccuracy: 8078.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9466\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 42 [0/1000 (0%)]\tLosses F.softmax: 0.014067 log_softmax: 0.006992\n",
            "Train Epoch: 42 [200/1000 (20%)]\tLosses F.softmax: 0.000006 log_softmax: 0.000024\n",
            "Train Epoch: 42 [400/1000 (40%)]\tLosses F.softmax: 0.001183 log_softmax: 0.000342\n",
            "Train Epoch: 42 [600/1000 (60%)]\tLosses F.softmax: 0.000253 log_softmax: 0.000216\n",
            "Train Epoch: 42 [800/1000 (80%)]\tLosses F.softmax: 0.004804 log_softmax: 0.004356\n",
            "Train Epoch: 42 [1000/1000 (100%)]\tLosses F.softmax: 0.028932 log_softmax: 0.020173\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9325\tAccuracy: 8084.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9474\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 43 [0/1000 (0%)]\tLosses F.softmax: 0.010413 log_softmax: 0.011616\n",
            "Train Epoch: 43 [200/1000 (20%)]\tLosses F.softmax: 0.009590 log_softmax: 0.037497\n",
            "Train Epoch: 43 [400/1000 (40%)]\tLosses F.softmax: 0.000046 log_softmax: 0.000018\n",
            "Train Epoch: 43 [600/1000 (60%)]\tLosses F.softmax: 0.000464 log_softmax: 0.000305\n",
            "Train Epoch: 43 [800/1000 (80%)]\tLosses F.softmax: 0.000024 log_softmax: 0.000037\n",
            "Train Epoch: 43 [1000/1000 (100%)]\tLosses F.softmax: 0.000644 log_softmax: 0.000539\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9345\tAccuracy: 8075.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9487\tAccuracy: 8048.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 44 [0/1000 (0%)]\tLosses F.softmax: 0.004888 log_softmax: 0.015660\n",
            "Train Epoch: 44 [200/1000 (20%)]\tLosses F.softmax: 0.028070 log_softmax: 0.100968\n",
            "Train Epoch: 44 [400/1000 (40%)]\tLosses F.softmax: 0.001378 log_softmax: 0.000045\n",
            "Train Epoch: 44 [600/1000 (60%)]\tLosses F.softmax: 0.005283 log_softmax: 0.001899\n",
            "Train Epoch: 44 [800/1000 (80%)]\tLosses F.softmax: 0.000973 log_softmax: 0.002786\n",
            "Train Epoch: 44 [1000/1000 (100%)]\tLosses F.softmax: 0.015732 log_softmax: 0.003584\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9370\tAccuracy: 8065.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9517\tAccuracy: 8024.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 45 [0/1000 (0%)]\tLosses F.softmax: 0.010036 log_softmax: 0.009722\n",
            "Train Epoch: 45 [200/1000 (20%)]\tLosses F.softmax: 0.000752 log_softmax: 0.000282\n",
            "Train Epoch: 45 [400/1000 (40%)]\tLosses F.softmax: 0.035915 log_softmax: 0.017935\n",
            "Train Epoch: 45 [600/1000 (60%)]\tLosses F.softmax: 0.008598 log_softmax: 0.001447\n",
            "Train Epoch: 45 [800/1000 (80%)]\tLosses F.softmax: 0.122397 log_softmax: 0.041196\n",
            "Train Epoch: 45 [1000/1000 (100%)]\tLosses F.softmax: 0.001720 log_softmax: 0.020055\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9354\tAccuracy: 8077.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9498\tAccuracy: 8061.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 46 [0/1000 (0%)]\tLosses F.softmax: 0.005155 log_softmax: 0.002222\n",
            "Train Epoch: 46 [200/1000 (20%)]\tLosses F.softmax: 0.087649 log_softmax: 0.095654\n",
            "Train Epoch: 46 [400/1000 (40%)]\tLosses F.softmax: 0.011739 log_softmax: 0.003464\n",
            "Train Epoch: 46 [600/1000 (60%)]\tLosses F.softmax: 0.004646 log_softmax: 0.002818\n",
            "Train Epoch: 46 [800/1000 (80%)]\tLosses F.softmax: 0.027304 log_softmax: 0.062374\n",
            "Train Epoch: 46 [1000/1000 (100%)]\tLosses F.softmax: 0.000011 log_softmax: 0.000010\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9359\tAccuracy: 8080.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9502\tAccuracy: 8060.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 47 [0/1000 (0%)]\tLosses F.softmax: 0.003488 log_softmax: 0.001266\n",
            "Train Epoch: 47 [200/1000 (20%)]\tLosses F.softmax: 0.040118 log_softmax: 0.023274\n",
            "Train Epoch: 47 [400/1000 (40%)]\tLosses F.softmax: 0.018613 log_softmax: 0.009972\n",
            "Train Epoch: 47 [600/1000 (60%)]\tLosses F.softmax: 0.008716 log_softmax: 0.012040\n",
            "Train Epoch: 47 [800/1000 (80%)]\tLosses F.softmax: 0.000202 log_softmax: 0.000028\n",
            "Train Epoch: 47 [1000/1000 (100%)]\tLosses F.softmax: 0.000039 log_softmax: 0.000050\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9371\tAccuracy: 8081.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9525\tAccuracy: 8060.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 48 [0/1000 (0%)]\tLosses F.softmax: 0.000004 log_softmax: 0.000049\n",
            "Train Epoch: 48 [200/1000 (20%)]\tLosses F.softmax: 0.000016 log_softmax: 0.000064\n",
            "Train Epoch: 48 [400/1000 (40%)]\tLosses F.softmax: 0.028905 log_softmax: 0.067360\n",
            "Train Epoch: 48 [600/1000 (60%)]\tLosses F.softmax: 0.000431 log_softmax: 0.000117\n",
            "Train Epoch: 48 [800/1000 (80%)]\tLosses F.softmax: 0.000010 log_softmax: 0.000119\n",
            "Train Epoch: 48 [1000/1000 (100%)]\tLosses F.softmax: 0.003615 log_softmax: 0.005749\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9390\tAccuracy: 8074.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9539\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 49 [0/1000 (0%)]\tLosses F.softmax: 0.054797 log_softmax: 0.026626\n",
            "Train Epoch: 49 [200/1000 (20%)]\tLosses F.softmax: 0.000452 log_softmax: 0.000123\n",
            "Train Epoch: 49 [400/1000 (40%)]\tLosses F.softmax: 0.007414 log_softmax: 0.001109\n",
            "Train Epoch: 49 [600/1000 (60%)]\tLosses F.softmax: 0.001649 log_softmax: 0.008669\n",
            "Train Epoch: 49 [800/1000 (80%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000014\n",
            "Train Epoch: 49 [1000/1000 (100%)]\tLosses F.softmax: 0.033413 log_softmax: 0.026886\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9380\tAccuracy: 8088.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9533\tAccuracy: 8067.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 50 [0/1000 (0%)]\tLosses F.softmax: 0.008091 log_softmax: 0.001999\n",
            "Train Epoch: 50 [200/1000 (20%)]\tLosses F.softmax: 0.000005 log_softmax: 0.000007\n",
            "Train Epoch: 50 [400/1000 (40%)]\tLosses F.softmax: 0.000051 log_softmax: 0.000147\n",
            "Train Epoch: 50 [600/1000 (60%)]\tLosses F.softmax: 0.021128 log_softmax: 0.014343\n",
            "Train Epoch: 50 [800/1000 (80%)]\tLosses F.softmax: 0.002794 log_softmax: 0.000226\n",
            "Train Epoch: 50 [1000/1000 (100%)]\tLosses F.softmax: 0.029942 log_softmax: 0.006255\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9387\tAccuracy: 8092.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9548\tAccuracy: 8042.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 51 [0/1000 (0%)]\tLosses F.softmax: 0.082675 log_softmax: 0.023708\n",
            "Train Epoch: 51 [200/1000 (20%)]\tLosses F.softmax: 0.013604 log_softmax: 0.013442\n",
            "Train Epoch: 51 [400/1000 (40%)]\tLosses F.softmax: 0.001251 log_softmax: 0.000736\n",
            "Train Epoch: 51 [600/1000 (60%)]\tLosses F.softmax: 0.000006 log_softmax: 0.000193\n",
            "Train Epoch: 51 [800/1000 (80%)]\tLosses F.softmax: 0.000024 log_softmax: 0.000031\n",
            "Train Epoch: 51 [1000/1000 (100%)]\tLosses F.softmax: 0.001659 log_softmax: 0.002293\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9397\tAccuracy: 8094.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9556\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 52 [0/1000 (0%)]\tLosses F.softmax: 0.000227 log_softmax: 0.000058\n",
            "Train Epoch: 52 [200/1000 (20%)]\tLosses F.softmax: 0.063725 log_softmax: 0.033980\n",
            "Train Epoch: 52 [400/1000 (40%)]\tLosses F.softmax: 0.004362 log_softmax: 0.001265\n",
            "Train Epoch: 52 [600/1000 (60%)]\tLosses F.softmax: 0.001306 log_softmax: 0.008281\n",
            "Train Epoch: 52 [800/1000 (80%)]\tLosses F.softmax: 0.072029 log_softmax: 0.049735\n",
            "Train Epoch: 52 [1000/1000 (100%)]\tLosses F.softmax: 0.000690 log_softmax: 0.006973\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9416\tAccuracy: 8082.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9567\tAccuracy: 8068.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 53 [0/1000 (0%)]\tLosses F.softmax: 0.032440 log_softmax: 0.030185\n",
            "Train Epoch: 53 [200/1000 (20%)]\tLosses F.softmax: 0.000935 log_softmax: 0.000327\n",
            "Train Epoch: 53 [400/1000 (40%)]\tLosses F.softmax: 0.000205 log_softmax: 0.000088\n",
            "Train Epoch: 53 [600/1000 (60%)]\tLosses F.softmax: 0.040847 log_softmax: 0.030620\n",
            "Train Epoch: 53 [800/1000 (80%)]\tLosses F.softmax: 0.020969 log_softmax: 0.002373\n",
            "Train Epoch: 53 [1000/1000 (100%)]\tLosses F.softmax: 0.033974 log_softmax: 0.051454\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9428\tAccuracy: 8093.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9581\tAccuracy: 8060.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 54 [0/1000 (0%)]\tLosses F.softmax: 0.059248 log_softmax: 0.097656\n",
            "Train Epoch: 54 [200/1000 (20%)]\tLosses F.softmax: 0.000090 log_softmax: 0.000021\n",
            "Train Epoch: 54 [400/1000 (40%)]\tLosses F.softmax: 0.092799 log_softmax: 0.092392\n",
            "Train Epoch: 54 [600/1000 (60%)]\tLosses F.softmax: 0.000152 log_softmax: 0.000039\n",
            "Train Epoch: 54 [800/1000 (80%)]\tLosses F.softmax: 0.000065 log_softmax: 0.000011\n",
            "Train Epoch: 54 [1000/1000 (100%)]\tLosses F.softmax: 0.001056 log_softmax: 0.000648\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9432\tAccuracy: 8094.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9596\tAccuracy: 8047.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 55 [0/1000 (0%)]\tLosses F.softmax: 0.005660 log_softmax: 0.007948\n",
            "Train Epoch: 55 [200/1000 (20%)]\tLosses F.softmax: 0.050312 log_softmax: 0.137561\n",
            "Train Epoch: 55 [400/1000 (40%)]\tLosses F.softmax: 0.031557 log_softmax: 0.056932\n",
            "Train Epoch: 55 [600/1000 (60%)]\tLosses F.softmax: 0.097427 log_softmax: 0.103933\n",
            "Train Epoch: 55 [800/1000 (80%)]\tLosses F.softmax: 0.008849 log_softmax: 0.008791\n",
            "Train Epoch: 55 [1000/1000 (100%)]\tLosses F.softmax: 0.124983 log_softmax: 0.043874\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9447\tAccuracy: 8085.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9595\tAccuracy: 8065.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 56 [0/1000 (0%)]\tLosses F.softmax: 0.001019 log_softmax: 0.003625\n",
            "Train Epoch: 56 [200/1000 (20%)]\tLosses F.softmax: 0.005037 log_softmax: 0.000938\n",
            "Train Epoch: 56 [400/1000 (40%)]\tLosses F.softmax: 0.006230 log_softmax: 0.001429\n",
            "Train Epoch: 56 [600/1000 (60%)]\tLosses F.softmax: 0.009185 log_softmax: 0.008664\n",
            "Train Epoch: 56 [800/1000 (80%)]\tLosses F.softmax: 0.022124 log_softmax: 0.008111\n",
            "Train Epoch: 56 [1000/1000 (100%)]\tLosses F.softmax: 0.084070 log_softmax: 0.083683\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9455\tAccuracy: 8082.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9611\tAccuracy: 8063.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 57 [0/1000 (0%)]\tLosses F.softmax: 0.217573 log_softmax: 0.166976\n",
            "Train Epoch: 57 [200/1000 (20%)]\tLosses F.softmax: 0.007837 log_softmax: 0.017082\n",
            "Train Epoch: 57 [400/1000 (40%)]\tLosses F.softmax: 0.005319 log_softmax: 0.005173\n",
            "Train Epoch: 57 [600/1000 (60%)]\tLosses F.softmax: 0.142686 log_softmax: 0.265771\n",
            "Train Epoch: 57 [800/1000 (80%)]\tLosses F.softmax: 0.000839 log_softmax: 0.000134\n",
            "Train Epoch: 57 [1000/1000 (100%)]\tLosses F.softmax: 0.000882 log_softmax: 0.000197\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9460\tAccuracy: 8089.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9617\tAccuracy: 8071.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 58 [0/1000 (0%)]\tLosses F.softmax: 0.000920 log_softmax: 0.000307\n",
            "Train Epoch: 58 [200/1000 (20%)]\tLosses F.softmax: 0.040153 log_softmax: 0.034634\n",
            "Train Epoch: 58 [400/1000 (40%)]\tLosses F.softmax: 0.005453 log_softmax: 0.000824\n",
            "Train Epoch: 58 [600/1000 (60%)]\tLosses F.softmax: 0.000219 log_softmax: 0.004474\n",
            "Train Epoch: 58 [800/1000 (80%)]\tLosses F.softmax: 0.000264 log_softmax: 0.000030\n",
            "Train Epoch: 58 [1000/1000 (100%)]\tLosses F.softmax: 0.002493 log_softmax: 0.000564\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9478\tAccuracy: 8076.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9636\tAccuracy: 8040.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 59 [0/1000 (0%)]\tLosses F.softmax: 0.000063 log_softmax: 0.000016\n",
            "Train Epoch: 59 [200/1000 (20%)]\tLosses F.softmax: 0.005632 log_softmax: 0.000277\n",
            "Train Epoch: 59 [400/1000 (40%)]\tLosses F.softmax: 0.000002 log_softmax: 0.000001\n",
            "Train Epoch: 59 [600/1000 (60%)]\tLosses F.softmax: 0.034371 log_softmax: 0.022734\n",
            "Train Epoch: 59 [800/1000 (80%)]\tLosses F.softmax: 0.014921 log_softmax: 0.016115\n",
            "Train Epoch: 59 [1000/1000 (100%)]\tLosses F.softmax: 0.095628 log_softmax: 0.033703\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9488\tAccuracy: 8081.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9639\tAccuracy: 8063.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 60 [0/1000 (0%)]\tLosses F.softmax: 0.061194 log_softmax: 0.046130\n",
            "Train Epoch: 60 [200/1000 (20%)]\tLosses F.softmax: 0.041527 log_softmax: 0.005688\n",
            "Train Epoch: 60 [400/1000 (40%)]\tLosses F.softmax: 0.006928 log_softmax: 0.001344\n",
            "Train Epoch: 60 [600/1000 (60%)]\tLosses F.softmax: 0.000062 log_softmax: 0.000009\n",
            "Train Epoch: 60 [800/1000 (80%)]\tLosses F.softmax: 0.002215 log_softmax: 0.001101\n",
            "Train Epoch: 60 [1000/1000 (100%)]\tLosses F.softmax: 0.019533 log_softmax: 0.003636\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9501\tAccuracy: 8083.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9652\tAccuracy: 8062.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 61 [0/1000 (0%)]\tLosses F.softmax: 0.000187 log_softmax: 0.000033\n",
            "Train Epoch: 61 [200/1000 (20%)]\tLosses F.softmax: 0.000057 log_softmax: 0.000015\n",
            "Train Epoch: 61 [400/1000 (40%)]\tLosses F.softmax: 0.022026 log_softmax: 0.018580\n",
            "Train Epoch: 61 [600/1000 (60%)]\tLosses F.softmax: 0.006237 log_softmax: 0.005539\n",
            "Train Epoch: 61 [800/1000 (80%)]\tLosses F.softmax: 0.000309 log_softmax: 0.000035\n",
            "Train Epoch: 61 [1000/1000 (100%)]\tLosses F.softmax: 0.018018 log_softmax: 0.012460\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9508\tAccuracy: 8080.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9662\tAccuracy: 8041.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 62 [0/1000 (0%)]\tLosses F.softmax: 0.013890 log_softmax: 0.011796\n",
            "Train Epoch: 62 [200/1000 (20%)]\tLosses F.softmax: 0.008589 log_softmax: 0.004871\n",
            "Train Epoch: 62 [400/1000 (40%)]\tLosses F.softmax: 0.000262 log_softmax: 0.000918\n",
            "Train Epoch: 62 [600/1000 (60%)]\tLosses F.softmax: 0.000435 log_softmax: 0.000063\n",
            "Train Epoch: 62 [800/1000 (80%)]\tLosses F.softmax: 0.036951 log_softmax: 0.024650\n",
            "Train Epoch: 62 [1000/1000 (100%)]\tLosses F.softmax: 0.004267 log_softmax: 0.001906\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9515\tAccuracy: 8084.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9669\tAccuracy: 8067.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 63 [0/1000 (0%)]\tLosses F.softmax: 0.000031 log_softmax: 0.000007\n",
            "Train Epoch: 63 [200/1000 (20%)]\tLosses F.softmax: 0.000055 log_softmax: 0.000008\n",
            "Train Epoch: 63 [400/1000 (40%)]\tLosses F.softmax: 0.001711 log_softmax: 0.003007\n",
            "Train Epoch: 63 [600/1000 (60%)]\tLosses F.softmax: 0.002200 log_softmax: 0.000461\n",
            "Train Epoch: 63 [800/1000 (80%)]\tLosses F.softmax: 0.000416 log_softmax: 0.001220\n",
            "Train Epoch: 63 [1000/1000 (100%)]\tLosses F.softmax: 0.000026 log_softmax: 0.000004\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9526\tAccuracy: 8082.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9682\tAccuracy: 8042.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 64 [0/1000 (0%)]\tLosses F.softmax: 0.007570 log_softmax: 0.006683\n",
            "Train Epoch: 64 [200/1000 (20%)]\tLosses F.softmax: 0.000062 log_softmax: 0.000015\n",
            "Train Epoch: 64 [400/1000 (40%)]\tLosses F.softmax: 0.000172 log_softmax: 0.000491\n",
            "Train Epoch: 64 [600/1000 (60%)]\tLosses F.softmax: 0.000090 log_softmax: 0.000103\n",
            "Train Epoch: 64 [800/1000 (80%)]\tLosses F.softmax: 0.057271 log_softmax: 0.125583\n",
            "Train Epoch: 64 [1000/1000 (100%)]\tLosses F.softmax: 0.029652 log_softmax: 0.091092\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9535\tAccuracy: 8074.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9688\tAccuracy: 8044.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 65 [0/1000 (0%)]\tLosses F.softmax: 0.094258 log_softmax: 0.034048\n",
            "Train Epoch: 65 [200/1000 (20%)]\tLosses F.softmax: 0.000509 log_softmax: 0.001080\n",
            "Train Epoch: 65 [400/1000 (40%)]\tLosses F.softmax: 0.000387 log_softmax: 0.000328\n",
            "Train Epoch: 65 [600/1000 (60%)]\tLosses F.softmax: 0.002978 log_softmax: 0.000049\n",
            "Train Epoch: 65 [800/1000 (80%)]\tLosses F.softmax: 0.010931 log_softmax: 0.018243\n",
            "Train Epoch: 65 [1000/1000 (100%)]\tLosses F.softmax: 0.062725 log_softmax: 0.034370\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9550\tAccuracy: 8077.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9697\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 66 [0/1000 (0%)]\tLosses F.softmax: 0.028249 log_softmax: 0.028914\n",
            "Train Epoch: 66 [200/1000 (20%)]\tLosses F.softmax: 0.005756 log_softmax: 0.000404\n",
            "Train Epoch: 66 [400/1000 (40%)]\tLosses F.softmax: 0.024951 log_softmax: 0.045296\n",
            "Train Epoch: 66 [600/1000 (60%)]\tLosses F.softmax: 0.069869 log_softmax: 0.073982\n",
            "Train Epoch: 66 [800/1000 (80%)]\tLosses F.softmax: 0.000212 log_softmax: 0.000060\n",
            "Train Epoch: 66 [1000/1000 (100%)]\tLosses F.softmax: 0.008668 log_softmax: 0.004024\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9535\tAccuracy: 8094.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9693\tAccuracy: 8070.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 67 [0/1000 (0%)]\tLosses F.softmax: 0.002109 log_softmax: 0.002156\n",
            "Train Epoch: 67 [200/1000 (20%)]\tLosses F.softmax: 0.013651 log_softmax: 0.028519\n",
            "Train Epoch: 67 [400/1000 (40%)]\tLosses F.softmax: 0.010548 log_softmax: 0.009958\n",
            "Train Epoch: 67 [600/1000 (60%)]\tLosses F.softmax: 0.003516 log_softmax: 0.002772\n",
            "Train Epoch: 67 [800/1000 (80%)]\tLosses F.softmax: 0.083649 log_softmax: 0.097057\n",
            "Train Epoch: 67 [1000/1000 (100%)]\tLosses F.softmax: 0.002920 log_softmax: 0.003602\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9547\tAccuracy: 8086.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9711\tAccuracy: 8063.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 68 [0/1000 (0%)]\tLosses F.softmax: 0.011376 log_softmax: 0.011399\n",
            "Train Epoch: 68 [200/1000 (20%)]\tLosses F.softmax: 0.118114 log_softmax: 0.151365\n",
            "Train Epoch: 68 [400/1000 (40%)]\tLosses F.softmax: 0.016777 log_softmax: 0.017212\n",
            "Train Epoch: 68 [600/1000 (60%)]\tLosses F.softmax: 0.002127 log_softmax: 0.004266\n",
            "Train Epoch: 68 [800/1000 (80%)]\tLosses F.softmax: 0.001367 log_softmax: 0.000349\n",
            "Train Epoch: 68 [1000/1000 (100%)]\tLosses F.softmax: 0.000050 log_softmax: 0.000008\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9560\tAccuracy: 8087.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9716\tAccuracy: 8064.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 69 [0/1000 (0%)]\tLosses F.softmax: 0.001540 log_softmax: 0.001367\n",
            "Train Epoch: 69 [200/1000 (20%)]\tLosses F.softmax: 0.017524 log_softmax: 0.018229\n",
            "Train Epoch: 69 [400/1000 (40%)]\tLosses F.softmax: 0.094409 log_softmax: 0.057944\n",
            "Train Epoch: 69 [600/1000 (60%)]\tLosses F.softmax: 0.030500 log_softmax: 0.035011\n",
            "Train Epoch: 69 [800/1000 (80%)]\tLosses F.softmax: 0.017983 log_softmax: 0.010171\n",
            "Train Epoch: 69 [1000/1000 (100%)]\tLosses F.softmax: 0.120226 log_softmax: 0.166957\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9577\tAccuracy: 8074.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9741\tAccuracy: 8031.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 70 [0/1000 (0%)]\tLosses F.softmax: 0.000007 log_softmax: 0.000511\n",
            "Train Epoch: 70 [200/1000 (20%)]\tLosses F.softmax: 0.009838 log_softmax: 0.009435\n",
            "Train Epoch: 70 [400/1000 (40%)]\tLosses F.softmax: 0.000008 log_softmax: 0.000068\n",
            "Train Epoch: 70 [600/1000 (60%)]\tLosses F.softmax: 0.000188 log_softmax: 0.000052\n",
            "Train Epoch: 70 [800/1000 (80%)]\tLosses F.softmax: 0.063309 log_softmax: 0.063118\n",
            "Train Epoch: 70 [1000/1000 (100%)]\tLosses F.softmax: 0.019626 log_softmax: 0.019492\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9585\tAccuracy: 8084.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9746\tAccuracy: 8065.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 71 [0/1000 (0%)]\tLosses F.softmax: 0.092133 log_softmax: 0.030828\n",
            "Train Epoch: 71 [200/1000 (20%)]\tLosses F.softmax: 0.011649 log_softmax: 0.011902\n",
            "Train Epoch: 71 [400/1000 (40%)]\tLosses F.softmax: 0.002370 log_softmax: 0.022280\n",
            "Train Epoch: 71 [600/1000 (60%)]\tLosses F.softmax: 0.017666 log_softmax: 0.009775\n",
            "Train Epoch: 71 [800/1000 (80%)]\tLosses F.softmax: 0.040141 log_softmax: 0.030530\n",
            "Train Epoch: 71 [1000/1000 (100%)]\tLosses F.softmax: 0.000457 log_softmax: 0.001820\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9595\tAccuracy: 8085.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9757\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 72 [0/1000 (0%)]\tLosses F.softmax: 0.001819 log_softmax: 0.007739\n",
            "Train Epoch: 72 [200/1000 (20%)]\tLosses F.softmax: 0.000379 log_softmax: 0.006446\n",
            "Train Epoch: 72 [400/1000 (40%)]\tLosses F.softmax: 0.000015 log_softmax: 0.000019\n",
            "Train Epoch: 72 [600/1000 (60%)]\tLosses F.softmax: 0.007452 log_softmax: 0.003151\n",
            "Train Epoch: 72 [800/1000 (80%)]\tLosses F.softmax: 0.037451 log_softmax: 0.049586\n",
            "Train Epoch: 72 [1000/1000 (100%)]\tLosses F.softmax: 0.025759 log_softmax: 0.005307\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9612\tAccuracy: 8078.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9774\tAccuracy: 8046.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 73 [0/1000 (0%)]\tLosses F.softmax: 0.000018 log_softmax: 0.000010\n",
            "Train Epoch: 73 [200/1000 (20%)]\tLosses F.softmax: 0.001313 log_softmax: 0.000922\n",
            "Train Epoch: 73 [400/1000 (40%)]\tLosses F.softmax: 0.179523 log_softmax: 0.114413\n",
            "Train Epoch: 73 [600/1000 (60%)]\tLosses F.softmax: 0.089778 log_softmax: 0.069171\n",
            "Train Epoch: 73 [800/1000 (80%)]\tLosses F.softmax: 0.002152 log_softmax: 0.005112\n",
            "Train Epoch: 73 [1000/1000 (100%)]\tLosses F.softmax: 0.011463 log_softmax: 0.017044\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9609\tAccuracy: 8081.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9769\tAccuracy: 8068.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 74 [0/1000 (0%)]\tLosses F.softmax: 0.001305 log_softmax: 0.001167\n",
            "Train Epoch: 74 [200/1000 (20%)]\tLosses F.softmax: 0.000015 log_softmax: 0.000008\n",
            "Train Epoch: 74 [400/1000 (40%)]\tLosses F.softmax: 0.003824 log_softmax: 0.010866\n",
            "Train Epoch: 74 [600/1000 (60%)]\tLosses F.softmax: 0.066644 log_softmax: 0.029114\n",
            "Train Epoch: 74 [800/1000 (80%)]\tLosses F.softmax: 0.000327 log_softmax: 0.000045\n",
            "Train Epoch: 74 [1000/1000 (100%)]\tLosses F.softmax: 0.009787 log_softmax: 0.012901\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9616\tAccuracy: 8086.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9788\tAccuracy: 8050.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 75 [0/1000 (0%)]\tLosses F.softmax: 0.009839 log_softmax: 0.009205\n",
            "Train Epoch: 75 [200/1000 (20%)]\tLosses F.softmax: 0.016805 log_softmax: 0.017674\n",
            "Train Epoch: 75 [400/1000 (40%)]\tLosses F.softmax: 0.000071 log_softmax: 0.000085\n",
            "Train Epoch: 75 [600/1000 (60%)]\tLosses F.softmax: 0.003054 log_softmax: 0.004947\n",
            "Train Epoch: 75 [800/1000 (80%)]\tLosses F.softmax: 0.004034 log_softmax: 0.002091\n",
            "Train Epoch: 75 [1000/1000 (100%)]\tLosses F.softmax: 0.002060 log_softmax: 0.000254\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9625\tAccuracy: 8079.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9788\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 76 [0/1000 (0%)]\tLosses F.softmax: 0.000727 log_softmax: 0.000028\n",
            "Train Epoch: 76 [200/1000 (20%)]\tLosses F.softmax: 0.035958 log_softmax: 0.032631\n",
            "Train Epoch: 76 [400/1000 (40%)]\tLosses F.softmax: 0.117231 log_softmax: 0.236571\n",
            "Train Epoch: 76 [600/1000 (60%)]\tLosses F.softmax: 0.047492 log_softmax: 0.029681\n",
            "Train Epoch: 76 [800/1000 (80%)]\tLosses F.softmax: 0.002346 log_softmax: 0.002321\n",
            "Train Epoch: 76 [1000/1000 (100%)]\tLosses F.softmax: 0.000221 log_softmax: 0.000059\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9630\tAccuracy: 8085.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9796\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 77 [0/1000 (0%)]\tLosses F.softmax: 0.014840 log_softmax: 0.000137\n",
            "Train Epoch: 77 [200/1000 (20%)]\tLosses F.softmax: 0.023632 log_softmax: 0.004989\n",
            "Train Epoch: 77 [400/1000 (40%)]\tLosses F.softmax: 0.000013 log_softmax: 0.000090\n",
            "Train Epoch: 77 [600/1000 (60%)]\tLosses F.softmax: 0.030076 log_softmax: 0.025532\n",
            "Train Epoch: 77 [800/1000 (80%)]\tLosses F.softmax: 0.066921 log_softmax: 0.051521\n",
            "Train Epoch: 77 [1000/1000 (100%)]\tLosses F.softmax: 0.001491 log_softmax: 0.000786\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9640\tAccuracy: 8088.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9807\tAccuracy: 8059.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 78 [0/1000 (0%)]\tLosses F.softmax: 0.000006 log_softmax: 0.000009\n",
            "Train Epoch: 78 [200/1000 (20%)]\tLosses F.softmax: 0.017200 log_softmax: 0.019071\n",
            "Train Epoch: 78 [400/1000 (40%)]\tLosses F.softmax: 0.000022 log_softmax: 0.000025\n",
            "Train Epoch: 78 [600/1000 (60%)]\tLosses F.softmax: 0.022753 log_softmax: 0.002469\n",
            "Train Epoch: 78 [800/1000 (80%)]\tLosses F.softmax: 0.041579 log_softmax: 0.019972\n",
            "Train Epoch: 78 [1000/1000 (100%)]\tLosses F.softmax: 0.013186 log_softmax: 0.027451\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9641\tAccuracy: 8087.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9807\tAccuracy: 8059.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 79 [0/1000 (0%)]\tLosses F.softmax: 0.000197 log_softmax: 0.000056\n",
            "Train Epoch: 79 [200/1000 (20%)]\tLosses F.softmax: 0.000005 log_softmax: 0.000015\n",
            "Train Epoch: 79 [400/1000 (40%)]\tLosses F.softmax: 0.000610 log_softmax: 0.000395\n",
            "Train Epoch: 79 [600/1000 (60%)]\tLosses F.softmax: 0.013796 log_softmax: 0.001594\n",
            "Train Epoch: 79 [800/1000 (80%)]\tLosses F.softmax: 0.002906 log_softmax: 0.002469\n",
            "Train Epoch: 79 [1000/1000 (100%)]\tLosses F.softmax: 0.000128 log_softmax: 0.000015\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9648\tAccuracy: 8092.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9820\tAccuracy: 8065.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 80 [0/1000 (0%)]\tLosses F.softmax: 0.003421 log_softmax: 0.003682\n",
            "Train Epoch: 80 [200/1000 (20%)]\tLosses F.softmax: 0.000010 log_softmax: 0.000009\n",
            "Train Epoch: 80 [400/1000 (40%)]\tLosses F.softmax: 0.000123 log_softmax: 0.000313\n",
            "Train Epoch: 80 [600/1000 (60%)]\tLosses F.softmax: 0.000100 log_softmax: 0.000177\n",
            "Train Epoch: 80 [800/1000 (80%)]\tLosses F.softmax: 0.002513 log_softmax: 0.000647\n",
            "Train Epoch: 80 [1000/1000 (100%)]\tLosses F.softmax: 0.017824 log_softmax: 0.036622\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9674\tAccuracy: 8079.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9840\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 81 [0/1000 (0%)]\tLosses F.softmax: 0.000281 log_softmax: 0.000028\n",
            "Train Epoch: 81 [200/1000 (20%)]\tLosses F.softmax: 0.000894 log_softmax: 0.000960\n",
            "Train Epoch: 81 [400/1000 (40%)]\tLosses F.softmax: 0.112891 log_softmax: 0.084489\n",
            "Train Epoch: 81 [600/1000 (60%)]\tLosses F.softmax: 0.007298 log_softmax: 0.003256\n",
            "Train Epoch: 81 [800/1000 (80%)]\tLosses F.softmax: 0.002216 log_softmax: 0.002123\n",
            "Train Epoch: 81 [1000/1000 (100%)]\tLosses F.softmax: 0.012900 log_softmax: 0.003604\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9682\tAccuracy: 8082.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9846\tAccuracy: 8057.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 82 [0/1000 (0%)]\tLosses F.softmax: 0.000083 log_softmax: 0.000028\n",
            "Train Epoch: 82 [200/1000 (20%)]\tLosses F.softmax: 0.000041 log_softmax: 0.000074\n",
            "Train Epoch: 82 [400/1000 (40%)]\tLosses F.softmax: 0.100668 log_softmax: 0.082244\n",
            "Train Epoch: 82 [600/1000 (60%)]\tLosses F.softmax: 0.000693 log_softmax: 0.000026\n",
            "Train Epoch: 82 [800/1000 (80%)]\tLosses F.softmax: 0.097496 log_softmax: 0.060940\n",
            "Train Epoch: 82 [1000/1000 (100%)]\tLosses F.softmax: 0.003104 log_softmax: 0.009615\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9684\tAccuracy: 8084.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9853\tAccuracy: 8058.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 83 [0/1000 (0%)]\tLosses F.softmax: 0.109627 log_softmax: 0.123356\n",
            "Train Epoch: 83 [200/1000 (20%)]\tLosses F.softmax: 0.171312 log_softmax: 0.229990\n",
            "Train Epoch: 83 [400/1000 (40%)]\tLosses F.softmax: 0.000417 log_softmax: 0.001419\n",
            "Train Epoch: 83 [600/1000 (60%)]\tLosses F.softmax: 0.000055 log_softmax: 0.000021\n",
            "Train Epoch: 83 [800/1000 (80%)]\tLosses F.softmax: 0.014960 log_softmax: 0.006824\n",
            "Train Epoch: 83 [1000/1000 (100%)]\tLosses F.softmax: 0.000254 log_softmax: 0.000145\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9688\tAccuracy: 8086.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9853\tAccuracy: 8063.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 84 [0/1000 (0%)]\tLosses F.softmax: 0.046832 log_softmax: 0.040547\n",
            "Train Epoch: 84 [200/1000 (20%)]\tLosses F.softmax: 0.000292 log_softmax: 0.000254\n",
            "Train Epoch: 84 [400/1000 (40%)]\tLosses F.softmax: 0.000794 log_softmax: 0.009680\n",
            "Train Epoch: 84 [600/1000 (60%)]\tLosses F.softmax: 0.004926 log_softmax: 0.003833\n",
            "Train Epoch: 84 [800/1000 (80%)]\tLosses F.softmax: 0.000056 log_softmax: 0.000032\n",
            "Train Epoch: 84 [1000/1000 (100%)]\tLosses F.softmax: 0.003062 log_softmax: 0.003338\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9695\tAccuracy: 8090.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9864\tAccuracy: 8067.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 85 [0/1000 (0%)]\tLosses F.softmax: 0.015114 log_softmax: 0.000136\n",
            "Train Epoch: 85 [200/1000 (20%)]\tLosses F.softmax: 0.026673 log_softmax: 0.011727\n",
            "Train Epoch: 85 [400/1000 (40%)]\tLosses F.softmax: 0.003301 log_softmax: 0.007778\n",
            "Train Epoch: 85 [600/1000 (60%)]\tLosses F.softmax: 0.010381 log_softmax: 0.008327\n",
            "Train Epoch: 85 [800/1000 (80%)]\tLosses F.softmax: 0.096664 log_softmax: 0.090751\n",
            "Train Epoch: 85 [1000/1000 (100%)]\tLosses F.softmax: 0.000059 log_softmax: 0.000017\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9704\tAccuracy: 8090.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9883\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 86 [0/1000 (0%)]\tLosses F.softmax: 0.000019 log_softmax: 0.000022\n",
            "Train Epoch: 86 [200/1000 (20%)]\tLosses F.softmax: 0.009475 log_softmax: 0.016447\n",
            "Train Epoch: 86 [400/1000 (40%)]\tLosses F.softmax: 0.005185 log_softmax: 0.000042\n",
            "Train Epoch: 86 [600/1000 (60%)]\tLosses F.softmax: 0.010073 log_softmax: 0.004589\n",
            "Train Epoch: 86 [800/1000 (80%)]\tLosses F.softmax: 0.001013 log_softmax: 0.000361\n",
            "Train Epoch: 86 [1000/1000 (100%)]\tLosses F.softmax: 0.000438 log_softmax: 0.001207\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9727\tAccuracy: 8079.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9896\tAccuracy: 8044.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 87 [0/1000 (0%)]\tLosses F.softmax: 0.003509 log_softmax: 0.001254\n",
            "Train Epoch: 87 [200/1000 (20%)]\tLosses F.softmax: 0.000022 log_softmax: 0.000159\n",
            "Train Epoch: 87 [400/1000 (40%)]\tLosses F.softmax: 0.000924 log_softmax: 0.008553\n",
            "Train Epoch: 87 [600/1000 (60%)]\tLosses F.softmax: 0.000079 log_softmax: 0.000021\n",
            "Train Epoch: 87 [800/1000 (80%)]\tLosses F.softmax: 0.004995 log_softmax: 0.008016\n",
            "Train Epoch: 87 [1000/1000 (100%)]\tLosses F.softmax: 0.001970 log_softmax: 0.000068\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9731\tAccuracy: 8080.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9897\tAccuracy: 8059.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 88 [0/1000 (0%)]\tLosses F.softmax: 0.006821 log_softmax: 0.000819\n",
            "Train Epoch: 88 [200/1000 (20%)]\tLosses F.softmax: 0.010185 log_softmax: 0.001814\n",
            "Train Epoch: 88 [400/1000 (40%)]\tLosses F.softmax: 0.000227 log_softmax: 0.000133\n",
            "Train Epoch: 88 [600/1000 (60%)]\tLosses F.softmax: 0.024880 log_softmax: 0.029855\n",
            "Train Epoch: 88 [800/1000 (80%)]\tLosses F.softmax: 0.000006 log_softmax: 0.000005\n",
            "Train Epoch: 88 [1000/1000 (100%)]\tLosses F.softmax: 0.000529 log_softmax: 0.000844\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9736\tAccuracy: 8080.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9910\tAccuracy: 8047.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 89 [0/1000 (0%)]\tLosses F.softmax: 0.003590 log_softmax: 0.011474\n",
            "Train Epoch: 89 [200/1000 (20%)]\tLosses F.softmax: 0.005774 log_softmax: 0.000413\n",
            "Train Epoch: 89 [400/1000 (40%)]\tLosses F.softmax: 0.000026 log_softmax: 0.000005\n",
            "Train Epoch: 89 [600/1000 (60%)]\tLosses F.softmax: 0.004115 log_softmax: 0.008119\n",
            "Train Epoch: 89 [800/1000 (80%)]\tLosses F.softmax: 0.035966 log_softmax: 0.012948\n",
            "Train Epoch: 89 [1000/1000 (100%)]\tLosses F.softmax: 0.000668 log_softmax: 0.000225\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9745\tAccuracy: 8081.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9923\tAccuracy: 8047.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 90 [0/1000 (0%)]\tLosses F.softmax: 0.003008 log_softmax: 0.013314\n",
            "Train Epoch: 90 [200/1000 (20%)]\tLosses F.softmax: 0.000029 log_softmax: 0.000063\n",
            "Train Epoch: 90 [400/1000 (40%)]\tLosses F.softmax: 0.005338 log_softmax: 0.003839\n",
            "Train Epoch: 90 [600/1000 (60%)]\tLosses F.softmax: 0.011766 log_softmax: 0.013705\n",
            "Train Epoch: 90 [800/1000 (80%)]\tLosses F.softmax: 0.001242 log_softmax: 0.000415\n",
            "Train Epoch: 90 [1000/1000 (100%)]\tLosses F.softmax: 0.119042 log_softmax: 0.032052\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9738\tAccuracy: 8086.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9914\tAccuracy: 8068.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 91 [0/1000 (0%)]\tLosses F.softmax: 0.025622 log_softmax: 0.028464\n",
            "Train Epoch: 91 [200/1000 (20%)]\tLosses F.softmax: 0.012092 log_softmax: 0.002657\n",
            "Train Epoch: 91 [400/1000 (40%)]\tLosses F.softmax: 0.004512 log_softmax: 0.001499\n",
            "Train Epoch: 91 [600/1000 (60%)]\tLosses F.softmax: 0.032503 log_softmax: 0.005293\n",
            "Train Epoch: 91 [800/1000 (80%)]\tLosses F.softmax: 0.000632 log_softmax: 0.001100\n",
            "Train Epoch: 91 [1000/1000 (100%)]\tLosses F.softmax: 0.014249 log_softmax: 0.005844\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9758\tAccuracy: 8082.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9926\tAccuracy: 8063.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 92 [0/1000 (0%)]\tLosses F.softmax: 0.000270 log_softmax: 0.000033\n",
            "Train Epoch: 92 [200/1000 (20%)]\tLosses F.softmax: 0.060751 log_softmax: 0.046195\n",
            "Train Epoch: 92 [400/1000 (40%)]\tLosses F.softmax: 0.084965 log_softmax: 0.075616\n",
            "Train Epoch: 92 [600/1000 (60%)]\tLosses F.softmax: 0.004497 log_softmax: 0.000806\n",
            "Train Epoch: 92 [800/1000 (80%)]\tLosses F.softmax: 0.014822 log_softmax: 0.000124\n",
            "Train Epoch: 92 [1000/1000 (100%)]\tLosses F.softmax: 0.003110 log_softmax: 0.003247\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9758\tAccuracy: 8086.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9937\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 93 [0/1000 (0%)]\tLosses F.softmax: 0.011113 log_softmax: 0.013542\n",
            "Train Epoch: 93 [200/1000 (20%)]\tLosses F.softmax: 0.000285 log_softmax: 0.005079\n",
            "Train Epoch: 93 [400/1000 (40%)]\tLosses F.softmax: 0.029021 log_softmax: 0.017962\n",
            "Train Epoch: 93 [600/1000 (60%)]\tLosses F.softmax: 0.000942 log_softmax: 0.000577\n",
            "Train Epoch: 93 [800/1000 (80%)]\tLosses F.softmax: 0.015178 log_softmax: 0.009608\n",
            "Train Epoch: 93 [1000/1000 (100%)]\tLosses F.softmax: 0.001213 log_softmax: 0.000400\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9759\tAccuracy: 8092.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9942\tAccuracy: 8050.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 94 [0/1000 (0%)]\tLosses F.softmax: 0.001902 log_softmax: 0.001952\n",
            "Train Epoch: 94 [200/1000 (20%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000000\n",
            "Train Epoch: 94 [400/1000 (40%)]\tLosses F.softmax: 0.049092 log_softmax: 0.009518\n",
            "Train Epoch: 94 [600/1000 (60%)]\tLosses F.softmax: 0.001281 log_softmax: 0.003505\n",
            "Train Epoch: 94 [800/1000 (80%)]\tLosses F.softmax: 0.000226 log_softmax: 0.000021\n",
            "Train Epoch: 94 [1000/1000 (100%)]\tLosses F.softmax: 0.000305 log_softmax: 0.000019\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9788\tAccuracy: 8076.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9961\tAccuracy: 8048.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 95 [0/1000 (0%)]\tLosses F.softmax: 0.000095 log_softmax: 0.000026\n",
            "Train Epoch: 95 [200/1000 (20%)]\tLosses F.softmax: 0.000927 log_softmax: 0.001259\n",
            "Train Epoch: 95 [400/1000 (40%)]\tLosses F.softmax: 0.000048 log_softmax: 0.000007\n",
            "Train Epoch: 95 [600/1000 (60%)]\tLosses F.softmax: 0.038357 log_softmax: 0.013379\n",
            "Train Epoch: 95 [800/1000 (80%)]\tLosses F.softmax: 0.000719 log_softmax: 0.000159\n",
            "Train Epoch: 95 [1000/1000 (100%)]\tLosses F.softmax: 0.000614 log_softmax: 0.000163\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9787\tAccuracy: 8082.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9965\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 96 [0/1000 (0%)]\tLosses F.softmax: 0.000067 log_softmax: 0.000022\n",
            "Train Epoch: 96 [200/1000 (20%)]\tLosses F.softmax: 0.011498 log_softmax: 0.005351\n",
            "Train Epoch: 96 [400/1000 (40%)]\tLosses F.softmax: 0.000163 log_softmax: 0.000063\n",
            "Train Epoch: 96 [600/1000 (60%)]\tLosses F.softmax: 0.001721 log_softmax: 0.000343\n",
            "Train Epoch: 96 [800/1000 (80%)]\tLosses F.softmax: 0.022114 log_softmax: 0.017657\n",
            "Train Epoch: 96 [1000/1000 (100%)]\tLosses F.softmax: 0.051501 log_softmax: 0.028633\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9806\tAccuracy: 8078.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9983\tAccuracy: 8048.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 97 [0/1000 (0%)]\tLosses F.softmax: 0.002410 log_softmax: 0.002166\n",
            "Train Epoch: 97 [200/1000 (20%)]\tLosses F.softmax: 0.005675 log_softmax: 0.004813\n",
            "Train Epoch: 97 [400/1000 (40%)]\tLosses F.softmax: 0.000002 log_softmax: 0.000009\n",
            "Train Epoch: 97 [600/1000 (60%)]\tLosses F.softmax: 0.000429 log_softmax: 0.000509\n",
            "Train Epoch: 97 [800/1000 (80%)]\tLosses F.softmax: 0.009364 log_softmax: 0.010220\n",
            "Train Epoch: 97 [1000/1000 (100%)]\tLosses F.softmax: 0.000157 log_softmax: 0.000098\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9810\tAccuracy: 8082.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9993\tAccuracy: 8047.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 98 [0/1000 (0%)]\tLosses F.softmax: 0.001074 log_softmax: 0.001403\n",
            "Train Epoch: 98 [200/1000 (20%)]\tLosses F.softmax: 0.036466 log_softmax: 0.037733\n",
            "Train Epoch: 98 [400/1000 (40%)]\tLosses F.softmax: 0.000043 log_softmax: 0.000011\n",
            "Train Epoch: 98 [600/1000 (60%)]\tLosses F.softmax: 0.056773 log_softmax: 0.058136\n",
            "Train Epoch: 98 [800/1000 (80%)]\tLosses F.softmax: 0.000129 log_softmax: 0.000067\n",
            "Train Epoch: 98 [1000/1000 (100%)]\tLosses F.softmax: 0.001309 log_softmax: 0.001011\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9821\tAccuracy: 8082.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9997\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 99 [0/1000 (0%)]\tLosses F.softmax: 0.001307 log_softmax: 0.001009\n",
            "Train Epoch: 99 [200/1000 (20%)]\tLosses F.softmax: 0.000204 log_softmax: 0.000196\n",
            "Train Epoch: 99 [400/1000 (40%)]\tLosses F.softmax: 0.034371 log_softmax: 0.027323\n",
            "Train Epoch: 99 [600/1000 (60%)]\tLosses F.softmax: 0.000126 log_softmax: 0.000010\n",
            "Train Epoch: 99 [800/1000 (80%)]\tLosses F.softmax: 0.022779 log_softmax: 0.036534\n",
            "Train Epoch: 99 [1000/1000 (100%)]\tLosses F.softmax: 0.048351 log_softmax: 0.034974\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9809\tAccuracy: 8092.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9998\tAccuracy: 8062.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 100 [0/1000 (0%)]\tLosses F.softmax: 0.008137 log_softmax: 0.007672\n",
            "Train Epoch: 100 [200/1000 (20%)]\tLosses F.softmax: 0.000498 log_softmax: 0.000754\n",
            "Train Epoch: 100 [400/1000 (40%)]\tLosses F.softmax: 0.000018 log_softmax: 0.000024\n",
            "Train Epoch: 100 [600/1000 (60%)]\tLosses F.softmax: 0.002959 log_softmax: 0.003132\n",
            "Train Epoch: 100 [800/1000 (80%)]\tLosses F.softmax: 0.013196 log_softmax: 0.005532\n",
            "Train Epoch: 100 [1000/1000 (100%)]\tLosses F.softmax: 0.000630 log_softmax: 0.001007\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9825\tAccuracy: 8081.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0013\tAccuracy: 8048.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 101 [0/1000 (0%)]\tLosses F.softmax: 0.000465 log_softmax: 0.000667\n",
            "Train Epoch: 101 [200/1000 (20%)]\tLosses F.softmax: 0.015311 log_softmax: 0.001763\n",
            "Train Epoch: 101 [400/1000 (40%)]\tLosses F.softmax: 0.028888 log_softmax: 0.032487\n",
            "Train Epoch: 101 [600/1000 (60%)]\tLosses F.softmax: 0.000598 log_softmax: 0.000079\n",
            "Train Epoch: 101 [800/1000 (80%)]\tLosses F.softmax: 0.000302 log_softmax: 0.003839\n",
            "Train Epoch: 101 [1000/1000 (100%)]\tLosses F.softmax: 0.000813 log_softmax: 0.000499\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9834\tAccuracy: 8081.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0020\tAccuracy: 8050.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 102 [0/1000 (0%)]\tLosses F.softmax: 0.000311 log_softmax: 0.000394\n",
            "Train Epoch: 102 [200/1000 (20%)]\tLosses F.softmax: 0.051338 log_softmax: 0.029747\n",
            "Train Epoch: 102 [400/1000 (40%)]\tLosses F.softmax: 0.145957 log_softmax: 0.090776\n",
            "Train Epoch: 102 [600/1000 (60%)]\tLosses F.softmax: 0.004999 log_softmax: 0.004004\n",
            "Train Epoch: 102 [800/1000 (80%)]\tLosses F.softmax: 0.000616 log_softmax: 0.000690\n",
            "Train Epoch: 102 [1000/1000 (100%)]\tLosses F.softmax: 0.032842 log_softmax: 0.043201\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9833\tAccuracy: 8086.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0019\tAccuracy: 8068.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 103 [0/1000 (0%)]\tLosses F.softmax: 0.000502 log_softmax: 0.001523\n",
            "Train Epoch: 103 [200/1000 (20%)]\tLosses F.softmax: 0.019176 log_softmax: 0.012891\n",
            "Train Epoch: 103 [400/1000 (40%)]\tLosses F.softmax: 0.000054 log_softmax: 0.000038\n",
            "Train Epoch: 103 [600/1000 (60%)]\tLosses F.softmax: 0.000552 log_softmax: 0.000015\n",
            "Train Epoch: 103 [800/1000 (80%)]\tLosses F.softmax: 0.000005 log_softmax: 0.000048\n",
            "Train Epoch: 103 [1000/1000 (100%)]\tLosses F.softmax: 0.000004 log_softmax: 0.000009\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9844\tAccuracy: 8084.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0032\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 104 [0/1000 (0%)]\tLosses F.softmax: 0.014516 log_softmax: 0.010808\n",
            "Train Epoch: 104 [200/1000 (20%)]\tLosses F.softmax: 0.018218 log_softmax: 0.025949\n",
            "Train Epoch: 104 [400/1000 (40%)]\tLosses F.softmax: 0.061101 log_softmax: 0.042335\n",
            "Train Epoch: 104 [600/1000 (60%)]\tLosses F.softmax: 0.000099 log_softmax: 0.000018\n",
            "Train Epoch: 104 [800/1000 (80%)]\tLosses F.softmax: 0.078934 log_softmax: 0.085375\n",
            "Train Epoch: 104 [1000/1000 (100%)]\tLosses F.softmax: 0.023067 log_softmax: 0.039927\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9855\tAccuracy: 8082.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0038\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 105 [0/1000 (0%)]\tLosses F.softmax: 0.038785 log_softmax: 0.015030\n",
            "Train Epoch: 105 [200/1000 (20%)]\tLosses F.softmax: 0.018393 log_softmax: 0.006214\n",
            "Train Epoch: 105 [400/1000 (40%)]\tLosses F.softmax: 0.000206 log_softmax: 0.000086\n",
            "Train Epoch: 105 [600/1000 (60%)]\tLosses F.softmax: 0.035795 log_softmax: 0.048705\n",
            "Train Epoch: 105 [800/1000 (80%)]\tLosses F.softmax: 0.000835 log_softmax: 0.015182\n",
            "Train Epoch: 105 [1000/1000 (100%)]\tLosses F.softmax: 0.002368 log_softmax: 0.003137\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9879\tAccuracy: 8078.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0051\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 106 [0/1000 (0%)]\tLosses F.softmax: 0.000355 log_softmax: 0.000026\n",
            "Train Epoch: 106 [200/1000 (20%)]\tLosses F.softmax: 0.000459 log_softmax: 0.000506\n",
            "Train Epoch: 106 [400/1000 (40%)]\tLosses F.softmax: 0.005639 log_softmax: 0.000748\n",
            "Train Epoch: 106 [600/1000 (60%)]\tLosses F.softmax: 0.000081 log_softmax: 0.000024\n",
            "Train Epoch: 106 [800/1000 (80%)]\tLosses F.softmax: 0.013939 log_softmax: 0.001800\n",
            "Train Epoch: 106 [1000/1000 (100%)]\tLosses F.softmax: 0.001107 log_softmax: 0.000363\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9882\tAccuracy: 8078.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0066\tAccuracy: 8047.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 107 [0/1000 (0%)]\tLosses F.softmax: 0.000343 log_softmax: 0.000043\n",
            "Train Epoch: 107 [200/1000 (20%)]\tLosses F.softmax: 0.000006 log_softmax: 0.000000\n",
            "Train Epoch: 107 [400/1000 (40%)]\tLosses F.softmax: 0.000018 log_softmax: 0.000017\n",
            "Train Epoch: 107 [600/1000 (60%)]\tLosses F.softmax: 0.000011 log_softmax: 0.000002\n",
            "Train Epoch: 107 [800/1000 (80%)]\tLosses F.softmax: 0.043649 log_softmax: 0.039341\n",
            "Train Epoch: 107 [1000/1000 (100%)]\tLosses F.softmax: 0.044473 log_softmax: 0.031005\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9879\tAccuracy: 8082.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0065\tAccuracy: 8058.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 108 [0/1000 (0%)]\tLosses F.softmax: 0.004186 log_softmax: 0.000486\n",
            "Train Epoch: 108 [200/1000 (20%)]\tLosses F.softmax: 0.001699 log_softmax: 0.002248\n",
            "Train Epoch: 108 [400/1000 (40%)]\tLosses F.softmax: 0.001050 log_softmax: 0.000099\n",
            "Train Epoch: 108 [600/1000 (60%)]\tLosses F.softmax: 0.000011 log_softmax: 0.000002\n",
            "Train Epoch: 108 [800/1000 (80%)]\tLosses F.softmax: 0.039054 log_softmax: 0.016015\n",
            "Train Epoch: 108 [1000/1000 (100%)]\tLosses F.softmax: 0.035162 log_softmax: 0.012123\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9880\tAccuracy: 8089.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0071\tAccuracy: 8057.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 109 [0/1000 (0%)]\tLosses F.softmax: 0.000049 log_softmax: 0.000014\n",
            "Train Epoch: 109 [200/1000 (20%)]\tLosses F.softmax: 0.021655 log_softmax: 0.035303\n",
            "Train Epoch: 109 [400/1000 (40%)]\tLosses F.softmax: 0.027802 log_softmax: 0.020478\n",
            "Train Epoch: 109 [600/1000 (60%)]\tLosses F.softmax: 0.039839 log_softmax: 0.066643\n",
            "Train Epoch: 109 [800/1000 (80%)]\tLosses F.softmax: 0.000064 log_softmax: 0.000020\n",
            "Train Epoch: 109 [1000/1000 (100%)]\tLosses F.softmax: 0.000274 log_softmax: 0.003461\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9900\tAccuracy: 8083.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0087\tAccuracy: 8059.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 110 [0/1000 (0%)]\tLosses F.softmax: 0.052699 log_softmax: 0.016019\n",
            "Train Epoch: 110 [200/1000 (20%)]\tLosses F.softmax: 0.001726 log_softmax: 0.006992\n",
            "Train Epoch: 110 [400/1000 (40%)]\tLosses F.softmax: 0.022093 log_softmax: 0.016232\n",
            "Train Epoch: 110 [600/1000 (60%)]\tLosses F.softmax: 0.011231 log_softmax: 0.010127\n",
            "Train Epoch: 110 [800/1000 (80%)]\tLosses F.softmax: 0.000007 log_softmax: 0.000249\n",
            "Train Epoch: 110 [1000/1000 (100%)]\tLosses F.softmax: 0.000857 log_softmax: 0.002849\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9905\tAccuracy: 8087.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0094\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 111 [0/1000 (0%)]\tLosses F.softmax: 0.003062 log_softmax: 0.001216\n",
            "Train Epoch: 111 [200/1000 (20%)]\tLosses F.softmax: 0.063374 log_softmax: 0.121932\n",
            "Train Epoch: 111 [400/1000 (40%)]\tLosses F.softmax: 0.003629 log_softmax: 0.006355\n",
            "Train Epoch: 111 [600/1000 (60%)]\tLosses F.softmax: 0.000181 log_softmax: 0.000020\n",
            "Train Epoch: 111 [800/1000 (80%)]\tLosses F.softmax: 0.007244 log_softmax: 0.017388\n",
            "Train Epoch: 111 [1000/1000 (100%)]\tLosses F.softmax: 0.000451 log_softmax: 0.000303\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9907\tAccuracy: 8084.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0093\tAccuracy: 8066.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 112 [0/1000 (0%)]\tLosses F.softmax: 0.031831 log_softmax: 0.038615\n",
            "Train Epoch: 112 [200/1000 (20%)]\tLosses F.softmax: 0.005460 log_softmax: 0.002207\n",
            "Train Epoch: 112 [400/1000 (40%)]\tLosses F.softmax: 0.000004 log_softmax: 0.000043\n",
            "Train Epoch: 112 [600/1000 (60%)]\tLosses F.softmax: 0.041209 log_softmax: 0.026187\n",
            "Train Epoch: 112 [800/1000 (80%)]\tLosses F.softmax: 0.001410 log_softmax: 0.003797\n",
            "Train Epoch: 112 [1000/1000 (100%)]\tLosses F.softmax: 0.000189 log_softmax: 0.000202\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9912\tAccuracy: 8085.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0104\tAccuracy: 8050.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 113 [0/1000 (0%)]\tLosses F.softmax: 0.012314 log_softmax: 0.024244\n",
            "Train Epoch: 113 [200/1000 (20%)]\tLosses F.softmax: 0.027686 log_softmax: 0.019411\n",
            "Train Epoch: 113 [400/1000 (40%)]\tLosses F.softmax: 0.002094 log_softmax: 0.002558\n",
            "Train Epoch: 113 [600/1000 (60%)]\tLosses F.softmax: 0.000983 log_softmax: 0.000156\n",
            "Train Epoch: 113 [800/1000 (80%)]\tLosses F.softmax: 0.032010 log_softmax: 0.015569\n",
            "Train Epoch: 113 [1000/1000 (100%)]\tLosses F.softmax: 0.002648 log_softmax: 0.000659\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9928\tAccuracy: 8088.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0117\tAccuracy: 8052.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 114 [0/1000 (0%)]\tLosses F.softmax: 0.002935 log_softmax: 0.003460\n",
            "Train Epoch: 114 [200/1000 (20%)]\tLosses F.softmax: 0.000151 log_softmax: 0.000061\n",
            "Train Epoch: 114 [400/1000 (40%)]\tLosses F.softmax: 0.000022 log_softmax: 0.000017\n",
            "Train Epoch: 114 [600/1000 (60%)]\tLosses F.softmax: 0.000043 log_softmax: 0.000011\n",
            "Train Epoch: 114 [800/1000 (80%)]\tLosses F.softmax: 0.002089 log_softmax: 0.002617\n",
            "Train Epoch: 114 [1000/1000 (100%)]\tLosses F.softmax: 0.000640 log_softmax: 0.000683\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9921\tAccuracy: 8087.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0116\tAccuracy: 8061.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 115 [0/1000 (0%)]\tLosses F.softmax: 0.005794 log_softmax: 0.001253\n",
            "Train Epoch: 115 [200/1000 (20%)]\tLosses F.softmax: 0.000005 log_softmax: 0.000012\n",
            "Train Epoch: 115 [400/1000 (40%)]\tLosses F.softmax: 0.021730 log_softmax: 0.010723\n",
            "Train Epoch: 115 [600/1000 (60%)]\tLosses F.softmax: 0.024456 log_softmax: 0.014659\n",
            "Train Epoch: 115 [800/1000 (80%)]\tLosses F.softmax: 0.000008 log_softmax: 0.000003\n",
            "Train Epoch: 115 [1000/1000 (100%)]\tLosses F.softmax: 0.067228 log_softmax: 0.044734\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9938\tAccuracy: 8090.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0136\tAccuracy: 8050.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 116 [0/1000 (0%)]\tLosses F.softmax: 0.000277 log_softmax: 0.000322\n",
            "Train Epoch: 116 [200/1000 (20%)]\tLosses F.softmax: 0.040462 log_softmax: 0.076896\n",
            "Train Epoch: 116 [400/1000 (40%)]\tLosses F.softmax: 0.057997 log_softmax: 0.090486\n",
            "Train Epoch: 116 [600/1000 (60%)]\tLosses F.softmax: 0.006874 log_softmax: 0.002337\n",
            "Train Epoch: 116 [800/1000 (80%)]\tLosses F.softmax: 0.005219 log_softmax: 0.006553\n",
            "Train Epoch: 116 [1000/1000 (100%)]\tLosses F.softmax: 0.000733 log_softmax: 0.002126\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9945\tAccuracy: 8081.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0136\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 117 [0/1000 (0%)]\tLosses F.softmax: 0.019500 log_softmax: 0.031242\n",
            "Train Epoch: 117 [200/1000 (20%)]\tLosses F.softmax: 0.004452 log_softmax: 0.003693\n",
            "Train Epoch: 117 [400/1000 (40%)]\tLosses F.softmax: 0.000011 log_softmax: 0.000005\n",
            "Train Epoch: 117 [600/1000 (60%)]\tLosses F.softmax: 0.000725 log_softmax: 0.004374\n",
            "Train Epoch: 117 [800/1000 (80%)]\tLosses F.softmax: 0.000627 log_softmax: 0.000692\n",
            "Train Epoch: 117 [1000/1000 (100%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000024\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9974\tAccuracy: 8077.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0158\tAccuracy: 8048.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 118 [0/1000 (0%)]\tLosses F.softmax: 0.133235 log_softmax: 0.077349\n",
            "Train Epoch: 118 [200/1000 (20%)]\tLosses F.softmax: 0.011599 log_softmax: 0.001143\n",
            "Train Epoch: 118 [400/1000 (40%)]\tLosses F.softmax: 0.000004 log_softmax: 0.000000\n",
            "Train Epoch: 118 [600/1000 (60%)]\tLosses F.softmax: 0.001214 log_softmax: 0.000017\n",
            "Train Epoch: 118 [800/1000 (80%)]\tLosses F.softmax: 0.000011 log_softmax: 0.000003\n",
            "Train Epoch: 118 [1000/1000 (100%)]\tLosses F.softmax: 0.062852 log_softmax: 0.015525\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9964\tAccuracy: 8083.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0156\tAccuracy: 8057.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 119 [0/1000 (0%)]\tLosses F.softmax: 0.012060 log_softmax: 0.012484\n",
            "Train Epoch: 119 [200/1000 (20%)]\tLosses F.softmax: 0.009733 log_softmax: 0.010970\n",
            "Train Epoch: 119 [400/1000 (40%)]\tLosses F.softmax: 0.041022 log_softmax: 0.029443\n",
            "Train Epoch: 119 [600/1000 (60%)]\tLosses F.softmax: 0.010055 log_softmax: 0.003097\n",
            "Train Epoch: 119 [800/1000 (80%)]\tLosses F.softmax: 0.011802 log_softmax: 0.019295\n",
            "Train Epoch: 119 [1000/1000 (100%)]\tLosses F.softmax: 0.001088 log_softmax: 0.002468\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9962\tAccuracy: 8089.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0160\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 120 [0/1000 (0%)]\tLosses F.softmax: 0.002748 log_softmax: 0.005420\n",
            "Train Epoch: 120 [200/1000 (20%)]\tLosses F.softmax: 0.058251 log_softmax: 0.061668\n",
            "Train Epoch: 120 [400/1000 (40%)]\tLosses F.softmax: 0.000956 log_softmax: 0.004535\n",
            "Train Epoch: 120 [600/1000 (60%)]\tLosses F.softmax: 0.127843 log_softmax: 0.053981\n",
            "Train Epoch: 120 [800/1000 (80%)]\tLosses F.softmax: 0.010603 log_softmax: 0.010356\n",
            "Train Epoch: 120 [1000/1000 (100%)]\tLosses F.softmax: 0.000204 log_softmax: 0.000023\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9976\tAccuracy: 8084.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0169\tAccuracy: 8058.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 121 [0/1000 (0%)]\tLosses F.softmax: 0.022311 log_softmax: 0.014382\n",
            "Train Epoch: 121 [200/1000 (20%)]\tLosses F.softmax: 0.000004 log_softmax: 0.000051\n",
            "Train Epoch: 121 [400/1000 (40%)]\tLosses F.softmax: 0.004511 log_softmax: 0.009834\n",
            "Train Epoch: 121 [600/1000 (60%)]\tLosses F.softmax: 0.006006 log_softmax: 0.001543\n",
            "Train Epoch: 121 [800/1000 (80%)]\tLosses F.softmax: 0.006483 log_softmax: 0.004120\n",
            "Train Epoch: 121 [1000/1000 (100%)]\tLosses F.softmax: 0.077488 log_softmax: 0.101033\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9975\tAccuracy: 8090.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0175\tAccuracy: 8052.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 122 [0/1000 (0%)]\tLosses F.softmax: 0.001983 log_softmax: 0.003560\n",
            "Train Epoch: 122 [200/1000 (20%)]\tLosses F.softmax: 0.052564 log_softmax: 0.037589\n",
            "Train Epoch: 122 [400/1000 (40%)]\tLosses F.softmax: 0.000042 log_softmax: 0.000012\n",
            "Train Epoch: 122 [600/1000 (60%)]\tLosses F.softmax: 0.041795 log_softmax: 0.024237\n",
            "Train Epoch: 122 [800/1000 (80%)]\tLosses F.softmax: 0.000157 log_softmax: 0.000069\n",
            "Train Epoch: 122 [1000/1000 (100%)]\tLosses F.softmax: 0.000020 log_softmax: 0.000016\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9986\tAccuracy: 8089.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0181\tAccuracy: 8052.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 123 [0/1000 (0%)]\tLosses F.softmax: 0.046105 log_softmax: 0.066370\n",
            "Train Epoch: 123 [200/1000 (20%)]\tLosses F.softmax: 0.000961 log_softmax: 0.000018\n",
            "Train Epoch: 123 [400/1000 (40%)]\tLosses F.softmax: 0.101104 log_softmax: 0.127120\n",
            "Train Epoch: 123 [600/1000 (60%)]\tLosses F.softmax: 0.000741 log_softmax: 0.000253\n",
            "Train Epoch: 123 [800/1000 (80%)]\tLosses F.softmax: 0.140240 log_softmax: 0.057894\n",
            "Train Epoch: 123 [1000/1000 (100%)]\tLosses F.softmax: 0.002319 log_softmax: 0.011321\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9985\tAccuracy: 8085.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0181\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 124 [0/1000 (0%)]\tLosses F.softmax: 0.000124 log_softmax: 0.000020\n",
            "Train Epoch: 124 [200/1000 (20%)]\tLosses F.softmax: 0.016689 log_softmax: 0.016186\n",
            "Train Epoch: 124 [400/1000 (40%)]\tLosses F.softmax: 0.000759 log_softmax: 0.000813\n",
            "Train Epoch: 124 [600/1000 (60%)]\tLosses F.softmax: 0.009148 log_softmax: 0.007974\n",
            "Train Epoch: 124 [800/1000 (80%)]\tLosses F.softmax: 0.000009 log_softmax: 0.000002\n",
            "Train Epoch: 124 [1000/1000 (100%)]\tLosses F.softmax: 0.005571 log_softmax: 0.000062\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9994\tAccuracy: 8083.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0190\tAccuracy: 8059.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 125 [0/1000 (0%)]\tLosses F.softmax: 0.000568 log_softmax: 0.000923\n",
            "Train Epoch: 125 [200/1000 (20%)]\tLosses F.softmax: 0.037140 log_softmax: 0.099430\n",
            "Train Epoch: 125 [400/1000 (40%)]\tLosses F.softmax: 0.000733 log_softmax: 0.000422\n",
            "Train Epoch: 125 [600/1000 (60%)]\tLosses F.softmax: 0.114722 log_softmax: 0.080172\n",
            "Train Epoch: 125 [800/1000 (80%)]\tLosses F.softmax: 0.000696 log_softmax: 0.004216\n",
            "Train Epoch: 125 [1000/1000 (100%)]\tLosses F.softmax: 0.022542 log_softmax: 0.023382\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0001\tAccuracy: 8085.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0198\tAccuracy: 8064.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 126 [0/1000 (0%)]\tLosses F.softmax: 0.000005 log_softmax: 0.000007\n",
            "Train Epoch: 126 [200/1000 (20%)]\tLosses F.softmax: 0.042469 log_softmax: 0.032204\n",
            "Train Epoch: 126 [400/1000 (40%)]\tLosses F.softmax: 0.021051 log_softmax: 0.045158\n",
            "Train Epoch: 126 [600/1000 (60%)]\tLosses F.softmax: 0.077313 log_softmax: 0.064214\n",
            "Train Epoch: 126 [800/1000 (80%)]\tLosses F.softmax: 0.000018 log_softmax: 0.000013\n",
            "Train Epoch: 126 [1000/1000 (100%)]\tLosses F.softmax: 0.000084 log_softmax: 0.000020\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0007\tAccuracy: 8087.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0204\tAccuracy: 8060.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 127 [0/1000 (0%)]\tLosses F.softmax: 0.000347 log_softmax: 0.000042\n",
            "Train Epoch: 127 [200/1000 (20%)]\tLosses F.softmax: 0.002217 log_softmax: 0.000609\n",
            "Train Epoch: 127 [400/1000 (40%)]\tLosses F.softmax: 0.034841 log_softmax: 0.012727\n",
            "Train Epoch: 127 [600/1000 (60%)]\tLosses F.softmax: 0.000286 log_softmax: 0.000841\n",
            "Train Epoch: 127 [800/1000 (80%)]\tLosses F.softmax: 0.009040 log_softmax: 0.001395\n",
            "Train Epoch: 127 [1000/1000 (100%)]\tLosses F.softmax: 0.000028 log_softmax: 0.000005\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0018\tAccuracy: 8087.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0216\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 128 [0/1000 (0%)]\tLosses F.softmax: 0.074213 log_softmax: 0.067855\n",
            "Train Epoch: 128 [200/1000 (20%)]\tLosses F.softmax: 0.111178 log_softmax: 0.105296\n",
            "Train Epoch: 128 [400/1000 (40%)]\tLosses F.softmax: 0.000022 log_softmax: 0.000016\n",
            "Train Epoch: 128 [600/1000 (60%)]\tLosses F.softmax: 0.003993 log_softmax: 0.001023\n",
            "Train Epoch: 128 [800/1000 (80%)]\tLosses F.softmax: 0.000600 log_softmax: 0.000293\n",
            "Train Epoch: 128 [1000/1000 (100%)]\tLosses F.softmax: 0.005615 log_softmax: 0.019625\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0042\tAccuracy: 8079.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0240\tAccuracy: 8046.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 129 [0/1000 (0%)]\tLosses F.softmax: 0.000102 log_softmax: 0.000235\n",
            "Train Epoch: 129 [200/1000 (20%)]\tLosses F.softmax: 0.000019 log_softmax: 0.001672\n",
            "Train Epoch: 129 [400/1000 (40%)]\tLosses F.softmax: 0.084676 log_softmax: 0.072079\n",
            "Train Epoch: 129 [600/1000 (60%)]\tLosses F.softmax: 0.000505 log_softmax: 0.000013\n",
            "Train Epoch: 129 [800/1000 (80%)]\tLosses F.softmax: 0.000254 log_softmax: 0.000753\n",
            "Train Epoch: 129 [1000/1000 (100%)]\tLosses F.softmax: 0.001618 log_softmax: 0.000627\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0030\tAccuracy: 8093.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0233\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 130 [0/1000 (0%)]\tLosses F.softmax: 0.000057 log_softmax: 0.000004\n",
            "Train Epoch: 130 [200/1000 (20%)]\tLosses F.softmax: 0.005860 log_softmax: 0.015661\n",
            "Train Epoch: 130 [400/1000 (40%)]\tLosses F.softmax: 0.001610 log_softmax: 0.001830\n",
            "Train Epoch: 130 [600/1000 (60%)]\tLosses F.softmax: 0.001931 log_softmax: 0.000573\n",
            "Train Epoch: 130 [800/1000 (80%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000000\n",
            "Train Epoch: 130 [1000/1000 (100%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000001\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0051\tAccuracy: 8080.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0254\tAccuracy: 8046.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 131 [0/1000 (0%)]\tLosses F.softmax: 0.000130 log_softmax: 0.000090\n",
            "Train Epoch: 131 [200/1000 (20%)]\tLosses F.softmax: 0.005596 log_softmax: 0.003211\n",
            "Train Epoch: 131 [400/1000 (40%)]\tLosses F.softmax: 0.013669 log_softmax: 0.008564\n",
            "Train Epoch: 131 [600/1000 (60%)]\tLosses F.softmax: 0.002400 log_softmax: 0.004433\n",
            "Train Epoch: 131 [800/1000 (80%)]\tLosses F.softmax: 0.000003 log_softmax: 0.000058\n",
            "Train Epoch: 131 [1000/1000 (100%)]\tLosses F.softmax: 0.000092 log_softmax: 0.000101\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0049\tAccuracy: 8085.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0250\tAccuracy: 8059.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 132 [0/1000 (0%)]\tLosses F.softmax: 0.000379 log_softmax: 0.000141\n",
            "Train Epoch: 132 [200/1000 (20%)]\tLosses F.softmax: 0.012784 log_softmax: 0.003828\n",
            "Train Epoch: 132 [400/1000 (40%)]\tLosses F.softmax: 0.002856 log_softmax: 0.005191\n",
            "Train Epoch: 132 [600/1000 (60%)]\tLosses F.softmax: 0.002608 log_softmax: 0.000495\n",
            "Train Epoch: 132 [800/1000 (80%)]\tLosses F.softmax: 0.000843 log_softmax: 0.004752\n",
            "Train Epoch: 132 [1000/1000 (100%)]\tLosses F.softmax: 0.115044 log_softmax: 0.080898\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0061\tAccuracy: 8081.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0262\tAccuracy: 8050.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 133 [0/1000 (0%)]\tLosses F.softmax: 0.007797 log_softmax: 0.011112\n",
            "Train Epoch: 133 [200/1000 (20%)]\tLosses F.softmax: 0.014549 log_softmax: 0.029102\n",
            "Train Epoch: 133 [400/1000 (40%)]\tLosses F.softmax: 0.001227 log_softmax: 0.001128\n",
            "Train Epoch: 133 [600/1000 (60%)]\tLosses F.softmax: 0.000081 log_softmax: 0.000024\n",
            "Train Epoch: 133 [800/1000 (80%)]\tLosses F.softmax: 0.000152 log_softmax: 0.000165\n",
            "Train Epoch: 133 [1000/1000 (100%)]\tLosses F.softmax: 0.005602 log_softmax: 0.004282\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0057\tAccuracy: 8089.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0264\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 134 [0/1000 (0%)]\tLosses F.softmax: 0.000044 log_softmax: 0.000005\n",
            "Train Epoch: 134 [200/1000 (20%)]\tLosses F.softmax: 0.000010 log_softmax: 0.000003\n",
            "Train Epoch: 134 [400/1000 (40%)]\tLosses F.softmax: 0.002003 log_softmax: 0.003276\n",
            "Train Epoch: 134 [600/1000 (60%)]\tLosses F.softmax: 0.000041 log_softmax: 0.000278\n",
            "Train Epoch: 134 [800/1000 (80%)]\tLosses F.softmax: 0.002387 log_softmax: 0.000347\n",
            "Train Epoch: 134 [1000/1000 (100%)]\tLosses F.softmax: 0.000004 log_softmax: 0.000000\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0070\tAccuracy: 8087.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0270\tAccuracy: 8051.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 135 [0/1000 (0%)]\tLosses F.softmax: 0.001496 log_softmax: 0.001627\n",
            "Train Epoch: 135 [200/1000 (20%)]\tLosses F.softmax: 0.002044 log_softmax: 0.003552\n",
            "Train Epoch: 135 [400/1000 (40%)]\tLosses F.softmax: 0.001812 log_softmax: 0.001330\n",
            "Train Epoch: 135 [600/1000 (60%)]\tLosses F.softmax: 0.016288 log_softmax: 0.010216\n",
            "Train Epoch: 135 [800/1000 (80%)]\tLosses F.softmax: 0.000051 log_softmax: 0.000040\n",
            "Train Epoch: 135 [1000/1000 (100%)]\tLosses F.softmax: 0.004106 log_softmax: 0.000842\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0069\tAccuracy: 8091.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0281\tAccuracy: 8051.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 136 [0/1000 (0%)]\tLosses F.softmax: 0.043031 log_softmax: 0.051340\n",
            "Train Epoch: 136 [200/1000 (20%)]\tLosses F.softmax: 0.000467 log_softmax: 0.000068\n",
            "Train Epoch: 136 [400/1000 (40%)]\tLosses F.softmax: 0.000082 log_softmax: 0.000010\n",
            "Train Epoch: 136 [600/1000 (60%)]\tLosses F.softmax: 0.026867 log_softmax: 0.073152\n",
            "Train Epoch: 136 [800/1000 (80%)]\tLosses F.softmax: 0.000002 log_softmax: 0.000243\n",
            "Train Epoch: 136 [1000/1000 (100%)]\tLosses F.softmax: 0.002575 log_softmax: 0.007492\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0091\tAccuracy: 8086.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0292\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 137 [0/1000 (0%)]\tLosses F.softmax: 0.024083 log_softmax: 0.035041\n",
            "Train Epoch: 137 [200/1000 (20%)]\tLosses F.softmax: 0.001419 log_softmax: 0.001481\n",
            "Train Epoch: 137 [400/1000 (40%)]\tLosses F.softmax: 0.000056 log_softmax: 0.000003\n",
            "Train Epoch: 137 [600/1000 (60%)]\tLosses F.softmax: 0.001251 log_softmax: 0.005951\n",
            "Train Epoch: 137 [800/1000 (80%)]\tLosses F.softmax: 0.000782 log_softmax: 0.000227\n",
            "Train Epoch: 137 [1000/1000 (100%)]\tLosses F.softmax: 0.000598 log_softmax: 0.000298\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0086\tAccuracy: 8088.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0291\tAccuracy: 8065.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 138 [0/1000 (0%)]\tLosses F.softmax: 0.022995 log_softmax: 0.013323\n",
            "Train Epoch: 138 [200/1000 (20%)]\tLosses F.softmax: 0.002792 log_softmax: 0.003254\n",
            "Train Epoch: 138 [400/1000 (40%)]\tLosses F.softmax: 0.036941 log_softmax: 0.034051\n",
            "Train Epoch: 138 [600/1000 (60%)]\tLosses F.softmax: 0.000018 log_softmax: 0.001568\n",
            "Train Epoch: 138 [800/1000 (80%)]\tLosses F.softmax: 0.024112 log_softmax: 0.021537\n",
            "Train Epoch: 138 [1000/1000 (100%)]\tLosses F.softmax: 0.000031 log_softmax: 0.000018\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0097\tAccuracy: 8088.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0303\tAccuracy: 8057.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 139 [0/1000 (0%)]\tLosses F.softmax: 0.000080 log_softmax: 0.001820\n",
            "Train Epoch: 139 [200/1000 (20%)]\tLosses F.softmax: 0.120660 log_softmax: 0.052896\n",
            "Train Epoch: 139 [400/1000 (40%)]\tLosses F.softmax: 0.028760 log_softmax: 0.067423\n",
            "Train Epoch: 139 [600/1000 (60%)]\tLosses F.softmax: 0.001073 log_softmax: 0.000019\n",
            "Train Epoch: 139 [800/1000 (80%)]\tLosses F.softmax: 0.057586 log_softmax: 0.030087\n",
            "Train Epoch: 139 [1000/1000 (100%)]\tLosses F.softmax: 0.000010 log_softmax: 0.000003\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0102\tAccuracy: 8086.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0309\tAccuracy: 8052.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 140 [0/1000 (0%)]\tLosses F.softmax: 0.004727 log_softmax: 0.000205\n",
            "Train Epoch: 140 [200/1000 (20%)]\tLosses F.softmax: 0.019271 log_softmax: 0.010083\n",
            "Train Epoch: 140 [400/1000 (40%)]\tLosses F.softmax: 0.001481 log_softmax: 0.000044\n",
            "Train Epoch: 140 [600/1000 (60%)]\tLosses F.softmax: 0.034956 log_softmax: 0.033542\n",
            "Train Epoch: 140 [800/1000 (80%)]\tLosses F.softmax: 0.009516 log_softmax: 0.007391\n",
            "Train Epoch: 140 [1000/1000 (100%)]\tLosses F.softmax: 0.010272 log_softmax: 0.019866\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0103\tAccuracy: 8089.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0311\tAccuracy: 8059.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 141 [0/1000 (0%)]\tLosses F.softmax: 0.000002 log_softmax: 0.000002\n",
            "Train Epoch: 141 [200/1000 (20%)]\tLosses F.softmax: 0.059751 log_softmax: 0.026246\n",
            "Train Epoch: 141 [400/1000 (40%)]\tLosses F.softmax: 0.001874 log_softmax: 0.011582\n",
            "Train Epoch: 141 [600/1000 (60%)]\tLosses F.softmax: 0.001506 log_softmax: 0.000516\n",
            "Train Epoch: 141 [800/1000 (80%)]\tLosses F.softmax: 0.000007 log_softmax: 0.000013\n",
            "Train Epoch: 141 [1000/1000 (100%)]\tLosses F.softmax: 0.003578 log_softmax: 0.007767\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0116\tAccuracy: 8087.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0328\tAccuracy: 8051.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 142 [0/1000 (0%)]\tLosses F.softmax: 0.000706 log_softmax: 0.000164\n",
            "Train Epoch: 142 [200/1000 (20%)]\tLosses F.softmax: 0.000007 log_softmax: 0.000055\n",
            "Train Epoch: 142 [400/1000 (40%)]\tLosses F.softmax: 0.009436 log_softmax: 0.004572\n",
            "Train Epoch: 142 [600/1000 (60%)]\tLosses F.softmax: 0.003087 log_softmax: 0.009293\n",
            "Train Epoch: 142 [800/1000 (80%)]\tLosses F.softmax: 0.038274 log_softmax: 0.047743\n",
            "Train Epoch: 142 [1000/1000 (100%)]\tLosses F.softmax: 0.022842 log_softmax: 0.023308\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0140\tAccuracy: 8079.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0351\tAccuracy: 8044.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 143 [0/1000 (0%)]\tLosses F.softmax: 0.021991 log_softmax: 0.016947\n",
            "Train Epoch: 143 [200/1000 (20%)]\tLosses F.softmax: 0.008724 log_softmax: 0.006147\n",
            "Train Epoch: 143 [400/1000 (40%)]\tLosses F.softmax: 0.023826 log_softmax: 0.013914\n",
            "Train Epoch: 143 [600/1000 (60%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000004\n",
            "Train Epoch: 143 [800/1000 (80%)]\tLosses F.softmax: 0.000404 log_softmax: 0.000059\n",
            "Train Epoch: 143 [1000/1000 (100%)]\tLosses F.softmax: 0.074491 log_softmax: 0.060975\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0143\tAccuracy: 8081.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0349\tAccuracy: 8047.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 144 [0/1000 (0%)]\tLosses F.softmax: 0.000021 log_softmax: 0.000020\n",
            "Train Epoch: 144 [200/1000 (20%)]\tLosses F.softmax: 0.002036 log_softmax: 0.003677\n",
            "Train Epoch: 144 [400/1000 (40%)]\tLosses F.softmax: 0.050216 log_softmax: 0.047906\n",
            "Train Epoch: 144 [600/1000 (60%)]\tLosses F.softmax: 0.136463 log_softmax: 0.106364\n",
            "Train Epoch: 144 [800/1000 (80%)]\tLosses F.softmax: 0.000620 log_softmax: 0.002659\n",
            "Train Epoch: 144 [1000/1000 (100%)]\tLosses F.softmax: 0.011664 log_softmax: 0.003596\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0145\tAccuracy: 8085.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0349\tAccuracy: 8051.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 145 [0/1000 (0%)]\tLosses F.softmax: 0.000412 log_softmax: 0.000083\n",
            "Train Epoch: 145 [200/1000 (20%)]\tLosses F.softmax: 0.006645 log_softmax: 0.002007\n",
            "Train Epoch: 145 [400/1000 (40%)]\tLosses F.softmax: 0.005434 log_softmax: 0.006682\n",
            "Train Epoch: 145 [600/1000 (60%)]\tLosses F.softmax: 0.025042 log_softmax: 0.015491\n",
            "Train Epoch: 145 [800/1000 (80%)]\tLosses F.softmax: 0.002781 log_softmax: 0.000987\n",
            "Train Epoch: 145 [1000/1000 (100%)]\tLosses F.softmax: 0.018585 log_softmax: 0.014733\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0142\tAccuracy: 8086.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0354\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 146 [0/1000 (0%)]\tLosses F.softmax: 0.008921 log_softmax: 0.027774\n",
            "Train Epoch: 146 [200/1000 (20%)]\tLosses F.softmax: 0.000017 log_softmax: 0.001645\n",
            "Train Epoch: 146 [400/1000 (40%)]\tLosses F.softmax: 0.004581 log_softmax: 0.012739\n",
            "Train Epoch: 146 [600/1000 (60%)]\tLosses F.softmax: 0.003380 log_softmax: 0.002627\n",
            "Train Epoch: 146 [800/1000 (80%)]\tLosses F.softmax: 0.028983 log_softmax: 0.010616\n",
            "Train Epoch: 146 [1000/1000 (100%)]\tLosses F.softmax: 0.018791 log_softmax: 0.026656\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0154\tAccuracy: 8083.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0368\tAccuracy: 8051.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 147 [0/1000 (0%)]\tLosses F.softmax: 0.000094 log_softmax: 0.000220\n",
            "Train Epoch: 147 [200/1000 (20%)]\tLosses F.softmax: 0.001904 log_softmax: 0.002827\n",
            "Train Epoch: 147 [400/1000 (40%)]\tLosses F.softmax: 0.000164 log_softmax: 0.000556\n",
            "Train Epoch: 147 [600/1000 (60%)]\tLosses F.softmax: 0.000013 log_softmax: 0.000166\n",
            "Train Epoch: 147 [800/1000 (80%)]\tLosses F.softmax: 0.022358 log_softmax: 0.039877\n",
            "Train Epoch: 147 [1000/1000 (100%)]\tLosses F.softmax: 0.032778 log_softmax: 0.033681\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0153\tAccuracy: 8088.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0373\tAccuracy: 8051.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 148 [0/1000 (0%)]\tLosses F.softmax: 0.001315 log_softmax: 0.003948\n",
            "Train Epoch: 148 [200/1000 (20%)]\tLosses F.softmax: 0.101495 log_softmax: 0.042413\n",
            "Train Epoch: 148 [400/1000 (40%)]\tLosses F.softmax: 0.010245 log_softmax: 0.017279\n",
            "Train Epoch: 148 [600/1000 (60%)]\tLosses F.softmax: 0.010447 log_softmax: 0.000955\n",
            "Train Epoch: 148 [800/1000 (80%)]\tLosses F.softmax: 0.000011 log_softmax: 0.000001\n",
            "Train Epoch: 148 [1000/1000 (100%)]\tLosses F.softmax: 0.006519 log_softmax: 0.001293\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0155\tAccuracy: 8092.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0369\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 149 [0/1000 (0%)]\tLosses F.softmax: 0.000100 log_softmax: 0.000283\n",
            "Train Epoch: 149 [200/1000 (20%)]\tLosses F.softmax: 0.000077 log_softmax: 0.000109\n",
            "Train Epoch: 149 [400/1000 (40%)]\tLosses F.softmax: 0.003779 log_softmax: 0.010182\n",
            "Train Epoch: 149 [600/1000 (60%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000001\n",
            "Train Epoch: 149 [800/1000 (80%)]\tLosses F.softmax: 0.015456 log_softmax: 0.019546\n",
            "Train Epoch: 149 [1000/1000 (100%)]\tLosses F.softmax: 0.008733 log_softmax: 0.027781\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0166\tAccuracy: 8087.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0377\tAccuracy: 8052.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 150 [0/1000 (0%)]\tLosses F.softmax: 0.005692 log_softmax: 0.013781\n",
            "Train Epoch: 150 [200/1000 (20%)]\tLosses F.softmax: 0.000123 log_softmax: 0.000092\n",
            "Train Epoch: 150 [400/1000 (40%)]\tLosses F.softmax: 0.031339 log_softmax: 0.020715\n",
            "Train Epoch: 150 [600/1000 (60%)]\tLosses F.softmax: 0.000018 log_softmax: 0.000042\n",
            "Train Epoch: 150 [800/1000 (80%)]\tLosses F.softmax: 0.002298 log_softmax: 0.001269\n",
            "Train Epoch: 150 [1000/1000 (100%)]\tLosses F.softmax: 0.000007 log_softmax: 0.000004\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0184\tAccuracy: 8085.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0392\tAccuracy: 8051.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 151 [0/1000 (0%)]\tLosses F.softmax: 0.032860 log_softmax: 0.023315\n",
            "Train Epoch: 151 [200/1000 (20%)]\tLosses F.softmax: 0.000059 log_softmax: 0.000369\n",
            "Train Epoch: 151 [400/1000 (40%)]\tLosses F.softmax: 0.118076 log_softmax: 0.105765\n",
            "Train Epoch: 151 [600/1000 (60%)]\tLosses F.softmax: 0.000121 log_softmax: 0.000123\n",
            "Train Epoch: 151 [800/1000 (80%)]\tLosses F.softmax: 0.002320 log_softmax: 0.004676\n",
            "Train Epoch: 151 [1000/1000 (100%)]\tLosses F.softmax: 0.046120 log_softmax: 0.062967\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0176\tAccuracy: 8088.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0393\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 152 [0/1000 (0%)]\tLosses F.softmax: 0.000112 log_softmax: 0.000048\n",
            "Train Epoch: 152 [200/1000 (20%)]\tLosses F.softmax: 0.024601 log_softmax: 0.024768\n",
            "Train Epoch: 152 [400/1000 (40%)]\tLosses F.softmax: 0.020024 log_softmax: 0.011248\n",
            "Train Epoch: 152 [600/1000 (60%)]\tLosses F.softmax: 0.001961 log_softmax: 0.000542\n",
            "Train Epoch: 152 [800/1000 (80%)]\tLosses F.softmax: 0.012888 log_softmax: 0.000228\n",
            "Train Epoch: 152 [1000/1000 (100%)]\tLosses F.softmax: 0.003207 log_softmax: 0.020559\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0201\tAccuracy: 8080.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0412\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 153 [0/1000 (0%)]\tLosses F.softmax: 0.000436 log_softmax: 0.007115\n",
            "Train Epoch: 153 [200/1000 (20%)]\tLosses F.softmax: 0.023086 log_softmax: 0.024513\n",
            "Train Epoch: 153 [400/1000 (40%)]\tLosses F.softmax: 0.004879 log_softmax: 0.000215\n",
            "Train Epoch: 153 [600/1000 (60%)]\tLosses F.softmax: 0.002926 log_softmax: 0.002351\n",
            "Train Epoch: 153 [800/1000 (80%)]\tLosses F.softmax: 0.070976 log_softmax: 0.062908\n",
            "Train Epoch: 153 [1000/1000 (100%)]\tLosses F.softmax: 0.000062 log_softmax: 0.000014\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0188\tAccuracy: 8099.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0404\tAccuracy: 8061.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 154 [0/1000 (0%)]\tLosses F.softmax: 0.036901 log_softmax: 0.032600\n",
            "Train Epoch: 154 [200/1000 (20%)]\tLosses F.softmax: 0.000328 log_softmax: 0.000123\n",
            "Train Epoch: 154 [400/1000 (40%)]\tLosses F.softmax: 0.000479 log_softmax: 0.007070\n",
            "Train Epoch: 154 [600/1000 (60%)]\tLosses F.softmax: 0.008311 log_softmax: 0.005618\n",
            "Train Epoch: 154 [800/1000 (80%)]\tLosses F.softmax: 0.000840 log_softmax: 0.000268\n",
            "Train Epoch: 154 [1000/1000 (100%)]\tLosses F.softmax: 0.000011 log_softmax: 0.000001\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0206\tAccuracy: 8085.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0419\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 155 [0/1000 (0%)]\tLosses F.softmax: 0.017762 log_softmax: 0.025522\n",
            "Train Epoch: 155 [200/1000 (20%)]\tLosses F.softmax: 0.001495 log_softmax: 0.000160\n",
            "Train Epoch: 155 [400/1000 (40%)]\tLosses F.softmax: 0.008402 log_softmax: 0.001793\n",
            "Train Epoch: 155 [600/1000 (60%)]\tLosses F.softmax: 0.000015 log_softmax: 0.000047\n",
            "Train Epoch: 155 [800/1000 (80%)]\tLosses F.softmax: 0.055730 log_softmax: 0.060254\n",
            "Train Epoch: 155 [1000/1000 (100%)]\tLosses F.softmax: 0.000015 log_softmax: 0.000007\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0205\tAccuracy: 8087.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0419\tAccuracy: 8057.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 156 [0/1000 (0%)]\tLosses F.softmax: 0.000396 log_softmax: 0.000265\n",
            "Train Epoch: 156 [200/1000 (20%)]\tLosses F.softmax: 0.004808 log_softmax: 0.001223\n",
            "Train Epoch: 156 [400/1000 (40%)]\tLosses F.softmax: 0.000273 log_softmax: 0.001258\n",
            "Train Epoch: 156 [600/1000 (60%)]\tLosses F.softmax: 0.002891 log_softmax: 0.043013\n",
            "Train Epoch: 156 [800/1000 (80%)]\tLosses F.softmax: 0.005154 log_softmax: 0.000899\n",
            "Train Epoch: 156 [1000/1000 (100%)]\tLosses F.softmax: 0.001133 log_softmax: 0.000022\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0207\tAccuracy: 8090.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0426\tAccuracy: 8058.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 157 [0/1000 (0%)]\tLosses F.softmax: 0.002201 log_softmax: 0.001141\n",
            "Train Epoch: 157 [200/1000 (20%)]\tLosses F.softmax: 0.041723 log_softmax: 0.027907\n",
            "Train Epoch: 157 [400/1000 (40%)]\tLosses F.softmax: 0.001593 log_softmax: 0.003028\n",
            "Train Epoch: 157 [600/1000 (60%)]\tLosses F.softmax: 0.016496 log_softmax: 0.035399\n",
            "Train Epoch: 157 [800/1000 (80%)]\tLosses F.softmax: 0.000023 log_softmax: 0.000042\n",
            "Train Epoch: 157 [1000/1000 (100%)]\tLosses F.softmax: 0.021036 log_softmax: 0.024817\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0227\tAccuracy: 8086.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0442\tAccuracy: 8057.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 158 [0/1000 (0%)]\tLosses F.softmax: 0.015131 log_softmax: 0.029128\n",
            "Train Epoch: 158 [200/1000 (20%)]\tLosses F.softmax: 0.002998 log_softmax: 0.002336\n",
            "Train Epoch: 158 [400/1000 (40%)]\tLosses F.softmax: 0.001761 log_softmax: 0.000408\n",
            "Train Epoch: 158 [600/1000 (60%)]\tLosses F.softmax: 0.003246 log_softmax: 0.007047\n",
            "Train Epoch: 158 [800/1000 (80%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000000\n",
            "Train Epoch: 158 [1000/1000 (100%)]\tLosses F.softmax: 0.030048 log_softmax: 0.020920\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0227\tAccuracy: 8088.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0443\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 159 [0/1000 (0%)]\tLosses F.softmax: 0.043455 log_softmax: 0.033959\n",
            "Train Epoch: 159 [200/1000 (20%)]\tLosses F.softmax: 0.024428 log_softmax: 0.022433\n",
            "Train Epoch: 159 [400/1000 (40%)]\tLosses F.softmax: 0.028929 log_softmax: 0.015907\n",
            "Train Epoch: 159 [600/1000 (60%)]\tLosses F.softmax: 0.007502 log_softmax: 0.010804\n",
            "Train Epoch: 159 [800/1000 (80%)]\tLosses F.softmax: 0.002304 log_softmax: 0.001778\n",
            "Train Epoch: 159 [1000/1000 (100%)]\tLosses F.softmax: 0.003103 log_softmax: 0.020012\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0241\tAccuracy: 8087.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0459\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 160 [0/1000 (0%)]\tLosses F.softmax: 0.008813 log_softmax: 0.027219\n",
            "Train Epoch: 160 [200/1000 (20%)]\tLosses F.softmax: 0.000155 log_softmax: 0.000582\n",
            "Train Epoch: 160 [400/1000 (40%)]\tLosses F.softmax: 0.002005 log_softmax: 0.000025\n",
            "Train Epoch: 160 [600/1000 (60%)]\tLosses F.softmax: 0.003465 log_softmax: 0.006144\n",
            "Train Epoch: 160 [800/1000 (80%)]\tLosses F.softmax: 0.026928 log_softmax: 0.047127\n",
            "Train Epoch: 160 [1000/1000 (100%)]\tLosses F.softmax: 0.008804 log_softmax: 0.001424\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0241\tAccuracy: 8087.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0454\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 161 [0/1000 (0%)]\tLosses F.softmax: 0.001789 log_softmax: 0.000528\n",
            "Train Epoch: 161 [200/1000 (20%)]\tLosses F.softmax: 0.004359 log_softmax: 0.001130\n",
            "Train Epoch: 161 [400/1000 (40%)]\tLosses F.softmax: 0.028316 log_softmax: 0.017582\n",
            "Train Epoch: 161 [600/1000 (60%)]\tLosses F.softmax: 0.000015 log_softmax: 0.001391\n",
            "Train Epoch: 161 [800/1000 (80%)]\tLosses F.softmax: 0.007471 log_softmax: 0.003187\n",
            "Train Epoch: 161 [1000/1000 (100%)]\tLosses F.softmax: 0.043802 log_softmax: 0.033537\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0242\tAccuracy: 8088.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0465\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 162 [0/1000 (0%)]\tLosses F.softmax: 0.000169 log_softmax: 0.000692\n",
            "Train Epoch: 162 [200/1000 (20%)]\tLosses F.softmax: 0.000258 log_softmax: 0.000170\n",
            "Train Epoch: 162 [400/1000 (40%)]\tLosses F.softmax: 0.030414 log_softmax: 0.011028\n",
            "Train Epoch: 162 [600/1000 (60%)]\tLosses F.softmax: 0.001339 log_softmax: 0.000435\n",
            "Train Epoch: 162 [800/1000 (80%)]\tLosses F.softmax: 0.008045 log_softmax: 0.025382\n",
            "Train Epoch: 162 [1000/1000 (100%)]\tLosses F.softmax: 0.000015 log_softmax: 0.000002\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0250\tAccuracy: 8090.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0472\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 163 [0/1000 (0%)]\tLosses F.softmax: 0.000045 log_softmax: 0.000012\n",
            "Train Epoch: 163 [200/1000 (20%)]\tLosses F.softmax: 0.016834 log_softmax: 0.015101\n",
            "Train Epoch: 163 [400/1000 (40%)]\tLosses F.softmax: 0.003297 log_softmax: 0.005895\n",
            "Train Epoch: 163 [600/1000 (60%)]\tLosses F.softmax: 0.026608 log_softmax: 0.009418\n",
            "Train Epoch: 163 [800/1000 (80%)]\tLosses F.softmax: 0.037198 log_softmax: 0.022107\n",
            "Train Epoch: 163 [1000/1000 (100%)]\tLosses F.softmax: 0.000044 log_softmax: 0.000002\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0262\tAccuracy: 8089.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0481\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 164 [0/1000 (0%)]\tLosses F.softmax: 0.006182 log_softmax: 0.006589\n",
            "Train Epoch: 164 [200/1000 (20%)]\tLosses F.softmax: 0.044810 log_softmax: 0.062085\n",
            "Train Epoch: 164 [400/1000 (40%)]\tLosses F.softmax: 0.118676 log_softmax: 0.166043\n",
            "Train Epoch: 164 [600/1000 (60%)]\tLosses F.softmax: 0.035165 log_softmax: 0.036002\n",
            "Train Epoch: 164 [800/1000 (80%)]\tLosses F.softmax: 0.000026 log_softmax: 0.000020\n",
            "Train Epoch: 164 [1000/1000 (100%)]\tLosses F.softmax: 0.043349 log_softmax: 0.018529\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0264\tAccuracy: 8092.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0489\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 165 [0/1000 (0%)]\tLosses F.softmax: 0.001870 log_softmax: 0.004831\n",
            "Train Epoch: 165 [200/1000 (20%)]\tLosses F.softmax: 0.016300 log_softmax: 0.020966\n",
            "Train Epoch: 165 [400/1000 (40%)]\tLosses F.softmax: 0.009951 log_softmax: 0.010088\n",
            "Train Epoch: 165 [600/1000 (60%)]\tLosses F.softmax: 0.000172 log_softmax: 0.000522\n",
            "Train Epoch: 165 [800/1000 (80%)]\tLosses F.softmax: 0.000316 log_softmax: 0.000102\n",
            "Train Epoch: 165 [1000/1000 (100%)]\tLosses F.softmax: 0.046239 log_softmax: 0.040302\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0280\tAccuracy: 8086.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0497\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 166 [0/1000 (0%)]\tLosses F.softmax: 0.030033 log_softmax: 0.019706\n",
            "Train Epoch: 166 [200/1000 (20%)]\tLosses F.softmax: 0.046502 log_softmax: 0.044325\n",
            "Train Epoch: 166 [400/1000 (40%)]\tLosses F.softmax: 0.003517 log_softmax: 0.000723\n",
            "Train Epoch: 166 [600/1000 (60%)]\tLosses F.softmax: 0.001625 log_softmax: 0.000045\n",
            "Train Epoch: 166 [800/1000 (80%)]\tLosses F.softmax: 0.001396 log_softmax: 0.000324\n",
            "Train Epoch: 166 [1000/1000 (100%)]\tLosses F.softmax: 0.000605 log_softmax: 0.000141\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0281\tAccuracy: 8090.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0502\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 167 [0/1000 (0%)]\tLosses F.softmax: 0.004884 log_softmax: 0.000301\n",
            "Train Epoch: 167 [200/1000 (20%)]\tLosses F.softmax: 0.016957 log_softmax: 0.025138\n",
            "Train Epoch: 167 [400/1000 (40%)]\tLosses F.softmax: 0.000019 log_softmax: 0.000003\n",
            "Train Epoch: 167 [600/1000 (60%)]\tLosses F.softmax: 0.001203 log_softmax: 0.000130\n",
            "Train Epoch: 167 [800/1000 (80%)]\tLosses F.softmax: 0.001888 log_softmax: 0.002617\n",
            "Train Epoch: 167 [1000/1000 (100%)]\tLosses F.softmax: 0.002446 log_softmax: 0.002729\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0285\tAccuracy: 8087.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0510\tAccuracy: 8052.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 168 [0/1000 (0%)]\tLosses F.softmax: 0.008566 log_softmax: 0.011614\n",
            "Train Epoch: 168 [200/1000 (20%)]\tLosses F.softmax: 0.000292 log_softmax: 0.000048\n",
            "Train Epoch: 168 [400/1000 (40%)]\tLosses F.softmax: 0.075395 log_softmax: 0.071057\n",
            "Train Epoch: 168 [600/1000 (60%)]\tLosses F.softmax: 0.000005 log_softmax: 0.000005\n",
            "Train Epoch: 168 [800/1000 (80%)]\tLosses F.softmax: 0.002191 log_softmax: 0.004415\n",
            "Train Epoch: 168 [1000/1000 (100%)]\tLosses F.softmax: 0.000087 log_softmax: 0.000228\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0287\tAccuracy: 8084.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0514\tAccuracy: 8051.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 169 [0/1000 (0%)]\tLosses F.softmax: 0.000404 log_softmax: 0.000041\n",
            "Train Epoch: 169 [200/1000 (20%)]\tLosses F.softmax: 0.000028 log_softmax: 0.000006\n",
            "Train Epoch: 169 [400/1000 (40%)]\tLosses F.softmax: 0.009126 log_softmax: 0.004111\n",
            "Train Epoch: 169 [600/1000 (60%)]\tLosses F.softmax: 0.000002 log_softmax: 0.000022\n",
            "Train Epoch: 169 [800/1000 (80%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000006\n",
            "Train Epoch: 169 [1000/1000 (100%)]\tLosses F.softmax: 0.011004 log_softmax: 0.003320\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0296\tAccuracy: 8087.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0518\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 170 [0/1000 (0%)]\tLosses F.softmax: 0.000231 log_softmax: 0.001012\n",
            "Train Epoch: 170 [200/1000 (20%)]\tLosses F.softmax: 0.011613 log_softmax: 0.013867\n",
            "Train Epoch: 170 [400/1000 (40%)]\tLosses F.softmax: 0.029920 log_softmax: 0.022367\n",
            "Train Epoch: 170 [600/1000 (60%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000001\n",
            "Train Epoch: 170 [800/1000 (80%)]\tLosses F.softmax: 0.000019 log_softmax: 0.007879\n",
            "Train Epoch: 170 [1000/1000 (100%)]\tLosses F.softmax: 0.000092 log_softmax: 0.000524\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0308\tAccuracy: 8089.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0530\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 171 [0/1000 (0%)]\tLosses F.softmax: 0.000199 log_softmax: 0.000218\n",
            "Train Epoch: 171 [200/1000 (20%)]\tLosses F.softmax: 0.000170 log_softmax: 0.000710\n",
            "Train Epoch: 171 [400/1000 (40%)]\tLosses F.softmax: 0.046225 log_softmax: 0.064227\n",
            "Train Epoch: 171 [600/1000 (60%)]\tLosses F.softmax: 0.004789 log_softmax: 0.000294\n",
            "Train Epoch: 171 [800/1000 (80%)]\tLosses F.softmax: 0.001907 log_softmax: 0.000023\n",
            "Train Epoch: 171 [1000/1000 (100%)]\tLosses F.softmax: 0.000083 log_softmax: 0.000008\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0315\tAccuracy: 8086.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0540\tAccuracy: 8051.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 172 [0/1000 (0%)]\tLosses F.softmax: 0.028361 log_softmax: 0.054292\n",
            "Train Epoch: 172 [200/1000 (20%)]\tLosses F.softmax: 0.091208 log_softmax: 0.134713\n",
            "Train Epoch: 172 [400/1000 (40%)]\tLosses F.softmax: 0.004510 log_softmax: 0.005931\n",
            "Train Epoch: 172 [600/1000 (60%)]\tLosses F.softmax: 0.056971 log_softmax: 0.020809\n",
            "Train Epoch: 172 [800/1000 (80%)]\tLosses F.softmax: 0.024678 log_softmax: 0.012160\n",
            "Train Epoch: 172 [1000/1000 (100%)]\tLosses F.softmax: 0.008786 log_softmax: 0.008494\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0325\tAccuracy: 8090.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0546\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 173 [0/1000 (0%)]\tLosses F.softmax: 0.000005 log_softmax: 0.000005\n",
            "Train Epoch: 173 [200/1000 (20%)]\tLosses F.softmax: 0.090453 log_softmax: 0.067018\n",
            "Train Epoch: 173 [400/1000 (40%)]\tLosses F.softmax: 0.000555 log_softmax: 0.000189\n",
            "Train Epoch: 173 [600/1000 (60%)]\tLosses F.softmax: 0.001958 log_softmax: 0.005185\n",
            "Train Epoch: 173 [800/1000 (80%)]\tLosses F.softmax: 0.016034 log_softmax: 0.008296\n",
            "Train Epoch: 173 [1000/1000 (100%)]\tLosses F.softmax: 0.000083 log_softmax: 0.003033\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0338\tAccuracy: 8085.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0566\tAccuracy: 8045.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 174 [0/1000 (0%)]\tLosses F.softmax: 0.067345 log_softmax: 0.052500\n",
            "Train Epoch: 174 [200/1000 (20%)]\tLosses F.softmax: 0.000037 log_softmax: 0.000013\n",
            "Train Epoch: 174 [400/1000 (40%)]\tLosses F.softmax: 0.000484 log_softmax: 0.003055\n",
            "Train Epoch: 174 [600/1000 (60%)]\tLosses F.softmax: 0.000007 log_softmax: 0.000015\n",
            "Train Epoch: 174 [800/1000 (80%)]\tLosses F.softmax: 0.000447 log_softmax: 0.000059\n",
            "Train Epoch: 174 [1000/1000 (100%)]\tLosses F.softmax: 0.000017 log_softmax: 0.000015\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0336\tAccuracy: 8086.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0564\tAccuracy: 8049.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 175 [0/1000 (0%)]\tLosses F.softmax: 0.000002 log_softmax: 0.000020\n",
            "Train Epoch: 175 [200/1000 (20%)]\tLosses F.softmax: 0.000023 log_softmax: 0.000003\n",
            "Train Epoch: 175 [400/1000 (40%)]\tLosses F.softmax: 0.000628 log_softmax: 0.000003\n",
            "Train Epoch: 175 [600/1000 (60%)]\tLosses F.softmax: 0.000373 log_softmax: 0.000069\n",
            "Train Epoch: 175 [800/1000 (80%)]\tLosses F.softmax: 0.000031 log_softmax: 0.000004\n",
            "Train Epoch: 175 [1000/1000 (100%)]\tLosses F.softmax: 0.001710 log_softmax: 0.005000\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0337\tAccuracy: 8086.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0563\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 176 [0/1000 (0%)]\tLosses F.softmax: 0.003739 log_softmax: 0.000874\n",
            "Train Epoch: 176 [200/1000 (20%)]\tLosses F.softmax: 0.003544 log_softmax: 0.000429\n",
            "Train Epoch: 176 [400/1000 (40%)]\tLosses F.softmax: 0.000302 log_softmax: 0.000209\n",
            "Train Epoch: 176 [600/1000 (60%)]\tLosses F.softmax: 0.001306 log_softmax: 0.002434\n",
            "Train Epoch: 176 [800/1000 (80%)]\tLosses F.softmax: 0.018545 log_softmax: 0.025112\n",
            "Train Epoch: 176 [1000/1000 (100%)]\tLosses F.softmax: 0.010644 log_softmax: 0.030293\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0341\tAccuracy: 8089.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0575\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 177 [0/1000 (0%)]\tLosses F.softmax: 0.000020 log_softmax: 0.000010\n",
            "Train Epoch: 177 [200/1000 (20%)]\tLosses F.softmax: 0.033208 log_softmax: 0.005687\n",
            "Train Epoch: 177 [400/1000 (40%)]\tLosses F.softmax: 0.008458 log_softmax: 0.015262\n",
            "Train Epoch: 177 [600/1000 (60%)]\tLosses F.softmax: 0.004264 log_softmax: 0.002962\n",
            "Train Epoch: 177 [800/1000 (80%)]\tLosses F.softmax: 0.027038 log_softmax: 0.019387\n",
            "Train Epoch: 177 [1000/1000 (100%)]\tLosses F.softmax: 0.000160 log_softmax: 0.000694\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0347\tAccuracy: 8091.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0575\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 178 [0/1000 (0%)]\tLosses F.softmax: 0.000006 log_softmax: 0.000003\n",
            "Train Epoch: 178 [200/1000 (20%)]\tLosses F.softmax: 0.000013 log_softmax: 0.000000\n",
            "Train Epoch: 178 [400/1000 (40%)]\tLosses F.softmax: 0.010622 log_softmax: 0.008561\n",
            "Train Epoch: 178 [600/1000 (60%)]\tLosses F.softmax: 0.000257 log_softmax: 0.000581\n",
            "Train Epoch: 178 [800/1000 (80%)]\tLosses F.softmax: 0.000263 log_softmax: 0.000087\n",
            "Train Epoch: 178 [1000/1000 (100%)]\tLosses F.softmax: 0.000234 log_softmax: 0.000045\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0350\tAccuracy: 8091.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0576\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 179 [0/1000 (0%)]\tLosses F.softmax: 0.001055 log_softmax: 0.000500\n",
            "Train Epoch: 179 [200/1000 (20%)]\tLosses F.softmax: 0.098175 log_softmax: 0.138526\n",
            "Train Epoch: 179 [400/1000 (40%)]\tLosses F.softmax: 0.001641 log_softmax: 0.008230\n",
            "Train Epoch: 179 [600/1000 (60%)]\tLosses F.softmax: 0.012083 log_softmax: 0.015620\n",
            "Train Epoch: 179 [800/1000 (80%)]\tLosses F.softmax: 0.013511 log_softmax: 0.006351\n",
            "Train Epoch: 179 [1000/1000 (100%)]\tLosses F.softmax: 0.001143 log_softmax: 0.000881\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0358\tAccuracy: 8089.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0590\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 180 [0/1000 (0%)]\tLosses F.softmax: 0.001992 log_softmax: 0.003875\n",
            "Train Epoch: 180 [200/1000 (20%)]\tLosses F.softmax: 0.000002 log_softmax: 0.000017\n",
            "Train Epoch: 180 [400/1000 (40%)]\tLosses F.softmax: 0.001096 log_softmax: 0.000102\n",
            "Train Epoch: 180 [600/1000 (60%)]\tLosses F.softmax: 0.000032 log_softmax: 0.000200\n",
            "Train Epoch: 180 [800/1000 (80%)]\tLosses F.softmax: 0.018253 log_softmax: 0.008350\n",
            "Train Epoch: 180 [1000/1000 (100%)]\tLosses F.softmax: 0.015534 log_softmax: 0.012291\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0363\tAccuracy: 8090.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0592\tAccuracy: 8060.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 181 [0/1000 (0%)]\tLosses F.softmax: 0.040523 log_softmax: 0.023179\n",
            "Train Epoch: 181 [200/1000 (20%)]\tLosses F.softmax: 0.038521 log_softmax: 0.011435\n",
            "Train Epoch: 181 [400/1000 (40%)]\tLosses F.softmax: 0.000002 log_softmax: 0.000032\n",
            "Train Epoch: 181 [600/1000 (60%)]\tLosses F.softmax: 0.003989 log_softmax: 0.000252\n",
            "Train Epoch: 181 [800/1000 (80%)]\tLosses F.softmax: 0.027183 log_softmax: 0.044411\n",
            "Train Epoch: 181 [1000/1000 (100%)]\tLosses F.softmax: 0.008180 log_softmax: 0.000855\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0379\tAccuracy: 8086.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0605\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 182 [0/1000 (0%)]\tLosses F.softmax: 0.003704 log_softmax: 0.000708\n",
            "Train Epoch: 182 [200/1000 (20%)]\tLosses F.softmax: 0.014307 log_softmax: 0.031665\n",
            "Train Epoch: 182 [400/1000 (40%)]\tLosses F.softmax: 0.000291 log_softmax: 0.000162\n",
            "Train Epoch: 182 [600/1000 (60%)]\tLosses F.softmax: 0.000072 log_softmax: 0.000027\n",
            "Train Epoch: 182 [800/1000 (80%)]\tLosses F.softmax: 0.000291 log_softmax: 0.000086\n",
            "Train Epoch: 182 [1000/1000 (100%)]\tLosses F.softmax: 0.000275 log_softmax: 0.000089\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0389\tAccuracy: 8089.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0613\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 183 [0/1000 (0%)]\tLosses F.softmax: 0.000016 log_softmax: 0.000316\n",
            "Train Epoch: 183 [200/1000 (20%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000000\n",
            "Train Epoch: 183 [400/1000 (40%)]\tLosses F.softmax: 0.002530 log_softmax: 0.003370\n",
            "Train Epoch: 183 [600/1000 (60%)]\tLosses F.softmax: 0.007275 log_softmax: 0.003441\n",
            "Train Epoch: 183 [800/1000 (80%)]\tLosses F.softmax: 0.019539 log_softmax: 0.005045\n",
            "Train Epoch: 183 [1000/1000 (100%)]\tLosses F.softmax: 0.008607 log_softmax: 0.016434\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0380\tAccuracy: 8092.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0608\tAccuracy: 8058.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 184 [0/1000 (0%)]\tLosses F.softmax: 0.001070 log_softmax: 0.000113\n",
            "Train Epoch: 184 [200/1000 (20%)]\tLosses F.softmax: 0.000987 log_softmax: 0.001078\n",
            "Train Epoch: 184 [400/1000 (40%)]\tLosses F.softmax: 0.000800 log_softmax: 0.002606\n",
            "Train Epoch: 184 [600/1000 (60%)]\tLosses F.softmax: 0.000249 log_softmax: 0.004209\n",
            "Train Epoch: 184 [800/1000 (80%)]\tLosses F.softmax: 0.020425 log_softmax: 0.015058\n",
            "Train Epoch: 184 [1000/1000 (100%)]\tLosses F.softmax: 0.009601 log_softmax: 0.000926\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0384\tAccuracy: 8088.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0615\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 185 [0/1000 (0%)]\tLosses F.softmax: 0.000473 log_softmax: 0.003662\n",
            "Train Epoch: 185 [200/1000 (20%)]\tLosses F.softmax: 0.001083 log_softmax: 0.003809\n",
            "Train Epoch: 185 [400/1000 (40%)]\tLosses F.softmax: 0.000080 log_softmax: 0.000176\n",
            "Train Epoch: 185 [600/1000 (60%)]\tLosses F.softmax: 0.001219 log_softmax: 0.002808\n",
            "Train Epoch: 185 [800/1000 (80%)]\tLosses F.softmax: 0.000097 log_softmax: 0.000008\n",
            "Train Epoch: 185 [1000/1000 (100%)]\tLosses F.softmax: 0.057583 log_softmax: 0.050374\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0394\tAccuracy: 8092.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0626\tAccuracy: 8052.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 186 [0/1000 (0%)]\tLosses F.softmax: 0.002933 log_softmax: 0.006378\n",
            "Train Epoch: 186 [200/1000 (20%)]\tLosses F.softmax: 0.000012 log_softmax: 0.001381\n",
            "Train Epoch: 186 [400/1000 (40%)]\tLosses F.softmax: 0.002744 log_softmax: 0.000104\n",
            "Train Epoch: 186 [600/1000 (60%)]\tLosses F.softmax: 0.048322 log_softmax: 0.020055\n",
            "Train Epoch: 186 [800/1000 (80%)]\tLosses F.softmax: 0.003835 log_softmax: 0.000950\n",
            "Train Epoch: 186 [1000/1000 (100%)]\tLosses F.softmax: 0.029470 log_softmax: 0.047709\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0399\tAccuracy: 8089.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0632\tAccuracy: 8059.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 187 [0/1000 (0%)]\tLosses F.softmax: 0.015056 log_softmax: 0.021687\n",
            "Train Epoch: 187 [200/1000 (20%)]\tLosses F.softmax: 0.016100 log_softmax: 0.017570\n",
            "Train Epoch: 187 [400/1000 (40%)]\tLosses F.softmax: 0.000076 log_softmax: 0.000056\n",
            "Train Epoch: 187 [600/1000 (60%)]\tLosses F.softmax: 0.000010 log_softmax: 0.000144\n",
            "Train Epoch: 187 [800/1000 (80%)]\tLosses F.softmax: 0.000029 log_softmax: 0.000016\n",
            "Train Epoch: 187 [1000/1000 (100%)]\tLosses F.softmax: 0.007692 log_softmax: 0.000132\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0403\tAccuracy: 8090.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0631\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 188 [0/1000 (0%)]\tLosses F.softmax: 0.061559 log_softmax: 0.023679\n",
            "Train Epoch: 188 [200/1000 (20%)]\tLosses F.softmax: 0.000333 log_softmax: 0.000058\n",
            "Train Epoch: 188 [400/1000 (40%)]\tLosses F.softmax: 0.000204 log_softmax: 0.000107\n",
            "Train Epoch: 188 [600/1000 (60%)]\tLosses F.softmax: 0.000035 log_softmax: 0.000002\n",
            "Train Epoch: 188 [800/1000 (80%)]\tLosses F.softmax: 0.001076 log_softmax: 0.001065\n",
            "Train Epoch: 188 [1000/1000 (100%)]\tLosses F.softmax: 0.003153 log_softmax: 0.009062\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0416\tAccuracy: 8090.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0645\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 189 [0/1000 (0%)]\tLosses F.softmax: 0.042340 log_softmax: 0.046020\n",
            "Train Epoch: 189 [200/1000 (20%)]\tLosses F.softmax: 0.009636 log_softmax: 0.003595\n",
            "Train Epoch: 189 [400/1000 (40%)]\tLosses F.softmax: 0.000103 log_softmax: 0.000040\n",
            "Train Epoch: 189 [600/1000 (60%)]\tLosses F.softmax: 0.032178 log_softmax: 0.024797\n",
            "Train Epoch: 189 [800/1000 (80%)]\tLosses F.softmax: 0.000006 log_softmax: 0.000003\n",
            "Train Epoch: 189 [1000/1000 (100%)]\tLosses F.softmax: 0.000002 log_softmax: 0.000028\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0422\tAccuracy: 8087.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0654\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 190 [0/1000 (0%)]\tLosses F.softmax: 0.000759 log_softmax: 0.002333\n",
            "Train Epoch: 190 [200/1000 (20%)]\tLosses F.softmax: 0.000004 log_softmax: 0.000003\n",
            "Train Epoch: 190 [400/1000 (40%)]\tLosses F.softmax: 0.038263 log_softmax: 0.052957\n",
            "Train Epoch: 190 [600/1000 (60%)]\tLosses F.softmax: 0.000666 log_softmax: 0.000494\n",
            "Train Epoch: 190 [800/1000 (80%)]\tLosses F.softmax: 0.001391 log_softmax: 0.003795\n",
            "Train Epoch: 190 [1000/1000 (100%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000000\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0437\tAccuracy: 8086.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0665\tAccuracy: 8052.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 191 [0/1000 (0%)]\tLosses F.softmax: 0.000003 log_softmax: 0.000013\n",
            "Train Epoch: 191 [200/1000 (20%)]\tLosses F.softmax: 0.001499 log_softmax: 0.004250\n",
            "Train Epoch: 191 [400/1000 (40%)]\tLosses F.softmax: 0.003625 log_softmax: 0.000824\n",
            "Train Epoch: 191 [600/1000 (60%)]\tLosses F.softmax: 0.000008 log_softmax: 0.000077\n",
            "Train Epoch: 191 [800/1000 (80%)]\tLosses F.softmax: 0.032016 log_softmax: 0.028411\n",
            "Train Epoch: 191 [1000/1000 (100%)]\tLosses F.softmax: 0.009724 log_softmax: 0.000994\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0436\tAccuracy: 8089.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0671\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 192 [0/1000 (0%)]\tLosses F.softmax: 0.004513 log_softmax: 0.003425\n",
            "Train Epoch: 192 [200/1000 (20%)]\tLosses F.softmax: 0.000549 log_softmax: 0.000175\n",
            "Train Epoch: 192 [400/1000 (40%)]\tLosses F.softmax: 0.000194 log_softmax: 0.000022\n",
            "Train Epoch: 192 [600/1000 (60%)]\tLosses F.softmax: 0.000031 log_softmax: 0.000011\n",
            "Train Epoch: 192 [800/1000 (80%)]\tLosses F.softmax: 0.000015 log_softmax: 0.000170\n",
            "Train Epoch: 192 [1000/1000 (100%)]\tLosses F.softmax: 0.005413 log_softmax: 0.007719\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0440\tAccuracy: 8091.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0675\tAccuracy: 8057.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 193 [0/1000 (0%)]\tLosses F.softmax: 0.000527 log_softmax: 0.000653\n",
            "Train Epoch: 193 [200/1000 (20%)]\tLosses F.softmax: 0.000034 log_softmax: 0.000013\n",
            "Train Epoch: 193 [400/1000 (40%)]\tLosses F.softmax: 0.028311 log_softmax: 0.005431\n",
            "Train Epoch: 193 [600/1000 (60%)]\tLosses F.softmax: 0.009845 log_softmax: 0.001050\n",
            "Train Epoch: 193 [800/1000 (80%)]\tLosses F.softmax: 0.002110 log_softmax: 0.001663\n",
            "Train Epoch: 193 [1000/1000 (100%)]\tLosses F.softmax: 0.000022 log_softmax: 0.000009\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0444\tAccuracy: 8086.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0679\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 194 [0/1000 (0%)]\tLosses F.softmax: 0.007316 log_softmax: 0.002347\n",
            "Train Epoch: 194 [200/1000 (20%)]\tLosses F.softmax: 0.000099 log_softmax: 0.000446\n",
            "Train Epoch: 194 [400/1000 (40%)]\tLosses F.softmax: 0.002598 log_softmax: 0.005202\n",
            "Train Epoch: 194 [600/1000 (60%)]\tLosses F.softmax: 0.001687 log_softmax: 0.000421\n",
            "Train Epoch: 194 [800/1000 (80%)]\tLosses F.softmax: 0.016878 log_softmax: 0.023454\n",
            "Train Epoch: 194 [1000/1000 (100%)]\tLosses F.softmax: 0.031495 log_softmax: 0.008986\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0465\tAccuracy: 8084.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0695\tAccuracy: 8050.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 195 [0/1000 (0%)]\tLosses F.softmax: 0.007427 log_softmax: 0.015902\n",
            "Train Epoch: 195 [200/1000 (20%)]\tLosses F.softmax: 0.000920 log_softmax: 0.000106\n",
            "Train Epoch: 195 [400/1000 (40%)]\tLosses F.softmax: 0.020493 log_softmax: 0.021739\n",
            "Train Epoch: 195 [600/1000 (60%)]\tLosses F.softmax: 0.010340 log_softmax: 0.002202\n",
            "Train Epoch: 195 [800/1000 (80%)]\tLosses F.softmax: 0.000019 log_softmax: 0.000010\n",
            "Train Epoch: 195 [1000/1000 (100%)]\tLosses F.softmax: 0.000035 log_softmax: 0.000011\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0464\tAccuracy: 8089.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0701\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 196 [0/1000 (0%)]\tLosses F.softmax: 0.040021 log_softmax: 0.026228\n",
            "Train Epoch: 196 [200/1000 (20%)]\tLosses F.softmax: 0.010377 log_softmax: 0.005394\n",
            "Train Epoch: 196 [400/1000 (40%)]\tLosses F.softmax: 0.004925 log_softmax: 0.002941\n",
            "Train Epoch: 196 [600/1000 (60%)]\tLosses F.softmax: 0.025516 log_softmax: 0.018460\n",
            "Train Epoch: 196 [800/1000 (80%)]\tLosses F.softmax: 0.021663 log_softmax: 0.017871\n",
            "Train Epoch: 196 [1000/1000 (100%)]\tLosses F.softmax: 0.000037 log_softmax: 0.000015\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0468\tAccuracy: 8089.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0704\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 197 [0/1000 (0%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000000\n",
            "Train Epoch: 197 [200/1000 (20%)]\tLosses F.softmax: 0.001463 log_softmax: 0.000524\n",
            "Train Epoch: 197 [400/1000 (40%)]\tLosses F.softmax: 0.030572 log_softmax: 0.037381\n",
            "Train Epoch: 197 [600/1000 (60%)]\tLosses F.softmax: 0.000404 log_softmax: 0.002721\n",
            "Train Epoch: 197 [800/1000 (80%)]\tLosses F.softmax: 0.009394 log_softmax: 0.015109\n",
            "Train Epoch: 197 [1000/1000 (100%)]\tLosses F.softmax: 0.000141 log_softmax: 0.000236\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0469\tAccuracy: 8092.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0702\tAccuracy: 8061.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 198 [0/1000 (0%)]\tLosses F.softmax: 0.002990 log_softmax: 0.005519\n",
            "Train Epoch: 198 [200/1000 (20%)]\tLosses F.softmax: 0.000003 log_softmax: 0.000000\n",
            "Train Epoch: 198 [400/1000 (40%)]\tLosses F.softmax: 0.000004 log_softmax: 0.000234\n",
            "Train Epoch: 198 [600/1000 (60%)]\tLosses F.softmax: 0.000148 log_softmax: 0.001301\n",
            "Train Epoch: 198 [800/1000 (80%)]\tLosses F.softmax: 0.000261 log_softmax: 0.000049\n",
            "Train Epoch: 198 [1000/1000 (100%)]\tLosses F.softmax: 0.025538 log_softmax: 0.010320\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0477\tAccuracy: 8088.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0714\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 199 [0/1000 (0%)]\tLosses F.softmax: 0.012941 log_softmax: 0.020359\n",
            "Train Epoch: 199 [200/1000 (20%)]\tLosses F.softmax: 0.000379 log_softmax: 0.000027\n",
            "Train Epoch: 199 [400/1000 (40%)]\tLosses F.softmax: 0.000583 log_softmax: 0.000157\n",
            "Train Epoch: 199 [600/1000 (60%)]\tLosses F.softmax: 0.000025 log_softmax: 0.000003\n",
            "Train Epoch: 199 [800/1000 (80%)]\tLosses F.softmax: 0.000049 log_softmax: 0.000018\n",
            "Train Epoch: 199 [1000/1000 (100%)]\tLosses F.softmax: 0.000189 log_softmax: 0.000022\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0477\tAccuracy: 8094.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0717\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 200 [0/1000 (0%)]\tLosses F.softmax: 0.000105 log_softmax: 0.000096\n",
            "Train Epoch: 200 [200/1000 (20%)]\tLosses F.softmax: 0.001994 log_softmax: 0.000565\n",
            "Train Epoch: 200 [400/1000 (40%)]\tLosses F.softmax: 0.013088 log_softmax: 0.007710\n",
            "Train Epoch: 200 [600/1000 (60%)]\tLosses F.softmax: 0.000215 log_softmax: 0.001186\n",
            "Train Epoch: 200 [800/1000 (80%)]\tLosses F.softmax: 0.001315 log_softmax: 0.012884\n",
            "Train Epoch: 200 [1000/1000 (100%)]\tLosses F.softmax: 0.014233 log_softmax: 0.007545\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0477\tAccuracy: 8095.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0719\tAccuracy: 8060.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 201 [0/1000 (0%)]\tLosses F.softmax: 0.000021 log_softmax: 0.000034\n",
            "Train Epoch: 201 [200/1000 (20%)]\tLosses F.softmax: 0.002955 log_softmax: 0.001537\n",
            "Train Epoch: 201 [400/1000 (40%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000001\n",
            "Train Epoch: 201 [600/1000 (60%)]\tLosses F.softmax: 0.000022 log_softmax: 0.000004\n",
            "Train Epoch: 201 [800/1000 (80%)]\tLosses F.softmax: 0.000402 log_softmax: 0.000224\n",
            "Train Epoch: 201 [1000/1000 (100%)]\tLosses F.softmax: 0.000446 log_softmax: 0.010412\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0483\tAccuracy: 8099.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0723\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 202 [0/1000 (0%)]\tLosses F.softmax: 0.004034 log_softmax: 0.001371\n",
            "Train Epoch: 202 [200/1000 (20%)]\tLosses F.softmax: 0.000032 log_softmax: 0.000225\n",
            "Train Epoch: 202 [400/1000 (40%)]\tLosses F.softmax: 0.000610 log_softmax: 0.000526\n",
            "Train Epoch: 202 [600/1000 (60%)]\tLosses F.softmax: 0.001329 log_softmax: 0.002020\n",
            "Train Epoch: 202 [800/1000 (80%)]\tLosses F.softmax: 0.000937 log_softmax: 0.003431\n",
            "Train Epoch: 202 [1000/1000 (100%)]\tLosses F.softmax: 0.023278 log_softmax: 0.009048\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0495\tAccuracy: 8093.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0735\tAccuracy: 8059.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 203 [0/1000 (0%)]\tLosses F.softmax: 0.009484 log_softmax: 0.016313\n",
            "Train Epoch: 203 [200/1000 (20%)]\tLosses F.softmax: 0.017477 log_softmax: 0.011009\n",
            "Train Epoch: 203 [400/1000 (40%)]\tLosses F.softmax: 0.000218 log_softmax: 0.000041\n",
            "Train Epoch: 203 [600/1000 (60%)]\tLosses F.softmax: 0.000914 log_softmax: 0.000939\n",
            "Train Epoch: 203 [800/1000 (80%)]\tLosses F.softmax: 0.000087 log_softmax: 0.000018\n",
            "Train Epoch: 203 [1000/1000 (100%)]\tLosses F.softmax: 0.009138 log_softmax: 0.008419\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0503\tAccuracy: 8089.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0743\tAccuracy: 8049.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 204 [0/1000 (0%)]\tLosses F.softmax: 0.000900 log_softmax: 0.005451\n",
            "Train Epoch: 204 [200/1000 (20%)]\tLosses F.softmax: 0.000281 log_softmax: 0.000153\n",
            "Train Epoch: 204 [400/1000 (40%)]\tLosses F.softmax: 0.039914 log_softmax: 0.037909\n",
            "Train Epoch: 204 [600/1000 (60%)]\tLosses F.softmax: 0.000130 log_softmax: 0.000007\n",
            "Train Epoch: 204 [800/1000 (80%)]\tLosses F.softmax: 0.002327 log_softmax: 0.001438\n",
            "Train Epoch: 204 [1000/1000 (100%)]\tLosses F.softmax: 0.014709 log_softmax: 0.007809\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0510\tAccuracy: 8087.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0748\tAccuracy: 8050.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 205 [0/1000 (0%)]\tLosses F.softmax: 0.009413 log_softmax: 0.000910\n",
            "Train Epoch: 205 [200/1000 (20%)]\tLosses F.softmax: 0.000005 log_softmax: 0.000001\n",
            "Train Epoch: 205 [400/1000 (40%)]\tLosses F.softmax: 0.000276 log_softmax: 0.000034\n",
            "Train Epoch: 205 [600/1000 (60%)]\tLosses F.softmax: 0.000007 log_softmax: 0.000005\n",
            "Train Epoch: 205 [800/1000 (80%)]\tLosses F.softmax: 0.000046 log_softmax: 0.000399\n",
            "Train Epoch: 205 [1000/1000 (100%)]\tLosses F.softmax: 0.000045 log_softmax: 0.000001\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0520\tAccuracy: 8088.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0762\tAccuracy: 8049.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 206 [0/1000 (0%)]\tLosses F.softmax: 0.000334 log_softmax: 0.000166\n",
            "Train Epoch: 206 [200/1000 (20%)]\tLosses F.softmax: 0.001544 log_softmax: 0.002769\n",
            "Train Epoch: 206 [400/1000 (40%)]\tLosses F.softmax: 0.000015 log_softmax: 0.000011\n",
            "Train Epoch: 206 [600/1000 (60%)]\tLosses F.softmax: 0.001810 log_softmax: 0.000778\n",
            "Train Epoch: 206 [800/1000 (80%)]\tLosses F.softmax: 0.002435 log_softmax: 0.000373\n",
            "Train Epoch: 206 [1000/1000 (100%)]\tLosses F.softmax: 0.002063 log_softmax: 0.005968\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0520\tAccuracy: 8092.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0759\tAccuracy: 8058.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 207 [0/1000 (0%)]\tLosses F.softmax: 0.000118 log_softmax: 0.000034\n",
            "Train Epoch: 207 [200/1000 (20%)]\tLosses F.softmax: 0.000034 log_softmax: 0.000125\n",
            "Train Epoch: 207 [400/1000 (40%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000000\n",
            "Train Epoch: 207 [600/1000 (60%)]\tLosses F.softmax: 0.034422 log_softmax: 0.027400\n",
            "Train Epoch: 207 [800/1000 (80%)]\tLosses F.softmax: 0.000039 log_softmax: 0.000547\n",
            "Train Epoch: 207 [1000/1000 (100%)]\tLosses F.softmax: 0.000124 log_softmax: 0.000290\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0520\tAccuracy: 8095.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0763\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 208 [0/1000 (0%)]\tLosses F.softmax: 0.027461 log_softmax: 0.027242\n",
            "Train Epoch: 208 [200/1000 (20%)]\tLosses F.softmax: 0.008273 log_softmax: 0.008574\n",
            "Train Epoch: 208 [400/1000 (40%)]\tLosses F.softmax: 0.001080 log_softmax: 0.000121\n",
            "Train Epoch: 208 [600/1000 (60%)]\tLosses F.softmax: 0.000113 log_softmax: 0.000095\n",
            "Train Epoch: 208 [800/1000 (80%)]\tLosses F.softmax: 0.001042 log_softmax: 0.002945\n",
            "Train Epoch: 208 [1000/1000 (100%)]\tLosses F.softmax: 0.003248 log_softmax: 0.011333\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0534\tAccuracy: 8089.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0773\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 209 [0/1000 (0%)]\tLosses F.softmax: 0.006445 log_softmax: 0.014023\n",
            "Train Epoch: 209 [200/1000 (20%)]\tLosses F.softmax: 0.002486 log_softmax: 0.007301\n",
            "Train Epoch: 209 [400/1000 (40%)]\tLosses F.softmax: 0.000099 log_softmax: 0.000173\n",
            "Train Epoch: 209 [600/1000 (60%)]\tLosses F.softmax: 0.000107 log_softmax: 0.000011\n",
            "Train Epoch: 209 [800/1000 (80%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000000\n",
            "Train Epoch: 209 [1000/1000 (100%)]\tLosses F.softmax: 0.000416 log_softmax: 0.000759\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0534\tAccuracy: 8088.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0779\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 210 [0/1000 (0%)]\tLosses F.softmax: 0.009280 log_softmax: 0.011473\n",
            "Train Epoch: 210 [200/1000 (20%)]\tLosses F.softmax: 0.011980 log_softmax: 0.023617\n",
            "Train Epoch: 210 [400/1000 (40%)]\tLosses F.softmax: 0.004383 log_softmax: 0.005218\n",
            "Train Epoch: 210 [600/1000 (60%)]\tLosses F.softmax: 0.005512 log_softmax: 0.009011\n",
            "Train Epoch: 210 [800/1000 (80%)]\tLosses F.softmax: 0.001480 log_softmax: 0.000401\n",
            "Train Epoch: 210 [1000/1000 (100%)]\tLosses F.softmax: 0.000327 log_softmax: 0.000029\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0541\tAccuracy: 8087.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0789\tAccuracy: 8048.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 211 [0/1000 (0%)]\tLosses F.softmax: 0.000143 log_softmax: 0.000960\n",
            "Train Epoch: 211 [200/1000 (20%)]\tLosses F.softmax: 0.013307 log_softmax: 0.007230\n",
            "Train Epoch: 211 [400/1000 (40%)]\tLosses F.softmax: 0.010879 log_softmax: 0.000176\n",
            "Train Epoch: 211 [600/1000 (60%)]\tLosses F.softmax: 0.049719 log_softmax: 0.021167\n",
            "Train Epoch: 211 [800/1000 (80%)]\tLosses F.softmax: 0.014409 log_softmax: 0.015404\n",
            "Train Epoch: 211 [1000/1000 (100%)]\tLosses F.softmax: 0.000391 log_softmax: 0.000724\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0539\tAccuracy: 8090.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0787\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 212 [0/1000 (0%)]\tLosses F.softmax: 0.002051 log_softmax: 0.001219\n",
            "Train Epoch: 212 [200/1000 (20%)]\tLosses F.softmax: 0.006549 log_softmax: 0.000104\n",
            "Train Epoch: 212 [400/1000 (40%)]\tLosses F.softmax: 0.000235 log_softmax: 0.000284\n",
            "Train Epoch: 212 [600/1000 (60%)]\tLosses F.softmax: 0.001240 log_softmax: 0.000900\n",
            "Train Epoch: 212 [800/1000 (80%)]\tLosses F.softmax: 0.000029 log_softmax: 0.000043\n",
            "Train Epoch: 212 [1000/1000 (100%)]\tLosses F.softmax: 0.000051 log_softmax: 0.000011\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0551\tAccuracy: 8092.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0793\tAccuracy: 8062.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 213 [0/1000 (0%)]\tLosses F.softmax: 0.000040 log_softmax: 0.000088\n",
            "Train Epoch: 213 [200/1000 (20%)]\tLosses F.softmax: 0.000142 log_softmax: 0.000010\n",
            "Train Epoch: 213 [400/1000 (40%)]\tLosses F.softmax: 0.001781 log_softmax: 0.003612\n",
            "Train Epoch: 213 [600/1000 (60%)]\tLosses F.softmax: 0.000967 log_softmax: 0.002822\n",
            "Train Epoch: 213 [800/1000 (80%)]\tLosses F.softmax: 0.000141 log_softmax: 0.000897\n",
            "Train Epoch: 213 [1000/1000 (100%)]\tLosses F.softmax: 0.010169 log_softmax: 0.005235\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0562\tAccuracy: 8086.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0805\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 214 [0/1000 (0%)]\tLosses F.softmax: 0.001363 log_softmax: 0.001393\n",
            "Train Epoch: 214 [200/1000 (20%)]\tLosses F.softmax: 0.001261 log_softmax: 0.000282\n",
            "Train Epoch: 214 [400/1000 (40%)]\tLosses F.softmax: 0.006706 log_softmax: 0.002123\n",
            "Train Epoch: 214 [600/1000 (60%)]\tLosses F.softmax: 0.000539 log_softmax: 0.002099\n",
            "Train Epoch: 214 [800/1000 (80%)]\tLosses F.softmax: 0.000153 log_softmax: 0.000078\n",
            "Train Epoch: 214 [1000/1000 (100%)]\tLosses F.softmax: 0.067061 log_softmax: 0.062886\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0566\tAccuracy: 8086.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0813\tAccuracy: 8047.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 215 [0/1000 (0%)]\tLosses F.softmax: 0.000342 log_softmax: 0.011009\n",
            "Train Epoch: 215 [200/1000 (20%)]\tLosses F.softmax: 0.000111 log_softmax: 0.000057\n",
            "Train Epoch: 215 [400/1000 (40%)]\tLosses F.softmax: 0.002490 log_softmax: 0.000374\n",
            "Train Epoch: 215 [600/1000 (60%)]\tLosses F.softmax: 0.001136 log_softmax: 0.001213\n",
            "Train Epoch: 215 [800/1000 (80%)]\tLosses F.softmax: 0.000552 log_softmax: 0.000410\n",
            "Train Epoch: 215 [1000/1000 (100%)]\tLosses F.softmax: 0.087449 log_softmax: 0.076407\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0575\tAccuracy: 8089.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0817\tAccuracy: 8059.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 216 [0/1000 (0%)]\tLosses F.softmax: 0.000011 log_softmax: 0.000004\n",
            "Train Epoch: 216 [200/1000 (20%)]\tLosses F.softmax: 0.000962 log_softmax: 0.000359\n",
            "Train Epoch: 216 [400/1000 (40%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000001\n",
            "Train Epoch: 216 [600/1000 (60%)]\tLosses F.softmax: 0.000016 log_softmax: 0.000009\n",
            "Train Epoch: 216 [800/1000 (80%)]\tLosses F.softmax: 0.000259 log_softmax: 0.000005\n",
            "Train Epoch: 216 [1000/1000 (100%)]\tLosses F.softmax: 0.000040 log_softmax: 0.000008\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0574\tAccuracy: 8097.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0818\tAccuracy: 8062.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 217 [0/1000 (0%)]\tLosses F.softmax: 0.000260 log_softmax: 0.000276\n",
            "Train Epoch: 217 [200/1000 (20%)]\tLosses F.softmax: 0.000059 log_softmax: 0.000254\n",
            "Train Epoch: 217 [400/1000 (40%)]\tLosses F.softmax: 0.018997 log_softmax: 0.023233\n",
            "Train Epoch: 217 [600/1000 (60%)]\tLosses F.softmax: 0.001569 log_softmax: 0.003059\n",
            "Train Epoch: 217 [800/1000 (80%)]\tLosses F.softmax: 0.000036 log_softmax: 0.000008\n",
            "Train Epoch: 217 [1000/1000 (100%)]\tLosses F.softmax: 0.000043 log_softmax: 0.000010\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0578\tAccuracy: 8097.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0821\tAccuracy: 8060.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 218 [0/1000 (0%)]\tLosses F.softmax: 0.033798 log_softmax: 0.047022\n",
            "Train Epoch: 218 [200/1000 (20%)]\tLosses F.softmax: 0.000235 log_softmax: 0.000052\n",
            "Train Epoch: 218 [400/1000 (40%)]\tLosses F.softmax: 0.000012 log_softmax: 0.000029\n",
            "Train Epoch: 218 [600/1000 (60%)]\tLosses F.softmax: 0.000043 log_softmax: 0.000422\n",
            "Train Epoch: 218 [800/1000 (80%)]\tLosses F.softmax: 0.005804 log_softmax: 0.002272\n",
            "Train Epoch: 218 [1000/1000 (100%)]\tLosses F.softmax: 0.000031 log_softmax: 0.000234\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0592\tAccuracy: 8086.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0839\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 219 [0/1000 (0%)]\tLosses F.softmax: 0.017054 log_softmax: 0.027152\n",
            "Train Epoch: 219 [200/1000 (20%)]\tLosses F.softmax: 0.001400 log_softmax: 0.000027\n",
            "Train Epoch: 219 [400/1000 (40%)]\tLosses F.softmax: 0.000160 log_softmax: 0.000020\n",
            "Train Epoch: 219 [600/1000 (60%)]\tLosses F.softmax: 0.020133 log_softmax: 0.005868\n",
            "Train Epoch: 219 [800/1000 (80%)]\tLosses F.softmax: 0.000007 log_softmax: 0.000001\n",
            "Train Epoch: 219 [1000/1000 (100%)]\tLosses F.softmax: 0.006461 log_softmax: 0.016186\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0588\tAccuracy: 8095.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0835\tAccuracy: 8059.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 220 [0/1000 (0%)]\tLosses F.softmax: 0.001807 log_softmax: 0.003528\n",
            "Train Epoch: 220 [200/1000 (20%)]\tLosses F.softmax: 0.031665 log_softmax: 0.030043\n",
            "Train Epoch: 220 [400/1000 (40%)]\tLosses F.softmax: 0.000107 log_softmax: 0.000032\n",
            "Train Epoch: 220 [600/1000 (60%)]\tLosses F.softmax: 0.001274 log_softmax: 0.006172\n",
            "Train Epoch: 220 [800/1000 (80%)]\tLosses F.softmax: 0.000749 log_softmax: 0.000818\n",
            "Train Epoch: 220 [1000/1000 (100%)]\tLosses F.softmax: 0.019285 log_softmax: 0.003810\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0593\tAccuracy: 8095.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0843\tAccuracy: 8058.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 221 [0/1000 (0%)]\tLosses F.softmax: 0.000096 log_softmax: 0.000010\n",
            "Train Epoch: 221 [200/1000 (20%)]\tLosses F.softmax: 0.026191 log_softmax: 0.015990\n",
            "Train Epoch: 221 [400/1000 (40%)]\tLosses F.softmax: 0.002916 log_softmax: 0.000151\n",
            "Train Epoch: 221 [600/1000 (60%)]\tLosses F.softmax: 0.000012 log_softmax: 0.000000\n",
            "Train Epoch: 221 [800/1000 (80%)]\tLosses F.softmax: 0.000449 log_softmax: 0.000006\n",
            "Train Epoch: 221 [1000/1000 (100%)]\tLosses F.softmax: 0.000032 log_softmax: 0.000027\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0602\tAccuracy: 8091.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0850\tAccuracy: 8058.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 222 [0/1000 (0%)]\tLosses F.softmax: 0.000143 log_softmax: 0.000077\n",
            "Train Epoch: 222 [200/1000 (20%)]\tLosses F.softmax: 0.017250 log_softmax: 0.027852\n",
            "Train Epoch: 222 [400/1000 (40%)]\tLosses F.softmax: 0.004236 log_softmax: 0.009980\n",
            "Train Epoch: 222 [600/1000 (60%)]\tLosses F.softmax: 0.000125 log_softmax: 0.000006\n",
            "Train Epoch: 222 [800/1000 (80%)]\tLosses F.softmax: 0.000223 log_softmax: 0.000148\n",
            "Train Epoch: 222 [1000/1000 (100%)]\tLosses F.softmax: 0.005256 log_softmax: 0.001673\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0603\tAccuracy: 8100.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0853\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 223 [0/1000 (0%)]\tLosses F.softmax: 0.000032 log_softmax: 0.000484\n",
            "Train Epoch: 223 [200/1000 (20%)]\tLosses F.softmax: 0.000003 log_softmax: 0.000002\n",
            "Train Epoch: 223 [400/1000 (40%)]\tLosses F.softmax: 0.008863 log_softmax: 0.008278\n",
            "Train Epoch: 223 [600/1000 (60%)]\tLosses F.softmax: 0.001205 log_softmax: 0.000814\n",
            "Train Epoch: 223 [800/1000 (80%)]\tLosses F.softmax: 0.000013 log_softmax: 0.000002\n",
            "Train Epoch: 223 [1000/1000 (100%)]\tLosses F.softmax: 0.000015 log_softmax: 0.000014\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0617\tAccuracy: 8088.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0863\tAccuracy: 8057.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 224 [0/1000 (0%)]\tLosses F.softmax: 0.000002 log_softmax: 0.000003\n",
            "Train Epoch: 224 [200/1000 (20%)]\tLosses F.softmax: 0.000768 log_softmax: 0.000395\n",
            "Train Epoch: 224 [400/1000 (40%)]\tLosses F.softmax: 0.000004 log_softmax: 0.000007\n",
            "Train Epoch: 224 [600/1000 (60%)]\tLosses F.softmax: 0.009588 log_softmax: 0.011875\n",
            "Train Epoch: 224 [800/1000 (80%)]\tLosses F.softmax: 0.008924 log_softmax: 0.026522\n",
            "Train Epoch: 224 [1000/1000 (100%)]\tLosses F.softmax: 0.000015 log_softmax: 0.000008\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0615\tAccuracy: 8096.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0868\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 225 [0/1000 (0%)]\tLosses F.softmax: 0.002147 log_softmax: 0.000498\n",
            "Train Epoch: 225 [200/1000 (20%)]\tLosses F.softmax: 0.012015 log_softmax: 0.019835\n",
            "Train Epoch: 225 [400/1000 (40%)]\tLosses F.softmax: 0.004928 log_softmax: 0.008738\n",
            "Train Epoch: 225 [600/1000 (60%)]\tLosses F.softmax: 0.010130 log_softmax: 0.000849\n",
            "Train Epoch: 225 [800/1000 (80%)]\tLosses F.softmax: 0.000855 log_softmax: 0.000576\n",
            "Train Epoch: 225 [1000/1000 (100%)]\tLosses F.softmax: 0.000102 log_softmax: 0.000390\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0617\tAccuracy: 8101.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0869\tAccuracy: 8057.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 226 [0/1000 (0%)]\tLosses F.softmax: 0.000595 log_softmax: 0.004518\n",
            "Train Epoch: 226 [200/1000 (20%)]\tLosses F.softmax: 0.001119 log_softmax: 0.001322\n",
            "Train Epoch: 226 [400/1000 (40%)]\tLosses F.softmax: 0.018214 log_softmax: 0.015678\n",
            "Train Epoch: 226 [600/1000 (60%)]\tLosses F.softmax: 0.000328 log_softmax: 0.000307\n",
            "Train Epoch: 226 [800/1000 (80%)]\tLosses F.softmax: 0.000009 log_softmax: 0.000008\n",
            "Train Epoch: 226 [1000/1000 (100%)]\tLosses F.softmax: 0.000200 log_softmax: 0.000748\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0628\tAccuracy: 8092.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0880\tAccuracy: 8052.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 227 [0/1000 (0%)]\tLosses F.softmax: 0.021332 log_softmax: 0.023236\n",
            "Train Epoch: 227 [200/1000 (20%)]\tLosses F.softmax: 0.001853 log_softmax: 0.001182\n",
            "Train Epoch: 227 [400/1000 (40%)]\tLosses F.softmax: 0.059102 log_softmax: 0.067509\n",
            "Train Epoch: 227 [600/1000 (60%)]\tLosses F.softmax: 0.056400 log_softmax: 0.023505\n",
            "Train Epoch: 227 [800/1000 (80%)]\tLosses F.softmax: 0.000225 log_softmax: 0.000396\n",
            "Train Epoch: 227 [1000/1000 (100%)]\tLosses F.softmax: 0.000283 log_softmax: 0.000149\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0627\tAccuracy: 8094.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0883\tAccuracy: 8059.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 228 [0/1000 (0%)]\tLosses F.softmax: 0.000477 log_softmax: 0.000023\n",
            "Train Epoch: 228 [200/1000 (20%)]\tLosses F.softmax: 0.004870 log_softmax: 0.002631\n",
            "Train Epoch: 228 [400/1000 (40%)]\tLosses F.softmax: 0.000003 log_softmax: 0.000003\n",
            "Train Epoch: 228 [600/1000 (60%)]\tLosses F.softmax: 0.000012 log_softmax: 0.000000\n",
            "Train Epoch: 228 [800/1000 (80%)]\tLosses F.softmax: 0.000400 log_softmax: 0.000814\n",
            "Train Epoch: 228 [1000/1000 (100%)]\tLosses F.softmax: 0.000039 log_softmax: 0.000035\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0636\tAccuracy: 8095.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0890\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 229 [0/1000 (0%)]\tLosses F.softmax: 0.000149 log_softmax: 0.000329\n",
            "Train Epoch: 229 [200/1000 (20%)]\tLosses F.softmax: 0.022497 log_softmax: 0.010786\n",
            "Train Epoch: 229 [400/1000 (40%)]\tLosses F.softmax: 0.000313 log_softmax: 0.000141\n",
            "Train Epoch: 229 [600/1000 (60%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000001\n",
            "Train Epoch: 229 [800/1000 (80%)]\tLosses F.softmax: 0.000014 log_softmax: 0.000010\n",
            "Train Epoch: 229 [1000/1000 (100%)]\tLosses F.softmax: 0.006249 log_softmax: 0.020579\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0652\tAccuracy: 8087.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0903\tAccuracy: 8050.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 230 [0/1000 (0%)]\tLosses F.softmax: 0.000648 log_softmax: 0.000027\n",
            "Train Epoch: 230 [200/1000 (20%)]\tLosses F.softmax: 0.000137 log_softmax: 0.000007\n",
            "Train Epoch: 230 [400/1000 (40%)]\tLosses F.softmax: 0.000294 log_softmax: 0.000291\n",
            "Train Epoch: 230 [600/1000 (60%)]\tLosses F.softmax: 0.000149 log_softmax: 0.000071\n",
            "Train Epoch: 230 [800/1000 (80%)]\tLosses F.softmax: 0.034553 log_softmax: 0.014847\n",
            "Train Epoch: 230 [1000/1000 (100%)]\tLosses F.softmax: 0.000222 log_softmax: 0.000073\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0647\tAccuracy: 8102.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0900\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 231 [0/1000 (0%)]\tLosses F.softmax: 0.001169 log_softmax: 0.010953\n",
            "Train Epoch: 231 [200/1000 (20%)]\tLosses F.softmax: 0.017878 log_softmax: 0.003462\n",
            "Train Epoch: 231 [400/1000 (40%)]\tLosses F.softmax: 0.000038 log_softmax: 0.000356\n",
            "Train Epoch: 231 [600/1000 (60%)]\tLosses F.softmax: 0.008492 log_softmax: 0.017860\n",
            "Train Epoch: 231 [800/1000 (80%)]\tLosses F.softmax: 0.000177 log_softmax: 0.000491\n",
            "Train Epoch: 231 [1000/1000 (100%)]\tLosses F.softmax: 0.024197 log_softmax: 0.004778\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0664\tAccuracy: 8088.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0914\tAccuracy: 8048.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 232 [0/1000 (0%)]\tLosses F.softmax: 0.000132 log_softmax: 0.000870\n",
            "Train Epoch: 232 [200/1000 (20%)]\tLosses F.softmax: 0.000014 log_softmax: 0.000013\n",
            "Train Epoch: 232 [400/1000 (40%)]\tLosses F.softmax: 0.001645 log_softmax: 0.000458\n",
            "Train Epoch: 232 [600/1000 (60%)]\tLosses F.softmax: 0.015522 log_softmax: 0.009818\n",
            "Train Epoch: 232 [800/1000 (80%)]\tLosses F.softmax: 0.000012 log_softmax: 0.000001\n",
            "Train Epoch: 232 [1000/1000 (100%)]\tLosses F.softmax: 0.000005 log_softmax: 0.000063\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0667\tAccuracy: 8089.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0922\tAccuracy: 8050.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 233 [0/1000 (0%)]\tLosses F.softmax: 0.002526 log_softmax: 0.001767\n",
            "Train Epoch: 233 [200/1000 (20%)]\tLosses F.softmax: 0.000156 log_softmax: 0.000008\n",
            "Train Epoch: 233 [400/1000 (40%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000000\n",
            "Train Epoch: 233 [600/1000 (60%)]\tLosses F.softmax: 0.000208 log_softmax: 0.000038\n",
            "Train Epoch: 233 [800/1000 (80%)]\tLosses F.softmax: 0.000055 log_softmax: 0.000149\n",
            "Train Epoch: 233 [1000/1000 (100%)]\tLosses F.softmax: 0.000207 log_softmax: 0.000065\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0668\tAccuracy: 8097.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0921\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 234 [0/1000 (0%)]\tLosses F.softmax: 0.013358 log_softmax: 0.013803\n",
            "Train Epoch: 234 [200/1000 (20%)]\tLosses F.softmax: 0.000161 log_softmax: 0.001577\n",
            "Train Epoch: 234 [400/1000 (40%)]\tLosses F.softmax: 0.017215 log_softmax: 0.011618\n",
            "Train Epoch: 234 [600/1000 (60%)]\tLosses F.softmax: 0.001022 log_softmax: 0.000879\n",
            "Train Epoch: 234 [800/1000 (80%)]\tLosses F.softmax: 0.000213 log_softmax: 0.000050\n",
            "Train Epoch: 234 [1000/1000 (100%)]\tLosses F.softmax: 0.003123 log_softmax: 0.000121\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0667\tAccuracy: 8099.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0924\tAccuracy: 8058.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 235 [0/1000 (0%)]\tLosses F.softmax: 0.000243 log_softmax: 0.000104\n",
            "Train Epoch: 235 [200/1000 (20%)]\tLosses F.softmax: 0.010912 log_softmax: 0.009072\n",
            "Train Epoch: 235 [400/1000 (40%)]\tLosses F.softmax: 0.000014 log_softmax: 0.000002\n",
            "Train Epoch: 235 [600/1000 (60%)]\tLosses F.softmax: 0.035009 log_softmax: 0.024274\n",
            "Train Epoch: 235 [800/1000 (80%)]\tLosses F.softmax: 0.031086 log_softmax: 0.025138\n",
            "Train Epoch: 235 [1000/1000 (100%)]\tLosses F.softmax: 0.005450 log_softmax: 0.001941\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0676\tAccuracy: 8096.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0932\tAccuracy: 8057.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 236 [0/1000 (0%)]\tLosses F.softmax: 0.008849 log_softmax: 0.002787\n",
            "Train Epoch: 236 [200/1000 (20%)]\tLosses F.softmax: 0.002018 log_softmax: 0.001589\n",
            "Train Epoch: 236 [400/1000 (40%)]\tLosses F.softmax: 0.000303 log_softmax: 0.000854\n",
            "Train Epoch: 236 [600/1000 (60%)]\tLosses F.softmax: 0.000262 log_softmax: 0.000039\n",
            "Train Epoch: 236 [800/1000 (80%)]\tLosses F.softmax: 0.009693 log_softmax: 0.031745\n",
            "Train Epoch: 236 [1000/1000 (100%)]\tLosses F.softmax: 0.030271 log_softmax: 0.024425\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0686\tAccuracy: 8089.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0940\tAccuracy: 8058.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 237 [0/1000 (0%)]\tLosses F.softmax: 0.001177 log_softmax: 0.001237\n",
            "Train Epoch: 237 [200/1000 (20%)]\tLosses F.softmax: 0.040813 log_softmax: 0.037273\n",
            "Train Epoch: 237 [400/1000 (40%)]\tLosses F.softmax: 0.000217 log_softmax: 0.000437\n",
            "Train Epoch: 237 [600/1000 (60%)]\tLosses F.softmax: 0.029886 log_softmax: 0.041913\n",
            "Train Epoch: 237 [800/1000 (80%)]\tLosses F.softmax: 0.000002 log_softmax: 0.000002\n",
            "Train Epoch: 237 [1000/1000 (100%)]\tLosses F.softmax: 0.004600 log_softmax: 0.001506\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0690\tAccuracy: 8088.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0948\tAccuracy: 8052.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 238 [0/1000 (0%)]\tLosses F.softmax: 0.012601 log_softmax: 0.011907\n",
            "Train Epoch: 238 [200/1000 (20%)]\tLosses F.softmax: 0.001833 log_softmax: 0.000107\n",
            "Train Epoch: 238 [400/1000 (40%)]\tLosses F.softmax: 0.031643 log_softmax: 0.018454\n",
            "Train Epoch: 238 [600/1000 (60%)]\tLosses F.softmax: 0.020700 log_softmax: 0.059741\n",
            "Train Epoch: 238 [800/1000 (80%)]\tLosses F.softmax: 0.001124 log_softmax: 0.001756\n",
            "Train Epoch: 238 [1000/1000 (100%)]\tLosses F.softmax: 0.000003 log_softmax: 0.000006\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0695\tAccuracy: 8093.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0948\tAccuracy: 8060.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 239 [0/1000 (0%)]\tLosses F.softmax: 0.008707 log_softmax: 0.015617\n",
            "Train Epoch: 239 [200/1000 (20%)]\tLosses F.softmax: 0.000114 log_softmax: 0.001031\n",
            "Train Epoch: 239 [400/1000 (40%)]\tLosses F.softmax: 0.000082 log_softmax: 0.003116\n",
            "Train Epoch: 239 [600/1000 (60%)]\tLosses F.softmax: 0.000662 log_softmax: 0.001691\n",
            "Train Epoch: 239 [800/1000 (80%)]\tLosses F.softmax: 0.001115 log_softmax: 0.001200\n",
            "Train Epoch: 239 [1000/1000 (100%)]\tLosses F.softmax: 0.005585 log_softmax: 0.001729\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0704\tAccuracy: 8089.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0962\tAccuracy: 8051.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 240 [0/1000 (0%)]\tLosses F.softmax: 0.000312 log_softmax: 0.000890\n",
            "Train Epoch: 240 [200/1000 (20%)]\tLosses F.softmax: 0.000048 log_softmax: 0.000017\n",
            "Train Epoch: 240 [400/1000 (40%)]\tLosses F.softmax: 0.001413 log_softmax: 0.000008\n",
            "Train Epoch: 240 [600/1000 (60%)]\tLosses F.softmax: 0.047361 log_softmax: 0.020272\n",
            "Train Epoch: 240 [800/1000 (80%)]\tLosses F.softmax: 0.000577 log_softmax: 0.000006\n",
            "Train Epoch: 240 [1000/1000 (100%)]\tLosses F.softmax: 0.002845 log_softmax: 0.000644\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0701\tAccuracy: 8094.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0960\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 241 [0/1000 (0%)]\tLosses F.softmax: 0.000004 log_softmax: 0.000003\n",
            "Train Epoch: 241 [200/1000 (20%)]\tLosses F.softmax: 0.000003 log_softmax: 0.000006\n",
            "Train Epoch: 241 [400/1000 (40%)]\tLosses F.softmax: 0.010517 log_softmax: 0.000539\n",
            "Train Epoch: 241 [600/1000 (60%)]\tLosses F.softmax: 0.014616 log_softmax: 0.009377\n",
            "Train Epoch: 241 [800/1000 (80%)]\tLosses F.softmax: 0.001491 log_softmax: 0.000554\n",
            "Train Epoch: 241 [1000/1000 (100%)]\tLosses F.softmax: 0.018738 log_softmax: 0.003609\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0710\tAccuracy: 8092.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0965\tAccuracy: 8058.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 242 [0/1000 (0%)]\tLosses F.softmax: 0.000646 log_softmax: 0.001701\n",
            "Train Epoch: 242 [200/1000 (20%)]\tLosses F.softmax: 0.025257 log_softmax: 0.031463\n",
            "Train Epoch: 242 [400/1000 (40%)]\tLosses F.softmax: 0.039839 log_softmax: 0.014516\n",
            "Train Epoch: 242 [600/1000 (60%)]\tLosses F.softmax: 0.001464 log_softmax: 0.000424\n",
            "Train Epoch: 242 [800/1000 (80%)]\tLosses F.softmax: 0.000002 log_softmax: 0.000002\n",
            "Train Epoch: 242 [1000/1000 (100%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000000\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0713\tAccuracy: 8095.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0971\tAccuracy: 8058.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 243 [0/1000 (0%)]\tLosses F.softmax: 0.000428 log_softmax: 0.000055\n",
            "Train Epoch: 243 [200/1000 (20%)]\tLosses F.softmax: 0.000008 log_softmax: 0.000000\n",
            "Train Epoch: 243 [400/1000 (40%)]\tLosses F.softmax: 0.049428 log_softmax: 0.040238\n",
            "Train Epoch: 243 [600/1000 (60%)]\tLosses F.softmax: 0.000923 log_softmax: 0.000010\n",
            "Train Epoch: 243 [800/1000 (80%)]\tLosses F.softmax: 0.053353 log_softmax: 0.061724\n",
            "Train Epoch: 243 [1000/1000 (100%)]\tLosses F.softmax: 0.000668 log_softmax: 0.001252\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0714\tAccuracy: 8102.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0974\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 244 [0/1000 (0%)]\tLosses F.softmax: 0.000104 log_softmax: 0.000187\n",
            "Train Epoch: 244 [200/1000 (20%)]\tLosses F.softmax: 0.000342 log_softmax: 0.000163\n",
            "Train Epoch: 244 [400/1000 (40%)]\tLosses F.softmax: 0.000087 log_softmax: 0.001152\n",
            "Train Epoch: 244 [600/1000 (60%)]\tLosses F.softmax: 0.005873 log_softmax: 0.003660\n",
            "Train Epoch: 244 [800/1000 (80%)]\tLosses F.softmax: 0.018868 log_softmax: 0.011505\n",
            "Train Epoch: 244 [1000/1000 (100%)]\tLosses F.softmax: 0.002729 log_softmax: 0.000191\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0721\tAccuracy: 8101.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0981\tAccuracy: 8057.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 245 [0/1000 (0%)]\tLosses F.softmax: 0.000003 log_softmax: 0.000002\n",
            "Train Epoch: 245 [200/1000 (20%)]\tLosses F.softmax: 0.000655 log_softmax: 0.000731\n",
            "Train Epoch: 245 [400/1000 (40%)]\tLosses F.softmax: 0.000639 log_softmax: 0.000245\n",
            "Train Epoch: 245 [600/1000 (60%)]\tLosses F.softmax: 0.073338 log_softmax: 0.043181\n",
            "Train Epoch: 245 [800/1000 (80%)]\tLosses F.softmax: 0.000087 log_softmax: 0.000000\n",
            "Train Epoch: 245 [1000/1000 (100%)]\tLosses F.softmax: 0.000007 log_softmax: 0.000007\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0728\tAccuracy: 8098.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0986\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 246 [0/1000 (0%)]\tLosses F.softmax: 0.003551 log_softmax: 0.000021\n",
            "Train Epoch: 246 [200/1000 (20%)]\tLosses F.softmax: 0.009022 log_softmax: 0.008313\n",
            "Train Epoch: 246 [400/1000 (40%)]\tLosses F.softmax: 0.033481 log_softmax: 0.032929\n",
            "Train Epoch: 246 [600/1000 (60%)]\tLosses F.softmax: 0.001242 log_softmax: 0.000955\n",
            "Train Epoch: 246 [800/1000 (80%)]\tLosses F.softmax: 0.003176 log_softmax: 0.000371\n",
            "Train Epoch: 246 [1000/1000 (100%)]\tLosses F.softmax: 0.000003 log_softmax: 0.000003\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0732\tAccuracy: 8095.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0993\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 247 [0/1000 (0%)]\tLosses F.softmax: 0.008107 log_softmax: 0.007650\n",
            "Train Epoch: 247 [200/1000 (20%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000001\n",
            "Train Epoch: 247 [400/1000 (40%)]\tLosses F.softmax: 0.000319 log_softmax: 0.000760\n",
            "Train Epoch: 247 [600/1000 (60%)]\tLosses F.softmax: 0.008179 log_softmax: 0.011626\n",
            "Train Epoch: 247 [800/1000 (80%)]\tLosses F.softmax: 0.000934 log_softmax: 0.000184\n",
            "Train Epoch: 247 [1000/1000 (100%)]\tLosses F.softmax: 0.001585 log_softmax: 0.003212\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0739\tAccuracy: 8092.0/10000 (81%)\n",
            "log_softmax: Loss: 1.0998\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 248 [0/1000 (0%)]\tLosses F.softmax: 0.024996 log_softmax: 0.019187\n",
            "Train Epoch: 248 [200/1000 (20%)]\tLosses F.softmax: 0.007666 log_softmax: 0.005990\n",
            "Train Epoch: 248 [400/1000 (40%)]\tLosses F.softmax: 0.012197 log_softmax: 0.006625\n",
            "Train Epoch: 248 [600/1000 (60%)]\tLosses F.softmax: 0.000481 log_softmax: 0.000371\n",
            "Train Epoch: 248 [800/1000 (80%)]\tLosses F.softmax: 0.029182 log_softmax: 0.005508\n",
            "Train Epoch: 248 [1000/1000 (100%)]\tLosses F.softmax: 0.000002 log_softmax: 0.000002\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0740\tAccuracy: 8098.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1001\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 249 [0/1000 (0%)]\tLosses F.softmax: 0.000005 log_softmax: 0.000004\n",
            "Train Epoch: 249 [200/1000 (20%)]\tLosses F.softmax: 0.003329 log_softmax: 0.002322\n",
            "Train Epoch: 249 [400/1000 (40%)]\tLosses F.softmax: 0.010948 log_softmax: 0.009355\n",
            "Train Epoch: 249 [600/1000 (60%)]\tLosses F.softmax: 0.000074 log_softmax: 0.002899\n",
            "Train Epoch: 249 [800/1000 (80%)]\tLosses F.softmax: 0.012730 log_softmax: 0.009786\n",
            "Train Epoch: 249 [1000/1000 (100%)]\tLosses F.softmax: 0.000048 log_softmax: 0.000013\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0748\tAccuracy: 8095.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1010\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 250 [0/1000 (0%)]\tLosses F.softmax: 0.002287 log_softmax: 0.004701\n",
            "Train Epoch: 250 [200/1000 (20%)]\tLosses F.softmax: 0.000751 log_softmax: 0.002772\n",
            "Train Epoch: 250 [400/1000 (40%)]\tLosses F.softmax: 0.000003 log_softmax: 0.000002\n",
            "Train Epoch: 250 [600/1000 (60%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000011\n",
            "Train Epoch: 250 [800/1000 (80%)]\tLosses F.softmax: 0.000807 log_softmax: 0.004795\n",
            "Train Epoch: 250 [1000/1000 (100%)]\tLosses F.softmax: 0.005499 log_softmax: 0.008632\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0759\tAccuracy: 8090.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1018\tAccuracy: 8052.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 251 [0/1000 (0%)]\tLosses F.softmax: 0.005322 log_softmax: 0.008948\n",
            "Train Epoch: 251 [200/1000 (20%)]\tLosses F.softmax: 0.015144 log_softmax: 0.016244\n",
            "Train Epoch: 251 [400/1000 (40%)]\tLosses F.softmax: 0.000736 log_softmax: 0.000762\n",
            "Train Epoch: 251 [600/1000 (60%)]\tLosses F.softmax: 0.009107 log_softmax: 0.008113\n",
            "Train Epoch: 251 [800/1000 (80%)]\tLosses F.softmax: 0.000739 log_softmax: 0.001818\n",
            "Train Epoch: 251 [1000/1000 (100%)]\tLosses F.softmax: 0.025347 log_softmax: 0.019414\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0754\tAccuracy: 8101.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1016\tAccuracy: 8058.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 252 [0/1000 (0%)]\tLosses F.softmax: 0.000139 log_softmax: 0.000826\n",
            "Train Epoch: 252 [200/1000 (20%)]\tLosses F.softmax: 0.002638 log_softmax: 0.009343\n",
            "Train Epoch: 252 [400/1000 (40%)]\tLosses F.softmax: 0.000126 log_softmax: 0.000065\n",
            "Train Epoch: 252 [600/1000 (60%)]\tLosses F.softmax: 0.000075 log_softmax: 0.000007\n",
            "Train Epoch: 252 [800/1000 (80%)]\tLosses F.softmax: 0.002451 log_softmax: 0.004221\n",
            "Train Epoch: 252 [1000/1000 (100%)]\tLosses F.softmax: 0.050806 log_softmax: 0.039147\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0765\tAccuracy: 8096.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1025\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 253 [0/1000 (0%)]\tLosses F.softmax: 0.000004 log_softmax: 0.000001\n",
            "Train Epoch: 253 [200/1000 (20%)]\tLosses F.softmax: 0.010042 log_softmax: 0.009361\n",
            "Train Epoch: 253 [400/1000 (40%)]\tLosses F.softmax: 0.015267 log_softmax: 0.018488\n",
            "Train Epoch: 253 [600/1000 (60%)]\tLosses F.softmax: 0.005226 log_softmax: 0.005953\n",
            "Train Epoch: 253 [800/1000 (80%)]\tLosses F.softmax: 0.000126 log_softmax: 0.000006\n",
            "Train Epoch: 253 [1000/1000 (100%)]\tLosses F.softmax: 0.000096 log_softmax: 0.000496\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0770\tAccuracy: 8089.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1034\tAccuracy: 8050.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 254 [0/1000 (0%)]\tLosses F.softmax: 0.000202 log_softmax: 0.000138\n",
            "Train Epoch: 254 [200/1000 (20%)]\tLosses F.softmax: 0.000023 log_softmax: 0.000004\n",
            "Train Epoch: 254 [400/1000 (40%)]\tLosses F.softmax: 0.000003 log_softmax: 0.000080\n",
            "Train Epoch: 254 [600/1000 (60%)]\tLosses F.softmax: 0.007570 log_softmax: 0.000763\n",
            "Train Epoch: 254 [800/1000 (80%)]\tLosses F.softmax: 0.003115 log_softmax: 0.000013\n",
            "Train Epoch: 254 [1000/1000 (100%)]\tLosses F.softmax: 0.016728 log_softmax: 0.002319\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0778\tAccuracy: 8091.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1042\tAccuracy: 8049.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 255 [0/1000 (0%)]\tLosses F.softmax: 0.000290 log_softmax: 0.000157\n",
            "Train Epoch: 255 [200/1000 (20%)]\tLosses F.softmax: 0.002816 log_softmax: 0.001863\n",
            "Train Epoch: 255 [400/1000 (40%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000010\n",
            "Train Epoch: 255 [600/1000 (60%)]\tLosses F.softmax: 0.019432 log_softmax: 0.011850\n",
            "Train Epoch: 255 [800/1000 (80%)]\tLosses F.softmax: 0.000020 log_softmax: 0.000133\n",
            "Train Epoch: 255 [1000/1000 (100%)]\tLosses F.softmax: 0.000005 log_softmax: 0.000004\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0777\tAccuracy: 8101.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1040\tAccuracy: 8059.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 256 [0/1000 (0%)]\tLosses F.softmax: 0.001146 log_softmax: 0.000484\n",
            "Train Epoch: 256 [200/1000 (20%)]\tLosses F.softmax: 0.052472 log_softmax: 0.041027\n",
            "Train Epoch: 256 [400/1000 (40%)]\tLosses F.softmax: 0.002064 log_softmax: 0.000565\n",
            "Train Epoch: 256 [600/1000 (60%)]\tLosses F.softmax: 0.000015 log_softmax: 0.000010\n",
            "Train Epoch: 256 [800/1000 (80%)]\tLosses F.softmax: 0.002632 log_softmax: 0.000560\n",
            "Train Epoch: 256 [1000/1000 (100%)]\tLosses F.softmax: 0.000245 log_softmax: 0.000335\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0785\tAccuracy: 8104.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1047\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 257 [0/1000 (0%)]\tLosses F.softmax: 0.000037 log_softmax: 0.000002\n",
            "Train Epoch: 257 [200/1000 (20%)]\tLosses F.softmax: 0.000004 log_softmax: 0.000006\n",
            "Train Epoch: 257 [400/1000 (40%)]\tLosses F.softmax: 0.000004 log_softmax: 0.000054\n",
            "Train Epoch: 257 [600/1000 (60%)]\tLosses F.softmax: 0.000774 log_softmax: 0.000804\n",
            "Train Epoch: 257 [800/1000 (80%)]\tLosses F.softmax: 0.022141 log_softmax: 0.014759\n",
            "Train Epoch: 257 [1000/1000 (100%)]\tLosses F.softmax: 0.003280 log_softmax: 0.001644\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0797\tAccuracy: 8098.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1054\tAccuracy: 8058.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 258 [0/1000 (0%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000019\n",
            "Train Epoch: 258 [200/1000 (20%)]\tLosses F.softmax: 0.002032 log_softmax: 0.002121\n",
            "Train Epoch: 258 [400/1000 (40%)]\tLosses F.softmax: 0.005899 log_softmax: 0.002560\n",
            "Train Epoch: 258 [600/1000 (60%)]\tLosses F.softmax: 0.020962 log_softmax: 0.035238\n",
            "Train Epoch: 258 [800/1000 (80%)]\tLosses F.softmax: 0.000166 log_softmax: 0.000050\n",
            "Train Epoch: 258 [1000/1000 (100%)]\tLosses F.softmax: 0.000186 log_softmax: 0.000383\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0802\tAccuracy: 8093.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1060\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 259 [0/1000 (0%)]\tLosses F.softmax: 0.000273 log_softmax: 0.000022\n",
            "Train Epoch: 259 [200/1000 (20%)]\tLosses F.softmax: 0.001075 log_softmax: 0.000065\n",
            "Train Epoch: 259 [400/1000 (40%)]\tLosses F.softmax: 0.008659 log_softmax: 0.005097\n",
            "Train Epoch: 259 [600/1000 (60%)]\tLosses F.softmax: 0.001782 log_softmax: 0.012819\n",
            "Train Epoch: 259 [800/1000 (80%)]\tLosses F.softmax: 0.032428 log_softmax: 0.042303\n",
            "Train Epoch: 259 [1000/1000 (100%)]\tLosses F.softmax: 0.001509 log_softmax: 0.000264\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0806\tAccuracy: 8101.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1065\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 260 [0/1000 (0%)]\tLosses F.softmax: 0.000200 log_softmax: 0.000355\n",
            "Train Epoch: 260 [200/1000 (20%)]\tLosses F.softmax: 0.018245 log_softmax: 0.017317\n",
            "Train Epoch: 260 [400/1000 (40%)]\tLosses F.softmax: 0.000011 log_softmax: 0.000003\n",
            "Train Epoch: 260 [600/1000 (60%)]\tLosses F.softmax: 0.010177 log_softmax: 0.018962\n",
            "Train Epoch: 260 [800/1000 (80%)]\tLosses F.softmax: 0.000104 log_softmax: 0.001868\n",
            "Train Epoch: 260 [1000/1000 (100%)]\tLosses F.softmax: 0.040481 log_softmax: 0.029773\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0808\tAccuracy: 8101.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1071\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 261 [0/1000 (0%)]\tLosses F.softmax: 0.006457 log_softmax: 0.012547\n",
            "Train Epoch: 261 [200/1000 (20%)]\tLosses F.softmax: 0.000018 log_softmax: 0.000002\n",
            "Train Epoch: 261 [400/1000 (40%)]\tLosses F.softmax: 0.002448 log_softmax: 0.001073\n",
            "Train Epoch: 261 [600/1000 (60%)]\tLosses F.softmax: 0.001105 log_softmax: 0.001176\n",
            "Train Epoch: 261 [800/1000 (80%)]\tLosses F.softmax: 0.000177 log_softmax: 0.000029\n",
            "Train Epoch: 261 [1000/1000 (100%)]\tLosses F.softmax: 0.000725 log_softmax: 0.001006\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0809\tAccuracy: 8108.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1075\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 262 [0/1000 (0%)]\tLosses F.softmax: 0.020438 log_softmax: 0.017225\n",
            "Train Epoch: 262 [200/1000 (20%)]\tLosses F.softmax: 0.000089 log_softmax: 0.001221\n",
            "Train Epoch: 262 [400/1000 (40%)]\tLosses F.softmax: 0.005477 log_softmax: 0.005136\n",
            "Train Epoch: 262 [600/1000 (60%)]\tLosses F.softmax: 0.000017 log_softmax: 0.000001\n",
            "Train Epoch: 262 [800/1000 (80%)]\tLosses F.softmax: 0.000722 log_softmax: 0.000279\n",
            "Train Epoch: 262 [1000/1000 (100%)]\tLosses F.softmax: 0.002533 log_softmax: 0.007171\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0823\tAccuracy: 8091.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1084\tAccuracy: 8057.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 263 [0/1000 (0%)]\tLosses F.softmax: 0.000159 log_softmax: 0.000203\n",
            "Train Epoch: 263 [200/1000 (20%)]\tLosses F.softmax: 0.000009 log_softmax: 0.000006\n",
            "Train Epoch: 263 [400/1000 (40%)]\tLosses F.softmax: 0.035310 log_softmax: 0.035499\n",
            "Train Epoch: 263 [600/1000 (60%)]\tLosses F.softmax: 0.013931 log_softmax: 0.015775\n",
            "Train Epoch: 263 [800/1000 (80%)]\tLosses F.softmax: 0.031013 log_softmax: 0.021880\n",
            "Train Epoch: 263 [1000/1000 (100%)]\tLosses F.softmax: 0.000167 log_softmax: 0.006469\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0827\tAccuracy: 8091.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1089\tAccuracy: 8057.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 264 [0/1000 (0%)]\tLosses F.softmax: 0.006014 log_softmax: 0.001585\n",
            "Train Epoch: 264 [200/1000 (20%)]\tLosses F.softmax: 0.002366 log_softmax: 0.002575\n",
            "Train Epoch: 264 [400/1000 (40%)]\tLosses F.softmax: 0.001065 log_softmax: 0.000059\n",
            "Train Epoch: 264 [600/1000 (60%)]\tLosses F.softmax: 0.000060 log_softmax: 0.000050\n",
            "Train Epoch: 264 [800/1000 (80%)]\tLosses F.softmax: 0.001063 log_softmax: 0.002055\n",
            "Train Epoch: 264 [1000/1000 (100%)]\tLosses F.softmax: 0.000060 log_softmax: 0.000009\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0834\tAccuracy: 8091.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1097\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 265 [0/1000 (0%)]\tLosses F.softmax: 0.001299 log_softmax: 0.001701\n",
            "Train Epoch: 265 [200/1000 (20%)]\tLosses F.softmax: 0.000180 log_softmax: 0.000003\n",
            "Train Epoch: 265 [400/1000 (40%)]\tLosses F.softmax: 0.004062 log_softmax: 0.004238\n",
            "Train Epoch: 265 [600/1000 (60%)]\tLosses F.softmax: 0.000040 log_softmax: 0.000007\n",
            "Train Epoch: 265 [800/1000 (80%)]\tLosses F.softmax: 0.012379 log_softmax: 0.016625\n",
            "Train Epoch: 265 [1000/1000 (100%)]\tLosses F.softmax: 0.000005 log_softmax: 0.000000\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0831\tAccuracy: 8096.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1099\tAccuracy: 8052.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 266 [0/1000 (0%)]\tLosses F.softmax: 0.003208 log_softmax: 0.001659\n",
            "Train Epoch: 266 [200/1000 (20%)]\tLosses F.softmax: 0.026442 log_softmax: 0.036267\n",
            "Train Epoch: 266 [400/1000 (40%)]\tLosses F.softmax: 0.000380 log_softmax: 0.000300\n",
            "Train Epoch: 266 [600/1000 (60%)]\tLosses F.softmax: 0.000428 log_softmax: 0.000375\n",
            "Train Epoch: 266 [800/1000 (80%)]\tLosses F.softmax: 0.000158 log_softmax: 0.000409\n",
            "Train Epoch: 266 [1000/1000 (100%)]\tLosses F.softmax: 0.000170 log_softmax: 0.000051\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0838\tAccuracy: 8094.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1104\tAccuracy: 8052.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 267 [0/1000 (0%)]\tLosses F.softmax: 0.011017 log_softmax: 0.015138\n",
            "Train Epoch: 267 [200/1000 (20%)]\tLosses F.softmax: 0.001462 log_softmax: 0.000382\n",
            "Train Epoch: 267 [400/1000 (40%)]\tLosses F.softmax: 0.000619 log_softmax: 0.000561\n",
            "Train Epoch: 267 [600/1000 (60%)]\tLosses F.softmax: 0.002051 log_softmax: 0.007170\n",
            "Train Epoch: 267 [800/1000 (80%)]\tLosses F.softmax: 0.000202 log_softmax: 0.000032\n",
            "Train Epoch: 267 [1000/1000 (100%)]\tLosses F.softmax: 0.023324 log_softmax: 0.015801\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0837\tAccuracy: 8098.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1104\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 268 [0/1000 (0%)]\tLosses F.softmax: 0.002452 log_softmax: 0.001641\n",
            "Train Epoch: 268 [200/1000 (20%)]\tLosses F.softmax: 0.000038 log_softmax: 0.000499\n",
            "Train Epoch: 268 [400/1000 (40%)]\tLosses F.softmax: 0.019767 log_softmax: 0.019665\n",
            "Train Epoch: 268 [600/1000 (60%)]\tLosses F.softmax: 0.001435 log_softmax: 0.000292\n",
            "Train Epoch: 268 [800/1000 (80%)]\tLosses F.softmax: 0.000096 log_softmax: 0.000062\n",
            "Train Epoch: 268 [1000/1000 (100%)]\tLosses F.softmax: 0.000013 log_softmax: 0.000005\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0846\tAccuracy: 8094.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1112\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 269 [0/1000 (0%)]\tLosses F.softmax: 0.015938 log_softmax: 0.008109\n",
            "Train Epoch: 269 [200/1000 (20%)]\tLosses F.softmax: 0.002507 log_softmax: 0.002516\n",
            "Train Epoch: 269 [400/1000 (40%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000001\n",
            "Train Epoch: 269 [600/1000 (60%)]\tLosses F.softmax: 0.008619 log_softmax: 0.007747\n",
            "Train Epoch: 269 [800/1000 (80%)]\tLosses F.softmax: 0.005626 log_softmax: 0.004640\n",
            "Train Epoch: 269 [1000/1000 (100%)]\tLosses F.softmax: 0.000003 log_softmax: 0.000033\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0850\tAccuracy: 8098.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1117\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 270 [0/1000 (0%)]\tLosses F.softmax: 0.000034 log_softmax: 0.000025\n",
            "Train Epoch: 270 [200/1000 (20%)]\tLosses F.softmax: 0.000028 log_softmax: 0.000226\n",
            "Train Epoch: 270 [400/1000 (40%)]\tLosses F.softmax: 0.000414 log_softmax: 0.000002\n",
            "Train Epoch: 270 [600/1000 (60%)]\tLosses F.softmax: 0.002275 log_softmax: 0.003620\n",
            "Train Epoch: 270 [800/1000 (80%)]\tLosses F.softmax: 0.000600 log_softmax: 0.000750\n",
            "Train Epoch: 270 [1000/1000 (100%)]\tLosses F.softmax: 0.000002 log_softmax: 0.000007\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0858\tAccuracy: 8092.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1126\tAccuracy: 8052.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 271 [0/1000 (0%)]\tLosses F.softmax: 0.000325 log_softmax: 0.000105\n",
            "Train Epoch: 271 [200/1000 (20%)]\tLosses F.softmax: 0.000344 log_softmax: 0.000455\n",
            "Train Epoch: 271 [400/1000 (40%)]\tLosses F.softmax: 0.007352 log_softmax: 0.005277\n",
            "Train Epoch: 271 [600/1000 (60%)]\tLosses F.softmax: 0.009106 log_softmax: 0.011375\n",
            "Train Epoch: 271 [800/1000 (80%)]\tLosses F.softmax: 0.006170 log_softmax: 0.001582\n",
            "Train Epoch: 271 [1000/1000 (100%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000001\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0861\tAccuracy: 8095.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1130\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 272 [0/1000 (0%)]\tLosses F.softmax: 0.001311 log_softmax: 0.000537\n",
            "Train Epoch: 272 [200/1000 (20%)]\tLosses F.softmax: 0.000017 log_softmax: 0.000122\n",
            "Train Epoch: 272 [400/1000 (40%)]\tLosses F.softmax: 0.002185 log_softmax: 0.000275\n",
            "Train Epoch: 272 [600/1000 (60%)]\tLosses F.softmax: 0.003985 log_softmax: 0.005764\n",
            "Train Epoch: 272 [800/1000 (80%)]\tLosses F.softmax: 0.000022 log_softmax: 0.000056\n",
            "Train Epoch: 272 [1000/1000 (100%)]\tLosses F.softmax: 0.013310 log_softmax: 0.003460\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0867\tAccuracy: 8093.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1135\tAccuracy: 8052.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 273 [0/1000 (0%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000017\n",
            "Train Epoch: 273 [200/1000 (20%)]\tLosses F.softmax: 0.005891 log_softmax: 0.005180\n",
            "Train Epoch: 273 [400/1000 (40%)]\tLosses F.softmax: 0.000016 log_softmax: 0.000004\n",
            "Train Epoch: 273 [600/1000 (60%)]\tLosses F.softmax: 0.002987 log_softmax: 0.003752\n",
            "Train Epoch: 273 [800/1000 (80%)]\tLosses F.softmax: 0.030398 log_softmax: 0.038983\n",
            "Train Epoch: 273 [1000/1000 (100%)]\tLosses F.softmax: 0.007097 log_softmax: 0.013364\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0870\tAccuracy: 8097.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1140\tAccuracy: 8058.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 274 [0/1000 (0%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000000\n",
            "Train Epoch: 274 [200/1000 (20%)]\tLosses F.softmax: 0.005804 log_softmax: 0.011504\n",
            "Train Epoch: 274 [400/1000 (40%)]\tLosses F.softmax: 0.000044 log_softmax: 0.000110\n",
            "Train Epoch: 274 [600/1000 (60%)]\tLosses F.softmax: 0.010839 log_softmax: 0.006353\n",
            "Train Epoch: 274 [800/1000 (80%)]\tLosses F.softmax: 0.000191 log_softmax: 0.000059\n",
            "Train Epoch: 274 [1000/1000 (100%)]\tLosses F.softmax: 0.000388 log_softmax: 0.000100\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0875\tAccuracy: 8102.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1144\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 275 [0/1000 (0%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000000\n",
            "Train Epoch: 275 [200/1000 (20%)]\tLosses F.softmax: 0.013485 log_softmax: 0.009536\n",
            "Train Epoch: 275 [400/1000 (40%)]\tLosses F.softmax: 0.000163 log_softmax: 0.000054\n",
            "Train Epoch: 275 [600/1000 (60%)]\tLosses F.softmax: 0.000011 log_softmax: 0.000010\n",
            "Train Epoch: 275 [800/1000 (80%)]\tLosses F.softmax: 0.000331 log_softmax: 0.001035\n",
            "Train Epoch: 275 [1000/1000 (100%)]\tLosses F.softmax: 0.023467 log_softmax: 0.021159\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0879\tAccuracy: 8096.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1150\tAccuracy: 8052.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 276 [0/1000 (0%)]\tLosses F.softmax: 0.007603 log_softmax: 0.008311\n",
            "Train Epoch: 276 [200/1000 (20%)]\tLosses F.softmax: 0.000975 log_softmax: 0.000261\n",
            "Train Epoch: 276 [400/1000 (40%)]\tLosses F.softmax: 0.023425 log_softmax: 0.018607\n",
            "Train Epoch: 276 [600/1000 (60%)]\tLosses F.softmax: 0.000151 log_softmax: 0.001057\n",
            "Train Epoch: 276 [800/1000 (80%)]\tLosses F.softmax: 0.000578 log_softmax: 0.001077\n",
            "Train Epoch: 276 [1000/1000 (100%)]\tLosses F.softmax: 0.008319 log_softmax: 0.007717\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0883\tAccuracy: 8095.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1156\tAccuracy: 8052.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 277 [0/1000 (0%)]\tLosses F.softmax: 0.016298 log_softmax: 0.010054\n",
            "Train Epoch: 277 [200/1000 (20%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000002\n",
            "Train Epoch: 277 [400/1000 (40%)]\tLosses F.softmax: 0.000856 log_softmax: 0.000173\n",
            "Train Epoch: 277 [600/1000 (60%)]\tLosses F.softmax: 0.001418 log_softmax: 0.002892\n",
            "Train Epoch: 277 [800/1000 (80%)]\tLosses F.softmax: 0.012583 log_softmax: 0.010195\n",
            "Train Epoch: 277 [1000/1000 (100%)]\tLosses F.softmax: 0.003639 log_softmax: 0.002452\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0889\tAccuracy: 8096.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1158\tAccuracy: 8059.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 278 [0/1000 (0%)]\tLosses F.softmax: 0.000088 log_softmax: 0.000071\n",
            "Train Epoch: 278 [200/1000 (20%)]\tLosses F.softmax: 0.005644 log_softmax: 0.012193\n",
            "Train Epoch: 278 [400/1000 (40%)]\tLosses F.softmax: 0.000002 log_softmax: 0.000001\n",
            "Train Epoch: 278 [600/1000 (60%)]\tLosses F.softmax: 0.015943 log_softmax: 0.009977\n",
            "Train Epoch: 278 [800/1000 (80%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000001\n",
            "Train Epoch: 278 [1000/1000 (100%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000003\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0890\tAccuracy: 8104.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1161\tAccuracy: 8058.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 279 [0/1000 (0%)]\tLosses F.softmax: 0.000002 log_softmax: 0.000000\n",
            "Train Epoch: 279 [200/1000 (20%)]\tLosses F.softmax: 0.018875 log_softmax: 0.006165\n",
            "Train Epoch: 279 [400/1000 (40%)]\tLosses F.softmax: 0.002376 log_softmax: 0.002066\n",
            "Train Epoch: 279 [600/1000 (60%)]\tLosses F.softmax: 0.000445 log_softmax: 0.000342\n",
            "Train Epoch: 279 [800/1000 (80%)]\tLosses F.softmax: 0.001163 log_softmax: 0.000162\n",
            "Train Epoch: 279 [1000/1000 (100%)]\tLosses F.softmax: 0.001404 log_softmax: 0.000353\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0902\tAccuracy: 8093.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1171\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 280 [0/1000 (0%)]\tLosses F.softmax: 0.000060 log_softmax: 0.000014\n",
            "Train Epoch: 280 [200/1000 (20%)]\tLosses F.softmax: 0.005988 log_softmax: 0.006806\n",
            "Train Epoch: 280 [400/1000 (40%)]\tLosses F.softmax: 0.002435 log_softmax: 0.000483\n",
            "Train Epoch: 280 [600/1000 (60%)]\tLosses F.softmax: 0.000273 log_softmax: 0.000313\n",
            "Train Epoch: 280 [800/1000 (80%)]\tLosses F.softmax: 0.004074 log_softmax: 0.003866\n",
            "Train Epoch: 280 [1000/1000 (100%)]\tLosses F.softmax: 0.000071 log_softmax: 0.001062\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0907\tAccuracy: 8095.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1179\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 281 [0/1000 (0%)]\tLosses F.softmax: 0.004187 log_softmax: 0.000901\n",
            "Train Epoch: 281 [200/1000 (20%)]\tLosses F.softmax: 0.000518 log_softmax: 0.001269\n",
            "Train Epoch: 281 [400/1000 (40%)]\tLosses F.softmax: 0.002761 log_softmax: 0.000164\n",
            "Train Epoch: 281 [600/1000 (60%)]\tLosses F.softmax: 0.000584 log_softmax: 0.001071\n",
            "Train Epoch: 281 [800/1000 (80%)]\tLosses F.softmax: 0.000022 log_softmax: 0.000019\n",
            "Train Epoch: 281 [1000/1000 (100%)]\tLosses F.softmax: 0.000167 log_softmax: 0.000004\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0916\tAccuracy: 8092.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1185\tAccuracy: 8050.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 282 [0/1000 (0%)]\tLosses F.softmax: 0.000903 log_softmax: 0.008782\n",
            "Train Epoch: 282 [200/1000 (20%)]\tLosses F.softmax: 0.010957 log_softmax: 0.006141\n",
            "Train Epoch: 282 [400/1000 (40%)]\tLosses F.softmax: 0.002879 log_softmax: 0.003465\n",
            "Train Epoch: 282 [600/1000 (60%)]\tLosses F.softmax: 0.000162 log_softmax: 0.000038\n",
            "Train Epoch: 282 [800/1000 (80%)]\tLosses F.softmax: 0.000002 log_softmax: 0.000071\n",
            "Train Epoch: 282 [1000/1000 (100%)]\tLosses F.softmax: 0.000284 log_softmax: 0.000856\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0914\tAccuracy: 8095.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1186\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 283 [0/1000 (0%)]\tLosses F.softmax: 0.005320 log_softmax: 0.013025\n",
            "Train Epoch: 283 [200/1000 (20%)]\tLosses F.softmax: 0.000009 log_softmax: 0.000009\n",
            "Train Epoch: 283 [400/1000 (40%)]\tLosses F.softmax: 0.011534 log_softmax: 0.012671\n",
            "Train Epoch: 283 [600/1000 (60%)]\tLosses F.softmax: 0.000004 log_softmax: 0.000053\n",
            "Train Epoch: 283 [800/1000 (80%)]\tLosses F.softmax: 0.003379 log_softmax: 0.011434\n",
            "Train Epoch: 283 [1000/1000 (100%)]\tLosses F.softmax: 0.000103 log_softmax: 0.000005\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0916\tAccuracy: 8108.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1187\tAccuracy: 8058.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 284 [0/1000 (0%)]\tLosses F.softmax: 0.000952 log_softmax: 0.000058\n",
            "Train Epoch: 284 [200/1000 (20%)]\tLosses F.softmax: 0.000448 log_softmax: 0.000164\n",
            "Train Epoch: 284 [400/1000 (40%)]\tLosses F.softmax: 0.024558 log_softmax: 0.030278\n",
            "Train Epoch: 284 [600/1000 (60%)]\tLosses F.softmax: 0.000002 log_softmax: 0.000002\n",
            "Train Epoch: 284 [800/1000 (80%)]\tLosses F.softmax: 0.000040 log_softmax: 0.001851\n",
            "Train Epoch: 284 [1000/1000 (100%)]\tLosses F.softmax: 0.005845 log_softmax: 0.006555\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0918\tAccuracy: 8106.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1190\tAccuracy: 8058.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 285 [0/1000 (0%)]\tLosses F.softmax: 0.000049 log_softmax: 0.000078\n",
            "Train Epoch: 285 [200/1000 (20%)]\tLosses F.softmax: 0.000996 log_softmax: 0.000229\n",
            "Train Epoch: 285 [400/1000 (40%)]\tLosses F.softmax: 0.001182 log_softmax: 0.000392\n",
            "Train Epoch: 285 [600/1000 (60%)]\tLosses F.softmax: 0.005159 log_softmax: 0.003367\n",
            "Train Epoch: 285 [800/1000 (80%)]\tLosses F.softmax: 0.002617 log_softmax: 0.000164\n",
            "Train Epoch: 285 [1000/1000 (100%)]\tLosses F.softmax: 0.001226 log_softmax: 0.000484\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0926\tAccuracy: 8094.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1200\tAccuracy: 8057.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 286 [0/1000 (0%)]\tLosses F.softmax: 0.007483 log_softmax: 0.003427\n",
            "Train Epoch: 286 [200/1000 (20%)]\tLosses F.softmax: 0.000189 log_softmax: 0.000747\n",
            "Train Epoch: 286 [400/1000 (40%)]\tLosses F.softmax: 0.000066 log_softmax: 0.000008\n",
            "Train Epoch: 286 [600/1000 (60%)]\tLosses F.softmax: 0.025198 log_softmax: 0.022740\n",
            "Train Epoch: 286 [800/1000 (80%)]\tLosses F.softmax: 0.000022 log_softmax: 0.000366\n",
            "Train Epoch: 286 [1000/1000 (100%)]\tLosses F.softmax: 0.000564 log_softmax: 0.001521\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0932\tAccuracy: 8095.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1207\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 287 [0/1000 (0%)]\tLosses F.softmax: 0.000952 log_softmax: 0.000058\n",
            "Train Epoch: 287 [200/1000 (20%)]\tLosses F.softmax: 0.004512 log_softmax: 0.012128\n",
            "Train Epoch: 287 [400/1000 (40%)]\tLosses F.softmax: 0.040804 log_softmax: 0.049152\n",
            "Train Epoch: 287 [600/1000 (60%)]\tLosses F.softmax: 0.000021 log_softmax: 0.000001\n",
            "Train Epoch: 287 [800/1000 (80%)]\tLosses F.softmax: 0.000010 log_softmax: 0.000001\n",
            "Train Epoch: 287 [1000/1000 (100%)]\tLosses F.softmax: 0.000843 log_softmax: 0.001491\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0936\tAccuracy: 8094.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1212\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 288 [0/1000 (0%)]\tLosses F.softmax: 0.000676 log_softmax: 0.003668\n",
            "Train Epoch: 288 [200/1000 (20%)]\tLosses F.softmax: 0.034580 log_softmax: 0.031757\n",
            "Train Epoch: 288 [400/1000 (40%)]\tLosses F.softmax: 0.014284 log_softmax: 0.012511\n",
            "Train Epoch: 288 [600/1000 (60%)]\tLosses F.softmax: 0.003171 log_softmax: 0.002752\n",
            "Train Epoch: 288 [800/1000 (80%)]\tLosses F.softmax: 0.001692 log_softmax: 0.012557\n",
            "Train Epoch: 288 [1000/1000 (100%)]\tLosses F.softmax: 0.000165 log_softmax: 0.000109\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0937\tAccuracy: 8100.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1211\tAccuracy: 8057.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 289 [0/1000 (0%)]\tLosses F.softmax: 0.000147 log_softmax: 0.000234\n",
            "Train Epoch: 289 [200/1000 (20%)]\tLosses F.softmax: 0.000065 log_softmax: 0.000013\n",
            "Train Epoch: 289 [400/1000 (40%)]\tLosses F.softmax: 0.000151 log_softmax: 0.000021\n",
            "Train Epoch: 289 [600/1000 (60%)]\tLosses F.softmax: 0.001170 log_softmax: 0.001120\n",
            "Train Epoch: 289 [800/1000 (80%)]\tLosses F.softmax: 0.026535 log_softmax: 0.018475\n",
            "Train Epoch: 289 [1000/1000 (100%)]\tLosses F.softmax: 0.003342 log_softmax: 0.002361\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0946\tAccuracy: 8104.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1218\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 290 [0/1000 (0%)]\tLosses F.softmax: 0.001627 log_softmax: 0.001603\n",
            "Train Epoch: 290 [200/1000 (20%)]\tLosses F.softmax: 0.000269 log_softmax: 0.000023\n",
            "Train Epoch: 290 [400/1000 (40%)]\tLosses F.softmax: 0.001720 log_softmax: 0.000481\n",
            "Train Epoch: 290 [600/1000 (60%)]\tLosses F.softmax: 0.000054 log_softmax: 0.000118\n",
            "Train Epoch: 290 [800/1000 (80%)]\tLosses F.softmax: 0.005603 log_softmax: 0.001008\n",
            "Train Epoch: 290 [1000/1000 (100%)]\tLosses F.softmax: 0.012104 log_softmax: 0.008020\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0948\tAccuracy: 8101.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1223\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 291 [0/1000 (0%)]\tLosses F.softmax: 0.000099 log_softmax: 0.000094\n",
            "Train Epoch: 291 [200/1000 (20%)]\tLosses F.softmax: 0.010214 log_softmax: 0.028003\n",
            "Train Epoch: 291 [400/1000 (40%)]\tLosses F.softmax: 0.000005 log_softmax: 0.000009\n",
            "Train Epoch: 291 [600/1000 (60%)]\tLosses F.softmax: 0.000159 log_softmax: 0.000035\n",
            "Train Epoch: 291 [800/1000 (80%)]\tLosses F.softmax: 0.000144 log_softmax: 0.005879\n",
            "Train Epoch: 291 [1000/1000 (100%)]\tLosses F.softmax: 0.000988 log_softmax: 0.001838\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0948\tAccuracy: 8108.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1224\tAccuracy: 8059.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 292 [0/1000 (0%)]\tLosses F.softmax: 0.000022 log_softmax: 0.000006\n",
            "Train Epoch: 292 [200/1000 (20%)]\tLosses F.softmax: 0.000025 log_softmax: 0.001087\n",
            "Train Epoch: 292 [400/1000 (40%)]\tLosses F.softmax: 0.000729 log_softmax: 0.001074\n",
            "Train Epoch: 292 [600/1000 (60%)]\tLosses F.softmax: 0.000054 log_softmax: 0.000004\n",
            "Train Epoch: 292 [800/1000 (80%)]\tLosses F.softmax: 0.001913 log_softmax: 0.006034\n",
            "Train Epoch: 292 [1000/1000 (100%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000000\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0963\tAccuracy: 8094.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1241\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 293 [0/1000 (0%)]\tLosses F.softmax: 0.008869 log_softmax: 0.017126\n",
            "Train Epoch: 293 [200/1000 (20%)]\tLosses F.softmax: 0.000004 log_softmax: 0.000006\n",
            "Train Epoch: 293 [400/1000 (40%)]\tLosses F.softmax: 0.022864 log_softmax: 0.021687\n",
            "Train Epoch: 293 [600/1000 (60%)]\tLosses F.softmax: 0.025188 log_softmax: 0.026044\n",
            "Train Epoch: 293 [800/1000 (80%)]\tLosses F.softmax: 0.008612 log_softmax: 0.012434\n",
            "Train Epoch: 293 [1000/1000 (100%)]\tLosses F.softmax: 0.020170 log_softmax: 0.015049\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0966\tAccuracy: 8095.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1239\tAccuracy: 8059.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 294 [0/1000 (0%)]\tLosses F.softmax: 0.000119 log_softmax: 0.000645\n",
            "Train Epoch: 294 [200/1000 (20%)]\tLosses F.softmax: 0.012632 log_softmax: 0.022289\n",
            "Train Epoch: 294 [400/1000 (40%)]\tLosses F.softmax: 0.007670 log_softmax: 0.004463\n",
            "Train Epoch: 294 [600/1000 (60%)]\tLosses F.softmax: 0.032228 log_softmax: 0.013697\n",
            "Train Epoch: 294 [800/1000 (80%)]\tLosses F.softmax: 0.026400 log_softmax: 0.015197\n",
            "Train Epoch: 294 [1000/1000 (100%)]\tLosses F.softmax: 0.025693 log_softmax: 0.018132\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0965\tAccuracy: 8101.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1243\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 295 [0/1000 (0%)]\tLosses F.softmax: 0.001390 log_softmax: 0.001712\n",
            "Train Epoch: 295 [200/1000 (20%)]\tLosses F.softmax: 0.004826 log_softmax: 0.001247\n",
            "Train Epoch: 295 [400/1000 (40%)]\tLosses F.softmax: 0.001316 log_softmax: 0.003732\n",
            "Train Epoch: 295 [600/1000 (60%)]\tLosses F.softmax: 0.000011 log_softmax: 0.000000\n",
            "Train Epoch: 295 [800/1000 (80%)]\tLosses F.softmax: 0.001841 log_softmax: 0.000177\n",
            "Train Epoch: 295 [1000/1000 (100%)]\tLosses F.softmax: 0.007282 log_softmax: 0.006522\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0970\tAccuracy: 8103.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1246\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 296 [0/1000 (0%)]\tLosses F.softmax: 0.000924 log_softmax: 0.008759\n",
            "Train Epoch: 296 [200/1000 (20%)]\tLosses F.softmax: 0.000237 log_softmax: 0.001143\n",
            "Train Epoch: 296 [400/1000 (40%)]\tLosses F.softmax: 0.001028 log_softmax: 0.000383\n",
            "Train Epoch: 296 [600/1000 (60%)]\tLosses F.softmax: 0.002877 log_softmax: 0.000015\n",
            "Train Epoch: 296 [800/1000 (80%)]\tLosses F.softmax: 0.000638 log_softmax: 0.000227\n",
            "Train Epoch: 296 [1000/1000 (100%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000000\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0978\tAccuracy: 8095.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1254\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 297 [0/1000 (0%)]\tLosses F.softmax: 0.000081 log_softmax: 0.000050\n",
            "Train Epoch: 297 [200/1000 (20%)]\tLosses F.softmax: 0.000589 log_softmax: 0.000562\n",
            "Train Epoch: 297 [400/1000 (40%)]\tLosses F.softmax: 0.002191 log_softmax: 0.008233\n",
            "Train Epoch: 297 [600/1000 (60%)]\tLosses F.softmax: 0.041361 log_softmax: 0.033122\n",
            "Train Epoch: 297 [800/1000 (80%)]\tLosses F.softmax: 0.000008 log_softmax: 0.000006\n",
            "Train Epoch: 297 [1000/1000 (100%)]\tLosses F.softmax: 0.005267 log_softmax: 0.001335\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0981\tAccuracy: 8099.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1258\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 298 [0/1000 (0%)]\tLosses F.softmax: 0.001607 log_softmax: 0.000085\n",
            "Train Epoch: 298 [200/1000 (20%)]\tLosses F.softmax: 0.017100 log_softmax: 0.011541\n",
            "Train Epoch: 298 [400/1000 (40%)]\tLosses F.softmax: 0.001169 log_softmax: 0.000422\n",
            "Train Epoch: 298 [600/1000 (60%)]\tLosses F.softmax: 0.017574 log_softmax: 0.028035\n",
            "Train Epoch: 298 [800/1000 (80%)]\tLosses F.softmax: 0.001594 log_softmax: 0.005130\n",
            "Train Epoch: 298 [1000/1000 (100%)]\tLosses F.softmax: 0.002877 log_softmax: 0.000015\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0987\tAccuracy: 8098.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1267\tAccuracy: 8057.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 299 [0/1000 (0%)]\tLosses F.softmax: 0.001771 log_softmax: 0.025481\n",
            "Train Epoch: 299 [200/1000 (20%)]\tLosses F.softmax: 0.000025 log_softmax: 0.000005\n",
            "Train Epoch: 299 [400/1000 (40%)]\tLosses F.softmax: 0.000007 log_softmax: 0.004332\n",
            "Train Epoch: 299 [600/1000 (60%)]\tLosses F.softmax: 0.002976 log_softmax: 0.009262\n",
            "Train Epoch: 299 [800/1000 (80%)]\tLosses F.softmax: 0.000049 log_softmax: 0.000003\n",
            "Train Epoch: 299 [1000/1000 (100%)]\tLosses F.softmax: 0.000018 log_softmax: 0.000006\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0990\tAccuracy: 8097.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1269\tAccuracy: 8059.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 300 [0/1000 (0%)]\tLosses F.softmax: 0.000143 log_softmax: 0.000062\n",
            "Train Epoch: 300 [200/1000 (20%)]\tLosses F.softmax: 0.000786 log_softmax: 0.003987\n",
            "Train Epoch: 300 [400/1000 (40%)]\tLosses F.softmax: 0.014423 log_softmax: 0.011341\n",
            "Train Epoch: 300 [600/1000 (60%)]\tLosses F.softmax: 0.004434 log_softmax: 0.004472\n",
            "Train Epoch: 300 [800/1000 (80%)]\tLosses F.softmax: 0.000023 log_softmax: 0.000254\n",
            "Train Epoch: 300 [1000/1000 (100%)]\tLosses F.softmax: 0.001718 log_softmax: 0.000707\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0995\tAccuracy: 8104.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1273\tAccuracy: 8059.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 301 [0/1000 (0%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000001\n",
            "Train Epoch: 301 [200/1000 (20%)]\tLosses F.softmax: 0.000003 log_softmax: 0.000001\n",
            "Train Epoch: 301 [400/1000 (40%)]\tLosses F.softmax: 0.000004 log_softmax: 0.000003\n",
            "Train Epoch: 301 [600/1000 (60%)]\tLosses F.softmax: 0.016388 log_softmax: 0.020349\n",
            "Train Epoch: 301 [800/1000 (80%)]\tLosses F.softmax: 0.000014 log_softmax: 0.000002\n",
            "Train Epoch: 301 [1000/1000 (100%)]\tLosses F.softmax: 0.000023 log_softmax: 0.000259\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0998\tAccuracy: 8100.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1278\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 302 [0/1000 (0%)]\tLosses F.softmax: 0.000153 log_softmax: 0.000046\n",
            "Train Epoch: 302 [200/1000 (20%)]\tLosses F.softmax: 0.002698 log_softmax: 0.000011\n",
            "Train Epoch: 302 [400/1000 (40%)]\tLosses F.softmax: 0.000011 log_softmax: 0.000001\n",
            "Train Epoch: 302 [600/1000 (60%)]\tLosses F.softmax: 0.000533 log_softmax: 0.001398\n",
            "Train Epoch: 302 [800/1000 (80%)]\tLosses F.softmax: 0.001549 log_softmax: 0.002393\n",
            "Train Epoch: 302 [1000/1000 (100%)]\tLosses F.softmax: 0.000489 log_softmax: 0.001825\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1007\tAccuracy: 8096.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1284\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 303 [0/1000 (0%)]\tLosses F.softmax: 0.000017 log_softmax: 0.000012\n",
            "Train Epoch: 303 [200/1000 (20%)]\tLosses F.softmax: 0.000958 log_softmax: 0.000459\n",
            "Train Epoch: 303 [400/1000 (40%)]\tLosses F.softmax: 0.000117 log_softmax: 0.000005\n",
            "Train Epoch: 303 [600/1000 (60%)]\tLosses F.softmax: 0.001007 log_softmax: 0.000907\n",
            "Train Epoch: 303 [800/1000 (80%)]\tLosses F.softmax: 0.001304 log_softmax: 0.001293\n",
            "Train Epoch: 303 [1000/1000 (100%)]\tLosses F.softmax: 0.000097 log_softmax: 0.001198\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1008\tAccuracy: 8100.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1291\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 304 [0/1000 (0%)]\tLosses F.softmax: 0.000008 log_softmax: 0.000000\n",
            "Train Epoch: 304 [200/1000 (20%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000003\n",
            "Train Epoch: 304 [400/1000 (40%)]\tLosses F.softmax: 0.000007 log_softmax: 0.000005\n",
            "Train Epoch: 304 [600/1000 (60%)]\tLosses F.softmax: 0.007954 log_softmax: 0.004360\n",
            "Train Epoch: 304 [800/1000 (80%)]\tLosses F.softmax: 0.000148 log_softmax: 0.000023\n",
            "Train Epoch: 304 [1000/1000 (100%)]\tLosses F.softmax: 0.000972 log_softmax: 0.000420\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1011\tAccuracy: 8098.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1295\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 305 [0/1000 (0%)]\tLosses F.softmax: 0.001529 log_softmax: 0.000055\n",
            "Train Epoch: 305 [200/1000 (20%)]\tLosses F.softmax: 0.000692 log_softmax: 0.000168\n",
            "Train Epoch: 305 [400/1000 (40%)]\tLosses F.softmax: 0.000058 log_softmax: 0.000005\n",
            "Train Epoch: 305 [600/1000 (60%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000021\n",
            "Train Epoch: 305 [800/1000 (80%)]\tLosses F.softmax: 0.003527 log_softmax: 0.000704\n",
            "Train Epoch: 305 [1000/1000 (100%)]\tLosses F.softmax: 0.000070 log_softmax: 0.000168\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1015\tAccuracy: 8104.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1297\tAccuracy: 8058.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 306 [0/1000 (0%)]\tLosses F.softmax: 0.000599 log_softmax: 0.004336\n",
            "Train Epoch: 306 [200/1000 (20%)]\tLosses F.softmax: 0.000025 log_softmax: 0.000005\n",
            "Train Epoch: 306 [400/1000 (40%)]\tLosses F.softmax: 0.000604 log_softmax: 0.003451\n",
            "Train Epoch: 306 [600/1000 (60%)]\tLosses F.softmax: 0.023256 log_softmax: 0.015900\n",
            "Train Epoch: 306 [800/1000 (80%)]\tLosses F.softmax: 0.000153 log_softmax: 0.000026\n",
            "Train Epoch: 306 [1000/1000 (100%)]\tLosses F.softmax: 0.001145 log_softmax: 0.000398\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1022\tAccuracy: 8096.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1305\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 307 [0/1000 (0%)]\tLosses F.softmax: 0.000030 log_softmax: 0.000051\n",
            "Train Epoch: 307 [200/1000 (20%)]\tLosses F.softmax: 0.000454 log_softmax: 0.002391\n",
            "Train Epoch: 307 [400/1000 (40%)]\tLosses F.softmax: 0.007121 log_softmax: 0.024392\n",
            "Train Epoch: 307 [600/1000 (60%)]\tLosses F.softmax: 0.000236 log_softmax: 0.000201\n",
            "Train Epoch: 307 [800/1000 (80%)]\tLosses F.softmax: 0.000564 log_softmax: 0.000546\n",
            "Train Epoch: 307 [1000/1000 (100%)]\tLosses F.softmax: 0.000338 log_softmax: 0.000170\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1024\tAccuracy: 8097.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1309\tAccuracy: 8058.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 308 [0/1000 (0%)]\tLosses F.softmax: 0.004927 log_softmax: 0.004260\n",
            "Train Epoch: 308 [200/1000 (20%)]\tLosses F.softmax: 0.000006 log_softmax: 0.000015\n",
            "Train Epoch: 308 [400/1000 (40%)]\tLosses F.softmax: 0.000223 log_softmax: 0.000015\n",
            "Train Epoch: 308 [600/1000 (60%)]\tLosses F.softmax: 0.006558 log_softmax: 0.000944\n",
            "Train Epoch: 308 [800/1000 (80%)]\tLosses F.softmax: 0.001110 log_softmax: 0.000289\n",
            "Train Epoch: 308 [1000/1000 (100%)]\tLosses F.softmax: 0.004929 log_softmax: 0.009544\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1028\tAccuracy: 8100.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1312\tAccuracy: 8057.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 309 [0/1000 (0%)]\tLosses F.softmax: 0.011472 log_softmax: 0.011476\n",
            "Train Epoch: 309 [200/1000 (20%)]\tLosses F.softmax: 0.000317 log_softmax: 0.001445\n",
            "Train Epoch: 309 [400/1000 (40%)]\tLosses F.softmax: 0.000007 log_softmax: 0.000003\n",
            "Train Epoch: 309 [600/1000 (60%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000001\n",
            "Train Epoch: 309 [800/1000 (80%)]\tLosses F.softmax: 0.000031 log_softmax: 0.000011\n",
            "Train Epoch: 309 [1000/1000 (100%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000000\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1035\tAccuracy: 8099.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1317\tAccuracy: 8058.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 310 [0/1000 (0%)]\tLosses F.softmax: 0.000201 log_softmax: 0.000188\n",
            "Train Epoch: 310 [200/1000 (20%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000000\n",
            "Train Epoch: 310 [400/1000 (40%)]\tLosses F.softmax: 0.023308 log_softmax: 0.004488\n",
            "Train Epoch: 310 [600/1000 (60%)]\tLosses F.softmax: 0.001651 log_softmax: 0.000597\n",
            "Train Epoch: 310 [800/1000 (80%)]\tLosses F.softmax: 0.013755 log_softmax: 0.009248\n",
            "Train Epoch: 310 [1000/1000 (100%)]\tLosses F.softmax: 0.004027 log_softmax: 0.001622\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1041\tAccuracy: 8096.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1325\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 311 [0/1000 (0%)]\tLosses F.softmax: 0.003868 log_softmax: 0.005062\n",
            "Train Epoch: 311 [200/1000 (20%)]\tLosses F.softmax: 0.001021 log_softmax: 0.007834\n",
            "Train Epoch: 311 [400/1000 (40%)]\tLosses F.softmax: 0.000137 log_softmax: 0.000057\n",
            "Train Epoch: 311 [600/1000 (60%)]\tLosses F.softmax: 0.013164 log_softmax: 0.013180\n",
            "Train Epoch: 311 [800/1000 (80%)]\tLosses F.softmax: 0.005327 log_softmax: 0.000531\n",
            "Train Epoch: 311 [1000/1000 (100%)]\tLosses F.softmax: 0.000003 log_softmax: 0.000003\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1040\tAccuracy: 8107.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1322\tAccuracy: 8057.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 312 [0/1000 (0%)]\tLosses F.softmax: 0.000022 log_softmax: 0.001056\n",
            "Train Epoch: 312 [200/1000 (20%)]\tLosses F.softmax: 0.000052 log_softmax: 0.000023\n",
            "Train Epoch: 312 [400/1000 (40%)]\tLosses F.softmax: 0.000115 log_softmax: 0.000199\n",
            "Train Epoch: 312 [600/1000 (60%)]\tLosses F.softmax: 0.000035 log_softmax: 0.000009\n",
            "Train Epoch: 312 [800/1000 (80%)]\tLosses F.softmax: 0.000534 log_softmax: 0.000509\n",
            "Train Epoch: 312 [1000/1000 (100%)]\tLosses F.softmax: 0.012550 log_softmax: 0.008790\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1053\tAccuracy: 8097.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1334\tAccuracy: 8052.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 313 [0/1000 (0%)]\tLosses F.softmax: 0.008246 log_softmax: 0.010292\n",
            "Train Epoch: 313 [200/1000 (20%)]\tLosses F.softmax: 0.000203 log_softmax: 0.000435\n",
            "Train Epoch: 313 [400/1000 (40%)]\tLosses F.softmax: 0.005159 log_softmax: 0.000455\n",
            "Train Epoch: 313 [600/1000 (60%)]\tLosses F.softmax: 0.000066 log_softmax: 0.000252\n",
            "Train Epoch: 313 [800/1000 (80%)]\tLosses F.softmax: 0.025966 log_softmax: 0.035186\n",
            "Train Epoch: 313 [1000/1000 (100%)]\tLosses F.softmax: 0.022279 log_softmax: 0.013720\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1048\tAccuracy: 8107.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1330\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 314 [0/1000 (0%)]\tLosses F.softmax: 0.001455 log_softmax: 0.001783\n",
            "Train Epoch: 314 [200/1000 (20%)]\tLosses F.softmax: 0.000966 log_softmax: 0.001399\n",
            "Train Epoch: 314 [400/1000 (40%)]\tLosses F.softmax: 0.000167 log_softmax: 0.000047\n",
            "Train Epoch: 314 [600/1000 (60%)]\tLosses F.softmax: 0.007522 log_softmax: 0.003421\n",
            "Train Epoch: 314 [800/1000 (80%)]\tLosses F.softmax: 0.000647 log_softmax: 0.000006\n",
            "Train Epoch: 314 [1000/1000 (100%)]\tLosses F.softmax: 0.004807 log_softmax: 0.001492\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1052\tAccuracy: 8111.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1335\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 315 [0/1000 (0%)]\tLosses F.softmax: 0.014184 log_softmax: 0.010032\n",
            "Train Epoch: 315 [200/1000 (20%)]\tLosses F.softmax: 0.003353 log_softmax: 0.003488\n",
            "Train Epoch: 315 [400/1000 (40%)]\tLosses F.softmax: 0.007013 log_softmax: 0.006882\n",
            "Train Epoch: 315 [600/1000 (60%)]\tLosses F.softmax: 0.000244 log_softmax: 0.000817\n",
            "Train Epoch: 315 [800/1000 (80%)]\tLosses F.softmax: 0.002460 log_softmax: 0.000444\n",
            "Train Epoch: 315 [1000/1000 (100%)]\tLosses F.softmax: 0.002668 log_softmax: 0.008356\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1061\tAccuracy: 8108.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1343\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 316 [0/1000 (0%)]\tLosses F.softmax: 0.000145 log_softmax: 0.000151\n",
            "Train Epoch: 316 [200/1000 (20%)]\tLosses F.softmax: 0.013531 log_softmax: 0.004941\n",
            "Train Epoch: 316 [400/1000 (40%)]\tLosses F.softmax: 0.000566 log_softmax: 0.000848\n",
            "Train Epoch: 316 [600/1000 (60%)]\tLosses F.softmax: 0.030249 log_softmax: 0.042595\n",
            "Train Epoch: 316 [800/1000 (80%)]\tLosses F.softmax: 0.000219 log_softmax: 0.000316\n",
            "Train Epoch: 316 [1000/1000 (100%)]\tLosses F.softmax: 0.000081 log_softmax: 0.000003\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1066\tAccuracy: 8103.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1349\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 317 [0/1000 (0%)]\tLosses F.softmax: 0.013910 log_softmax: 0.003763\n",
            "Train Epoch: 317 [200/1000 (20%)]\tLosses F.softmax: 0.003881 log_softmax: 0.001359\n",
            "Train Epoch: 317 [400/1000 (40%)]\tLosses F.softmax: 0.017540 log_softmax: 0.011223\n",
            "Train Epoch: 317 [600/1000 (60%)]\tLosses F.softmax: 0.040691 log_softmax: 0.035530\n",
            "Train Epoch: 317 [800/1000 (80%)]\tLosses F.softmax: 0.000047 log_softmax: 0.000294\n",
            "Train Epoch: 317 [1000/1000 (100%)]\tLosses F.softmax: 0.000012 log_softmax: 0.000003\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1069\tAccuracy: 8103.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1354\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 318 [0/1000 (0%)]\tLosses F.softmax: 0.000211 log_softmax: 0.000518\n",
            "Train Epoch: 318 [200/1000 (20%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000001\n",
            "Train Epoch: 318 [400/1000 (40%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000010\n",
            "Train Epoch: 318 [600/1000 (60%)]\tLosses F.softmax: 0.001289 log_softmax: 0.003066\n",
            "Train Epoch: 318 [800/1000 (80%)]\tLosses F.softmax: 0.018000 log_softmax: 0.014496\n",
            "Train Epoch: 318 [1000/1000 (100%)]\tLosses F.softmax: 0.000008 log_softmax: 0.000055\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1069\tAccuracy: 8108.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1357\tAccuracy: 8058.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 319 [0/1000 (0%)]\tLosses F.softmax: 0.000222 log_softmax: 0.000524\n",
            "Train Epoch: 319 [200/1000 (20%)]\tLosses F.softmax: 0.012790 log_softmax: 0.014961\n",
            "Train Epoch: 319 [400/1000 (40%)]\tLosses F.softmax: 0.000025 log_softmax: 0.000092\n",
            "Train Epoch: 319 [600/1000 (60%)]\tLosses F.softmax: 0.000072 log_softmax: 0.000557\n",
            "Train Epoch: 319 [800/1000 (80%)]\tLosses F.softmax: 0.000066 log_softmax: 0.000292\n",
            "Train Epoch: 319 [1000/1000 (100%)]\tLosses F.softmax: 0.004072 log_softmax: 0.004272\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1079\tAccuracy: 8100.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1365\tAccuracy: 8057.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 320 [0/1000 (0%)]\tLosses F.softmax: 0.002782 log_softmax: 0.000547\n",
            "Train Epoch: 320 [200/1000 (20%)]\tLosses F.softmax: 0.000002 log_softmax: 0.000060\n",
            "Train Epoch: 320 [400/1000 (40%)]\tLosses F.softmax: 0.015875 log_softmax: 0.038667\n",
            "Train Epoch: 320 [600/1000 (60%)]\tLosses F.softmax: 0.001052 log_softmax: 0.000139\n",
            "Train Epoch: 320 [800/1000 (80%)]\tLosses F.softmax: 0.000125 log_softmax: 0.000008\n",
            "Train Epoch: 320 [1000/1000 (100%)]\tLosses F.softmax: 0.000137 log_softmax: 0.000002\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1077\tAccuracy: 8104.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1365\tAccuracy: 8058.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 321 [0/1000 (0%)]\tLosses F.softmax: 0.000298 log_softmax: 0.000017\n",
            "Train Epoch: 321 [200/1000 (20%)]\tLosses F.softmax: 0.000011 log_softmax: 0.002382\n",
            "Train Epoch: 321 [400/1000 (40%)]\tLosses F.softmax: 0.000136 log_softmax: 0.000013\n",
            "Train Epoch: 321 [600/1000 (60%)]\tLosses F.softmax: 0.000018 log_softmax: 0.000007\n",
            "Train Epoch: 321 [800/1000 (80%)]\tLosses F.softmax: 0.030405 log_softmax: 0.018714\n",
            "Train Epoch: 321 [1000/1000 (100%)]\tLosses F.softmax: 0.000063 log_softmax: 0.000285\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1086\tAccuracy: 8104.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1373\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 322 [0/1000 (0%)]\tLosses F.softmax: 0.000080 log_softmax: 0.000003\n",
            "Train Epoch: 322 [200/1000 (20%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000001\n",
            "Train Epoch: 322 [400/1000 (40%)]\tLosses F.softmax: 0.001927 log_softmax: 0.003013\n",
            "Train Epoch: 322 [600/1000 (60%)]\tLosses F.softmax: 0.000053 log_softmax: 0.000153\n",
            "Train Epoch: 322 [800/1000 (80%)]\tLosses F.softmax: 0.019790 log_softmax: 0.015039\n",
            "Train Epoch: 322 [1000/1000 (100%)]\tLosses F.softmax: 0.000549 log_softmax: 0.000003\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1090\tAccuracy: 8104.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1377\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 323 [0/1000 (0%)]\tLosses F.softmax: 0.000125 log_softmax: 0.000027\n",
            "Train Epoch: 323 [200/1000 (20%)]\tLosses F.softmax: 0.002873 log_softmax: 0.000665\n",
            "Train Epoch: 323 [400/1000 (40%)]\tLosses F.softmax: 0.007129 log_softmax: 0.004168\n",
            "Train Epoch: 323 [600/1000 (60%)]\tLosses F.softmax: 0.001706 log_softmax: 0.003395\n",
            "Train Epoch: 323 [800/1000 (80%)]\tLosses F.softmax: 0.002084 log_softmax: 0.009054\n",
            "Train Epoch: 323 [1000/1000 (100%)]\tLosses F.softmax: 0.000006 log_softmax: 0.000017\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1099\tAccuracy: 8102.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1384\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 324 [0/1000 (0%)]\tLosses F.softmax: 0.000010 log_softmax: 0.000005\n",
            "Train Epoch: 324 [200/1000 (20%)]\tLosses F.softmax: 0.000005 log_softmax: 0.000924\n",
            "Train Epoch: 324 [400/1000 (40%)]\tLosses F.softmax: 0.002055 log_softmax: 0.000446\n",
            "Train Epoch: 324 [600/1000 (60%)]\tLosses F.softmax: 0.001320 log_softmax: 0.003012\n",
            "Train Epoch: 324 [800/1000 (80%)]\tLosses F.softmax: 0.000002 log_softmax: 0.000122\n",
            "Train Epoch: 324 [1000/1000 (100%)]\tLosses F.softmax: 0.030382 log_softmax: 0.028049\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1099\tAccuracy: 8112.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1385\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 325 [0/1000 (0%)]\tLosses F.softmax: 0.000149 log_softmax: 0.000267\n",
            "Train Epoch: 325 [200/1000 (20%)]\tLosses F.softmax: 0.022170 log_softmax: 0.000553\n",
            "Train Epoch: 325 [400/1000 (40%)]\tLosses F.softmax: 0.000072 log_softmax: 0.000020\n",
            "Train Epoch: 325 [600/1000 (60%)]\tLosses F.softmax: 0.006777 log_softmax: 0.007433\n",
            "Train Epoch: 325 [800/1000 (80%)]\tLosses F.softmax: 0.001599 log_softmax: 0.000999\n",
            "Train Epoch: 325 [1000/1000 (100%)]\tLosses F.softmax: 0.009681 log_softmax: 0.013454\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1107\tAccuracy: 8099.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1398\tAccuracy: 8051.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 326 [0/1000 (0%)]\tLosses F.softmax: 0.000012 log_softmax: 0.000003\n",
            "Train Epoch: 326 [200/1000 (20%)]\tLosses F.softmax: 0.000009 log_softmax: 0.000003\n",
            "Train Epoch: 326 [400/1000 (40%)]\tLosses F.softmax: 0.000002 log_softmax: 0.000016\n",
            "Train Epoch: 326 [600/1000 (60%)]\tLosses F.softmax: 0.000119 log_softmax: 0.000019\n",
            "Train Epoch: 326 [800/1000 (80%)]\tLosses F.softmax: 0.003286 log_softmax: 0.000650\n",
            "Train Epoch: 326 [1000/1000 (100%)]\tLosses F.softmax: 0.000183 log_softmax: 0.003953\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1107\tAccuracy: 8107.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1398\tAccuracy: 8058.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 327 [0/1000 (0%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000000\n",
            "Train Epoch: 327 [200/1000 (20%)]\tLosses F.softmax: 0.019833 log_softmax: 0.014596\n",
            "Train Epoch: 327 [400/1000 (40%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000000\n",
            "Train Epoch: 327 [600/1000 (60%)]\tLosses F.softmax: 0.013867 log_softmax: 0.006876\n",
            "Train Epoch: 327 [800/1000 (80%)]\tLosses F.softmax: 0.006930 log_softmax: 0.000292\n",
            "Train Epoch: 327 [1000/1000 (100%)]\tLosses F.softmax: 0.000337 log_softmax: 0.000001\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1111\tAccuracy: 8103.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1399\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 328 [0/1000 (0%)]\tLosses F.softmax: 0.000857 log_softmax: 0.001668\n",
            "Train Epoch: 328 [200/1000 (20%)]\tLosses F.softmax: 0.000105 log_softmax: 0.000618\n",
            "Train Epoch: 328 [400/1000 (40%)]\tLosses F.softmax: 0.000176 log_softmax: 0.000254\n",
            "Train Epoch: 328 [600/1000 (60%)]\tLosses F.softmax: 0.000907 log_softmax: 0.000199\n",
            "Train Epoch: 328 [800/1000 (80%)]\tLosses F.softmax: 0.000065 log_softmax: 0.000079\n",
            "Train Epoch: 328 [1000/1000 (100%)]\tLosses F.softmax: 0.010345 log_softmax: 0.010538\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1114\tAccuracy: 8106.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1402\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 329 [0/1000 (0%)]\tLosses F.softmax: 0.002163 log_softmax: 0.000397\n",
            "Train Epoch: 329 [200/1000 (20%)]\tLosses F.softmax: 0.000275 log_softmax: 0.000033\n",
            "Train Epoch: 329 [400/1000 (40%)]\tLosses F.softmax: 0.001926 log_softmax: 0.002007\n",
            "Train Epoch: 329 [600/1000 (60%)]\tLosses F.softmax: 0.000195 log_softmax: 0.000182\n",
            "Train Epoch: 329 [800/1000 (80%)]\tLosses F.softmax: 0.000493 log_softmax: 0.000259\n",
            "Train Epoch: 329 [1000/1000 (100%)]\tLosses F.softmax: 0.001340 log_softmax: 0.001593\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1120\tAccuracy: 8110.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1410\tAccuracy: 8058.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 330 [0/1000 (0%)]\tLosses F.softmax: 0.022575 log_softmax: 0.018303\n",
            "Train Epoch: 330 [200/1000 (20%)]\tLosses F.softmax: 0.001290 log_softmax: 0.000952\n",
            "Train Epoch: 330 [400/1000 (40%)]\tLosses F.softmax: 0.016395 log_softmax: 0.007503\n",
            "Train Epoch: 330 [600/1000 (60%)]\tLosses F.softmax: 0.000012 log_softmax: 0.000116\n",
            "Train Epoch: 330 [800/1000 (80%)]\tLosses F.softmax: 0.006168 log_softmax: 0.000852\n",
            "Train Epoch: 330 [1000/1000 (100%)]\tLosses F.softmax: 0.000093 log_softmax: 0.000041\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1126\tAccuracy: 8108.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1416\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 331 [0/1000 (0%)]\tLosses F.softmax: 0.000139 log_softmax: 0.000088\n",
            "Train Epoch: 331 [200/1000 (20%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000000\n",
            "Train Epoch: 331 [400/1000 (40%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000001\n",
            "Train Epoch: 331 [600/1000 (60%)]\tLosses F.softmax: 0.000058 log_softmax: 0.000300\n",
            "Train Epoch: 331 [800/1000 (80%)]\tLosses F.softmax: 0.000186 log_softmax: 0.003999\n",
            "Train Epoch: 331 [1000/1000 (100%)]\tLosses F.softmax: 0.004831 log_softmax: 0.004243\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1133\tAccuracy: 8104.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1423\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 332 [0/1000 (0%)]\tLosses F.softmax: 0.000841 log_softmax: 0.000042\n",
            "Train Epoch: 332 [200/1000 (20%)]\tLosses F.softmax: 0.000025 log_softmax: 0.000001\n",
            "Train Epoch: 332 [400/1000 (40%)]\tLosses F.softmax: 0.010337 log_softmax: 0.007521\n",
            "Train Epoch: 332 [600/1000 (60%)]\tLosses F.softmax: 0.000474 log_softmax: 0.000233\n",
            "Train Epoch: 332 [800/1000 (80%)]\tLosses F.softmax: 0.000004 log_softmax: 0.000000\n",
            "Train Epoch: 332 [1000/1000 (100%)]\tLosses F.softmax: 0.002349 log_softmax: 0.003026\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1136\tAccuracy: 8099.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1428\tAccuracy: 8060.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 333 [0/1000 (0%)]\tLosses F.softmax: 0.017803 log_softmax: 0.025621\n",
            "Train Epoch: 333 [200/1000 (20%)]\tLosses F.softmax: 0.000709 log_softmax: 0.005471\n",
            "Train Epoch: 333 [400/1000 (40%)]\tLosses F.softmax: 0.002740 log_softmax: 0.001329\n",
            "Train Epoch: 333 [600/1000 (60%)]\tLosses F.softmax: 0.000003 log_softmax: 0.000000\n",
            "Train Epoch: 333 [800/1000 (80%)]\tLosses F.softmax: 0.015840 log_softmax: 0.021825\n",
            "Train Epoch: 333 [1000/1000 (100%)]\tLosses F.softmax: 0.000394 log_softmax: 0.000280\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1138\tAccuracy: 8106.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1430\tAccuracy: 8057.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 334 [0/1000 (0%)]\tLosses F.softmax: 0.030441 log_softmax: 0.036764\n",
            "Train Epoch: 334 [200/1000 (20%)]\tLosses F.softmax: 0.000327 log_softmax: 0.000001\n",
            "Train Epoch: 334 [400/1000 (40%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000001\n",
            "Train Epoch: 334 [600/1000 (60%)]\tLosses F.softmax: 0.000293 log_softmax: 0.000011\n",
            "Train Epoch: 334 [800/1000 (80%)]\tLosses F.softmax: 0.015857 log_softmax: 0.010988\n",
            "Train Epoch: 334 [1000/1000 (100%)]\tLosses F.softmax: 0.000893 log_softmax: 0.000333\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1141\tAccuracy: 8104.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1434\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 335 [0/1000 (0%)]\tLosses F.softmax: 0.000537 log_softmax: 0.000209\n",
            "Train Epoch: 335 [200/1000 (20%)]\tLosses F.softmax: 0.048071 log_softmax: 0.021852\n",
            "Train Epoch: 335 [400/1000 (40%)]\tLosses F.softmax: 0.004219 log_softmax: 0.002375\n",
            "Train Epoch: 335 [600/1000 (60%)]\tLosses F.softmax: 0.009507 log_softmax: 0.011539\n",
            "Train Epoch: 335 [800/1000 (80%)]\tLosses F.softmax: 0.000007 log_softmax: 0.000005\n",
            "Train Epoch: 335 [1000/1000 (100%)]\tLosses F.softmax: 0.000002 log_softmax: 0.000001\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1149\tAccuracy: 8101.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1442\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 336 [0/1000 (0%)]\tLosses F.softmax: 0.002673 log_softmax: 0.000987\n",
            "Train Epoch: 336 [200/1000 (20%)]\tLosses F.softmax: 0.021219 log_softmax: 0.015551\n",
            "Train Epoch: 336 [400/1000 (40%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000001\n",
            "Train Epoch: 336 [600/1000 (60%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000005\n",
            "Train Epoch: 336 [800/1000 (80%)]\tLosses F.softmax: 0.000005 log_softmax: 0.000908\n",
            "Train Epoch: 336 [1000/1000 (100%)]\tLosses F.softmax: 0.000014 log_softmax: 0.000330\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1150\tAccuracy: 8103.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1441\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 337 [0/1000 (0%)]\tLosses F.softmax: 0.010819 log_softmax: 0.010864\n",
            "Train Epoch: 337 [200/1000 (20%)]\tLosses F.softmax: 0.000428 log_softmax: 0.000047\n",
            "Train Epoch: 337 [400/1000 (40%)]\tLosses F.softmax: 0.015028 log_softmax: 0.014598\n",
            "Train Epoch: 337 [600/1000 (60%)]\tLosses F.softmax: 0.000004 log_softmax: 0.000075\n",
            "Train Epoch: 337 [800/1000 (80%)]\tLosses F.softmax: 0.001185 log_softmax: 0.000096\n",
            "Train Epoch: 337 [1000/1000 (100%)]\tLosses F.softmax: 0.002852 log_softmax: 0.000965\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1154\tAccuracy: 8106.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1446\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 338 [0/1000 (0%)]\tLosses F.softmax: 0.055271 log_softmax: 0.050459\n",
            "Train Epoch: 338 [200/1000 (20%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000001\n",
            "Train Epoch: 338 [400/1000 (40%)]\tLosses F.softmax: 0.002204 log_softmax: 0.002865\n",
            "Train Epoch: 338 [600/1000 (60%)]\tLosses F.softmax: 0.018647 log_softmax: 0.008376\n",
            "Train Epoch: 338 [800/1000 (80%)]\tLosses F.softmax: 0.004972 log_softmax: 0.004442\n",
            "Train Epoch: 338 [1000/1000 (100%)]\tLosses F.softmax: 0.000042 log_softmax: 0.002069\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1159\tAccuracy: 8106.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1449\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 339 [0/1000 (0%)]\tLosses F.softmax: 0.000042 log_softmax: 0.000005\n",
            "Train Epoch: 339 [200/1000 (20%)]\tLosses F.softmax: 0.000055 log_softmax: 0.000275\n",
            "Train Epoch: 339 [400/1000 (40%)]\tLosses F.softmax: 0.000045 log_softmax: 0.000287\n",
            "Train Epoch: 339 [600/1000 (60%)]\tLosses F.softmax: 0.002736 log_softmax: 0.005091\n",
            "Train Epoch: 339 [800/1000 (80%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000018\n",
            "Train Epoch: 339 [1000/1000 (100%)]\tLosses F.softmax: 0.000007 log_softmax: 0.000010\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1164\tAccuracy: 8106.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1456\tAccuracy: 8057.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 340 [0/1000 (0%)]\tLosses F.softmax: 0.000057 log_softmax: 0.000005\n",
            "Train Epoch: 340 [200/1000 (20%)]\tLosses F.softmax: 0.010927 log_softmax: 0.014974\n",
            "Train Epoch: 340 [400/1000 (40%)]\tLosses F.softmax: 0.000120 log_softmax: 0.000027\n",
            "Train Epoch: 340 [600/1000 (60%)]\tLosses F.softmax: 0.012111 log_softmax: 0.013963\n",
            "Train Epoch: 340 [800/1000 (80%)]\tLosses F.softmax: 0.002357 log_softmax: 0.000166\n",
            "Train Epoch: 340 [1000/1000 (100%)]\tLosses F.softmax: 0.000945 log_softmax: 0.002354\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1168\tAccuracy: 8101.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1461\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 341 [0/1000 (0%)]\tLosses F.softmax: 0.000005 log_softmax: 0.000910\n",
            "Train Epoch: 341 [200/1000 (20%)]\tLosses F.softmax: 0.000012 log_softmax: 0.000112\n",
            "Train Epoch: 341 [400/1000 (40%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000061\n",
            "Train Epoch: 341 [600/1000 (60%)]\tLosses F.softmax: 0.000265 log_softmax: 0.000076\n",
            "Train Epoch: 341 [800/1000 (80%)]\tLosses F.softmax: 0.000161 log_softmax: 0.000069\n",
            "Train Epoch: 341 [1000/1000 (100%)]\tLosses F.softmax: 0.015218 log_softmax: 0.024643\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1168\tAccuracy: 8106.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1463\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 342 [0/1000 (0%)]\tLosses F.softmax: 0.004514 log_softmax: 0.000474\n",
            "Train Epoch: 342 [200/1000 (20%)]\tLosses F.softmax: 0.019198 log_softmax: 0.014322\n",
            "Train Epoch: 342 [400/1000 (40%)]\tLosses F.softmax: 0.000455 log_softmax: 0.000004\n",
            "Train Epoch: 342 [600/1000 (60%)]\tLosses F.softmax: 0.000026 log_softmax: 0.000377\n",
            "Train Epoch: 342 [800/1000 (80%)]\tLosses F.softmax: 0.000248 log_softmax: 0.000835\n",
            "Train Epoch: 342 [1000/1000 (100%)]\tLosses F.softmax: 0.000072 log_softmax: 0.000357\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1173\tAccuracy: 8106.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1468\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 343 [0/1000 (0%)]\tLosses F.softmax: 0.000793 log_softmax: 0.000746\n",
            "Train Epoch: 343 [200/1000 (20%)]\tLosses F.softmax: 0.019267 log_softmax: 0.014079\n",
            "Train Epoch: 343 [400/1000 (40%)]\tLosses F.softmax: 0.000197 log_softmax: 0.000434\n",
            "Train Epoch: 343 [600/1000 (60%)]\tLosses F.softmax: 0.055206 log_softmax: 0.049739\n",
            "Train Epoch: 343 [800/1000 (80%)]\tLosses F.softmax: 0.000005 log_softmax: 0.000012\n",
            "Train Epoch: 343 [1000/1000 (100%)]\tLosses F.softmax: 0.004424 log_softmax: 0.002293\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1177\tAccuracy: 8109.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1469\tAccuracy: 8057.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 344 [0/1000 (0%)]\tLosses F.softmax: 0.000002 log_softmax: 0.000000\n",
            "Train Epoch: 344 [200/1000 (20%)]\tLosses F.softmax: 0.003311 log_softmax: 0.005792\n",
            "Train Epoch: 344 [400/1000 (40%)]\tLosses F.softmax: 0.004569 log_softmax: 0.002364\n",
            "Train Epoch: 344 [600/1000 (60%)]\tLosses F.softmax: 0.022928 log_softmax: 0.029216\n",
            "Train Epoch: 344 [800/1000 (80%)]\tLosses F.softmax: 0.009492 log_softmax: 0.011909\n",
            "Train Epoch: 344 [1000/1000 (100%)]\tLosses F.softmax: 0.013821 log_softmax: 0.005244\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1184\tAccuracy: 8108.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1477\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 345 [0/1000 (0%)]\tLosses F.softmax: 0.009117 log_softmax: 0.010835\n",
            "Train Epoch: 345 [200/1000 (20%)]\tLosses F.softmax: 0.009785 log_softmax: 0.001279\n",
            "Train Epoch: 345 [400/1000 (40%)]\tLosses F.softmax: 0.002118 log_softmax: 0.000125\n",
            "Train Epoch: 345 [600/1000 (60%)]\tLosses F.softmax: 0.010178 log_softmax: 0.015754\n",
            "Train Epoch: 345 [800/1000 (80%)]\tLosses F.softmax: 0.001242 log_softmax: 0.000966\n",
            "Train Epoch: 345 [1000/1000 (100%)]\tLosses F.softmax: 0.005420 log_softmax: 0.004263\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1192\tAccuracy: 8099.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1484\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 346 [0/1000 (0%)]\tLosses F.softmax: 0.007909 log_softmax: 0.017642\n",
            "Train Epoch: 346 [200/1000 (20%)]\tLosses F.softmax: 0.000061 log_softmax: 0.000028\n",
            "Train Epoch: 346 [400/1000 (40%)]\tLosses F.softmax: 0.000009 log_softmax: 0.000001\n",
            "Train Epoch: 346 [600/1000 (60%)]\tLosses F.softmax: 0.000476 log_softmax: 0.000625\n",
            "Train Epoch: 346 [800/1000 (80%)]\tLosses F.softmax: 0.014314 log_softmax: 0.017010\n",
            "Train Epoch: 346 [1000/1000 (100%)]\tLosses F.softmax: 0.006549 log_softmax: 0.000270\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1192\tAccuracy: 8106.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1486\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 347 [0/1000 (0%)]\tLosses F.softmax: 0.016706 log_softmax: 0.005356\n",
            "Train Epoch: 347 [200/1000 (20%)]\tLosses F.softmax: 0.003603 log_softmax: 0.000013\n",
            "Train Epoch: 347 [400/1000 (40%)]\tLosses F.softmax: 0.000487 log_softmax: 0.000008\n",
            "Train Epoch: 347 [600/1000 (60%)]\tLosses F.softmax: 0.047977 log_softmax: 0.036496\n",
            "Train Epoch: 347 [800/1000 (80%)]\tLosses F.softmax: 0.008814 log_softmax: 0.015282\n",
            "Train Epoch: 347 [1000/1000 (100%)]\tLosses F.softmax: 0.005474 log_softmax: 0.004479\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1199\tAccuracy: 8101.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1494\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 348 [0/1000 (0%)]\tLosses F.softmax: 0.003587 log_softmax: 0.003819\n",
            "Train Epoch: 348 [200/1000 (20%)]\tLosses F.softmax: 0.000056 log_softmax: 0.000264\n",
            "Train Epoch: 348 [400/1000 (40%)]\tLosses F.softmax: 0.007833 log_softmax: 0.012108\n",
            "Train Epoch: 348 [600/1000 (60%)]\tLosses F.softmax: 0.020887 log_softmax: 0.014872\n",
            "Train Epoch: 348 [800/1000 (80%)]\tLosses F.softmax: 0.032733 log_softmax: 0.027356\n",
            "Train Epoch: 348 [1000/1000 (100%)]\tLosses F.softmax: 0.000044 log_softmax: 0.000015\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1201\tAccuracy: 8107.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1495\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 349 [0/1000 (0%)]\tLosses F.softmax: 0.002127 log_softmax: 0.000795\n",
            "Train Epoch: 349 [200/1000 (20%)]\tLosses F.softmax: 0.003119 log_softmax: 0.000335\n",
            "Train Epoch: 349 [400/1000 (40%)]\tLosses F.softmax: 0.000195 log_softmax: 0.000526\n",
            "Train Epoch: 349 [600/1000 (60%)]\tLosses F.softmax: 0.000007 log_softmax: 0.000002\n",
            "Train Epoch: 349 [800/1000 (80%)]\tLosses F.softmax: 0.000277 log_softmax: 0.000524\n",
            "Train Epoch: 349 [1000/1000 (100%)]\tLosses F.softmax: 0.006929 log_softmax: 0.012564\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1208\tAccuracy: 8103.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1504\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 350 [0/1000 (0%)]\tLosses F.softmax: 0.012178 log_softmax: 0.006191\n",
            "Train Epoch: 350 [200/1000 (20%)]\tLosses F.softmax: 0.001525 log_softmax: 0.021987\n",
            "Train Epoch: 350 [400/1000 (40%)]\tLosses F.softmax: 0.002459 log_softmax: 0.000924\n",
            "Train Epoch: 350 [600/1000 (60%)]\tLosses F.softmax: 0.004428 log_softmax: 0.003035\n",
            "Train Epoch: 350 [800/1000 (80%)]\tLosses F.softmax: 0.000250 log_softmax: 0.000341\n",
            "Train Epoch: 350 [1000/1000 (100%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000000\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1210\tAccuracy: 8105.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1505\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 351 [0/1000 (0%)]\tLosses F.softmax: 0.000727 log_softmax: 0.000030\n",
            "Train Epoch: 351 [200/1000 (20%)]\tLosses F.softmax: 0.001115 log_softmax: 0.002282\n",
            "Train Epoch: 351 [400/1000 (40%)]\tLosses F.softmax: 0.033339 log_softmax: 0.038176\n",
            "Train Epoch: 351 [600/1000 (60%)]\tLosses F.softmax: 0.015723 log_softmax: 0.021795\n",
            "Train Epoch: 351 [800/1000 (80%)]\tLosses F.softmax: 0.006303 log_softmax: 0.005099\n",
            "Train Epoch: 351 [1000/1000 (100%)]\tLosses F.softmax: 0.010580 log_softmax: 0.002621\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1216\tAccuracy: 8104.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1512\tAccuracy: 8052.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 352 [0/1000 (0%)]\tLosses F.softmax: 0.005448 log_softmax: 0.010398\n",
            "Train Epoch: 352 [200/1000 (20%)]\tLosses F.softmax: 0.049847 log_softmax: 0.028101\n",
            "Train Epoch: 352 [400/1000 (40%)]\tLosses F.softmax: 0.000115 log_softmax: 0.000022\n",
            "Train Epoch: 352 [600/1000 (60%)]\tLosses F.softmax: 0.003717 log_softmax: 0.006785\n",
            "Train Epoch: 352 [800/1000 (80%)]\tLosses F.softmax: 0.000062 log_softmax: 0.000342\n",
            "Train Epoch: 352 [1000/1000 (100%)]\tLosses F.softmax: 0.000102 log_softmax: 0.000049\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1215\tAccuracy: 8108.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1510\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 353 [0/1000 (0%)]\tLosses F.softmax: 0.000015 log_softmax: 0.000002\n",
            "Train Epoch: 353 [200/1000 (20%)]\tLosses F.softmax: 0.000134 log_softmax: 0.000040\n",
            "Train Epoch: 353 [400/1000 (40%)]\tLosses F.softmax: 0.014231 log_softmax: 0.004419\n",
            "Train Epoch: 353 [600/1000 (60%)]\tLosses F.softmax: 0.003549 log_softmax: 0.000013\n",
            "Train Epoch: 353 [800/1000 (80%)]\tLosses F.softmax: 0.001897 log_softmax: 0.000077\n",
            "Train Epoch: 353 [1000/1000 (100%)]\tLosses F.softmax: 0.001556 log_softmax: 0.001145\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1220\tAccuracy: 8105.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1517\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 354 [0/1000 (0%)]\tLosses F.softmax: 0.000050 log_softmax: 0.000072\n",
            "Train Epoch: 354 [200/1000 (20%)]\tLosses F.softmax: 0.000039 log_softmax: 0.000092\n",
            "Train Epoch: 354 [400/1000 (40%)]\tLosses F.softmax: 0.013613 log_softmax: 0.009035\n",
            "Train Epoch: 354 [600/1000 (60%)]\tLosses F.softmax: 0.000008 log_softmax: 0.000002\n",
            "Train Epoch: 354 [800/1000 (80%)]\tLosses F.softmax: 0.000213 log_softmax: 0.000643\n",
            "Train Epoch: 354 [1000/1000 (100%)]\tLosses F.softmax: 0.000017 log_softmax: 0.000214\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1225\tAccuracy: 8105.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1520\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 355 [0/1000 (0%)]\tLosses F.softmax: 0.000071 log_softmax: 0.000420\n",
            "Train Epoch: 355 [200/1000 (20%)]\tLosses F.softmax: 0.000005 log_softmax: 0.000014\n",
            "Train Epoch: 355 [400/1000 (40%)]\tLosses F.softmax: 0.000006 log_softmax: 0.000004\n",
            "Train Epoch: 355 [600/1000 (60%)]\tLosses F.softmax: 0.000902 log_softmax: 0.000007\n",
            "Train Epoch: 355 [800/1000 (80%)]\tLosses F.softmax: 0.000008 log_softmax: 0.000000\n",
            "Train Epoch: 355 [1000/1000 (100%)]\tLosses F.softmax: 0.002175 log_softmax: 0.001749\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1228\tAccuracy: 8106.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1526\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 356 [0/1000 (0%)]\tLosses F.softmax: 0.000488 log_softmax: 0.000003\n",
            "Train Epoch: 356 [200/1000 (20%)]\tLosses F.softmax: 0.008508 log_softmax: 0.024032\n",
            "Train Epoch: 356 [400/1000 (40%)]\tLosses F.softmax: 0.000191 log_softmax: 0.000535\n",
            "Train Epoch: 356 [600/1000 (60%)]\tLosses F.softmax: 0.001021 log_softmax: 0.000004\n",
            "Train Epoch: 356 [800/1000 (80%)]\tLosses F.softmax: 0.001974 log_softmax: 0.003877\n",
            "Train Epoch: 356 [1000/1000 (100%)]\tLosses F.softmax: 0.000262 log_softmax: 0.000133\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1237\tAccuracy: 8100.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1536\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 357 [0/1000 (0%)]\tLosses F.softmax: 0.000078 log_softmax: 0.000058\n",
            "Train Epoch: 357 [200/1000 (20%)]\tLosses F.softmax: 0.007140 log_softmax: 0.008803\n",
            "Train Epoch: 357 [400/1000 (40%)]\tLosses F.softmax: 0.002465 log_softmax: 0.002221\n",
            "Train Epoch: 357 [600/1000 (60%)]\tLosses F.softmax: 0.000016 log_softmax: 0.000006\n",
            "Train Epoch: 357 [800/1000 (80%)]\tLosses F.softmax: 0.000050 log_softmax: 0.000187\n",
            "Train Epoch: 357 [1000/1000 (100%)]\tLosses F.softmax: 0.000002 log_softmax: 0.000002\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1238\tAccuracy: 8102.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1539\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 358 [0/1000 (0%)]\tLosses F.softmax: 0.002337 log_softmax: 0.001862\n",
            "Train Epoch: 358 [200/1000 (20%)]\tLosses F.softmax: 0.021425 log_softmax: 0.021223\n",
            "Train Epoch: 358 [400/1000 (40%)]\tLosses F.softmax: 0.001109 log_softmax: 0.003218\n",
            "Train Epoch: 358 [600/1000 (60%)]\tLosses F.softmax: 0.003535 log_softmax: 0.012924\n",
            "Train Epoch: 358 [800/1000 (80%)]\tLosses F.softmax: 0.000050 log_softmax: 0.000878\n",
            "Train Epoch: 358 [1000/1000 (100%)]\tLosses F.softmax: 0.000003 log_softmax: 0.000001\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1242\tAccuracy: 8106.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1541\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 359 [0/1000 (0%)]\tLosses F.softmax: 0.000073 log_softmax: 0.000071\n",
            "Train Epoch: 359 [200/1000 (20%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000001\n",
            "Train Epoch: 359 [400/1000 (40%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000000\n",
            "Train Epoch: 359 [600/1000 (60%)]\tLosses F.softmax: 0.001044 log_softmax: 0.001592\n",
            "Train Epoch: 359 [800/1000 (80%)]\tLosses F.softmax: 0.004871 log_softmax: 0.005550\n",
            "Train Epoch: 359 [1000/1000 (100%)]\tLosses F.softmax: 0.000164 log_softmax: 0.000083\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1247\tAccuracy: 8108.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1544\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 360 [0/1000 (0%)]\tLosses F.softmax: 0.018228 log_softmax: 0.020443\n",
            "Train Epoch: 360 [200/1000 (20%)]\tLosses F.softmax: 0.001792 log_softmax: 0.001324\n",
            "Train Epoch: 360 [400/1000 (40%)]\tLosses F.softmax: 0.000261 log_softmax: 0.000082\n",
            "Train Epoch: 360 [600/1000 (60%)]\tLosses F.softmax: 0.000750 log_softmax: 0.007481\n",
            "Train Epoch: 360 [800/1000 (80%)]\tLosses F.softmax: 0.000462 log_softmax: 0.000312\n",
            "Train Epoch: 360 [1000/1000 (100%)]\tLosses F.softmax: 0.000314 log_softmax: 0.000287\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1248\tAccuracy: 8111.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1547\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 361 [0/1000 (0%)]\tLosses F.softmax: 0.014139 log_softmax: 0.010360\n",
            "Train Epoch: 361 [200/1000 (20%)]\tLosses F.softmax: 0.002612 log_softmax: 0.002021\n",
            "Train Epoch: 361 [400/1000 (40%)]\tLosses F.softmax: 0.000174 log_softmax: 0.000262\n",
            "Train Epoch: 361 [600/1000 (60%)]\tLosses F.softmax: 0.000186 log_softmax: 0.000011\n",
            "Train Epoch: 361 [800/1000 (80%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000000\n",
            "Train Epoch: 361 [1000/1000 (100%)]\tLosses F.softmax: 0.003503 log_softmax: 0.013192\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1257\tAccuracy: 8105.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1553\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 362 [0/1000 (0%)]\tLosses F.softmax: 0.000104 log_softmax: 0.000024\n",
            "Train Epoch: 362 [200/1000 (20%)]\tLosses F.softmax: 0.002046 log_softmax: 0.000752\n",
            "Train Epoch: 362 [400/1000 (40%)]\tLosses F.softmax: 0.000436 log_softmax: 0.000003\n",
            "Train Epoch: 362 [600/1000 (60%)]\tLosses F.softmax: 0.000834 log_softmax: 0.000714\n",
            "Train Epoch: 362 [800/1000 (80%)]\tLosses F.softmax: 0.021155 log_softmax: 0.012255\n",
            "Train Epoch: 362 [1000/1000 (100%)]\tLosses F.softmax: 0.001459 log_softmax: 0.004351\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1259\tAccuracy: 8106.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1557\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 363 [0/1000 (0%)]\tLosses F.softmax: 0.014595 log_softmax: 0.036980\n",
            "Train Epoch: 363 [200/1000 (20%)]\tLosses F.softmax: 0.004832 log_softmax: 0.000146\n",
            "Train Epoch: 363 [400/1000 (40%)]\tLosses F.softmax: 0.000551 log_softmax: 0.003152\n",
            "Train Epoch: 363 [600/1000 (60%)]\tLosses F.softmax: 0.016459 log_softmax: 0.010511\n",
            "Train Epoch: 363 [800/1000 (80%)]\tLosses F.softmax: 0.000166 log_softmax: 0.000070\n",
            "Train Epoch: 363 [1000/1000 (100%)]\tLosses F.softmax: 0.007211 log_softmax: 0.011401\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1262\tAccuracy: 8109.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1561\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 364 [0/1000 (0%)]\tLosses F.softmax: 0.001069 log_softmax: 0.000092\n",
            "Train Epoch: 364 [200/1000 (20%)]\tLosses F.softmax: 0.000498 log_softmax: 0.000122\n",
            "Train Epoch: 364 [400/1000 (40%)]\tLosses F.softmax: 0.000068 log_softmax: 0.000002\n",
            "Train Epoch: 364 [600/1000 (60%)]\tLosses F.softmax: 0.000019 log_softmax: 0.000007\n",
            "Train Epoch: 364 [800/1000 (80%)]\tLosses F.softmax: 0.000697 log_softmax: 0.003594\n",
            "Train Epoch: 364 [1000/1000 (100%)]\tLosses F.softmax: 0.000003 log_softmax: 0.000000\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1268\tAccuracy: 8106.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1568\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 365 [0/1000 (0%)]\tLosses F.softmax: 0.004508 log_softmax: 0.002902\n",
            "Train Epoch: 365 [200/1000 (20%)]\tLosses F.softmax: 0.000904 log_softmax: 0.000286\n",
            "Train Epoch: 365 [400/1000 (40%)]\tLosses F.softmax: 0.037154 log_softmax: 0.062706\n",
            "Train Epoch: 365 [600/1000 (60%)]\tLosses F.softmax: 0.004864 log_softmax: 0.000280\n",
            "Train Epoch: 365 [800/1000 (80%)]\tLosses F.softmax: 0.022526 log_softmax: 0.032008\n",
            "Train Epoch: 365 [1000/1000 (100%)]\tLosses F.softmax: 0.001848 log_softmax: 0.002669\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1269\tAccuracy: 8111.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1569\tAccuracy: 8051.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 366 [0/1000 (0%)]\tLosses F.softmax: 0.000102 log_softmax: 0.000002\n",
            "Train Epoch: 366 [200/1000 (20%)]\tLosses F.softmax: 0.003023 log_softmax: 0.004928\n",
            "Train Epoch: 366 [400/1000 (40%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000002\n",
            "Train Epoch: 366 [600/1000 (60%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000000\n",
            "Train Epoch: 366 [800/1000 (80%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000002\n",
            "Train Epoch: 366 [1000/1000 (100%)]\tLosses F.softmax: 0.006800 log_softmax: 0.000348\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1271\tAccuracy: 8110.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1572\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 367 [0/1000 (0%)]\tLosses F.softmax: 0.009028 log_softmax: 0.010132\n",
            "Train Epoch: 367 [200/1000 (20%)]\tLosses F.softmax: 0.000045 log_softmax: 0.000170\n",
            "Train Epoch: 367 [400/1000 (40%)]\tLosses F.softmax: 0.039461 log_softmax: 0.037303\n",
            "Train Epoch: 367 [600/1000 (60%)]\tLosses F.softmax: 0.000613 log_softmax: 0.000658\n",
            "Train Epoch: 367 [800/1000 (80%)]\tLosses F.softmax: 0.049144 log_softmax: 0.040044\n",
            "Train Epoch: 367 [1000/1000 (100%)]\tLosses F.softmax: 0.000068 log_softmax: 0.001350\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1277\tAccuracy: 8109.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1579\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 368 [0/1000 (0%)]\tLosses F.softmax: 0.000070 log_softmax: 0.000096\n",
            "Train Epoch: 368 [200/1000 (20%)]\tLosses F.softmax: 0.000969 log_softmax: 0.002472\n",
            "Train Epoch: 368 [400/1000 (40%)]\tLosses F.softmax: 0.000099 log_softmax: 0.004883\n",
            "Train Epoch: 368 [600/1000 (60%)]\tLosses F.softmax: 0.003474 log_softmax: 0.006329\n",
            "Train Epoch: 368 [800/1000 (80%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000000\n",
            "Train Epoch: 368 [1000/1000 (100%)]\tLosses F.softmax: 0.036687 log_softmax: 0.065910\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1282\tAccuracy: 8110.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1583\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 369 [0/1000 (0%)]\tLosses F.softmax: 0.002979 log_softmax: 0.001218\n",
            "Train Epoch: 369 [200/1000 (20%)]\tLosses F.softmax: 0.000003 log_softmax: 0.000005\n",
            "Train Epoch: 369 [400/1000 (40%)]\tLosses F.softmax: 0.000137 log_softmax: 0.000073\n",
            "Train Epoch: 369 [600/1000 (60%)]\tLosses F.softmax: 0.000660 log_softmax: 0.004696\n",
            "Train Epoch: 369 [800/1000 (80%)]\tLosses F.softmax: 0.016441 log_softmax: 0.017960\n",
            "Train Epoch: 369 [1000/1000 (100%)]\tLosses F.softmax: 0.004374 log_softmax: 0.000639\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1285\tAccuracy: 8109.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1589\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 370 [0/1000 (0%)]\tLosses F.softmax: 0.000464 log_softmax: 0.000410\n",
            "Train Epoch: 370 [200/1000 (20%)]\tLosses F.softmax: 0.000010 log_softmax: 0.000015\n",
            "Train Epoch: 370 [400/1000 (40%)]\tLosses F.softmax: 0.000022 log_softmax: 0.000007\n",
            "Train Epoch: 370 [600/1000 (60%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000056\n",
            "Train Epoch: 370 [800/1000 (80%)]\tLosses F.softmax: 0.000102 log_softmax: 0.000015\n",
            "Train Epoch: 370 [1000/1000 (100%)]\tLosses F.softmax: 0.000108 log_softmax: 0.000030\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1290\tAccuracy: 8104.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1593\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 371 [0/1000 (0%)]\tLosses F.softmax: 0.000047 log_softmax: 0.000004\n",
            "Train Epoch: 371 [200/1000 (20%)]\tLosses F.softmax: 0.000258 log_softmax: 0.000068\n",
            "Train Epoch: 371 [400/1000 (40%)]\tLosses F.softmax: 0.001909 log_softmax: 0.000669\n",
            "Train Epoch: 371 [600/1000 (60%)]\tLosses F.softmax: 0.001046 log_softmax: 0.003192\n",
            "Train Epoch: 371 [800/1000 (80%)]\tLosses F.softmax: 0.000801 log_softmax: 0.002048\n",
            "Train Epoch: 371 [1000/1000 (100%)]\tLosses F.softmax: 0.000335 log_softmax: 0.000254\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1294\tAccuracy: 8106.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1596\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 372 [0/1000 (0%)]\tLosses F.softmax: 0.001240 log_softmax: 0.000068\n",
            "Train Epoch: 372 [200/1000 (20%)]\tLosses F.softmax: 0.000154 log_softmax: 0.000351\n",
            "Train Epoch: 372 [400/1000 (40%)]\tLosses F.softmax: 0.005216 log_softmax: 0.004053\n",
            "Train Epoch: 372 [600/1000 (60%)]\tLosses F.softmax: 0.000402 log_softmax: 0.000953\n",
            "Train Epoch: 372 [800/1000 (80%)]\tLosses F.softmax: 0.000023 log_softmax: 0.000007\n",
            "Train Epoch: 372 [1000/1000 (100%)]\tLosses F.softmax: 0.017181 log_softmax: 0.009763\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1300\tAccuracy: 8104.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1602\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 373 [0/1000 (0%)]\tLosses F.softmax: 0.000290 log_softmax: 0.000001\n",
            "Train Epoch: 373 [200/1000 (20%)]\tLosses F.softmax: 0.002417 log_softmax: 0.001672\n",
            "Train Epoch: 373 [400/1000 (40%)]\tLosses F.softmax: 0.000096 log_softmax: 0.000295\n",
            "Train Epoch: 373 [600/1000 (60%)]\tLosses F.softmax: 0.004557 log_softmax: 0.001793\n",
            "Train Epoch: 373 [800/1000 (80%)]\tLosses F.softmax: 0.003488 log_softmax: 0.009467\n",
            "Train Epoch: 373 [1000/1000 (100%)]\tLosses F.softmax: 0.000630 log_softmax: 0.000644\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1304\tAccuracy: 8107.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1605\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 374 [0/1000 (0%)]\tLosses F.softmax: 0.000002 log_softmax: 0.000001\n",
            "Train Epoch: 374 [200/1000 (20%)]\tLosses F.softmax: 0.000717 log_softmax: 0.000348\n",
            "Train Epoch: 374 [400/1000 (40%)]\tLosses F.softmax: 0.000070 log_softmax: 0.000094\n",
            "Train Epoch: 374 [600/1000 (60%)]\tLosses F.softmax: 0.020245 log_softmax: 0.018842\n",
            "Train Epoch: 374 [800/1000 (80%)]\tLosses F.softmax: 0.000039 log_softmax: 0.000025\n",
            "Train Epoch: 374 [1000/1000 (100%)]\tLosses F.softmax: 0.000004 log_softmax: 0.000000\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1304\tAccuracy: 8109.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1609\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 375 [0/1000 (0%)]\tLosses F.softmax: 0.000744 log_softmax: 0.001033\n",
            "Train Epoch: 375 [200/1000 (20%)]\tLosses F.softmax: 0.000003 log_softmax: 0.000000\n",
            "Train Epoch: 375 [400/1000 (40%)]\tLosses F.softmax: 0.000168 log_softmax: 0.000066\n",
            "Train Epoch: 375 [600/1000 (60%)]\tLosses F.softmax: 0.000500 log_softmax: 0.000002\n",
            "Train Epoch: 375 [800/1000 (80%)]\tLosses F.softmax: 0.020170 log_softmax: 0.029167\n",
            "Train Epoch: 375 [1000/1000 (100%)]\tLosses F.softmax: 0.017236 log_softmax: 0.009856\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1312\tAccuracy: 8103.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1618\tAccuracy: 8051.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 376 [0/1000 (0%)]\tLosses F.softmax: 0.000365 log_softmax: 0.000372\n",
            "Train Epoch: 376 [200/1000 (20%)]\tLosses F.softmax: 0.041221 log_softmax: 0.037041\n",
            "Train Epoch: 376 [400/1000 (40%)]\tLosses F.softmax: 0.001950 log_softmax: 0.000198\n",
            "Train Epoch: 376 [600/1000 (60%)]\tLosses F.softmax: 0.000966 log_softmax: 0.002072\n",
            "Train Epoch: 376 [800/1000 (80%)]\tLosses F.softmax: 0.000758 log_softmax: 0.000360\n",
            "Train Epoch: 376 [1000/1000 (100%)]\tLosses F.softmax: 0.000338 log_softmax: 0.000770\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1316\tAccuracy: 8102.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1621\tAccuracy: 8049.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 377 [0/1000 (0%)]\tLosses F.softmax: 0.000005 log_softmax: 0.000002\n",
            "Train Epoch: 377 [200/1000 (20%)]\tLosses F.softmax: 0.001880 log_softmax: 0.005116\n",
            "Train Epoch: 377 [400/1000 (40%)]\tLosses F.softmax: 0.050359 log_softmax: 0.068878\n",
            "Train Epoch: 377 [600/1000 (60%)]\tLosses F.softmax: 0.000004 log_softmax: 0.000000\n",
            "Train Epoch: 377 [800/1000 (80%)]\tLosses F.softmax: 0.000203 log_softmax: 0.000111\n",
            "Train Epoch: 377 [1000/1000 (100%)]\tLosses F.softmax: 0.027543 log_softmax: 0.012959\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1314\tAccuracy: 8108.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1621\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 378 [0/1000 (0%)]\tLosses F.softmax: 0.000009 log_softmax: 0.001281\n",
            "Train Epoch: 378 [200/1000 (20%)]\tLosses F.softmax: 0.000004 log_softmax: 0.000000\n",
            "Train Epoch: 378 [400/1000 (40%)]\tLosses F.softmax: 0.003027 log_softmax: 0.005658\n",
            "Train Epoch: 378 [600/1000 (60%)]\tLosses F.softmax: 0.000156 log_softmax: 0.000081\n",
            "Train Epoch: 378 [800/1000 (80%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000001\n",
            "Train Epoch: 378 [1000/1000 (100%)]\tLosses F.softmax: 0.002217 log_softmax: 0.000066\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1323\tAccuracy: 8103.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1629\tAccuracy: 8050.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 379 [0/1000 (0%)]\tLosses F.softmax: 0.012118 log_softmax: 0.002761\n",
            "Train Epoch: 379 [200/1000 (20%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000000\n",
            "Train Epoch: 379 [400/1000 (40%)]\tLosses F.softmax: 0.006182 log_softmax: 0.005697\n",
            "Train Epoch: 379 [600/1000 (60%)]\tLosses F.softmax: 0.000003 log_softmax: 0.000000\n",
            "Train Epoch: 379 [800/1000 (80%)]\tLosses F.softmax: 0.007785 log_softmax: 0.007806\n",
            "Train Epoch: 379 [1000/1000 (100%)]\tLosses F.softmax: 0.002616 log_softmax: 0.001718\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1318\tAccuracy: 8113.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1626\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 380 [0/1000 (0%)]\tLosses F.softmax: 0.000046 log_softmax: 0.000000\n",
            "Train Epoch: 380 [200/1000 (20%)]\tLosses F.softmax: 0.009750 log_softmax: 0.006640\n",
            "Train Epoch: 380 [400/1000 (40%)]\tLosses F.softmax: 0.000145 log_softmax: 0.003252\n",
            "Train Epoch: 380 [600/1000 (60%)]\tLosses F.softmax: 0.008316 log_softmax: 0.013728\n",
            "Train Epoch: 380 [800/1000 (80%)]\tLosses F.softmax: 0.000104 log_softmax: 0.000013\n",
            "Train Epoch: 380 [1000/1000 (100%)]\tLosses F.softmax: 0.039595 log_softmax: 0.036927\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1326\tAccuracy: 8109.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1633\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 381 [0/1000 (0%)]\tLosses F.softmax: 0.000087 log_softmax: 0.000700\n",
            "Train Epoch: 381 [200/1000 (20%)]\tLosses F.softmax: 0.003250 log_softmax: 0.001966\n",
            "Train Epoch: 381 [400/1000 (40%)]\tLosses F.softmax: 0.001705 log_softmax: 0.000111\n",
            "Train Epoch: 381 [600/1000 (60%)]\tLosses F.softmax: 0.000032 log_softmax: 0.000008\n",
            "Train Epoch: 381 [800/1000 (80%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000004\n",
            "Train Epoch: 381 [1000/1000 (100%)]\tLosses F.softmax: 0.002112 log_softmax: 0.000373\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1334\tAccuracy: 8104.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1640\tAccuracy: 8052.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 382 [0/1000 (0%)]\tLosses F.softmax: 0.000006 log_softmax: 0.000000\n",
            "Train Epoch: 382 [200/1000 (20%)]\tLosses F.softmax: 0.000006 log_softmax: 0.000002\n",
            "Train Epoch: 382 [400/1000 (40%)]\tLosses F.softmax: 0.001255 log_softmax: 0.001006\n",
            "Train Epoch: 382 [600/1000 (60%)]\tLosses F.softmax: 0.000406 log_softmax: 0.000409\n",
            "Train Epoch: 382 [800/1000 (80%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000000\n",
            "Train Epoch: 382 [1000/1000 (100%)]\tLosses F.softmax: 0.000146 log_softmax: 0.000207\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1334\tAccuracy: 8110.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1640\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 383 [0/1000 (0%)]\tLosses F.softmax: 0.000005 log_softmax: 0.000004\n",
            "Train Epoch: 383 [200/1000 (20%)]\tLosses F.softmax: 0.000560 log_softmax: 0.001939\n",
            "Train Epoch: 383 [400/1000 (40%)]\tLosses F.softmax: 0.000917 log_softmax: 0.000215\n",
            "Train Epoch: 383 [600/1000 (60%)]\tLosses F.softmax: 0.045800 log_softmax: 0.065798\n",
            "Train Epoch: 383 [800/1000 (80%)]\tLosses F.softmax: 0.001693 log_softmax: 0.002929\n",
            "Train Epoch: 383 [1000/1000 (100%)]\tLosses F.softmax: 0.000009 log_softmax: 0.000011\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1335\tAccuracy: 8114.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1642\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 384 [0/1000 (0%)]\tLosses F.softmax: 0.000709 log_softmax: 0.001788\n",
            "Train Epoch: 384 [200/1000 (20%)]\tLosses F.softmax: 0.000017 log_softmax: 0.000009\n",
            "Train Epoch: 384 [400/1000 (40%)]\tLosses F.softmax: 0.000100 log_softmax: 0.000013\n",
            "Train Epoch: 384 [600/1000 (60%)]\tLosses F.softmax: 0.001778 log_softmax: 0.000517\n",
            "Train Epoch: 384 [800/1000 (80%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000001\n",
            "Train Epoch: 384 [1000/1000 (100%)]\tLosses F.softmax: 0.000025 log_softmax: 0.000447\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1343\tAccuracy: 8110.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1648\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 385 [0/1000 (0%)]\tLosses F.softmax: 0.000933 log_softmax: 0.002079\n",
            "Train Epoch: 385 [200/1000 (20%)]\tLosses F.softmax: 0.001427 log_softmax: 0.001062\n",
            "Train Epoch: 385 [400/1000 (40%)]\tLosses F.softmax: 0.001163 log_softmax: 0.002800\n",
            "Train Epoch: 385 [600/1000 (60%)]\tLosses F.softmax: 0.004536 log_softmax: 0.000050\n",
            "Train Epoch: 385 [800/1000 (80%)]\tLosses F.softmax: 0.002732 log_softmax: 0.001224\n",
            "Train Epoch: 385 [1000/1000 (100%)]\tLosses F.softmax: 0.000353 log_softmax: 0.000082\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1347\tAccuracy: 8108.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1655\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 386 [0/1000 (0%)]\tLosses F.softmax: 0.000003 log_softmax: 0.000000\n",
            "Train Epoch: 386 [200/1000 (20%)]\tLosses F.softmax: 0.033896 log_softmax: 0.014766\n",
            "Train Epoch: 386 [400/1000 (40%)]\tLosses F.softmax: 0.000002 log_softmax: 0.000003\n",
            "Train Epoch: 386 [600/1000 (60%)]\tLosses F.softmax: 0.000008 log_softmax: 0.000007\n",
            "Train Epoch: 386 [800/1000 (80%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000000\n",
            "Train Epoch: 386 [1000/1000 (100%)]\tLosses F.softmax: 0.002087 log_softmax: 0.000366\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1354\tAccuracy: 8105.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1661\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 387 [0/1000 (0%)]\tLosses F.softmax: 0.000435 log_softmax: 0.001522\n",
            "Train Epoch: 387 [200/1000 (20%)]\tLosses F.softmax: 0.000058 log_softmax: 0.000006\n",
            "Train Epoch: 387 [400/1000 (40%)]\tLosses F.softmax: 0.000278 log_softmax: 0.000001\n",
            "Train Epoch: 387 [600/1000 (60%)]\tLosses F.softmax: 0.000291 log_softmax: 0.000268\n",
            "Train Epoch: 387 [800/1000 (80%)]\tLosses F.softmax: 0.000239 log_softmax: 0.000523\n",
            "Train Epoch: 387 [1000/1000 (100%)]\tLosses F.softmax: 0.000051 log_softmax: 0.000023\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1357\tAccuracy: 8106.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1665\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 388 [0/1000 (0%)]\tLosses F.softmax: 0.001093 log_softmax: 0.000342\n",
            "Train Epoch: 388 [200/1000 (20%)]\tLosses F.softmax: 0.005708 log_softmax: 0.004512\n",
            "Train Epoch: 388 [400/1000 (40%)]\tLosses F.softmax: 0.000986 log_softmax: 0.000165\n",
            "Train Epoch: 388 [600/1000 (60%)]\tLosses F.softmax: 0.003772 log_softmax: 0.003422\n",
            "Train Epoch: 388 [800/1000 (80%)]\tLosses F.softmax: 0.000216 log_softmax: 0.000052\n",
            "Train Epoch: 388 [1000/1000 (100%)]\tLosses F.softmax: 0.000675 log_softmax: 0.001845\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1361\tAccuracy: 8105.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1671\tAccuracy: 8049.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 389 [0/1000 (0%)]\tLosses F.softmax: 0.020278 log_softmax: 0.026286\n",
            "Train Epoch: 389 [200/1000 (20%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000002\n",
            "Train Epoch: 389 [400/1000 (40%)]\tLosses F.softmax: 0.000005 log_softmax: 0.000003\n",
            "Train Epoch: 389 [600/1000 (60%)]\tLosses F.softmax: 0.006292 log_softmax: 0.003553\n",
            "Train Epoch: 389 [800/1000 (80%)]\tLosses F.softmax: 0.013046 log_softmax: 0.015483\n",
            "Train Epoch: 389 [1000/1000 (100%)]\tLosses F.softmax: 0.006532 log_softmax: 0.000084\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1365\tAccuracy: 8105.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1675\tAccuracy: 8050.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 390 [0/1000 (0%)]\tLosses F.softmax: 0.003561 log_softmax: 0.003286\n",
            "Train Epoch: 390 [200/1000 (20%)]\tLosses F.softmax: 0.007811 log_softmax: 0.009316\n",
            "Train Epoch: 390 [400/1000 (40%)]\tLosses F.softmax: 0.000357 log_softmax: 0.000359\n",
            "Train Epoch: 390 [600/1000 (60%)]\tLosses F.softmax: 0.024490 log_softmax: 0.021390\n",
            "Train Epoch: 390 [800/1000 (80%)]\tLosses F.softmax: 0.000040 log_softmax: 0.000167\n",
            "Train Epoch: 390 [1000/1000 (100%)]\tLosses F.softmax: 0.025020 log_softmax: 0.019685\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1362\tAccuracy: 8111.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1673\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 391 [0/1000 (0%)]\tLosses F.softmax: 0.001947 log_softmax: 0.000007\n",
            "Train Epoch: 391 [200/1000 (20%)]\tLosses F.softmax: 0.000012 log_softmax: 0.000008\n",
            "Train Epoch: 391 [400/1000 (40%)]\tLosses F.softmax: 0.000103 log_softmax: 0.000080\n",
            "Train Epoch: 391 [600/1000 (60%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000000\n",
            "Train Epoch: 391 [800/1000 (80%)]\tLosses F.softmax: 0.000826 log_softmax: 0.000106\n",
            "Train Epoch: 391 [1000/1000 (100%)]\tLosses F.softmax: 0.007941 log_softmax: 0.009077\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1367\tAccuracy: 8109.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1679\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 392 [0/1000 (0%)]\tLosses F.softmax: 0.000044 log_softmax: 0.000298\n",
            "Train Epoch: 392 [200/1000 (20%)]\tLosses F.softmax: 0.000549 log_softmax: 0.000424\n",
            "Train Epoch: 392 [400/1000 (40%)]\tLosses F.softmax: 0.001613 log_softmax: 0.000332\n",
            "Train Epoch: 392 [600/1000 (60%)]\tLosses F.softmax: 0.000093 log_softmax: 0.000032\n",
            "Train Epoch: 392 [800/1000 (80%)]\tLosses F.softmax: 0.000098 log_softmax: 0.000006\n",
            "Train Epoch: 392 [1000/1000 (100%)]\tLosses F.softmax: 0.000434 log_softmax: 0.000410\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1367\tAccuracy: 8113.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1679\tAccuracy: 8052.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 393 [0/1000 (0%)]\tLosses F.softmax: 0.000014 log_softmax: 0.000004\n",
            "Train Epoch: 393 [200/1000 (20%)]\tLosses F.softmax: 0.001594 log_softmax: 0.001190\n",
            "Train Epoch: 393 [400/1000 (40%)]\tLosses F.softmax: 0.000051 log_softmax: 0.000293\n",
            "Train Epoch: 393 [600/1000 (60%)]\tLosses F.softmax: 0.000032 log_softmax: 0.000030\n",
            "Train Epoch: 393 [800/1000 (80%)]\tLosses F.softmax: 0.000018 log_softmax: 0.000003\n",
            "Train Epoch: 393 [1000/1000 (100%)]\tLosses F.softmax: 0.010053 log_softmax: 0.014031\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1376\tAccuracy: 8106.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1685\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 394 [0/1000 (0%)]\tLosses F.softmax: 0.000143 log_softmax: 0.000073\n",
            "Train Epoch: 394 [200/1000 (20%)]\tLosses F.softmax: 0.023460 log_softmax: 0.014149\n",
            "Train Epoch: 394 [400/1000 (40%)]\tLosses F.softmax: 0.000159 log_softmax: 0.000080\n",
            "Train Epoch: 394 [600/1000 (60%)]\tLosses F.softmax: 0.000536 log_softmax: 0.000563\n",
            "Train Epoch: 394 [800/1000 (80%)]\tLosses F.softmax: 0.000013 log_softmax: 0.000003\n",
            "Train Epoch: 394 [1000/1000 (100%)]\tLosses F.softmax: 0.027062 log_softmax: 0.032473\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1377\tAccuracy: 8110.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1687\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 395 [0/1000 (0%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000004\n",
            "Train Epoch: 395 [200/1000 (20%)]\tLosses F.softmax: 0.016851 log_softmax: 0.010379\n",
            "Train Epoch: 395 [400/1000 (40%)]\tLosses F.softmax: 0.000047 log_softmax: 0.000184\n",
            "Train Epoch: 395 [600/1000 (60%)]\tLosses F.softmax: 0.000042 log_softmax: 0.000023\n",
            "Train Epoch: 395 [800/1000 (80%)]\tLosses F.softmax: 0.000032 log_softmax: 0.000007\n",
            "Train Epoch: 395 [1000/1000 (100%)]\tLosses F.softmax: 0.000117 log_softmax: 0.000002\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1387\tAccuracy: 8106.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1699\tAccuracy: 8047.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 396 [0/1000 (0%)]\tLosses F.softmax: 0.001809 log_softmax: 0.000609\n",
            "Train Epoch: 396 [200/1000 (20%)]\tLosses F.softmax: 0.002070 log_softmax: 0.000376\n",
            "Train Epoch: 396 [400/1000 (40%)]\tLosses F.softmax: 0.000678 log_softmax: 0.000519\n",
            "Train Epoch: 396 [600/1000 (60%)]\tLosses F.softmax: 0.010241 log_softmax: 0.009365\n",
            "Train Epoch: 396 [800/1000 (80%)]\tLosses F.softmax: 0.015758 log_softmax: 0.015982\n",
            "Train Epoch: 396 [1000/1000 (100%)]\tLosses F.softmax: 0.000036 log_softmax: 0.000002\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1390\tAccuracy: 8104.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1699\tAccuracy: 8051.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 397 [0/1000 (0%)]\tLosses F.softmax: 0.000605 log_softmax: 0.000091\n",
            "Train Epoch: 397 [200/1000 (20%)]\tLosses F.softmax: 0.005997 log_softmax: 0.011094\n",
            "Train Epoch: 397 [400/1000 (40%)]\tLosses F.softmax: 0.001152 log_softmax: 0.000748\n",
            "Train Epoch: 397 [600/1000 (60%)]\tLosses F.softmax: 0.000103 log_softmax: 0.000009\n",
            "Train Epoch: 397 [800/1000 (80%)]\tLosses F.softmax: 0.000021 log_softmax: 0.000318\n",
            "Train Epoch: 397 [1000/1000 (100%)]\tLosses F.softmax: 0.007356 log_softmax: 0.009512\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1394\tAccuracy: 8108.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1703\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 398 [0/1000 (0%)]\tLosses F.softmax: 0.000049 log_softmax: 0.000129\n",
            "Train Epoch: 398 [200/1000 (20%)]\tLosses F.softmax: 0.000035 log_softmax: 0.000011\n",
            "Train Epoch: 398 [400/1000 (40%)]\tLosses F.softmax: 0.004945 log_softmax: 0.000115\n",
            "Train Epoch: 398 [600/1000 (60%)]\tLosses F.softmax: 0.000007 log_softmax: 0.000004\n",
            "Train Epoch: 398 [800/1000 (80%)]\tLosses F.softmax: 0.024838 log_softmax: 0.012063\n",
            "Train Epoch: 398 [1000/1000 (100%)]\tLosses F.softmax: 0.010858 log_softmax: 0.001679\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1398\tAccuracy: 8107.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1707\tAccuracy: 8052.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 399 [0/1000 (0%)]\tLosses F.softmax: 0.000074 log_softmax: 0.000021\n",
            "Train Epoch: 399 [200/1000 (20%)]\tLosses F.softmax: 0.010011 log_softmax: 0.013616\n",
            "Train Epoch: 399 [400/1000 (40%)]\tLosses F.softmax: 0.006631 log_softmax: 0.012233\n",
            "Train Epoch: 399 [600/1000 (60%)]\tLosses F.softmax: 0.000160 log_softmax: 0.000133\n",
            "Train Epoch: 399 [800/1000 (80%)]\tLosses F.softmax: 0.012654 log_softmax: 0.009330\n",
            "Train Epoch: 399 [1000/1000 (100%)]\tLosses F.softmax: 0.000051 log_softmax: 0.000014\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1400\tAccuracy: 8110.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1711\tAccuracy: 8051.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 400 [0/1000 (0%)]\tLosses F.softmax: 0.000016 log_softmax: 0.000012\n",
            "Train Epoch: 400 [200/1000 (20%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000003\n",
            "Train Epoch: 400 [400/1000 (40%)]\tLosses F.softmax: 0.000137 log_softmax: 0.000197\n",
            "Train Epoch: 400 [600/1000 (60%)]\tLosses F.softmax: 0.003094 log_softmax: 0.005492\n",
            "Train Epoch: 400 [800/1000 (80%)]\tLosses F.softmax: 0.006559 log_softmax: 0.003947\n",
            "Train Epoch: 400 [1000/1000 (100%)]\tLosses F.softmax: 0.000099 log_softmax: 0.000132\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1403\tAccuracy: 8108.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1715\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 401 [0/1000 (0%)]\tLosses F.softmax: 0.010820 log_softmax: 0.004132\n",
            "Train Epoch: 401 [200/1000 (20%)]\tLosses F.softmax: 0.000005 log_softmax: 0.000005\n",
            "Train Epoch: 401 [400/1000 (40%)]\tLosses F.softmax: 0.000727 log_softmax: 0.000819\n",
            "Train Epoch: 401 [600/1000 (60%)]\tLosses F.softmax: 0.000004 log_softmax: 0.002921\n",
            "Train Epoch: 401 [800/1000 (80%)]\tLosses F.softmax: 0.002583 log_softmax: 0.000486\n",
            "Train Epoch: 401 [1000/1000 (100%)]\tLosses F.softmax: 0.000228 log_softmax: 0.000110\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1407\tAccuracy: 8109.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1718\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 402 [0/1000 (0%)]\tLosses F.softmax: 0.002942 log_softmax: 0.001547\n",
            "Train Epoch: 402 [200/1000 (20%)]\tLosses F.softmax: 0.000004 log_softmax: 0.000000\n",
            "Train Epoch: 402 [400/1000 (40%)]\tLosses F.softmax: 0.006767 log_softmax: 0.010509\n",
            "Train Epoch: 402 [600/1000 (60%)]\tLosses F.softmax: 0.000686 log_softmax: 0.000143\n",
            "Train Epoch: 402 [800/1000 (80%)]\tLosses F.softmax: 0.000113 log_softmax: 0.000017\n",
            "Train Epoch: 402 [1000/1000 (100%)]\tLosses F.softmax: 0.000422 log_softmax: 0.000102\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1409\tAccuracy: 8111.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1721\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 403 [0/1000 (0%)]\tLosses F.softmax: 0.000106 log_softmax: 0.000018\n",
            "Train Epoch: 403 [200/1000 (20%)]\tLosses F.softmax: 0.006419 log_softmax: 0.009060\n",
            "Train Epoch: 403 [400/1000 (40%)]\tLosses F.softmax: 0.000151 log_softmax: 0.000240\n",
            "Train Epoch: 403 [600/1000 (60%)]\tLosses F.softmax: 0.004709 log_softmax: 0.002551\n",
            "Train Epoch: 403 [800/1000 (80%)]\tLosses F.softmax: 0.001561 log_softmax: 0.001506\n",
            "Train Epoch: 403 [1000/1000 (100%)]\tLosses F.softmax: 0.000002 log_softmax: 0.000000\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1413\tAccuracy: 8111.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1726\tAccuracy: 8051.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 404 [0/1000 (0%)]\tLosses F.softmax: 0.012648 log_softmax: 0.033772\n",
            "Train Epoch: 404 [200/1000 (20%)]\tLosses F.softmax: 0.000002 log_softmax: 0.000000\n",
            "Train Epoch: 404 [400/1000 (40%)]\tLosses F.softmax: 0.000709 log_softmax: 0.001037\n",
            "Train Epoch: 404 [600/1000 (60%)]\tLosses F.softmax: 0.000091 log_softmax: 0.000005\n",
            "Train Epoch: 404 [800/1000 (80%)]\tLosses F.softmax: 0.000994 log_softmax: 0.001037\n",
            "Train Epoch: 404 [1000/1000 (100%)]\tLosses F.softmax: 0.002393 log_softmax: 0.002885\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1416\tAccuracy: 8112.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1728\tAccuracy: 8057.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 405 [0/1000 (0%)]\tLosses F.softmax: 0.008731 log_softmax: 0.003458\n",
            "Train Epoch: 405 [200/1000 (20%)]\tLosses F.softmax: 0.005177 log_softmax: 0.002429\n",
            "Train Epoch: 405 [400/1000 (40%)]\tLosses F.softmax: 0.007649 log_softmax: 0.007593\n",
            "Train Epoch: 405 [600/1000 (60%)]\tLosses F.softmax: 0.015662 log_softmax: 0.003120\n",
            "Train Epoch: 405 [800/1000 (80%)]\tLosses F.softmax: 0.015542 log_softmax: 0.015106\n",
            "Train Epoch: 405 [1000/1000 (100%)]\tLosses F.softmax: 0.002454 log_softmax: 0.004694\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1422\tAccuracy: 8109.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1735\tAccuracy: 8051.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 406 [0/1000 (0%)]\tLosses F.softmax: 0.000005 log_softmax: 0.000003\n",
            "Train Epoch: 406 [200/1000 (20%)]\tLosses F.softmax: 0.000002 log_softmax: 0.000000\n",
            "Train Epoch: 406 [400/1000 (40%)]\tLosses F.softmax: 0.007685 log_softmax: 0.008862\n",
            "Train Epoch: 406 [600/1000 (60%)]\tLosses F.softmax: 0.000124 log_softmax: 0.000293\n",
            "Train Epoch: 406 [800/1000 (80%)]\tLosses F.softmax: 0.000096 log_softmax: 0.000020\n",
            "Train Epoch: 406 [1000/1000 (100%)]\tLosses F.softmax: 0.006516 log_softmax: 0.010457\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1427\tAccuracy: 8112.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1740\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 407 [0/1000 (0%)]\tLosses F.softmax: 0.008223 log_softmax: 0.009278\n",
            "Train Epoch: 407 [200/1000 (20%)]\tLosses F.softmax: 0.005399 log_softmax: 0.006154\n",
            "Train Epoch: 407 [400/1000 (40%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000001\n",
            "Train Epoch: 407 [600/1000 (60%)]\tLosses F.softmax: 0.007212 log_softmax: 0.004327\n",
            "Train Epoch: 407 [800/1000 (80%)]\tLosses F.softmax: 0.000105 log_softmax: 0.000199\n",
            "Train Epoch: 407 [1000/1000 (100%)]\tLosses F.softmax: 0.005662 log_softmax: 0.002730\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1427\tAccuracy: 8110.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1741\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 408 [0/1000 (0%)]\tLosses F.softmax: 0.009368 log_softmax: 0.014335\n",
            "Train Epoch: 408 [200/1000 (20%)]\tLosses F.softmax: 0.001709 log_softmax: 0.003456\n",
            "Train Epoch: 408 [400/1000 (40%)]\tLosses F.softmax: 0.000084 log_softmax: 0.000038\n",
            "Train Epoch: 408 [600/1000 (60%)]\tLosses F.softmax: 0.018728 log_softmax: 0.018114\n",
            "Train Epoch: 408 [800/1000 (80%)]\tLosses F.softmax: 0.000002 log_softmax: 0.000000\n",
            "Train Epoch: 408 [1000/1000 (100%)]\tLosses F.softmax: 0.000080 log_softmax: 0.000677\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1426\tAccuracy: 8119.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1743\tAccuracy: 8057.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 409 [0/1000 (0%)]\tLosses F.softmax: 0.003196 log_softmax: 0.003087\n",
            "Train Epoch: 409 [200/1000 (20%)]\tLosses F.softmax: 0.001460 log_softmax: 0.002913\n",
            "Train Epoch: 409 [400/1000 (40%)]\tLosses F.softmax: 0.000021 log_softmax: 0.000024\n",
            "Train Epoch: 409 [600/1000 (60%)]\tLosses F.softmax: 0.000239 log_softmax: 0.000001\n",
            "Train Epoch: 409 [800/1000 (80%)]\tLosses F.softmax: 0.000125 log_softmax: 0.000062\n",
            "Train Epoch: 409 [1000/1000 (100%)]\tLosses F.softmax: 0.000042 log_softmax: 0.000176\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1434\tAccuracy: 8110.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1750\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 410 [0/1000 (0%)]\tLosses F.softmax: 0.000252 log_softmax: 0.004540\n",
            "Train Epoch: 410 [200/1000 (20%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000003\n",
            "Train Epoch: 410 [400/1000 (40%)]\tLosses F.softmax: 0.000241 log_softmax: 0.000267\n",
            "Train Epoch: 410 [600/1000 (60%)]\tLosses F.softmax: 0.018754 log_softmax: 0.014528\n",
            "Train Epoch: 410 [800/1000 (80%)]\tLosses F.softmax: 0.011678 log_softmax: 0.011403\n",
            "Train Epoch: 410 [1000/1000 (100%)]\tLosses F.softmax: 0.000147 log_softmax: 0.001364\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1434\tAccuracy: 8119.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1749\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 411 [0/1000 (0%)]\tLosses F.softmax: 0.000051 log_softmax: 0.000236\n",
            "Train Epoch: 411 [200/1000 (20%)]\tLosses F.softmax: 0.000028 log_softmax: 0.000072\n",
            "Train Epoch: 411 [400/1000 (40%)]\tLosses F.softmax: 0.000175 log_softmax: 0.000416\n",
            "Train Epoch: 411 [600/1000 (60%)]\tLosses F.softmax: 0.000340 log_softmax: 0.000126\n",
            "Train Epoch: 411 [800/1000 (80%)]\tLosses F.softmax: 0.008395 log_softmax: 0.007001\n",
            "Train Epoch: 411 [1000/1000 (100%)]\tLosses F.softmax: 0.001326 log_softmax: 0.005220\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1442\tAccuracy: 8108.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1759\tAccuracy: 8049.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 412 [0/1000 (0%)]\tLosses F.softmax: 0.000202 log_softmax: 0.000700\n",
            "Train Epoch: 412 [200/1000 (20%)]\tLosses F.softmax: 0.005938 log_softmax: 0.003143\n",
            "Train Epoch: 412 [400/1000 (40%)]\tLosses F.softmax: 0.028134 log_softmax: 0.033229\n",
            "Train Epoch: 412 [600/1000 (60%)]\tLosses F.softmax: 0.002519 log_softmax: 0.003126\n",
            "Train Epoch: 412 [800/1000 (80%)]\tLosses F.softmax: 0.000010 log_softmax: 0.000072\n",
            "Train Epoch: 412 [1000/1000 (100%)]\tLosses F.softmax: 0.000477 log_softmax: 0.000003\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1445\tAccuracy: 8111.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1762\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 413 [0/1000 (0%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000003\n",
            "Train Epoch: 413 [200/1000 (20%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000000\n",
            "Train Epoch: 413 [400/1000 (40%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000000\n",
            "Train Epoch: 413 [600/1000 (60%)]\tLosses F.softmax: 0.000005 log_softmax: 0.000000\n",
            "Train Epoch: 413 [800/1000 (80%)]\tLosses F.softmax: 0.000007 log_softmax: 0.000001\n",
            "Train Epoch: 413 [1000/1000 (100%)]\tLosses F.softmax: 0.013636 log_softmax: 0.004199\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1451\tAccuracy: 8109.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1767\tAccuracy: 8050.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 414 [0/1000 (0%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000000\n",
            "Train Epoch: 414 [200/1000 (20%)]\tLosses F.softmax: 0.000234 log_softmax: 0.000409\n",
            "Train Epoch: 414 [400/1000 (40%)]\tLosses F.softmax: 0.010400 log_softmax: 0.012009\n",
            "Train Epoch: 414 [600/1000 (60%)]\tLosses F.softmax: 0.001145 log_softmax: 0.000045\n",
            "Train Epoch: 414 [800/1000 (80%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000000\n",
            "Train Epoch: 414 [1000/1000 (100%)]\tLosses F.softmax: 0.000037 log_softmax: 0.000021\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1453\tAccuracy: 8112.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1769\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 415 [0/1000 (0%)]\tLosses F.softmax: 0.000105 log_softmax: 0.000207\n",
            "Train Epoch: 415 [200/1000 (20%)]\tLosses F.softmax: 0.016064 log_softmax: 0.015893\n",
            "Train Epoch: 415 [400/1000 (40%)]\tLosses F.softmax: 0.014397 log_softmax: 0.013172\n",
            "Train Epoch: 415 [600/1000 (60%)]\tLosses F.softmax: 0.000043 log_softmax: 0.000018\n",
            "Train Epoch: 415 [800/1000 (80%)]\tLosses F.softmax: 0.000013 log_softmax: 0.000003\n",
            "Train Epoch: 415 [1000/1000 (100%)]\tLosses F.softmax: 0.000007 log_softmax: 0.000004\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1458\tAccuracy: 8112.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1772\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 416 [0/1000 (0%)]\tLosses F.softmax: 0.000779 log_softmax: 0.000258\n",
            "Train Epoch: 416 [200/1000 (20%)]\tLosses F.softmax: 0.004511 log_softmax: 0.006634\n",
            "Train Epoch: 416 [400/1000 (40%)]\tLosses F.softmax: 0.016159 log_softmax: 0.015514\n",
            "Train Epoch: 416 [600/1000 (60%)]\tLosses F.softmax: 0.000166 log_softmax: 0.000401\n",
            "Train Epoch: 416 [800/1000 (80%)]\tLosses F.softmax: 0.011688 log_softmax: 0.009289\n",
            "Train Epoch: 416 [1000/1000 (100%)]\tLosses F.softmax: 0.002217 log_softmax: 0.000008\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1462\tAccuracy: 8112.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1778\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 417 [0/1000 (0%)]\tLosses F.softmax: 0.015256 log_softmax: 0.014995\n",
            "Train Epoch: 417 [200/1000 (20%)]\tLosses F.softmax: 0.000224 log_softmax: 0.001622\n",
            "Train Epoch: 417 [400/1000 (40%)]\tLosses F.softmax: 0.001677 log_softmax: 0.003387\n",
            "Train Epoch: 417 [600/1000 (60%)]\tLosses F.softmax: 0.000567 log_softmax: 0.004141\n",
            "Train Epoch: 417 [800/1000 (80%)]\tLosses F.softmax: 0.000079 log_softmax: 0.000036\n",
            "Train Epoch: 417 [1000/1000 (100%)]\tLosses F.softmax: 0.000034 log_softmax: 0.000021\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1462\tAccuracy: 8114.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1779\tAccuracy: 8057.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 418 [0/1000 (0%)]\tLosses F.softmax: 0.027519 log_softmax: 0.033189\n",
            "Train Epoch: 418 [200/1000 (20%)]\tLosses F.softmax: 0.004415 log_softmax: 0.009435\n",
            "Train Epoch: 418 [400/1000 (40%)]\tLosses F.softmax: 0.001881 log_softmax: 0.001930\n",
            "Train Epoch: 418 [600/1000 (60%)]\tLosses F.softmax: 0.000010 log_softmax: 0.000002\n",
            "Train Epoch: 418 [800/1000 (80%)]\tLosses F.softmax: 0.000891 log_softmax: 0.000382\n",
            "Train Epoch: 418 [1000/1000 (100%)]\tLosses F.softmax: 0.002210 log_softmax: 0.000421\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1466\tAccuracy: 8117.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1781\tAccuracy: 8057.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 419 [0/1000 (0%)]\tLosses F.softmax: 0.013983 log_softmax: 0.017905\n",
            "Train Epoch: 419 [200/1000 (20%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000000\n",
            "Train Epoch: 419 [400/1000 (40%)]\tLosses F.softmax: 0.000425 log_softmax: 0.001982\n",
            "Train Epoch: 419 [600/1000 (60%)]\tLosses F.softmax: 0.023053 log_softmax: 0.023358\n",
            "Train Epoch: 419 [800/1000 (80%)]\tLosses F.softmax: 0.025159 log_softmax: 0.030209\n",
            "Train Epoch: 419 [1000/1000 (100%)]\tLosses F.softmax: 0.000281 log_softmax: 0.001713\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1472\tAccuracy: 8110.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1789\tAccuracy: 8052.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 420 [0/1000 (0%)]\tLosses F.softmax: 0.000006 log_softmax: 0.000007\n",
            "Train Epoch: 420 [200/1000 (20%)]\tLosses F.softmax: 0.000104 log_softmax: 0.000062\n",
            "Train Epoch: 420 [400/1000 (40%)]\tLosses F.softmax: 0.005949 log_softmax: 0.004804\n",
            "Train Epoch: 420 [600/1000 (60%)]\tLosses F.softmax: 0.000089 log_softmax: 0.000019\n",
            "Train Epoch: 420 [800/1000 (80%)]\tLosses F.softmax: 0.000490 log_softmax: 0.000028\n",
            "Train Epoch: 420 [1000/1000 (100%)]\tLosses F.softmax: 0.001808 log_softmax: 0.001156\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1479\tAccuracy: 8109.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1796\tAccuracy: 8052.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 421 [0/1000 (0%)]\tLosses F.softmax: 0.002194 log_softmax: 0.002687\n",
            "Train Epoch: 421 [200/1000 (20%)]\tLosses F.softmax: 0.000092 log_softmax: 0.000008\n",
            "Train Epoch: 421 [400/1000 (40%)]\tLosses F.softmax: 0.003958 log_softmax: 0.000042\n",
            "Train Epoch: 421 [600/1000 (60%)]\tLosses F.softmax: 0.000200 log_softmax: 0.000022\n",
            "Train Epoch: 421 [800/1000 (80%)]\tLosses F.softmax: 0.000004 log_softmax: 0.000001\n",
            "Train Epoch: 421 [1000/1000 (100%)]\tLosses F.softmax: 0.000358 log_softmax: 0.000033\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1479\tAccuracy: 8110.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1798\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 422 [0/1000 (0%)]\tLosses F.softmax: 0.021771 log_softmax: 0.008229\n",
            "Train Epoch: 422 [200/1000 (20%)]\tLosses F.softmax: 0.008747 log_softmax: 0.008079\n",
            "Train Epoch: 422 [400/1000 (40%)]\tLosses F.softmax: 0.000478 log_softmax: 0.002526\n",
            "Train Epoch: 422 [600/1000 (60%)]\tLosses F.softmax: 0.000832 log_softmax: 0.000126\n",
            "Train Epoch: 422 [800/1000 (80%)]\tLosses F.softmax: 0.023219 log_softmax: 0.010373\n",
            "Train Epoch: 422 [1000/1000 (100%)]\tLosses F.softmax: 0.000044 log_softmax: 0.000093\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1484\tAccuracy: 8108.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1802\tAccuracy: 8051.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 423 [0/1000 (0%)]\tLosses F.softmax: 0.000026 log_softmax: 0.000018\n",
            "Train Epoch: 423 [200/1000 (20%)]\tLosses F.softmax: 0.009970 log_softmax: 0.004512\n",
            "Train Epoch: 423 [400/1000 (40%)]\tLosses F.softmax: 0.012423 log_softmax: 0.012678\n",
            "Train Epoch: 423 [600/1000 (60%)]\tLosses F.softmax: 0.003460 log_softmax: 0.002996\n",
            "Train Epoch: 423 [800/1000 (80%)]\tLosses F.softmax: 0.002236 log_softmax: 0.000422\n",
            "Train Epoch: 423 [1000/1000 (100%)]\tLosses F.softmax: 0.000002 log_softmax: 0.000000\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1487\tAccuracy: 8112.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1806\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 424 [0/1000 (0%)]\tLosses F.softmax: 0.015617 log_softmax: 0.009224\n",
            "Train Epoch: 424 [200/1000 (20%)]\tLosses F.softmax: 0.000562 log_softmax: 0.001672\n",
            "Train Epoch: 424 [400/1000 (40%)]\tLosses F.softmax: 0.000027 log_softmax: 0.000003\n",
            "Train Epoch: 424 [600/1000 (60%)]\tLosses F.softmax: 0.000031 log_softmax: 0.000230\n",
            "Train Epoch: 424 [800/1000 (80%)]\tLosses F.softmax: 0.012662 log_softmax: 0.020272\n",
            "Train Epoch: 424 [1000/1000 (100%)]\tLosses F.softmax: 0.002150 log_softmax: 0.001004\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1490\tAccuracy: 8110.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1811\tAccuracy: 8052.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 425 [0/1000 (0%)]\tLosses F.softmax: 0.000138 log_softmax: 0.000215\n",
            "Train Epoch: 425 [200/1000 (20%)]\tLosses F.softmax: 0.000010 log_softmax: 0.000000\n",
            "Train Epoch: 425 [400/1000 (40%)]\tLosses F.softmax: 0.000041 log_softmax: 0.000210\n",
            "Train Epoch: 425 [600/1000 (60%)]\tLosses F.softmax: 0.005290 log_softmax: 0.007054\n",
            "Train Epoch: 425 [800/1000 (80%)]\tLosses F.softmax: 0.001793 log_softmax: 0.000621\n",
            "Train Epoch: 425 [1000/1000 (100%)]\tLosses F.softmax: 0.000244 log_softmax: 0.000223\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1493\tAccuracy: 8113.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1811\tAccuracy: 8057.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 426 [0/1000 (0%)]\tLosses F.softmax: 0.000084 log_softmax: 0.000022\n",
            "Train Epoch: 426 [200/1000 (20%)]\tLosses F.softmax: 0.000035 log_softmax: 0.000015\n",
            "Train Epoch: 426 [400/1000 (40%)]\tLosses F.softmax: 0.008336 log_softmax: 0.003105\n",
            "Train Epoch: 426 [600/1000 (60%)]\tLosses F.softmax: 0.000100 log_softmax: 0.000209\n",
            "Train Epoch: 426 [800/1000 (80%)]\tLosses F.softmax: 0.000087 log_softmax: 0.000089\n",
            "Train Epoch: 426 [1000/1000 (100%)]\tLosses F.softmax: 0.017543 log_softmax: 0.010383\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1496\tAccuracy: 8111.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1815\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 427 [0/1000 (0%)]\tLosses F.softmax: 0.017368 log_softmax: 0.015711\n",
            "Train Epoch: 427 [200/1000 (20%)]\tLosses F.softmax: 0.000462 log_softmax: 0.000033\n",
            "Train Epoch: 427 [400/1000 (40%)]\tLosses F.softmax: 0.004210 log_softmax: 0.004750\n",
            "Train Epoch: 427 [600/1000 (60%)]\tLosses F.softmax: 0.000021 log_softmax: 0.000181\n",
            "Train Epoch: 427 [800/1000 (80%)]\tLosses F.softmax: 0.000010 log_softmax: 0.000221\n",
            "Train Epoch: 427 [1000/1000 (100%)]\tLosses F.softmax: 0.004355 log_softmax: 0.008657\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1502\tAccuracy: 8112.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1821\tAccuracy: 8050.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 428 [0/1000 (0%)]\tLosses F.softmax: 0.014163 log_softmax: 0.008169\n",
            "Train Epoch: 428 [200/1000 (20%)]\tLosses F.softmax: 0.020799 log_softmax: 0.024188\n",
            "Train Epoch: 428 [400/1000 (40%)]\tLosses F.softmax: 0.000095 log_softmax: 0.000628\n",
            "Train Epoch: 428 [600/1000 (60%)]\tLosses F.softmax: 0.000296 log_softmax: 0.001040\n",
            "Train Epoch: 428 [800/1000 (80%)]\tLosses F.softmax: 0.000275 log_softmax: 0.000092\n",
            "Train Epoch: 428 [1000/1000 (100%)]\tLosses F.softmax: 0.000361 log_softmax: 0.000531\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1502\tAccuracy: 8111.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1823\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 429 [0/1000 (0%)]\tLosses F.softmax: 0.000313 log_softmax: 0.000205\n",
            "Train Epoch: 429 [200/1000 (20%)]\tLosses F.softmax: 0.002370 log_softmax: 0.000614\n",
            "Train Epoch: 429 [400/1000 (40%)]\tLosses F.softmax: 0.010481 log_softmax: 0.005605\n",
            "Train Epoch: 429 [600/1000 (60%)]\tLosses F.softmax: 0.002228 log_softmax: 0.002246\n",
            "Train Epoch: 429 [800/1000 (80%)]\tLosses F.softmax: 0.005442 log_softmax: 0.003219\n",
            "Train Epoch: 429 [1000/1000 (100%)]\tLosses F.softmax: 0.001476 log_softmax: 0.000584\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1503\tAccuracy: 8116.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1825\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 430 [0/1000 (0%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000029\n",
            "Train Epoch: 430 [200/1000 (20%)]\tLosses F.softmax: 0.002359 log_softmax: 0.004688\n",
            "Train Epoch: 430 [400/1000 (40%)]\tLosses F.softmax: 0.000010 log_softmax: 0.000000\n",
            "Train Epoch: 430 [600/1000 (60%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000001\n",
            "Train Epoch: 430 [800/1000 (80%)]\tLosses F.softmax: 0.026023 log_softmax: 0.030776\n",
            "Train Epoch: 430 [1000/1000 (100%)]\tLosses F.softmax: 0.000019 log_softmax: 0.000169\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1514\tAccuracy: 8108.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1835\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 431 [0/1000 (0%)]\tLosses F.softmax: 0.000085 log_softmax: 0.000199\n",
            "Train Epoch: 431 [200/1000 (20%)]\tLosses F.softmax: 0.004175 log_softmax: 0.000223\n",
            "Train Epoch: 431 [400/1000 (40%)]\tLosses F.softmax: 0.004478 log_softmax: 0.001636\n",
            "Train Epoch: 431 [600/1000 (60%)]\tLosses F.softmax: 0.000878 log_softmax: 0.000572\n",
            "Train Epoch: 431 [800/1000 (80%)]\tLosses F.softmax: 0.000319 log_softmax: 0.000954\n",
            "Train Epoch: 431 [1000/1000 (100%)]\tLosses F.softmax: 0.012033 log_softmax: 0.011789\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1518\tAccuracy: 8109.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1839\tAccuracy: 8052.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 432 [0/1000 (0%)]\tLosses F.softmax: 0.000013 log_softmax: 0.000004\n",
            "Train Epoch: 432 [200/1000 (20%)]\tLosses F.softmax: 0.000006 log_softmax: 0.000003\n",
            "Train Epoch: 432 [400/1000 (40%)]\tLosses F.softmax: 0.001088 log_softmax: 0.000349\n",
            "Train Epoch: 432 [600/1000 (60%)]\tLosses F.softmax: 0.000142 log_softmax: 0.000433\n",
            "Train Epoch: 432 [800/1000 (80%)]\tLosses F.softmax: 0.000844 log_softmax: 0.000069\n",
            "Train Epoch: 432 [1000/1000 (100%)]\tLosses F.softmax: 0.000066 log_softmax: 0.000025\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1518\tAccuracy: 8113.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1839\tAccuracy: 8058.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 433 [0/1000 (0%)]\tLosses F.softmax: 0.000372 log_softmax: 0.000355\n",
            "Train Epoch: 433 [200/1000 (20%)]\tLosses F.softmax: 0.088001 log_softmax: 0.058452\n",
            "Train Epoch: 433 [400/1000 (40%)]\tLosses F.softmax: 0.005350 log_softmax: 0.000362\n",
            "Train Epoch: 433 [600/1000 (60%)]\tLosses F.softmax: 0.000233 log_softmax: 0.004242\n",
            "Train Epoch: 433 [800/1000 (80%)]\tLosses F.softmax: 0.000040 log_softmax: 0.000094\n",
            "Train Epoch: 433 [1000/1000 (100%)]\tLosses F.softmax: 0.000667 log_softmax: 0.000132\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1520\tAccuracy: 8114.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1842\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 434 [0/1000 (0%)]\tLosses F.softmax: 0.001140 log_softmax: 0.000235\n",
            "Train Epoch: 434 [200/1000 (20%)]\tLosses F.softmax: 0.003207 log_softmax: 0.003644\n",
            "Train Epoch: 434 [400/1000 (40%)]\tLosses F.softmax: 0.001924 log_softmax: 0.000051\n",
            "Train Epoch: 434 [600/1000 (60%)]\tLosses F.softmax: 0.000138 log_softmax: 0.000481\n",
            "Train Epoch: 434 [800/1000 (80%)]\tLosses F.softmax: 0.006583 log_softmax: 0.008379\n",
            "Train Epoch: 434 [1000/1000 (100%)]\tLosses F.softmax: 0.007362 log_softmax: 0.012545\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1525\tAccuracy: 8112.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1848\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 435 [0/1000 (0%)]\tLosses F.softmax: 0.000124 log_softmax: 0.001170\n",
            "Train Epoch: 435 [200/1000 (20%)]\tLosses F.softmax: 0.000073 log_softmax: 0.000002\n",
            "Train Epoch: 435 [400/1000 (40%)]\tLosses F.softmax: 0.001111 log_softmax: 0.003470\n",
            "Train Epoch: 435 [600/1000 (60%)]\tLosses F.softmax: 0.001131 log_softmax: 0.000254\n",
            "Train Epoch: 435 [800/1000 (80%)]\tLosses F.softmax: 0.004807 log_softmax: 0.007276\n",
            "Train Epoch: 435 [1000/1000 (100%)]\tLosses F.softmax: 0.000049 log_softmax: 0.000133\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1530\tAccuracy: 8108.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1851\tAccuracy: 8051.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 436 [0/1000 (0%)]\tLosses F.softmax: 0.002105 log_softmax: 0.000966\n",
            "Train Epoch: 436 [200/1000 (20%)]\tLosses F.softmax: 0.001355 log_softmax: 0.002110\n",
            "Train Epoch: 436 [400/1000 (40%)]\tLosses F.softmax: 0.001119 log_softmax: 0.000533\n",
            "Train Epoch: 436 [600/1000 (60%)]\tLosses F.softmax: 0.032918 log_softmax: 0.023745\n",
            "Train Epoch: 436 [800/1000 (80%)]\tLosses F.softmax: 0.000438 log_softmax: 0.002333\n",
            "Train Epoch: 436 [1000/1000 (100%)]\tLosses F.softmax: 0.000908 log_softmax: 0.001100\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1531\tAccuracy: 8112.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1855\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 437 [0/1000 (0%)]\tLosses F.softmax: 0.000074 log_softmax: 0.000004\n",
            "Train Epoch: 437 [200/1000 (20%)]\tLosses F.softmax: 0.000106 log_softmax: 0.000052\n",
            "Train Epoch: 437 [400/1000 (40%)]\tLosses F.softmax: 0.001299 log_softmax: 0.000411\n",
            "Train Epoch: 437 [600/1000 (60%)]\tLosses F.softmax: 0.009037 log_softmax: 0.007649\n",
            "Train Epoch: 437 [800/1000 (80%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000066\n",
            "Train Epoch: 437 [1000/1000 (100%)]\tLosses F.softmax: 0.004147 log_softmax: 0.005647\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1535\tAccuracy: 8111.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1859\tAccuracy: 8051.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 438 [0/1000 (0%)]\tLosses F.softmax: 0.007771 log_softmax: 0.007653\n",
            "Train Epoch: 438 [200/1000 (20%)]\tLosses F.softmax: 0.025927 log_softmax: 0.022719\n",
            "Train Epoch: 438 [400/1000 (40%)]\tLosses F.softmax: 0.000376 log_softmax: 0.000844\n",
            "Train Epoch: 438 [600/1000 (60%)]\tLosses F.softmax: 0.007444 log_softmax: 0.012016\n",
            "Train Epoch: 438 [800/1000 (80%)]\tLosses F.softmax: 0.000810 log_softmax: 0.000874\n",
            "Train Epoch: 438 [1000/1000 (100%)]\tLosses F.softmax: 0.000449 log_softmax: 0.000008\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1537\tAccuracy: 8112.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1860\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 439 [0/1000 (0%)]\tLosses F.softmax: 0.002736 log_softmax: 0.000008\n",
            "Train Epoch: 439 [200/1000 (20%)]\tLosses F.softmax: 0.000014 log_softmax: 0.000005\n",
            "Train Epoch: 439 [400/1000 (40%)]\tLosses F.softmax: 0.000132 log_softmax: 0.000479\n",
            "Train Epoch: 439 [600/1000 (60%)]\tLosses F.softmax: 0.000861 log_softmax: 0.000378\n",
            "Train Epoch: 439 [800/1000 (80%)]\tLosses F.softmax: 0.000006 log_softmax: 0.000001\n",
            "Train Epoch: 439 [1000/1000 (100%)]\tLosses F.softmax: 0.002489 log_softmax: 0.000239\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1539\tAccuracy: 8116.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1862\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 440 [0/1000 (0%)]\tLosses F.softmax: 0.000941 log_softmax: 0.005811\n",
            "Train Epoch: 440 [200/1000 (20%)]\tLosses F.softmax: 0.001041 log_softmax: 0.000678\n",
            "Train Epoch: 440 [400/1000 (40%)]\tLosses F.softmax: 0.004255 log_softmax: 0.003290\n",
            "Train Epoch: 440 [600/1000 (60%)]\tLosses F.softmax: 0.000747 log_softmax: 0.001869\n",
            "Train Epoch: 440 [800/1000 (80%)]\tLosses F.softmax: 0.000078 log_softmax: 0.000011\n",
            "Train Epoch: 440 [1000/1000 (100%)]\tLosses F.softmax: 0.000510 log_softmax: 0.000087\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1545\tAccuracy: 8112.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1867\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 441 [0/1000 (0%)]\tLosses F.softmax: 0.000054 log_softmax: 0.000085\n",
            "Train Epoch: 441 [200/1000 (20%)]\tLosses F.softmax: 0.019388 log_softmax: 0.009483\n",
            "Train Epoch: 441 [400/1000 (40%)]\tLosses F.softmax: 0.000003 log_softmax: 0.000000\n",
            "Train Epoch: 441 [600/1000 (60%)]\tLosses F.softmax: 0.001702 log_softmax: 0.000244\n",
            "Train Epoch: 441 [800/1000 (80%)]\tLosses F.softmax: 0.008397 log_softmax: 0.003029\n",
            "Train Epoch: 441 [1000/1000 (100%)]\tLosses F.softmax: 0.004841 log_softmax: 0.007181\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1552\tAccuracy: 8111.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1874\tAccuracy: 8051.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 442 [0/1000 (0%)]\tLosses F.softmax: 0.000162 log_softmax: 0.000186\n",
            "Train Epoch: 442 [200/1000 (20%)]\tLosses F.softmax: 0.003596 log_softmax: 0.001386\n",
            "Train Epoch: 442 [400/1000 (40%)]\tLosses F.softmax: 0.000339 log_softmax: 0.000178\n",
            "Train Epoch: 442 [600/1000 (60%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000000\n",
            "Train Epoch: 442 [800/1000 (80%)]\tLosses F.softmax: 0.003627 log_softmax: 0.001854\n",
            "Train Epoch: 442 [1000/1000 (100%)]\tLosses F.softmax: 0.000018 log_softmax: 0.000282\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1552\tAccuracy: 8112.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1876\tAccuracy: 8049.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 443 [0/1000 (0%)]\tLosses F.softmax: 0.000024 log_softmax: 0.000017\n",
            "Train Epoch: 443 [200/1000 (20%)]\tLosses F.softmax: 0.003492 log_softmax: 0.003878\n",
            "Train Epoch: 443 [400/1000 (40%)]\tLosses F.softmax: 0.000011 log_softmax: 0.000002\n",
            "Train Epoch: 443 [600/1000 (60%)]\tLosses F.softmax: 0.000038 log_softmax: 0.000000\n",
            "Train Epoch: 443 [800/1000 (80%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000000\n",
            "Train Epoch: 443 [1000/1000 (100%)]\tLosses F.softmax: 0.000143 log_softmax: 0.000045\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1556\tAccuracy: 8112.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1879\tAccuracy: 8052.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 444 [0/1000 (0%)]\tLosses F.softmax: 0.000165 log_softmax: 0.000514\n",
            "Train Epoch: 444 [200/1000 (20%)]\tLosses F.softmax: 0.000329 log_softmax: 0.000030\n",
            "Train Epoch: 444 [400/1000 (40%)]\tLosses F.softmax: 0.000179 log_softmax: 0.000019\n",
            "Train Epoch: 444 [600/1000 (60%)]\tLosses F.softmax: 0.025925 log_softmax: 0.028916\n",
            "Train Epoch: 444 [800/1000 (80%)]\tLosses F.softmax: 0.000012 log_softmax: 0.000047\n",
            "Train Epoch: 444 [1000/1000 (100%)]\tLosses F.softmax: 0.000039 log_softmax: 0.000085\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1560\tAccuracy: 8112.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1884\tAccuracy: 8051.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 445 [0/1000 (0%)]\tLosses F.softmax: 0.009733 log_softmax: 0.019482\n",
            "Train Epoch: 445 [200/1000 (20%)]\tLosses F.softmax: 0.010748 log_softmax: 0.007258\n",
            "Train Epoch: 445 [400/1000 (40%)]\tLosses F.softmax: 0.013219 log_softmax: 0.006101\n",
            "Train Epoch: 445 [600/1000 (60%)]\tLosses F.softmax: 0.000012 log_softmax: 0.000010\n",
            "Train Epoch: 445 [800/1000 (80%)]\tLosses F.softmax: 0.029838 log_softmax: 0.050200\n",
            "Train Epoch: 445 [1000/1000 (100%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000000\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1563\tAccuracy: 8111.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1886\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 446 [0/1000 (0%)]\tLosses F.softmax: 0.000299 log_softmax: 0.000068\n",
            "Train Epoch: 446 [200/1000 (20%)]\tLosses F.softmax: 0.000015 log_softmax: 0.000002\n",
            "Train Epoch: 446 [400/1000 (40%)]\tLosses F.softmax: 0.003471 log_softmax: 0.001850\n",
            "Train Epoch: 446 [600/1000 (60%)]\tLosses F.softmax: 0.011824 log_softmax: 0.008947\n",
            "Train Epoch: 446 [800/1000 (80%)]\tLosses F.softmax: 0.028415 log_softmax: 0.012291\n",
            "Train Epoch: 446 [1000/1000 (100%)]\tLosses F.softmax: 0.000010 log_softmax: 0.000001\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1565\tAccuracy: 8116.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1889\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 447 [0/1000 (0%)]\tLosses F.softmax: 0.000070 log_softmax: 0.000016\n",
            "Train Epoch: 447 [200/1000 (20%)]\tLosses F.softmax: 0.004160 log_softmax: 0.007055\n",
            "Train Epoch: 447 [400/1000 (40%)]\tLosses F.softmax: 0.000039 log_softmax: 0.000160\n",
            "Train Epoch: 447 [600/1000 (60%)]\tLosses F.softmax: 0.000401 log_softmax: 0.002423\n",
            "Train Epoch: 447 [800/1000 (80%)]\tLosses F.softmax: 0.000946 log_softmax: 0.001623\n",
            "Train Epoch: 447 [1000/1000 (100%)]\tLosses F.softmax: 0.005548 log_softmax: 0.005340\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1571\tAccuracy: 8110.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1895\tAccuracy: 8050.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 448 [0/1000 (0%)]\tLosses F.softmax: 0.000987 log_softmax: 0.004371\n",
            "Train Epoch: 448 [200/1000 (20%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000000\n",
            "Train Epoch: 448 [400/1000 (40%)]\tLosses F.softmax: 0.000265 log_softmax: 0.000028\n",
            "Train Epoch: 448 [600/1000 (60%)]\tLosses F.softmax: 0.000245 log_softmax: 0.000082\n",
            "Train Epoch: 448 [800/1000 (80%)]\tLosses F.softmax: 0.000312 log_softmax: 0.000115\n",
            "Train Epoch: 448 [1000/1000 (100%)]\tLosses F.softmax: 0.000080 log_softmax: 0.000084\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1573\tAccuracy: 8112.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1898\tAccuracy: 8051.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 449 [0/1000 (0%)]\tLosses F.softmax: 0.000666 log_softmax: 0.002197\n",
            "Train Epoch: 449 [200/1000 (20%)]\tLosses F.softmax: 0.000049 log_softmax: 0.000266\n",
            "Train Epoch: 449 [400/1000 (40%)]\tLosses F.softmax: 0.000006 log_softmax: 0.000001\n",
            "Train Epoch: 449 [600/1000 (60%)]\tLosses F.softmax: 0.015363 log_softmax: 0.014424\n",
            "Train Epoch: 449 [800/1000 (80%)]\tLosses F.softmax: 0.003955 log_softmax: 0.004576\n",
            "Train Epoch: 449 [1000/1000 (100%)]\tLosses F.softmax: 0.001067 log_softmax: 0.000039\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1579\tAccuracy: 8111.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1901\tAccuracy: 8052.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 450 [0/1000 (0%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000005\n",
            "Train Epoch: 450 [200/1000 (20%)]\tLosses F.softmax: 0.000006 log_softmax: 0.000003\n",
            "Train Epoch: 450 [400/1000 (40%)]\tLosses F.softmax: 0.009046 log_softmax: 0.006255\n",
            "Train Epoch: 450 [600/1000 (60%)]\tLosses F.softmax: 0.000360 log_softmax: 0.000339\n",
            "Train Epoch: 450 [800/1000 (80%)]\tLosses F.softmax: 0.000485 log_softmax: 0.000081\n",
            "Train Epoch: 450 [1000/1000 (100%)]\tLosses F.softmax: 0.000189 log_softmax: 0.000090\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1582\tAccuracy: 8113.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1905\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 451 [0/1000 (0%)]\tLosses F.softmax: 0.001051 log_softmax: 0.007997\n",
            "Train Epoch: 451 [200/1000 (20%)]\tLosses F.softmax: 0.001841 log_softmax: 0.001440\n",
            "Train Epoch: 451 [400/1000 (40%)]\tLosses F.softmax: 0.000256 log_softmax: 0.001487\n",
            "Train Epoch: 451 [600/1000 (60%)]\tLosses F.softmax: 0.000005 log_softmax: 0.000014\n",
            "Train Epoch: 451 [800/1000 (80%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000001\n",
            "Train Epoch: 451 [1000/1000 (100%)]\tLosses F.softmax: 0.004896 log_softmax: 0.007107\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1587\tAccuracy: 8110.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1911\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 452 [0/1000 (0%)]\tLosses F.softmax: 0.004488 log_softmax: 0.002154\n",
            "Train Epoch: 452 [200/1000 (20%)]\tLosses F.softmax: 0.000096 log_softmax: 0.000090\n",
            "Train Epoch: 452 [400/1000 (40%)]\tLosses F.softmax: 0.003882 log_softmax: 0.005316\n",
            "Train Epoch: 452 [600/1000 (60%)]\tLosses F.softmax: 0.000035 log_softmax: 0.000082\n",
            "Train Epoch: 452 [800/1000 (80%)]\tLosses F.softmax: 0.000070 log_softmax: 0.003821\n",
            "Train Epoch: 452 [1000/1000 (100%)]\tLosses F.softmax: 0.000443 log_softmax: 0.000097\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1585\tAccuracy: 8117.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1912\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 453 [0/1000 (0%)]\tLosses F.softmax: 0.005242 log_softmax: 0.002613\n",
            "Train Epoch: 453 [200/1000 (20%)]\tLosses F.softmax: 0.014737 log_softmax: 0.011093\n",
            "Train Epoch: 453 [400/1000 (40%)]\tLosses F.softmax: 0.000120 log_softmax: 0.000056\n",
            "Train Epoch: 453 [600/1000 (60%)]\tLosses F.softmax: 0.004976 log_softmax: 0.005688\n",
            "Train Epoch: 453 [800/1000 (80%)]\tLosses F.softmax: 0.000010 log_softmax: 0.000001\n",
            "Train Epoch: 453 [1000/1000 (100%)]\tLosses F.softmax: 0.000007 log_softmax: 0.000001\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1587\tAccuracy: 8118.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1914\tAccuracy: 8058.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 454 [0/1000 (0%)]\tLosses F.softmax: 0.001372 log_softmax: 0.000542\n",
            "Train Epoch: 454 [200/1000 (20%)]\tLosses F.softmax: 0.001596 log_softmax: 0.003095\n",
            "Train Epoch: 454 [400/1000 (40%)]\tLosses F.softmax: 0.002393 log_softmax: 0.000462\n",
            "Train Epoch: 454 [600/1000 (60%)]\tLosses F.softmax: 0.016404 log_softmax: 0.020788\n",
            "Train Epoch: 454 [800/1000 (80%)]\tLosses F.softmax: 0.013832 log_softmax: 0.016764\n",
            "Train Epoch: 454 [1000/1000 (100%)]\tLosses F.softmax: 0.003149 log_softmax: 0.002938\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1594\tAccuracy: 8112.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1921\tAccuracy: 8052.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 455 [0/1000 (0%)]\tLosses F.softmax: 0.001488 log_softmax: 0.000090\n",
            "Train Epoch: 455 [200/1000 (20%)]\tLosses F.softmax: 0.010897 log_softmax: 0.026155\n",
            "Train Epoch: 455 [400/1000 (40%)]\tLosses F.softmax: 0.000003 log_softmax: 0.000000\n",
            "Train Epoch: 455 [600/1000 (60%)]\tLosses F.softmax: 0.000179 log_softmax: 0.000089\n",
            "Train Epoch: 455 [800/1000 (80%)]\tLosses F.softmax: 0.007178 log_softmax: 0.000920\n",
            "Train Epoch: 455 [1000/1000 (100%)]\tLosses F.softmax: 0.000070 log_softmax: 0.000004\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1596\tAccuracy: 8116.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1923\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 456 [0/1000 (0%)]\tLosses F.softmax: 0.000210 log_softmax: 0.004099\n",
            "Train Epoch: 456 [200/1000 (20%)]\tLosses F.softmax: 0.002667 log_softmax: 0.007395\n",
            "Train Epoch: 456 [400/1000 (40%)]\tLosses F.softmax: 0.001639 log_softmax: 0.000235\n",
            "Train Epoch: 456 [600/1000 (60%)]\tLosses F.softmax: 0.016335 log_softmax: 0.009966\n",
            "Train Epoch: 456 [800/1000 (80%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000000\n",
            "Train Epoch: 456 [1000/1000 (100%)]\tLosses F.softmax: 0.002019 log_softmax: 0.000607\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1600\tAccuracy: 8115.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1927\tAccuracy: 8051.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 457 [0/1000 (0%)]\tLosses F.softmax: 0.000046 log_softmax: 0.000308\n",
            "Train Epoch: 457 [200/1000 (20%)]\tLosses F.softmax: 0.026519 log_softmax: 0.032358\n",
            "Train Epoch: 457 [400/1000 (40%)]\tLosses F.softmax: 0.015347 log_softmax: 0.009191\n",
            "Train Epoch: 457 [600/1000 (60%)]\tLosses F.softmax: 0.000037 log_softmax: 0.000007\n",
            "Train Epoch: 457 [800/1000 (80%)]\tLosses F.softmax: 0.000256 log_softmax: 0.000948\n",
            "Train Epoch: 457 [1000/1000 (100%)]\tLosses F.softmax: 0.000624 log_softmax: 0.000079\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1601\tAccuracy: 8117.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1928\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 458 [0/1000 (0%)]\tLosses F.softmax: 0.011057 log_softmax: 0.028448\n",
            "Train Epoch: 458 [200/1000 (20%)]\tLosses F.softmax: 0.001859 log_softmax: 0.000006\n",
            "Train Epoch: 458 [400/1000 (40%)]\tLosses F.softmax: 0.008591 log_softmax: 0.013011\n",
            "Train Epoch: 458 [600/1000 (60%)]\tLosses F.softmax: 0.022107 log_softmax: 0.021253\n",
            "Train Epoch: 458 [800/1000 (80%)]\tLosses F.softmax: 0.000934 log_softmax: 0.000008\n",
            "Train Epoch: 458 [1000/1000 (100%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000002\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1603\tAccuracy: 8119.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1932\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 459 [0/1000 (0%)]\tLosses F.softmax: 0.000010 log_softmax: 0.000002\n",
            "Train Epoch: 459 [200/1000 (20%)]\tLosses F.softmax: 0.085133 log_softmax: 0.055469\n",
            "Train Epoch: 459 [400/1000 (40%)]\tLosses F.softmax: 0.003975 log_softmax: 0.005408\n",
            "Train Epoch: 459 [600/1000 (60%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000002\n",
            "Train Epoch: 459 [800/1000 (80%)]\tLosses F.softmax: 0.005391 log_softmax: 0.001750\n",
            "Train Epoch: 459 [1000/1000 (100%)]\tLosses F.softmax: 0.000031 log_softmax: 0.000006\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1610\tAccuracy: 8115.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1939\tAccuracy: 8052.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 460 [0/1000 (0%)]\tLosses F.softmax: 0.007375 log_softmax: 0.008231\n",
            "Train Epoch: 460 [200/1000 (20%)]\tLosses F.softmax: 0.000113 log_softmax: 0.000046\n",
            "Train Epoch: 460 [400/1000 (40%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000000\n",
            "Train Epoch: 460 [600/1000 (60%)]\tLosses F.softmax: 0.018235 log_softmax: 0.010063\n",
            "Train Epoch: 460 [800/1000 (80%)]\tLosses F.softmax: 0.009262 log_softmax: 0.004200\n",
            "Train Epoch: 460 [1000/1000 (100%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000000\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1611\tAccuracy: 8116.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1941\tAccuracy: 8050.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 461 [0/1000 (0%)]\tLosses F.softmax: 0.000573 log_softmax: 0.001186\n",
            "Train Epoch: 461 [200/1000 (20%)]\tLosses F.softmax: 0.000004 log_softmax: 0.000010\n",
            "Train Epoch: 461 [400/1000 (40%)]\tLosses F.softmax: 0.000296 log_softmax: 0.000010\n",
            "Train Epoch: 461 [600/1000 (60%)]\tLosses F.softmax: 0.000816 log_softmax: 0.002369\n",
            "Train Epoch: 461 [800/1000 (80%)]\tLosses F.softmax: 0.000058 log_softmax: 0.000177\n",
            "Train Epoch: 461 [1000/1000 (100%)]\tLosses F.softmax: 0.000059 log_softmax: 0.000022\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1617\tAccuracy: 8116.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1946\tAccuracy: 8051.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 462 [0/1000 (0%)]\tLosses F.softmax: 0.000032 log_softmax: 0.000641\n",
            "Train Epoch: 462 [200/1000 (20%)]\tLosses F.softmax: 0.001669 log_softmax: 0.000291\n",
            "Train Epoch: 462 [400/1000 (40%)]\tLosses F.softmax: 0.000659 log_softmax: 0.001462\n",
            "Train Epoch: 462 [600/1000 (60%)]\tLosses F.softmax: 0.005444 log_softmax: 0.002933\n",
            "Train Epoch: 462 [800/1000 (80%)]\tLosses F.softmax: 0.000145 log_softmax: 0.000452\n",
            "Train Epoch: 462 [1000/1000 (100%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000000\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1618\tAccuracy: 8116.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1947\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 463 [0/1000 (0%)]\tLosses F.softmax: 0.000023 log_softmax: 0.000001\n",
            "Train Epoch: 463 [200/1000 (20%)]\tLosses F.softmax: 0.000955 log_softmax: 0.000224\n",
            "Train Epoch: 463 [400/1000 (40%)]\tLosses F.softmax: 0.000205 log_softmax: 0.000047\n",
            "Train Epoch: 463 [600/1000 (60%)]\tLosses F.softmax: 0.000083 log_softmax: 0.000163\n",
            "Train Epoch: 463 [800/1000 (80%)]\tLosses F.softmax: 0.000408 log_softmax: 0.000744\n",
            "Train Epoch: 463 [1000/1000 (100%)]\tLosses F.softmax: 0.008039 log_softmax: 0.001921\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1624\tAccuracy: 8116.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1954\tAccuracy: 8052.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 464 [0/1000 (0%)]\tLosses F.softmax: 0.001002 log_softmax: 0.000637\n",
            "Train Epoch: 464 [200/1000 (20%)]\tLosses F.softmax: 0.004770 log_softmax: 0.002916\n",
            "Train Epoch: 464 [400/1000 (40%)]\tLosses F.softmax: 0.000117 log_softmax: 0.000054\n",
            "Train Epoch: 464 [600/1000 (60%)]\tLosses F.softmax: 0.000003 log_softmax: 0.000001\n",
            "Train Epoch: 464 [800/1000 (80%)]\tLosses F.softmax: 0.003325 log_softmax: 0.000291\n",
            "Train Epoch: 464 [1000/1000 (100%)]\tLosses F.softmax: 0.009626 log_softmax: 0.003459\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1626\tAccuracy: 8116.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1956\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 465 [0/1000 (0%)]\tLosses F.softmax: 0.000035 log_softmax: 0.000043\n",
            "Train Epoch: 465 [200/1000 (20%)]\tLosses F.softmax: 0.001915 log_softmax: 0.000116\n",
            "Train Epoch: 465 [400/1000 (40%)]\tLosses F.softmax: 0.000027 log_softmax: 0.000001\n",
            "Train Epoch: 465 [600/1000 (60%)]\tLosses F.softmax: 0.004711 log_softmax: 0.004381\n",
            "Train Epoch: 465 [800/1000 (80%)]\tLosses F.softmax: 0.001257 log_softmax: 0.000504\n",
            "Train Epoch: 465 [1000/1000 (100%)]\tLosses F.softmax: 0.024800 log_softmax: 0.027230\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1626\tAccuracy: 8119.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1956\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 466 [0/1000 (0%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000042\n",
            "Train Epoch: 466 [200/1000 (20%)]\tLosses F.softmax: 0.000995 log_softmax: 0.004435\n",
            "Train Epoch: 466 [400/1000 (40%)]\tLosses F.softmax: 0.022741 log_softmax: 0.019345\n",
            "Train Epoch: 466 [600/1000 (60%)]\tLosses F.softmax: 0.026074 log_softmax: 0.021502\n",
            "Train Epoch: 466 [800/1000 (80%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000002\n",
            "Train Epoch: 466 [1000/1000 (100%)]\tLosses F.softmax: 0.000854 log_softmax: 0.000851\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1634\tAccuracy: 8118.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1963\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 467 [0/1000 (0%)]\tLosses F.softmax: 0.002739 log_softmax: 0.006215\n",
            "Train Epoch: 467 [200/1000 (20%)]\tLosses F.softmax: 0.001041 log_softmax: 0.000330\n",
            "Train Epoch: 467 [400/1000 (40%)]\tLosses F.softmax: 0.000558 log_softmax: 0.000258\n",
            "Train Epoch: 467 [600/1000 (60%)]\tLosses F.softmax: 0.000363 log_softmax: 0.001658\n",
            "Train Epoch: 467 [800/1000 (80%)]\tLosses F.softmax: 0.000666 log_softmax: 0.000163\n",
            "Train Epoch: 467 [1000/1000 (100%)]\tLosses F.softmax: 0.007396 log_softmax: 0.007375\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1638\tAccuracy: 8116.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1968\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 468 [0/1000 (0%)]\tLosses F.softmax: 0.000032 log_softmax: 0.000017\n",
            "Train Epoch: 468 [200/1000 (20%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000000\n",
            "Train Epoch: 468 [400/1000 (40%)]\tLosses F.softmax: 0.000243 log_softmax: 0.001442\n",
            "Train Epoch: 468 [600/1000 (60%)]\tLosses F.softmax: 0.000003 log_softmax: 0.002280\n",
            "Train Epoch: 468 [800/1000 (80%)]\tLosses F.softmax: 0.000022 log_softmax: 0.000016\n",
            "Train Epoch: 468 [1000/1000 (100%)]\tLosses F.softmax: 0.000002 log_softmax: 0.000003\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1643\tAccuracy: 8113.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1973\tAccuracy: 8052.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 469 [0/1000 (0%)]\tLosses F.softmax: 0.002511 log_softmax: 0.003174\n",
            "Train Epoch: 469 [200/1000 (20%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000017\n",
            "Train Epoch: 469 [400/1000 (40%)]\tLosses F.softmax: 0.000009 log_softmax: 0.000003\n",
            "Train Epoch: 469 [600/1000 (60%)]\tLosses F.softmax: 0.028072 log_softmax: 0.046904\n",
            "Train Epoch: 469 [800/1000 (80%)]\tLosses F.softmax: 0.007528 log_softmax: 0.005329\n",
            "Train Epoch: 469 [1000/1000 (100%)]\tLosses F.softmax: 0.000014 log_softmax: 0.000002\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1644\tAccuracy: 8116.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1974\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 470 [0/1000 (0%)]\tLosses F.softmax: 0.000540 log_softmax: 0.000786\n",
            "Train Epoch: 470 [200/1000 (20%)]\tLosses F.softmax: 0.002150 log_softmax: 0.003384\n",
            "Train Epoch: 470 [400/1000 (40%)]\tLosses F.softmax: 0.000006 log_softmax: 0.001717\n",
            "Train Epoch: 470 [600/1000 (60%)]\tLosses F.softmax: 0.000104 log_softmax: 0.000051\n",
            "Train Epoch: 470 [800/1000 (80%)]\tLosses F.softmax: 0.000094 log_softmax: 0.000002\n",
            "Train Epoch: 470 [1000/1000 (100%)]\tLosses F.softmax: 0.000152 log_softmax: 0.000172\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1647\tAccuracy: 8116.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1978\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 471 [0/1000 (0%)]\tLosses F.softmax: 0.000404 log_softmax: 0.003906\n",
            "Train Epoch: 471 [200/1000 (20%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000000\n",
            "Train Epoch: 471 [400/1000 (40%)]\tLosses F.softmax: 0.000024 log_softmax: 0.000005\n",
            "Train Epoch: 471 [600/1000 (60%)]\tLosses F.softmax: 0.000768 log_softmax: 0.000288\n",
            "Train Epoch: 471 [800/1000 (80%)]\tLosses F.softmax: 0.010304 log_softmax: 0.008286\n",
            "Train Epoch: 471 [1000/1000 (100%)]\tLosses F.softmax: 0.000024 log_softmax: 0.000022\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1651\tAccuracy: 8113.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1981\tAccuracy: 8052.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 472 [0/1000 (0%)]\tLosses F.softmax: 0.001023 log_softmax: 0.000233\n",
            "Train Epoch: 472 [200/1000 (20%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000001\n",
            "Train Epoch: 472 [400/1000 (40%)]\tLosses F.softmax: 0.000023 log_softmax: 0.000006\n",
            "Train Epoch: 472 [600/1000 (60%)]\tLosses F.softmax: 0.000296 log_softmax: 0.000002\n",
            "Train Epoch: 472 [800/1000 (80%)]\tLosses F.softmax: 0.015480 log_softmax: 0.000366\n",
            "Train Epoch: 472 [1000/1000 (100%)]\tLosses F.softmax: 0.001960 log_softmax: 0.000363\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1653\tAccuracy: 8114.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1985\tAccuracy: 8051.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 473 [0/1000 (0%)]\tLosses F.softmax: 0.002026 log_softmax: 0.004059\n",
            "Train Epoch: 473 [200/1000 (20%)]\tLosses F.softmax: 0.000056 log_softmax: 0.000177\n",
            "Train Epoch: 473 [400/1000 (40%)]\tLosses F.softmax: 0.007299 log_softmax: 0.002938\n",
            "Train Epoch: 473 [600/1000 (60%)]\tLosses F.softmax: 0.001245 log_softmax: 0.000202\n",
            "Train Epoch: 473 [800/1000 (80%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000037\n",
            "Train Epoch: 473 [1000/1000 (100%)]\tLosses F.softmax: 0.003454 log_softmax: 0.000917\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1661\tAccuracy: 8107.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1993\tAccuracy: 8051.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 474 [0/1000 (0%)]\tLosses F.softmax: 0.000002 log_softmax: 0.000000\n",
            "Train Epoch: 474 [200/1000 (20%)]\tLosses F.softmax: 0.011088 log_softmax: 0.015628\n",
            "Train Epoch: 474 [400/1000 (40%)]\tLosses F.softmax: 0.007647 log_softmax: 0.005394\n",
            "Train Epoch: 474 [600/1000 (60%)]\tLosses F.softmax: 0.000003 log_softmax: 0.000001\n",
            "Train Epoch: 474 [800/1000 (80%)]\tLosses F.softmax: 0.006686 log_softmax: 0.007904\n",
            "Train Epoch: 474 [1000/1000 (100%)]\tLosses F.softmax: 0.000023 log_softmax: 0.000064\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1659\tAccuracy: 8116.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1991\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 475 [0/1000 (0%)]\tLosses F.softmax: 0.003665 log_softmax: 0.007302\n",
            "Train Epoch: 475 [200/1000 (20%)]\tLosses F.softmax: 0.005334 log_softmax: 0.003151\n",
            "Train Epoch: 475 [400/1000 (40%)]\tLosses F.softmax: 0.002059 log_softmax: 0.002138\n",
            "Train Epoch: 475 [600/1000 (60%)]\tLosses F.softmax: 0.000014 log_softmax: 0.000004\n",
            "Train Epoch: 475 [800/1000 (80%)]\tLosses F.softmax: 0.000815 log_softmax: 0.002309\n",
            "Train Epoch: 475 [1000/1000 (100%)]\tLosses F.softmax: 0.033769 log_softmax: 0.015434\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1663\tAccuracy: 8117.0/10000 (81%)\n",
            "log_softmax: Loss: 1.1996\tAccuracy: 8051.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 476 [0/1000 (0%)]\tLosses F.softmax: 0.005169 log_softmax: 0.003057\n",
            "Train Epoch: 476 [200/1000 (20%)]\tLosses F.softmax: 0.000283 log_softmax: 0.000002\n",
            "Train Epoch: 476 [400/1000 (40%)]\tLosses F.softmax: 0.000004 log_softmax: 0.000001\n",
            "Train Epoch: 476 [600/1000 (60%)]\tLosses F.softmax: 0.000015 log_softmax: 0.000051\n",
            "Train Epoch: 476 [800/1000 (80%)]\tLosses F.softmax: 0.000268 log_softmax: 0.000001\n",
            "Train Epoch: 476 [1000/1000 (100%)]\tLosses F.softmax: 0.009859 log_softmax: 0.007987\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1668\tAccuracy: 8112.0/10000 (81%)\n",
            "log_softmax: Loss: 1.2000\tAccuracy: 8051.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 477 [0/1000 (0%)]\tLosses F.softmax: 0.000105 log_softmax: 0.001033\n",
            "Train Epoch: 477 [200/1000 (20%)]\tLosses F.softmax: 0.001433 log_softmax: 0.000295\n",
            "Train Epoch: 477 [400/1000 (40%)]\tLosses F.softmax: 0.000077 log_softmax: 0.000006\n",
            "Train Epoch: 477 [600/1000 (60%)]\tLosses F.softmax: 0.007162 log_softmax: 0.006408\n",
            "Train Epoch: 477 [800/1000 (80%)]\tLosses F.softmax: 0.002189 log_softmax: 0.000410\n",
            "Train Epoch: 477 [1000/1000 (100%)]\tLosses F.softmax: 0.002550 log_softmax: 0.000727\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1671\tAccuracy: 8116.0/10000 (81%)\n",
            "log_softmax: Loss: 1.2002\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 478 [0/1000 (0%)]\tLosses F.softmax: 0.000085 log_softmax: 0.000076\n",
            "Train Epoch: 478 [200/1000 (20%)]\tLosses F.softmax: 0.003688 log_softmax: 0.000318\n",
            "Train Epoch: 478 [400/1000 (40%)]\tLosses F.softmax: 0.000236 log_softmax: 0.001455\n",
            "Train Epoch: 478 [600/1000 (60%)]\tLosses F.softmax: 0.000187 log_softmax: 0.000144\n",
            "Train Epoch: 478 [800/1000 (80%)]\tLosses F.softmax: 0.003974 log_softmax: 0.001437\n",
            "Train Epoch: 478 [1000/1000 (100%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000003\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1674\tAccuracy: 8112.0/10000 (81%)\n",
            "log_softmax: Loss: 1.2006\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 479 [0/1000 (0%)]\tLosses F.softmax: 0.000978 log_softmax: 0.000049\n",
            "Train Epoch: 479 [200/1000 (20%)]\tLosses F.softmax: 0.000795 log_softmax: 0.000970\n",
            "Train Epoch: 479 [400/1000 (40%)]\tLosses F.softmax: 0.000004 log_softmax: 0.000005\n",
            "Train Epoch: 479 [600/1000 (60%)]\tLosses F.softmax: 0.016455 log_softmax: 0.007216\n",
            "Train Epoch: 479 [800/1000 (80%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000001\n",
            "Train Epoch: 479 [1000/1000 (100%)]\tLosses F.softmax: 0.001864 log_softmax: 0.000553\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1677\tAccuracy: 8113.0/10000 (81%)\n",
            "log_softmax: Loss: 1.2009\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 480 [0/1000 (0%)]\tLosses F.softmax: 0.000035 log_softmax: 0.000167\n",
            "Train Epoch: 480 [200/1000 (20%)]\tLosses F.softmax: 0.000006 log_softmax: 0.000007\n",
            "Train Epoch: 480 [400/1000 (40%)]\tLosses F.softmax: 0.000046 log_softmax: 0.000087\n",
            "Train Epoch: 480 [600/1000 (60%)]\tLosses F.softmax: 0.000326 log_softmax: 0.000312\n",
            "Train Epoch: 480 [800/1000 (80%)]\tLosses F.softmax: 0.001664 log_softmax: 0.000004\n",
            "Train Epoch: 480 [1000/1000 (100%)]\tLosses F.softmax: 0.012579 log_softmax: 0.012227\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1678\tAccuracy: 8116.0/10000 (81%)\n",
            "log_softmax: Loss: 1.2010\tAccuracy: 8058.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 481 [0/1000 (0%)]\tLosses F.softmax: 0.000022 log_softmax: 0.000001\n",
            "Train Epoch: 481 [200/1000 (20%)]\tLosses F.softmax: 0.006743 log_softmax: 0.007321\n",
            "Train Epoch: 481 [400/1000 (40%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000001\n",
            "Train Epoch: 481 [600/1000 (60%)]\tLosses F.softmax: 0.000926 log_softmax: 0.000307\n",
            "Train Epoch: 481 [800/1000 (80%)]\tLosses F.softmax: 0.005897 log_softmax: 0.010448\n",
            "Train Epoch: 481 [1000/1000 (100%)]\tLosses F.softmax: 0.001065 log_softmax: 0.001015\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1684\tAccuracy: 8113.0/10000 (81%)\n",
            "log_softmax: Loss: 1.2016\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 482 [0/1000 (0%)]\tLosses F.softmax: 0.000015 log_softmax: 0.000147\n",
            "Train Epoch: 482 [200/1000 (20%)]\tLosses F.softmax: 0.011063 log_softmax: 0.010896\n",
            "Train Epoch: 482 [400/1000 (40%)]\tLosses F.softmax: 0.000062 log_softmax: 0.000013\n",
            "Train Epoch: 482 [600/1000 (60%)]\tLosses F.softmax: 0.000064 log_softmax: 0.000003\n",
            "Train Epoch: 482 [800/1000 (80%)]\tLosses F.softmax: 0.011023 log_softmax: 0.008416\n",
            "Train Epoch: 482 [1000/1000 (100%)]\tLosses F.softmax: 0.002734 log_softmax: 0.003318\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1688\tAccuracy: 8112.0/10000 (81%)\n",
            "log_softmax: Loss: 1.2020\tAccuracy: 8052.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 483 [0/1000 (0%)]\tLosses F.softmax: 0.000559 log_softmax: 0.000109\n",
            "Train Epoch: 483 [200/1000 (20%)]\tLosses F.softmax: 0.008729 log_softmax: 0.005174\n",
            "Train Epoch: 483 [400/1000 (40%)]\tLosses F.softmax: 0.001012 log_softmax: 0.000035\n",
            "Train Epoch: 483 [600/1000 (60%)]\tLosses F.softmax: 0.001158 log_softmax: 0.001109\n",
            "Train Epoch: 483 [800/1000 (80%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000001\n",
            "Train Epoch: 483 [1000/1000 (100%)]\tLosses F.softmax: 0.001819 log_softmax: 0.003237\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1690\tAccuracy: 8117.0/10000 (81%)\n",
            "log_softmax: Loss: 1.2023\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 484 [0/1000 (0%)]\tLosses F.softmax: 0.000002 log_softmax: 0.000008\n",
            "Train Epoch: 484 [200/1000 (20%)]\tLosses F.softmax: 0.003696 log_softmax: 0.004295\n",
            "Train Epoch: 484 [400/1000 (40%)]\tLosses F.softmax: 0.006984 log_softmax: 0.005239\n",
            "Train Epoch: 484 [600/1000 (60%)]\tLosses F.softmax: 0.000001 log_softmax: 0.000037\n",
            "Train Epoch: 484 [800/1000 (80%)]\tLosses F.softmax: 0.004029 log_softmax: 0.002155\n",
            "Train Epoch: 484 [1000/1000 (100%)]\tLosses F.softmax: 0.002713 log_softmax: 0.007294\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1691\tAccuracy: 8120.0/10000 (81%)\n",
            "log_softmax: Loss: 1.2027\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 485 [0/1000 (0%)]\tLosses F.softmax: 0.015325 log_softmax: 0.013676\n",
            "Train Epoch: 485 [200/1000 (20%)]\tLosses F.softmax: 0.000079 log_softmax: 0.000158\n",
            "Train Epoch: 485 [400/1000 (40%)]\tLosses F.softmax: 0.003490 log_softmax: 0.003556\n",
            "Train Epoch: 485 [600/1000 (60%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000019\n",
            "Train Epoch: 485 [800/1000 (80%)]\tLosses F.softmax: 0.000009 log_softmax: 0.000708\n",
            "Train Epoch: 485 [1000/1000 (100%)]\tLosses F.softmax: 0.003445 log_softmax: 0.007069\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1692\tAccuracy: 8119.0/10000 (81%)\n",
            "log_softmax: Loss: 1.2029\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 486 [0/1000 (0%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000000\n",
            "Train Epoch: 486 [200/1000 (20%)]\tLosses F.softmax: 0.001428 log_softmax: 0.011622\n",
            "Train Epoch: 486 [400/1000 (40%)]\tLosses F.softmax: 0.000078 log_softmax: 0.000050\n",
            "Train Epoch: 486 [600/1000 (60%)]\tLosses F.softmax: 0.000032 log_softmax: 0.000000\n",
            "Train Epoch: 486 [800/1000 (80%)]\tLosses F.softmax: 0.000158 log_softmax: 0.000224\n",
            "Train Epoch: 486 [1000/1000 (100%)]\tLosses F.softmax: 0.005102 log_softmax: 0.000262\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1699\tAccuracy: 8112.0/10000 (81%)\n",
            "log_softmax: Loss: 1.2037\tAccuracy: 8052.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 487 [0/1000 (0%)]\tLosses F.softmax: 0.001067 log_softmax: 0.003086\n",
            "Train Epoch: 487 [200/1000 (20%)]\tLosses F.softmax: 0.000079 log_softmax: 0.000021\n",
            "Train Epoch: 487 [400/1000 (40%)]\tLosses F.softmax: 0.016636 log_softmax: 0.023990\n",
            "Train Epoch: 487 [600/1000 (60%)]\tLosses F.softmax: 0.000003 log_softmax: 0.000002\n",
            "Train Epoch: 487 [800/1000 (80%)]\tLosses F.softmax: 0.002261 log_softmax: 0.001178\n",
            "Train Epoch: 487 [1000/1000 (100%)]\tLosses F.softmax: 0.000054 log_softmax: 0.000493\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1701\tAccuracy: 8117.0/10000 (81%)\n",
            "log_softmax: Loss: 1.2038\tAccuracy: 8049.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 488 [0/1000 (0%)]\tLosses F.softmax: 0.000134 log_softmax: 0.000008\n",
            "Train Epoch: 488 [200/1000 (20%)]\tLosses F.softmax: 0.006312 log_softmax: 0.006494\n",
            "Train Epoch: 488 [400/1000 (40%)]\tLosses F.softmax: 0.000023 log_softmax: 0.000020\n",
            "Train Epoch: 488 [600/1000 (60%)]\tLosses F.softmax: 0.000031 log_softmax: 0.000641\n",
            "Train Epoch: 488 [800/1000 (80%)]\tLosses F.softmax: 0.000090 log_softmax: 0.000228\n",
            "Train Epoch: 488 [1000/1000 (100%)]\tLosses F.softmax: 0.000006 log_softmax: 0.000001\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1704\tAccuracy: 8119.0/10000 (81%)\n",
            "log_softmax: Loss: 1.2040\tAccuracy: 8048.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 489 [0/1000 (0%)]\tLosses F.softmax: 0.000025 log_softmax: 0.000112\n",
            "Train Epoch: 489 [200/1000 (20%)]\tLosses F.softmax: 0.000003 log_softmax: 0.000002\n",
            "Train Epoch: 489 [400/1000 (40%)]\tLosses F.softmax: 0.002111 log_softmax: 0.002221\n",
            "Train Epoch: 489 [600/1000 (60%)]\tLosses F.softmax: 0.001736 log_softmax: 0.001315\n",
            "Train Epoch: 489 [800/1000 (80%)]\tLosses F.softmax: 0.000126 log_softmax: 0.000480\n",
            "Train Epoch: 489 [1000/1000 (100%)]\tLosses F.softmax: 0.001535 log_softmax: 0.000133\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1711\tAccuracy: 8114.0/10000 (81%)\n",
            "log_softmax: Loss: 1.2047\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 490 [0/1000 (0%)]\tLosses F.softmax: 0.004088 log_softmax: 0.001938\n",
            "Train Epoch: 490 [200/1000 (20%)]\tLosses F.softmax: 0.000015 log_softmax: 0.000016\n",
            "Train Epoch: 490 [400/1000 (40%)]\tLosses F.softmax: 0.000030 log_softmax: 0.000037\n",
            "Train Epoch: 490 [600/1000 (60%)]\tLosses F.softmax: 0.000270 log_softmax: 0.000481\n",
            "Train Epoch: 490 [800/1000 (80%)]\tLosses F.softmax: 0.000031 log_softmax: 0.000017\n",
            "Train Epoch: 490 [1000/1000 (100%)]\tLosses F.softmax: 0.000005 log_softmax: 0.000001\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1710\tAccuracy: 8115.0/10000 (81%)\n",
            "log_softmax: Loss: 1.2047\tAccuracy: 8046.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 491 [0/1000 (0%)]\tLosses F.softmax: 0.002361 log_softmax: 0.001227\n",
            "Train Epoch: 491 [200/1000 (20%)]\tLosses F.softmax: 0.001729 log_softmax: 0.000675\n",
            "Train Epoch: 491 [400/1000 (40%)]\tLosses F.softmax: 0.013376 log_softmax: 0.012763\n",
            "Train Epoch: 491 [600/1000 (60%)]\tLosses F.softmax: 0.000212 log_softmax: 0.000167\n",
            "Train Epoch: 491 [800/1000 (80%)]\tLosses F.softmax: 0.000141 log_softmax: 0.000799\n",
            "Train Epoch: 491 [1000/1000 (100%)]\tLosses F.softmax: 0.015179 log_softmax: 0.015601\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1713\tAccuracy: 8119.0/10000 (81%)\n",
            "log_softmax: Loss: 1.2049\tAccuracy: 8050.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 492 [0/1000 (0%)]\tLosses F.softmax: 0.003819 log_softmax: 0.003042\n",
            "Train Epoch: 492 [200/1000 (20%)]\tLosses F.softmax: 0.007265 log_softmax: 0.005295\n",
            "Train Epoch: 492 [400/1000 (40%)]\tLosses F.softmax: 0.025142 log_softmax: 0.020997\n",
            "Train Epoch: 492 [600/1000 (60%)]\tLosses F.softmax: 0.000114 log_softmax: 0.000037\n",
            "Train Epoch: 492 [800/1000 (80%)]\tLosses F.softmax: 0.005905 log_softmax: 0.016061\n",
            "Train Epoch: 492 [1000/1000 (100%)]\tLosses F.softmax: 0.000081 log_softmax: 0.000047\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1716\tAccuracy: 8118.0/10000 (81%)\n",
            "log_softmax: Loss: 1.2052\tAccuracy: 8051.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 493 [0/1000 (0%)]\tLosses F.softmax: 0.008063 log_softmax: 0.007647\n",
            "Train Epoch: 493 [200/1000 (20%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000000\n",
            "Train Epoch: 493 [400/1000 (40%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000000\n",
            "Train Epoch: 493 [600/1000 (60%)]\tLosses F.softmax: 0.008008 log_softmax: 0.004498\n",
            "Train Epoch: 493 [800/1000 (80%)]\tLosses F.softmax: 0.000005 log_softmax: 0.000002\n",
            "Train Epoch: 493 [1000/1000 (100%)]\tLosses F.softmax: 0.000068 log_softmax: 0.000029\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1724\tAccuracy: 8114.0/10000 (81%)\n",
            "log_softmax: Loss: 1.2059\tAccuracy: 8049.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 494 [0/1000 (0%)]\tLosses F.softmax: 0.000890 log_softmax: 0.000296\n",
            "Train Epoch: 494 [200/1000 (20%)]\tLosses F.softmax: 0.005009 log_softmax: 0.005066\n",
            "Train Epoch: 494 [400/1000 (40%)]\tLosses F.softmax: 0.006726 log_softmax: 0.004835\n",
            "Train Epoch: 494 [600/1000 (60%)]\tLosses F.softmax: 0.000002 log_softmax: 0.000026\n",
            "Train Epoch: 494 [800/1000 (80%)]\tLosses F.softmax: 0.013976 log_softmax: 0.013169\n",
            "Train Epoch: 494 [1000/1000 (100%)]\tLosses F.softmax: 0.000130 log_softmax: 0.000021\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1726\tAccuracy: 8113.0/10000 (81%)\n",
            "log_softmax: Loss: 1.2063\tAccuracy: 8050.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 495 [0/1000 (0%)]\tLosses F.softmax: 0.000006 log_softmax: 0.000001\n",
            "Train Epoch: 495 [200/1000 (20%)]\tLosses F.softmax: 0.000028 log_softmax: 0.000109\n",
            "Train Epoch: 495 [400/1000 (40%)]\tLosses F.softmax: 0.001332 log_softmax: 0.001964\n",
            "Train Epoch: 495 [600/1000 (60%)]\tLosses F.softmax: 0.002195 log_softmax: 0.009398\n",
            "Train Epoch: 495 [800/1000 (80%)]\tLosses F.softmax: 0.011895 log_softmax: 0.009631\n",
            "Train Epoch: 495 [1000/1000 (100%)]\tLosses F.softmax: 0.000317 log_softmax: 0.000725\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1729\tAccuracy: 8113.0/10000 (81%)\n",
            "log_softmax: Loss: 1.2066\tAccuracy: 8050.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 496 [0/1000 (0%)]\tLosses F.softmax: 0.000021 log_softmax: 0.000005\n",
            "Train Epoch: 496 [200/1000 (20%)]\tLosses F.softmax: 0.000098 log_softmax: 0.002600\n",
            "Train Epoch: 496 [400/1000 (40%)]\tLosses F.softmax: 0.000039 log_softmax: 0.000004\n",
            "Train Epoch: 496 [600/1000 (60%)]\tLosses F.softmax: 0.000015 log_softmax: 0.000001\n",
            "Train Epoch: 496 [800/1000 (80%)]\tLosses F.softmax: 0.000354 log_softmax: 0.000023\n",
            "Train Epoch: 496 [1000/1000 (100%)]\tLosses F.softmax: 0.000588 log_softmax: 0.000488\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1731\tAccuracy: 8115.0/10000 (81%)\n",
            "log_softmax: Loss: 1.2069\tAccuracy: 8048.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 497 [0/1000 (0%)]\tLosses F.softmax: 0.000023 log_softmax: 0.000002\n",
            "Train Epoch: 497 [200/1000 (20%)]\tLosses F.softmax: 0.007355 log_softmax: 0.001755\n",
            "Train Epoch: 497 [400/1000 (40%)]\tLosses F.softmax: 0.000903 log_softmax: 0.000730\n",
            "Train Epoch: 497 [600/1000 (60%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000001\n",
            "Train Epoch: 497 [800/1000 (80%)]\tLosses F.softmax: 0.018725 log_softmax: 0.007213\n",
            "Train Epoch: 497 [1000/1000 (100%)]\tLosses F.softmax: 0.000070 log_softmax: 0.000008\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1732\tAccuracy: 8118.0/10000 (81%)\n",
            "log_softmax: Loss: 1.2070\tAccuracy: 8044.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 498 [0/1000 (0%)]\tLosses F.softmax: 0.000041 log_softmax: 0.000223\n",
            "Train Epoch: 498 [200/1000 (20%)]\tLosses F.softmax: 0.003669 log_softmax: 0.007830\n",
            "Train Epoch: 498 [400/1000 (40%)]\tLosses F.softmax: 0.000902 log_softmax: 0.004188\n",
            "Train Epoch: 498 [600/1000 (60%)]\tLosses F.softmax: 0.001929 log_softmax: 0.002368\n",
            "Train Epoch: 498 [800/1000 (80%)]\tLosses F.softmax: 0.000038 log_softmax: 0.000273\n",
            "Train Epoch: 498 [1000/1000 (100%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000000\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1738\tAccuracy: 8116.0/10000 (81%)\n",
            "log_softmax: Loss: 1.2077\tAccuracy: 8051.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 499 [0/1000 (0%)]\tLosses F.softmax: 0.000000 log_softmax: 0.000000\n",
            "Train Epoch: 499 [200/1000 (20%)]\tLosses F.softmax: 0.000036 log_softmax: 0.000003\n",
            "Train Epoch: 499 [400/1000 (40%)]\tLosses F.softmax: 0.000470 log_softmax: 0.001013\n",
            "Train Epoch: 499 [600/1000 (60%)]\tLosses F.softmax: 0.000895 log_softmax: 0.000027\n",
            "Train Epoch: 499 [800/1000 (80%)]\tLosses F.softmax: 0.000786 log_softmax: 0.003297\n",
            "Train Epoch: 499 [1000/1000 (100%)]\tLosses F.softmax: 0.001385 log_softmax: 0.011325\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1741\tAccuracy: 8116.0/10000 (81%)\n",
            "log_softmax: Loss: 1.2079\tAccuracy: 8048.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 500 [0/1000 (0%)]\tLosses F.softmax: 0.018619 log_softmax: 0.009239\n",
            "Train Epoch: 500 [200/1000 (20%)]\tLosses F.softmax: 0.001112 log_softmax: 0.004318\n",
            "Train Epoch: 500 [400/1000 (40%)]\tLosses F.softmax: 0.012970 log_softmax: 0.009718\n",
            "Train Epoch: 500 [600/1000 (60%)]\tLosses F.softmax: 0.004284 log_softmax: 0.000091\n",
            "Train Epoch: 500 [800/1000 (80%)]\tLosses F.softmax: 0.000069 log_softmax: 0.000023\n",
            "Train Epoch: 500 [1000/1000 (100%)]\tLosses F.softmax: 0.000516 log_softmax: 0.000148\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1744\tAccuracy: 8114.0/10000 (81%)\n",
            "log_softmax: Loss: 1.2081\tAccuracy: 8048.0/10000 (80%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jv5dQ_scbLX4",
        "colab_type": "code",
        "outputId": "04c6b561-60e4-4d6a-d1d4-8d672035bf29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        }
      },
      "source": [
        "from pylab import rcParams\n",
        "rcParams['figure.figsize'] = 20, 10\n",
        "plot_graphs(test_log, 'loss')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZJYmqLhmuAf",
        "colab_type": "code",
        "outputId": "a813a762-100a-4215-9a7e-df0576b4b87c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        }
      },
      "source": [
        "plot_graphs(test_log, 'accuracy')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYi8f4pRmw73",
        "colab_type": "code",
        "outputId": "7eee6176-12d8-4ffb-dd14-e9c053ed8b1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "test_log"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'F.softmax': [(2.304551475071907, tensor(8.9200)),\n",
              "  (2.303847157692909, tensor(8.9200)),\n",
              "  (2.303181861424446, tensor(8.9200)),\n",
              "  (2.3025501314640047, tensor(8.9200)),\n",
              "  (2.301946649646759, tensor(8.8900)),\n",
              "  (2.3013658689022063, tensor(8.8600)),\n",
              "  (2.3008060288906096, tensor(11.3800)),\n",
              "  (2.300259348535538, tensor(14.3000)),\n",
              "  (2.2997272054195403, tensor(15.1600)),\n",
              "  (2.299208303928375, tensor(16.5400)),\n",
              "  (2.298701072525978, tensor(17.8100)),\n",
              "  (2.2982047543764113, tensor(18.7200)),\n",
              "  (2.297715859556198, tensor(19.2100)),\n",
              "  (2.297231854367256, tensor(19.3600)),\n",
              "  (2.2967528824567793, tensor(19.4800)),\n",
              "  (2.2962812967538833, tensor(19.5900)),\n",
              "  (2.2958118710517885, tensor(19.4500)),\n",
              "  (2.295340613102913, tensor(19.4600)),\n",
              "  (2.2948691939353942, tensor(19.5100)),\n",
              "  (2.294390573835373, tensor(19.4800)),\n",
              "  (2.2939018749475477, tensor(19.4800)),\n",
              "  (2.293406449460983, tensor(19.4500)),\n",
              "  (2.2929005105495452, tensor(19.5000)),\n",
              "  (2.2923816130399706, tensor(19.4400)),\n",
              "  (2.291855415225029, tensor(19.5600)),\n",
              "  (2.291315243768692, tensor(19.5500)),\n",
              "  (2.2907617955207824, tensor(19.5700)),\n",
              "  (2.290190109705925, tensor(19.6300)),\n",
              "  (2.2896024260520935, tensor(19.6300)),\n",
              "  (2.2890008364677428, tensor(19.6800)),\n",
              "  (2.2883772092342376, tensor(19.7600)),\n",
              "  (2.287735017514229, tensor(19.8100)),\n",
              "  (2.287065384078026, tensor(19.8200)),\n",
              "  (2.2863743426561354, tensor(19.8700)),\n",
              "  (2.2856606293678285, tensor(19.9300)),\n",
              "  (2.2849152121782303, tensor(19.9900)),\n",
              "  (2.284142865204811, tensor(19.9700)),\n",
              "  (2.2833407975912094, tensor(19.9600)),\n",
              "  (2.282511142849922, tensor(20.0100)),\n",
              "  (2.281648454642296, tensor(20.0800)),\n",
              "  (2.280747130346298, tensor(20.1000)),\n",
              "  (2.2798085918188096, tensor(20.1500)),\n",
              "  (2.278831224155426, tensor(20.2000)),\n",
              "  (2.277806774234772, tensor(20.2100)),\n",
              "  (2.2767438103675843, tensor(20.2500)),\n",
              "  (2.2756306968450546, tensor(20.2700)),\n",
              "  (2.2744577265262604, tensor(20.2700)),\n",
              "  (2.2732416696548463, tensor(20.3000)),\n",
              "  (2.2719678364038467, tensor(20.3000)),\n",
              "  (2.2706438932180406, tensor(20.3200)),\n",
              "  (2.2692492731809617, tensor(20.3400)),\n",
              "  (2.267785398364067, tensor(20.3500)),\n",
              "  (2.266247190237045, tensor(20.3400)),\n",
              "  (2.26462723236084, tensor(20.3400)),\n",
              "  (2.2629243807792663, tensor(20.3400)),\n",
              "  (2.2611339611291887, tensor(20.3500)),\n",
              "  (2.259256744980812, tensor(20.3700)),\n",
              "  (2.2572698333501817, tensor(20.4600)),\n",
              "  (2.255191293859482, tensor(20.6200)),\n",
              "  (2.252987124836445, tensor(20.9000)),\n",
              "  (2.2506569699048997, tensor(21.1800)),\n",
              "  (2.248189132845402, tensor(21.5600)),\n",
              "  (2.2455806130170823, tensor(21.9900)),\n",
              "  (2.242809173321724, tensor(22.4500)),\n",
              "  (2.2398922568202018, tensor(22.7600)),\n",
              "  (2.2367921944737432, tensor(23.3100)),\n",
              "  (2.2335024356484414, tensor(23.7100)),\n",
              "  (2.230009541845322, tensor(24.2300)),\n",
              "  (2.2263026240348815, tensor(24.6700)),\n",
              "  (2.222326708614826, tensor(25.1600)),\n",
              "  (2.218104458510876, tensor(25.5700)),\n",
              "  (2.213590088570118, tensor(25.9300)),\n",
              "  (2.208810194694996, tensor(26.5000)),\n",
              "  (2.203687749505043, tensor(27.0500)),\n",
              "  (2.1982391971588133, tensor(27.5200)),\n",
              "  (2.1924233686447145, tensor(27.9600)),\n",
              "  (2.1861350621223448, tensor(28.3900)),\n",
              "  (2.1794649084329607, tensor(28.7300)),\n",
              "  (2.172303294992447, tensor(29.0700)),\n",
              "  (2.164649887907505, tensor(29.2900)),\n",
              "  (2.156460469841957, tensor(29.5900)),\n",
              "  (2.1475953814864157, tensor(30.0900)),\n",
              "  (2.1382015640854837, tensor(30.2400)),\n",
              "  (2.128120809316635, tensor(30.5500)),\n",
              "  (2.1173679050445555, tensor(30.8900)),\n",
              "  (2.105730950343609, tensor(31.1600)),\n",
              "  (2.093295254313946, tensor(31.4700)),\n",
              "  (2.0800073819637297, tensor(31.8600)),\n",
              "  (2.0657107633471488, tensor(32.2000)),\n",
              "  (2.0506116475105287, tensor(32.6000)),\n",
              "  (2.03472463953495, tensor(32.8500)),\n",
              "  (2.0173182436704638, tensor(33.0900)),\n",
              "  (1.9991043424606323, tensor(33.4200)),\n",
              "  (1.9799827070832252, tensor(33.4400)),\n",
              "  (1.9598618333876132, tensor(33.9800)),\n",
              "  (1.9385180675566196, tensor(34.3700)),\n",
              "  (1.9165001199901104, tensor(34.8300)),\n",
              "  (1.893794929176569, tensor(35.)),\n",
              "  (1.8700255234241485, tensor(35.7000)),\n",
              "  (1.8456313696861266, tensor(36.0500)),\n",
              "  (1.8212448925077915, tensor(36.3400)),\n",
              "  (1.7959960416674614, tensor(36.9300)),\n",
              "  (1.770829125869274, tensor(37.3500)),\n",
              "  (1.746529632538557, tensor(37.8900)),\n",
              "  (1.721499841940403, tensor(38.6500)),\n",
              "  (1.6970845614522696, tensor(39.4800)),\n",
              "  (1.6724743461221456, tensor(39.9300)),\n",
              "  (1.648720426094532, tensor(40.8500)),\n",
              "  (1.6250288347750903, tensor(41.8700)),\n",
              "  (1.6037268224090337, tensor(42.9600)),\n",
              "  (1.5815275649815799, tensor(43.9100)),\n",
              "  (1.560726956117153, tensor(44.8400)),\n",
              "  (1.5398805160045623, tensor(45.9200)),\n",
              "  (1.5210104154855013, tensor(46.8700)),\n",
              "  (1.5016662535518408, tensor(47.9100)),\n",
              "  (1.4826956912100315, tensor(48.8600)),\n",
              "  (1.465952423608303, tensor(49.1700)),\n",
              "  (1.4474505048885942, tensor(49.7500)),\n",
              "  (1.4309910327732562, tensor(50.4300)),\n",
              "  (1.4143349695026874, tensor(50.9100)),\n",
              "  (1.39842070209831, tensor(51.2800)),\n",
              "  (1.3824579004734754, tensor(51.9700)),\n",
              "  (1.3682076113805175, tensor(52.6600)),\n",
              "  (1.3560257462739944, tensor(52.8500)),\n",
              "  (1.3392193591833115, tensor(53.6100)),\n",
              "  (1.324914712318778, tensor(53.8600)),\n",
              "  (1.3130070241764187, tensor(54.4600)),\n",
              "  (1.2999740543544291, tensor(55.0700)),\n",
              "  (1.2893377731233835, tensor(55.1900)),\n",
              "  (1.277734935298562, tensor(55.4600)),\n",
              "  (1.2648402774557472, tensor(56.1800)),\n",
              "  (1.2524678105279803, tensor(56.6900)),\n",
              "  (1.2420934663891792, tensor(57.2700)),\n",
              "  (1.2309942489638925, tensor(58.2200)),\n",
              "  (1.2181053236737847, tensor(58.7200)),\n",
              "  (1.2099250106021762, tensor(58.6200)),\n",
              "  (1.2004589324519037, tensor(58.7500)),\n",
              "  (1.1904565375521778, tensor(59.5000)),\n",
              "  (1.1805469577252865, tensor(59.9500)),\n",
              "  (1.1726148856289684, tensor(60.4400)),\n",
              "  (1.1662555793367326, tensor(60.3400)),\n",
              "  (1.1540694239161908, tensor(60.8700)),\n",
              "  (1.1471812714606524, tensor(60.8700)),\n",
              "  (1.139867101957649, tensor(61.1900)),\n",
              "  (1.1304975237593056, tensor(63.2000)),\n",
              "  (1.1241849139049649, tensor(61.8900)),\n",
              "  (1.1170071047104895, tensor(63.4000)),\n",
              "  (1.1087928397253155, tensor(63.1400)),\n",
              "  (1.101013266339153, tensor(63.7200)),\n",
              "  (1.0938961716920137, tensor(64.0400)),\n",
              "  (1.085092305111885, tensor(63.8200)),\n",
              "  (1.0818766976639629, tensor(64.4600)),\n",
              "  (1.072647505439073, tensor(65.1400)),\n",
              "  (1.066662851525098, tensor(64.7800)),\n",
              "  (1.0582205416180193, tensor(65.0600)),\n",
              "  (1.0535191604927183, tensor(65.4900)),\n",
              "  (1.0448922615870833, tensor(66.0300)),\n",
              "  (1.0430120711714028, tensor(66.2300)),\n",
              "  (1.03401827224195, tensor(66.1400)),\n",
              "  (1.026183025931567, tensor(66.2300)),\n",
              "  (1.0193361859228462, tensor(66.5600)),\n",
              "  (1.0136479365617037, tensor(67.6000)),\n",
              "  (1.0145196699753403, tensor(67.3700)),\n",
              "  (1.0011443245202303, tensor(67.7300)),\n",
              "  (0.9956306411519646, tensor(67.6500)),\n",
              "  (0.9902662916615605, tensor(67.7800)),\n",
              "  (0.9862726002659649, tensor(68.0300)),\n",
              "  (0.9762466658439487, tensor(68.5900)),\n",
              "  (0.9693726123936475, tensor(68.8200)),\n",
              "  (0.9653564732603729, tensor(68.6500)),\n",
              "  (0.958416101232171, tensor(68.9700)),\n",
              "  (0.9543794637195766, tensor(69.0500)),\n",
              "  (0.9455321010552347, tensor(69.5700)),\n",
              "  (0.9419026329662651, tensor(69.9200)),\n",
              "  (0.9361678497515619, tensor(69.4400)),\n",
              "  (0.9338424078511074, tensor(69.9200)),\n",
              "  (0.920081972451508, tensor(70.6500)),\n",
              "  (0.9168374264769256, tensor(70.4500)),\n",
              "  (0.9138416306877509, tensor(70.7200)),\n",
              "  (0.9067089423030615, tensor(70.6800)),\n",
              "  (0.8974078223431483, tensor(71.2400)),\n",
              "  (0.8929406231394038, tensor(71.2100)),\n",
              "  (0.8961027463672683, tensor(70.4000)),\n",
              "  (0.8830399931732565, tensor(71.6800)),\n",
              "  (0.8750061880318448, tensor(71.7900)),\n",
              "  (0.8723606030927971, tensor(71.9500)),\n",
              "  (0.866997486906685, tensor(72.5200)),\n",
              "  (0.8619193984413519, tensor(72.0700)),\n",
              "  (0.8565714696314186, tensor(72.3200)),\n",
              "  (0.8471582936570048, tensor(72.7300)),\n",
              "  (0.8443780357422307, tensor(72.7400)),\n",
              "  (0.8370139234464615, tensor(73.5800)),\n",
              "  (0.8361239478253759, tensor(72.8600)),\n",
              "  (0.8288278396569192, tensor(73.5500)),\n",
              "  (0.8267591025408357, tensor(73.3300)),\n",
              "  (0.8239567509230227, tensor(73.4800)),\n",
              "  (0.8191569165523164, tensor(73.9800)),\n",
              "  (0.8108959409021772, tensor(73.9400)),\n",
              "  (0.8104390732329338, tensor(74.0500)),\n",
              "  (0.8049810338303447, tensor(74.1100)),\n",
              "  (0.8049130528986455, tensor(74.4800)),\n",
              "  (0.795056330345478, tensor(74.2200)),\n",
              "  (0.7960831387594343, tensor(74.1000)),\n",
              "  (0.7892057877491694, tensor(74.6600)),\n",
              "  (0.7877351687139366, tensor(74.4400)),\n",
              "  (0.7794541861434467, tensor(74.9100)),\n",
              "  (0.782511193374265, tensor(74.6300)),\n",
              "  (0.7767651378524024, tensor(75.1200)),\n",
              "  (0.775352007739013, tensor(75.2500)),\n",
              "  (0.7695524178855587, tensor(75.2300)),\n",
              "  (0.7760795959956944, tensor(74.7100)),\n",
              "  (0.7610578749340028, tensor(75.6000)),\n",
              "  (0.7618906391641125, tensor(75.4900)),\n",
              "  (0.7574388637026772, tensor(75.7200)),\n",
              "  (0.7534382515214384, tensor(75.9700)),\n",
              "  (0.7562253675259184, tensor(75.3500)),\n",
              "  (0.7483972010217607, tensor(76.2800)),\n",
              "  (0.7472750082308427, tensor(76.0200)),\n",
              "  (0.743298010803666, tensor(76.6200)),\n",
              "  (0.7410789869854226, tensor(76.5700)),\n",
              "  (0.7400589835789986, tensor(76.3900)),\n",
              "  (0.735801524351188, tensor(76.7400)),\n",
              "  (0.7327238550295122, tensor(76.9100)),\n",
              "  (0.7346610138209304, tensor(76.6300)),\n",
              "  (0.7311033191111405, tensor(77.0700)),\n",
              "  (0.7240602745486889, tensor(77.4600)),\n",
              "  (0.723816410242347, tensor(77.1000)),\n",
              "  (0.7257345512482105, tensor(77.0700)),\n",
              "  (0.7243413464002079, tensor(77.0900)),\n",
              "  (0.7190069484768435, tensor(77.5700)),\n",
              "  (0.7172198154369834, tensor(77.9900)),\n",
              "  (0.7153972386595793, tensor(77.5800)),\n",
              "  (0.7152178998229094, tensor(77.5300)),\n",
              "  (0.7149294407997281, tensor(77.5600)),\n",
              "  (0.7147248717410722, tensor(77.8900)),\n",
              "  (0.7128040168867447, tensor(77.6600)),\n",
              "  (0.7116199148666812, tensor(77.9900)),\n",
              "  (0.7057567808653926, tensor(77.9300)),\n",
              "  (0.7049000088743865, tensor(78.1500)),\n",
              "  (0.7050279498161748, tensor(78.0400)),\n",
              "  (0.7051743001237046, tensor(78.0800)),\n",
              "  (0.7066378731045173, tensor(78.2300)),\n",
              "  (0.7020878163164714, tensor(78.4300)),\n",
              "  (0.7012822060125764, tensor(78.3600)),\n",
              "  (0.7023742474962491, tensor(78.3300)),\n",
              "  (0.7049611033953843, tensor(77.9600)),\n",
              "  (0.6988495209389249, tensor(78.5600)),\n",
              "  (0.6947157086281455, tensor(78.7100)),\n",
              "  (0.6940074189346865, tensor(78.6100)),\n",
              "  (0.6914921870225226, tensor(78.9900)),\n",
              "  (0.6951346528335475, tensor(78.6700)),\n",
              "  (0.6966571059315465, tensor(78.3000)),\n",
              "  (0.692476738582889, tensor(78.9100)),\n",
              "  (0.6900342326005455, tensor(78.8600)),\n",
              "  (0.6882952068524435, tensor(78.9800)),\n",
              "  (0.6927559951479896, tensor(78.6200)),\n",
              "  (0.6868524590212037, tensor(79.2100)),\n",
              "  (0.6859348233293276, tensor(78.9900)),\n",
              "  (0.6864325884520542, tensor(79.5400)),\n",
              "  (0.6842004551543505, tensor(79.1500)),\n",
              "  (0.68523540207888, tensor(79.1800)),\n",
              "  (0.6812757313426817, tensor(79.3200)),\n",
              "  (0.6815348470443161, tensor(79.4000)),\n",
              "  (0.6823216004529852, tensor(79.5500)),\n",
              "  (0.6841429287868493, tensor(79.3200)),\n",
              "  (0.6816475748649274, tensor(79.6600)),\n",
              "  (0.6802287005238119, tensor(79.4000)),\n",
              "  (0.678502421070286, tensor(80.0300)),\n",
              "  (0.6832682689351961, tensor(79.4200)),\n",
              "  (0.6852654670378601, tensor(79.4900)),\n",
              "  (0.6843351399697305, tensor(79.2900)),\n",
              "  (0.6897321363611031, tensor(78.9600)),\n",
              "  (0.682521457217785, tensor(79.3200)),\n",
              "  (0.680912428785389, tensor(79.6600)),\n",
              "  (0.6777678224779841, tensor(79.6200)),\n",
              "  (0.6823197385751526, tensor(79.3700)),\n",
              "  (0.6775337588253489, tensor(79.6200)),\n",
              "  (0.6791072415423696, tensor(79.8100)),\n",
              "  (0.6755562058010605, tensor(79.7600)),\n",
              "  (0.673395721012092, tensor(80.0300)),\n",
              "  (0.6780404110052041, tensor(79.6800)),\n",
              "  (0.6761658394612255, tensor(79.8900)),\n",
              "  (0.6736223119774019, tensor(79.9700)),\n",
              "  (0.6819650223793986, tensor(79.5600)),\n",
              "  (0.677370664003538, tensor(79.8800)),\n",
              "  (0.6773019565322961, tensor(79.8300)),\n",
              "  (0.6759228432362259, tensor(79.9100)),\n",
              "  (0.6765547039027413, tensor(79.9100)),\n",
              "  (0.675928379673083, tensor(79.9800)),\n",
              "  (0.6788271277514636, tensor(79.8500)),\n",
              "  (0.6779622804335522, tensor(80.0500)),\n",
              "  (0.6770494283606531, tensor(80.0700)),\n",
              "  (0.6850617483707844, tensor(79.7900)),\n",
              "  (0.6757397166449053, tensor(80.1900)),\n",
              "  (0.6796324707481748, tensor(80.1800)),\n",
              "  (0.6746145492466458, tensor(80.2400)),\n",
              "  (0.6756539967987948, tensor(80.0800)),\n",
              "  (0.6733152077829465, tensor(80.3100)),\n",
              "  (0.6708414284741885, tensor(80.5100)),\n",
              "  (0.6732350839512016, tensor(80.1900)),\n",
              "  (0.6801796912526566, tensor(80.1800)),\n",
              "  (0.6751138733340893, tensor(80.2900)),\n",
              "  (0.6768313550592662, tensor(80.3400)),\n",
              "  (0.6807691129705403, tensor(79.9900)),\n",
              "  (0.6816841884434922, tensor(80.2900)),\n",
              "  (0.678620849487136, tensor(80.2900)),\n",
              "  (0.6737156031299237, tensor(80.3900)),\n",
              "  (0.6768400076872902, tensor(80.3100)),\n",
              "  (0.6774446821751772, tensor(80.2500)),\n",
              "  (0.680255964170958, tensor(80.2200)),\n",
              "  (0.6725244758830929, tensor(80.7200)),\n",
              "  (0.6785250319743107, tensor(80.4800)),\n",
              "  (0.6798988208201757, tensor(80.2200)),\n",
              "  (0.6779124596869311, tensor(80.3700)),\n",
              "  (0.6819383520340969, tensor(80.1200)),\n",
              "  (0.6843828343100162, tensor(80.1400)),\n",
              "  (0.6818089835564446, tensor(80.3000)),\n",
              "  (0.6825869775863844, tensor(80.3900)),\n",
              "  (0.6803516291896377, tensor(80.3700)),\n",
              "  (0.6806861323538076, tensor(80.5200)),\n",
              "  (0.6856619970517568, tensor(80.2000)),\n",
              "  (0.6908917412703318, tensor(80.0600)),\n",
              "  (0.6847375223933398, tensor(80.3800)),\n",
              "  (0.6879689762356953, tensor(80.2200)),\n",
              "  (0.6867474397660611, tensor(80.2900)),\n",
              "  (0.6870914074007757, tensor(80.2500)),\n",
              "  (0.6857843427187967, tensor(80.3500)),\n",
              "  (0.6850801237753309, tensor(80.5100)),\n",
              "  (0.69515991627201, tensor(80.2800)),\n",
              "  (0.6896116096622805, tensor(80.2100)),\n",
              "  (0.6923030683846955, tensor(80.0100)),\n",
              "  (0.6918538823199779, tensor(80.1100)),\n",
              "  (0.6915128207584974, tensor(80.4100)),\n",
              "  (0.6905021159292599, tensor(80.3400)),\n",
              "  (0.6941063149085676, tensor(80.3400)),\n",
              "  (0.6961153247104441, tensor(80.1700)),\n",
              "  (0.6923444351416147, tensor(80.3500)),\n",
              "  (0.6970932546135896, tensor(80.2900)),\n",
              "  (0.7016669094883072, tensor(80.1500)),\n",
              "  (0.7043439345054787, tensor(79.9400)),\n",
              "  (0.6992396820731818, tensor(80.2900)),\n",
              "  (0.6979660719624801, tensor(80.2500)),\n",
              "  (0.6988121521356297, tensor(80.4200)),\n",
              "  (0.6990871108675376, tensor(80.3300)),\n",
              "  (0.7012030293536525, tensor(80.3100)),\n",
              "  (0.7057235552657032, tensor(80.1300)),\n",
              "  (0.7014955430889306, tensor(80.3200)),\n",
              "  (0.7026905204724692, tensor(80.3500)),\n",
              "  (0.702563441358519, tensor(80.4700)),\n",
              "  (0.7021699508989834, tensor(80.4600)),\n",
              "  (0.7097058822987696, tensor(80.1400)),\n",
              "  (0.7087504296345094, tensor(80.3400)),\n",
              "  (0.705207664343221, tensor(80.6300)),\n",
              "  (0.7112186796172416, tensor(80.2800)),\n",
              "  (0.714275404597339, tensor(80.0900)),\n",
              "  (0.7102507284786952, tensor(80.3300)),\n",
              "  (0.7151775008789792, tensor(80.1800)),\n",
              "  (0.7136727843928571, tensor(80.1800)),\n",
              "  (0.7180474685044592, tensor(80.2200)),\n",
              "  (0.7193408558062063, tensor(80.4300)),\n",
              "  (0.7208634678441489, tensor(80.0900)),\n",
              "  (0.7256899413477491, tensor(79.9000)),\n",
              "  (0.7205959294946149, tensor(80.4500)),\n",
              "  (0.7191907568675104, tensor(80.4700)),\n",
              "  (0.7190500821817017, tensor(80.3600)),\n",
              "  (0.7215622323663898, tensor(80.3000)),\n",
              "  (0.7250125694421049, tensor(80.2200)),\n",
              "  (0.7248761834924909, tensor(80.3100)),\n",
              "  (0.7283978968543408, tensor(80.2200)),\n",
              "  (0.740256391883844, tensor(79.7100)),\n",
              "  (0.7281777227070173, tensor(80.4700)),\n",
              "  (0.7273897004189918, tensor(80.5400)),\n",
              "  (0.7320709607221355, tensor(80.1900)),\n",
              "  (0.7309982153607268, tensor(80.5900)),\n",
              "  (0.733353938380712, tensor(80.3100)),\n",
              "  (0.7355611002922018, tensor(80.2100)),\n",
              "  (0.7336693922929133, tensor(80.3500)),\n",
              "  (0.7405008412936802, tensor(80.1500)),\n",
              "  (0.7370089450723208, tensor(80.4400)),\n",
              "  (0.7406653272804564, tensor(80.3000)),\n",
              "  (0.7402691493666091, tensor(80.3200)),\n",
              "  (0.7473878296890591, tensor(80.0300)),\n",
              "  (0.7458912352777276, tensor(80.2200)),\n",
              "  (0.7490695261231644, tensor(80.0600)),\n",
              "  (0.7479986728988941, tensor(80.2000)),\n",
              "  (0.7471265452816342, tensor(80.3200)),\n",
              "  (0.7490697110773408, tensor(80.3200)),\n",
              "  (0.7499332841165053, tensor(80.3000)),\n",
              "  (0.7616358411977314, tensor(79.8900)),\n",
              "  (0.7550631518425904, tensor(80.2400)),\n",
              "  (0.7519346758161793, tensor(80.3800)),\n",
              "  (0.7552863139176257, tensor(80.2500)),\n",
              "  (0.7529698980283394, tensor(80.5900)),\n",
              "  (0.758593965286441, tensor(80.2200)),\n",
              "  (0.7570074973497334, tensor(80.4100)),\n",
              "  (0.7591276141659394, tensor(80.2600)),\n",
              "  (0.7636942207835138, tensor(80.2900)),\n",
              "  (0.7600762398834958, tensor(80.4200)),\n",
              "  (0.7613984736028425, tensor(80.4100)),\n",
              "  (0.7631266829359618, tensor(80.5500)),\n",
              "  (0.7725745982299383, tensor(80.1200)),\n",
              "  (0.7691870597482666, tensor(80.2500)),\n",
              "  (0.7734264737395345, tensor(80.2100)),\n",
              "  (0.7728975517866082, tensor(80.2200)),\n",
              "  (0.7823110775132651, tensor(79.9100)),\n",
              "  (0.7812308696669632, tensor(80.0100)),\n",
              "  (0.7768407720765331, tensor(80.3400)),\n",
              "  (0.7796816727449228, tensor(80.2000)),\n",
              "  (0.7748225093365644, tensor(80.4300)),\n",
              "  (0.7814951667992666, tensor(80.2600)),\n",
              "  (0.7803857636110926, tensor(80.4500)),\n",
              "  (0.7811759210205879, tensor(80.4600)),\n",
              "  (0.7803837902773254, tensor(80.4300)),\n",
              "  (0.7831662519068492, tensor(80.4900)),\n",
              "  (0.7867378542884026, tensor(80.2800)),\n",
              "  (0.7863143195876839, tensor(80.5500)),\n",
              "  (0.7865716217640476, tensor(80.3600)),\n",
              "  (0.7920970618219636, tensor(80.3100)),\n",
              "  (0.7975428264904917, tensor(80.2100)),\n",
              "  (0.7927905518692521, tensor(80.3800)),\n",
              "  (0.7948212250175114, tensor(80.3500)),\n",
              "  (0.7958104203936152, tensor(80.4700)),\n",
              "  (0.8031411474565513, tensor(80.1900)),\n",
              "  (0.7991310497393433, tensor(80.3100)),\n",
              "  (0.8008203062599281, tensor(80.2900)),\n",
              "  (0.7992602293041885, tensor(80.4900)),\n",
              "  (0.8035188067927302, tensor(80.2900)),\n",
              "  (0.8060889757219755, tensor(80.3600)),\n",
              "  (0.8063015559987521, tensor(80.4100)),\n",
              "  (0.8074365279458827, tensor(80.4400)),\n",
              "  (0.8095662209336459, tensor(80.3700)),\n",
              "  (0.8086818725081545, tensor(80.4600)),\n",
              "  (0.8077919777301094, tensor(80.4800)),\n",
              "  (0.8130207783020426, tensor(80.3900)),\n",
              "  (0.812264691958283, tensor(80.4200)),\n",
              "  (0.8160356621116699, tensor(80.2500)),\n",
              "  (0.817902868392917, tensor(80.4800)),\n",
              "  (0.8165008382816398, tensor(80.4900)),\n",
              "  (0.8189872572880893, tensor(80.4700)),\n",
              "  (0.8196777612749647, tensor(80.5200)),\n",
              "  (0.8180758103186805, tensor(80.5600)),\n",
              "  (0.8231940967774423, tensor(80.4100)),\n",
              "  (0.8215894335398931, tensor(80.5500)),\n",
              "  (0.8215360334597526, tensor(80.6000)),\n",
              "  (0.8263624802840446, tensor(80.4200)),\n",
              "  (0.8268656901828941, tensor(80.4300)),\n",
              "  (0.8273956559316921, tensor(80.5000)),\n",
              "  (0.8277591533413883, tensor(80.6400)),\n",
              "  (0.829113804348403, tensor(80.6500)),\n",
              "  (0.8290159280448208, tensor(80.7000)),\n",
              "  (0.8304872421547826, tensor(80.6400)),\n",
              "  (0.8342599662719876, tensor(80.5200)),\n",
              "  (0.8335997311339305, tensor(80.6100)),\n",
              "  (0.8375867581506407, tensor(80.5200)),\n",
              "  (0.8425621465411381, tensor(80.3400)),\n",
              "  (0.8385539883685562, tensor(80.7100)),\n",
              "  (0.8396949788035086, tensor(80.4700)),\n",
              "  (0.8399918382249132, tensor(80.7100)),\n",
              "  (0.8421549877026311, tensor(80.6200)),\n",
              "  (0.846056272461855, tensor(80.4700)),\n",
              "  (0.8439669477851337, tensor(80.7700)),\n",
              "  (0.845692866761588, tensor(80.7200)),\n",
              "  (0.8471864195999731, tensor(80.6100)),\n",
              "  (0.8487849861698719, tensor(80.6700)),\n",
              "  (0.847407444632523, tensor(80.7800)),\n",
              "  (0.8515964571466442, tensor(80.4500)),\n",
              "  (0.853844573730827, tensor(80.4900)),\n",
              "  (0.8543267543497132, tensor(80.6000)),\n",
              "  (0.8585870368164111, tensor(80.4800)),\n",
              "  (0.8555747958740183, tensor(80.6800)),\n",
              "  (0.8592677476253712, tensor(80.5100)),\n",
              "  (0.8564086374944553, tensor(80.8400)),\n",
              "  (0.8602554710518936, tensor(80.5700)),\n",
              "  (0.8613089410993369, tensor(80.6600)),\n",
              "  (0.8622567942340166, tensor(80.6700)),\n",
              "  (0.8633637777746453, tensor(80.6500)),\n",
              "  (0.8652861698996017, tensor(80.5400)),\n",
              "  (0.8652825215737022, tensor(80.6900)),\n",
              "  (0.8670121064458169, tensor(80.6800)),\n",
              "  (0.8674380806943147, tensor(80.6000)),\n",
              "  (0.8692034877354415, tensor(80.6500)),\n",
              "  (0.8686270779233187, tensor(80.7200)),\n",
              "  (0.8714187529757368, tensor(80.6100)),\n",
              "  (0.8735416013034403, tensor(80.6900)),\n",
              "  (0.8759638945565743, tensor(80.5700)),\n",
              "  (0.876584980987219, tensor(80.6800)),\n",
              "  (0.8745326820234928, tensor(80.6300)),\n",
              "  (0.8752767537904242, tensor(80.7400)),\n",
              "  (0.8791565465715375, tensor(80.6400)),\n",
              "  (0.8797898911770556, tensor(80.6700)),\n",
              "  (0.8828433205997529, tensor(80.5900)),\n",
              "  (0.8824388580806343, tensor(80.6400)),\n",
              "  (0.8834050169495069, tensor(80.6900)),\n",
              "  (0.8831093240829184, tensor(80.7100)),\n",
              "  (0.8840502570731922, tensor(80.7800)),\n",
              "  (0.8872786515527056, tensor(80.5400)),\n",
              "  (0.8879903027920045, tensor(80.6700)),\n",
              "  (0.8884785436707809, tensor(80.8200)),\n",
              "  (0.8909853601588195, tensor(80.6700)),\n",
              "  (0.8909463191819527, tensor(80.7200))],\n",
              " 'log_softmax': [(2.303101036286354, tensor(10.0900)),\n",
              "  (2.3024731009960173, tensor(10.0900)),\n",
              "  (2.301862487816811, tensor(10.1000)),\n",
              "  (2.301264090490341, tensor(9.9800)),\n",
              "  (2.3006789343833924, tensor(12.2700)),\n",
              "  (2.3001054943084718, tensor(13.0900)),\n",
              "  (2.2995424325466156, tensor(10.3900)),\n",
              "  (2.298986019849777, tensor(10.2800)),\n",
              "  (2.298438438630104, tensor(10.2800)),\n",
              "  (2.2978987448453903, tensor(10.2800)),\n",
              "  (2.2973667362213135, tensor(10.2800)),\n",
              "  (2.296843343114853, tensor(10.2800)),\n",
              "  (2.2963249523878098, tensor(10.2800)),\n",
              "  (2.2958114810943604, tensor(10.2800)),\n",
              "  (2.295297423386574, tensor(10.2800)),\n",
              "  (2.2947817198753357, tensor(10.2800)),\n",
              "  (2.294264592719078, tensor(10.2800)),\n",
              "  (2.2937445932626725, tensor(10.2800)),\n",
              "  (2.293219595837593, tensor(10.2800)),\n",
              "  (2.292685276222229, tensor(10.2900)),\n",
              "  (2.2921381835222245, tensor(10.3100)),\n",
              "  (2.291582089829445, tensor(10.3800)),\n",
              "  (2.291013379764557, tensor(10.4400)),\n",
              "  (2.290427771091461, tensor(10.5300)),\n",
              "  (2.2898301929712295, tensor(10.6800)),\n",
              "  (2.2892173222064973, tensor(10.8600)),\n",
              "  (2.2885857975006103, tensor(11.1700)),\n",
              "  (2.287932025051117, tensor(11.5000)),\n",
              "  (2.2872578048706056, tensor(11.8800)),\n",
              "  (2.286563755249977, tensor(12.2400)),\n",
              "  (2.2858407568216323, tensor(12.5400)),\n",
              "  (2.285094050073624, tensor(12.9700)),\n",
              "  (2.284312142896652, tensor(13.3900)),\n",
              "  (2.283504449224472, tensor(13.8800)),\n",
              "  (2.2826690838575363, tensor(14.3400)),\n",
              "  (2.2817917778730394, tensor(14.9000)),\n",
              "  (2.280878898191452, tensor(15.5900)),\n",
              "  (2.2799301167964936, tensor(16.2000)),\n",
              "  (2.278943868517876, tensor(16.7900)),\n",
              "  (2.277914681863785, tensor(17.0400)),\n",
              "  (2.276834213423729, tensor(17.5000)),\n",
              "  (2.2757052168130874, tensor(17.9300)),\n",
              "  (2.2745273856163024, tensor(18.3400)),\n",
              "  (2.27329123775959, tensor(18.7500)),\n",
              "  (2.2720062219142916, tensor(19.0900)),\n",
              "  (2.2706599888563157, tensor(19.4400)),\n",
              "  (2.2692406177282334, tensor(19.7000)),\n",
              "  (2.267759325695038, tensor(20.0200)),\n",
              "  (2.2662104074239733, tensor(20.2500)),\n",
              "  (2.264590957403183, tensor(20.3900)),\n",
              "  (2.262889904999733, tensor(20.5300)),\n",
              "  (2.2611003881931304, tensor(20.6300)),\n",
              "  (2.2592189123392106, tensor(20.8200)),\n",
              "  (2.2572354068517684, tensor(21.)),\n",
              "  (2.2551476386785505, tensor(21.3000)),\n",
              "  (2.252946000123024, tensor(21.7000)),\n",
              "  (2.250632824277878, tensor(22.0400)),\n",
              "  (2.248178599286079, tensor(22.6100)),\n",
              "  (2.245604249548912, tensor(23.1300)),\n",
              "  (2.2428773000240327, tensor(23.7100)),\n",
              "  (2.2399943423390387, tensor(24.1700)),\n",
              "  (2.236941632258892, tensor(24.8400)),\n",
              "  (2.23371836591959, tensor(25.3200)),\n",
              "  (2.2303002542972563, tensor(25.7800)),\n",
              "  (2.2266945323586462, tensor(26.1000)),\n",
              "  (2.2228639942646025, tensor(26.5100)),\n",
              "  (2.218792138683796, tensor(26.8200)),\n",
              "  (2.214464285647869, tensor(27.1000)),\n",
              "  (2.209888316512108, tensor(27.3700)),\n",
              "  (2.2049653083205225, tensor(27.7400)),\n",
              "  (2.1997350965619087, tensor(28.1100)),\n",
              "  (2.1941393447637556, tensor(28.5600)),\n",
              "  (2.1882197336554525, tensor(28.9400)),\n",
              "  (2.1818728526592253, tensor(29.2700)),\n",
              "  (2.1751237920165063, tensor(29.5200)),\n",
              "  (2.167935451388359, tensor(29.8200)),\n",
              "  (2.1601648359179495, tensor(30.2100)),\n",
              "  (2.1519337360024453, tensor(30.6000)),\n",
              "  (2.1431065133213996, tensor(30.9300)),\n",
              "  (2.133687694621086, tensor(31.1500)),\n",
              "  (2.123626660001278, tensor(31.4900)),\n",
              "  (2.1127537997603416, tensor(31.7600)),\n",
              "  (2.1012569850325584, tensor(31.8600)),\n",
              "  (2.0889331208467485, tensor(32.1100)),\n",
              "  (2.075822365796566, tensor(32.5700)),\n",
              "  (2.0617024853229524, tensor(32.9200)),\n",
              "  (2.0466410893440248, tensor(33.0400)),\n",
              "  (2.030626563692093, tensor(33.4800)),\n",
              "  (2.0135715499043463, tensor(33.8500)),\n",
              "  (1.9956686851620675, tensor(34.3600)),\n",
              "  (1.9769721494436263, tensor(34.5700)),\n",
              "  (1.9566123981833459, tensor(34.8000)),\n",
              "  (1.9355004559516906, tensor(35.3000)),\n",
              "  (1.9135804416298867, tensor(35.6900)),\n",
              "  (1.890798098707199, tensor(36.2200)),\n",
              "  (1.8666976365625858, tensor(36.8700)),\n",
              "  (1.8422232399702072, tensor(37.6300)),\n",
              "  (1.817113870859146, tensor(38.2800)),\n",
              "  (1.7913312433362008, tensor(39.1500)),\n",
              "  (1.7648028473734856, tensor(40.2100)),\n",
              "  (1.7383741011083127, tensor(40.9100)),\n",
              "  (1.7111021180033683, tensor(42.1900)),\n",
              "  (1.6841202868163585, tensor(42.9700)),\n",
              "  (1.657906843841076, tensor(43.8300)),\n",
              "  (1.631104106426239, tensor(44.8900)),\n",
              "  (1.6045335099458695, tensor(45.9300)),\n",
              "  (1.5775802139014006, tensor(46.8100)),\n",
              "  (1.5516045445382596, tensor(47.8900)),\n",
              "  (1.5258110055714846, tensor(48.9200)),\n",
              "  (1.502550866779685, tensor(50.3200)),\n",
              "  (1.4777829194426537, tensor(51.0300)),\n",
              "  (1.4549167248100043, tensor(52.1300)),\n",
              "  (1.4318216838777065, tensor(53.1000)),\n",
              "  (1.4112242056638002, tensor(53.8200)),\n",
              "  (1.3896456509232522, tensor(54.4800)),\n",
              "  (1.3686362110614776, tensor(55.1900)),\n",
              "  (1.3505273539185525, tensor(55.3900)),\n",
              "  (1.3306141659259796, tensor(55.9200)),\n",
              "  (1.31301074655056, tensor(56.6700)),\n",
              "  (1.2942802777826785, tensor(57.2000)),\n",
              "  (1.2771750945210456, tensor(57.4300)),\n",
              "  (1.2604380832403899, tensor(57.9300)),\n",
              "  (1.2451149720206858, tensor(58.8500)),\n",
              "  (1.2318210181459786, tensor(58.8300)),\n",
              "  (1.214006381240487, tensor(60.1700)),\n",
              "  (1.1992368917942047, tensor(59.8900)),\n",
              "  (1.1862646226361393, tensor(61.0100)),\n",
              "  (1.1717675639793277, tensor(61.6300)),\n",
              "  (1.1604936907216907, tensor(61.7700)),\n",
              "  (1.1476040471553803, tensor(61.9400)),\n",
              "  (1.1333678093716502, tensor(62.7100)),\n",
              "  (1.1192385027006269, tensor(63.4800)),\n",
              "  (1.1067173685133458, tensor(64.1600)),\n",
              "  (1.0940027615979313, tensor(64.6100)),\n",
              "  (1.0799374918609859, tensor(65.5500)),\n",
              "  (1.0707658853963018, tensor(65.0700)),\n",
              "  (1.0587288930170238, tensor(65.6500)),\n",
              "  (1.0466588810034096, tensor(66.2300)),\n",
              "  (1.0352331405594946, tensor(66.7700)),\n",
              "  (1.0251618491008878, tensor(66.9800)),\n",
              "  (1.0169158514395356, tensor(67.3700)),\n",
              "  (1.0026134077616036, tensor(67.2900)),\n",
              "  (0.9929245828084647, tensor(67.5100)),\n",
              "  (0.9843708949103952, tensor(68.0900)),\n",
              "  (0.9727126022234559, tensor(69.2200)),\n",
              "  (0.9628074741400778, tensor(68.5000)),\n",
              "  (0.9559756624516099, tensor(69.3200)),\n",
              "  (0.9443667995207011, tensor(69.6100)),\n",
              "  (0.9358112397141755, tensor(69.7900)),\n",
              "  (0.9257066362742334, tensor(70.4000)),\n",
              "  (0.9169410636473447, tensor(70.3100)),\n",
              "  (0.9106394560236484, tensor(71.1200)),\n",
              "  (0.9003798751872033, tensor(71.5300)),\n",
              "  (0.8931694899391383, tensor(71.2900)),\n",
              "  (0.8832689445741475, tensor(71.6900)),\n",
              "  (0.8781952542003244, tensor(72.4100)),\n",
              "  (0.8689027346406132, tensor(72.2400)),\n",
              "  (0.8662835763003677, tensor(72.3200)),\n",
              "  (0.8566749204682186, tensor(72.7200)),\n",
              "  (0.8497478245554492, tensor(72.7600)),\n",
              "  (0.8421442477937787, tensor(73.0200)),\n",
              "  (0.8370260281803087, tensor(73.3500)),\n",
              "  (0.8368898691635579, tensor(72.9200)),\n",
              "  (0.8234678600890561, tensor(73.6000)),\n",
              "  (0.8200939883641899, tensor(73.5300)),\n",
              "  (0.8157984122194349, tensor(73.5300)),\n",
              "  (0.8101828018248082, tensor(73.8800)),\n",
              "  (0.8040562605546787, tensor(74.3100)),\n",
              "  (0.7972510008173995, tensor(74.5200)),\n",
              "  (0.796355099689588, tensor(74.4100)),\n",
              "  (0.7896982541860081, tensor(74.5300)),\n",
              "  (0.7868942951506004, tensor(74.7500)),\n",
              "  (0.7812427902685478, tensor(74.7600)),\n",
              "  (0.7791613540682941, tensor(74.9000)),\n",
              "  (0.7751369366525672, tensor(75.0600)),\n",
              "  (0.7783750815682113, tensor(74.9300)),\n",
              "  (0.7650029069653712, tensor(75.5400)),\n",
              "  (0.7623392894037999, tensor(75.2700)),\n",
              "  (0.7621846343760379, tensor(75.5400)),\n",
              "  (0.7593242452085018, tensor(75.3600)),\n",
              "  (0.7534541405956261, tensor(75.8100)),\n",
              "  (0.7504657845273613, tensor(75.6200)),\n",
              "  (0.754886607259512, tensor(75.0800)),\n",
              "  (0.7473090559008997, tensor(75.7900)),\n",
              "  (0.7405529461751227, tensor(75.9300)),\n",
              "  (0.7421358199175913, tensor(76.1000)),\n",
              "  (0.7385415215140674, tensor(76.6200)),\n",
              "  (0.7367558803045656, tensor(76.1800)),\n",
              "  (0.7335513273413293, tensor(76.3200)),\n",
              "  (0.7267418449368793, tensor(76.7900)),\n",
              "  (0.7256811627669726, tensor(76.6600)),\n",
              "  (0.7230324811351951, tensor(77.2500)),\n",
              "  (0.7237347338321153, tensor(76.6000)),\n",
              "  (0.7185776686989702, tensor(77.2000)),\n",
              "  (0.7183135479559191, tensor(77.1300)),\n",
              "  (0.7186296854305547, tensor(77.1200)),\n",
              "  (0.7162199889113661, tensor(77.3100)),\n",
              "  (0.7097808320914861, tensor(77.5000)),\n",
              "  (0.7127609458069317, tensor(77.2800)),\n",
              "  (0.7061856853820849, tensor(77.6700)),\n",
              "  (0.7072996207136195, tensor(77.8300)),\n",
              "  (0.7020940297319088, tensor(77.5900)),\n",
              "  (0.704918027715804, tensor(77.3800)),\n",
              "  (0.7001229148338549, tensor(77.8700)),\n",
              "  (0.6998907544100658, tensor(77.5900)),\n",
              "  (0.6944494418998249, tensor(77.9200)),\n",
              "  (0.6995718881905777, tensor(77.4600)),\n",
              "  (0.6938126579840901, tensor(78.1000)),\n",
              "  (0.6933564977135044, tensor(78.0100)),\n",
              "  (0.6892419060538989, tensor(78.3300)),\n",
              "  (0.6977434010591591, tensor(77.5200)),\n",
              "  (0.6853465029103681, tensor(78.2400)),\n",
              "  (0.6874091465900186, tensor(78.2100)),\n",
              "  (0.6856504463787424, tensor(78.4300)),\n",
              "  (0.6811596245926339, tensor(78.7200)),\n",
              "  (0.6846304104606388, tensor(78.1500)),\n",
              "  (0.6799159727221821, tensor(78.6800)),\n",
              "  (0.6796433839621255, tensor(78.6800)),\n",
              "  (0.67779095502696, tensor(78.9100)),\n",
              "  (0.6756060217497172, tensor(79.0900)),\n",
              "  (0.6767787159637548, tensor(78.8600)),\n",
              "  (0.6764807514638873, tensor(78.7600)),\n",
              "  (0.6712539356436348, tensor(79.2500)),\n",
              "  (0.6728454757467378, tensor(79.3300)),\n",
              "  (0.6698487640873878, tensor(79.3100)),\n",
              "  (0.6644623723049066, tensor(79.4800)),\n",
              "  (0.6655985457491596, tensor(79.5100)),\n",
              "  (0.6665999872210203, tensor(79.4100)),\n",
              "  (0.666087614754634, tensor(79.4100)),\n",
              "  (0.663698067294457, tensor(79.6500)),\n",
              "  (0.661774502798228, tensor(79.7600)),\n",
              "  (0.6601404471027432, tensor(79.5600)),\n",
              "  (0.6602941581870662, tensor(79.6600)),\n",
              "  (0.6607887141485116, tensor(79.6000)),\n",
              "  (0.6600882959313341, tensor(79.9100)),\n",
              "  (0.6600854439711896, tensor(79.5600)),\n",
              "  (0.6608044210412889, tensor(79.6700)),\n",
              "  (0.6535054188674316, tensor(79.8700)),\n",
              "  (0.6533027276400942, tensor(79.9400)),\n",
              "  (0.6541791478510015, tensor(79.7900)),\n",
              "  (0.6567286847196985, tensor(79.7200)),\n",
              "  (0.654879934229271, tensor(79.6900)),\n",
              "  (0.6531959164433297, tensor(80.0700)),\n",
              "  (0.6513813080965075, tensor(80.0600)),\n",
              "  (0.653431668223883, tensor(79.9300)),\n",
              "  (0.6565040862625349, tensor(79.5900)),\n",
              "  (0.6507508197975694, tensor(80.1600)),\n",
              "  (0.6487151496401405, tensor(80.2400)),\n",
              "  (0.6479084607294179, tensor(80.1900)),\n",
              "  (0.6451458289732167, tensor(80.4200)),\n",
              "  (0.6487422892972944, tensor(80.1500)),\n",
              "  (0.6496964874379045, tensor(79.8400)),\n",
              "  (0.6468159571644791, tensor(80.1800)),\n",
              "  (0.6435127889985218, tensor(80.3600)),\n",
              "  (0.643350984224613, tensor(80.4100)),\n",
              "  (0.6465276258326368, tensor(80.2100)),\n",
              "  (0.6430365394907247, tensor(80.4800)),\n",
              "  (0.6426444229498971, tensor(80.2100)),\n",
              "  (0.6411650108069298, tensor(80.8700)),\n",
              "  (0.6410086771207454, tensor(80.4400)),\n",
              "  (0.6399418013521412, tensor(80.5000)),\n",
              "  (0.638091031097807, tensor(80.5800)),\n",
              "  (0.6387600758091081, tensor(80.6700)),\n",
              "  (0.6410620176604949, tensor(80.6600)),\n",
              "  (0.6416607715077466, tensor(80.6400)),\n",
              "  (0.639547577373602, tensor(80.7400)),\n",
              "  (0.6389667777833296, tensor(80.5300)),\n",
              "  (0.636817859038437, tensor(80.9000)),\n",
              "  (0.6401846217461687, tensor(80.6800)),\n",
              "  (0.6417080549600738, tensor(80.6300)),\n",
              "  (0.6434656474232295, tensor(80.4800)),\n",
              "  (0.6512916012031142, tensor(79.8500)),\n",
              "  (0.6442220517490059, tensor(80.4500)),\n",
              "  (0.6392741632309015, tensor(80.9000)),\n",
              "  (0.6409792571510042, tensor(80.8000)),\n",
              "  (0.6436626979533466, tensor(80.4700)),\n",
              "  (0.6408832660628483, tensor(80.6000)),\n",
              "  (0.6421392605214409, tensor(80.6000)),\n",
              "  (0.6415425985903508, tensor(80.7100)),\n",
              "  (0.6381780096515606, tensor(80.9800)),\n",
              "  (0.6417089489297796, tensor(80.6900)),\n",
              "  (0.640942794533752, tensor(80.8400)),\n",
              "  (0.640494841088244, tensor(80.8600)),\n",
              "  (0.6442358442417666, tensor(80.6700)),\n",
              "  (0.6415574056057202, tensor(80.8200)),\n",
              "  (0.642124085238688, tensor(80.8400)),\n",
              "  (0.6440214935637006, tensor(80.7600)),\n",
              "  (0.6417781827569837, tensor(81.0300)),\n",
              "  (0.6434422848741015, tensor(81.0500)),\n",
              "  (0.6465873177533941, tensor(80.7700)),\n",
              "  (0.6470547072764486, tensor(80.8400)),\n",
              "  (0.6455473930412234, tensor(80.9600)),\n",
              "  (0.6532079953519307, tensor(80.5300)),\n",
              "  (0.6454915494874905, tensor(80.9900)),\n",
              "  (0.6488838376470594, tensor(80.9400)),\n",
              "  (0.6464230044442578, tensor(81.0200)),\n",
              "  (0.6485015339650417, tensor(80.9000)),\n",
              "  (0.6464009802798333, tensor(80.9900)),\n",
              "  (0.6427945059762896, tensor(81.1700)),\n",
              "  (0.6467673023666132, tensor(81.0200)),\n",
              "  (0.6509782151319298, tensor(80.9900)),\n",
              "  (0.6481538203451244, tensor(81.1300)),\n",
              "  (0.6508029536433336, tensor(80.9300)),\n",
              "  (0.6565786810104117, tensor(80.9200)),\n",
              "  (0.6532599581146257, tensor(80.9600)),\n",
              "  (0.6537773293091647, tensor(81.0300)),\n",
              "  (0.65175393012446, tensor(81.0700)),\n",
              "  (0.6534710899401965, tensor(81.0600)),\n",
              "  (0.6554693551817574, tensor(80.9600)),\n",
              "  (0.6578952537654572, tensor(80.8900)),\n",
              "  (0.6519515500297879, tensor(81.1400)),\n",
              "  (0.6565946424597751, tensor(81.0300)),\n",
              "  (0.6593471254786367, tensor(80.9800)),\n",
              "  (0.6574695206794742, tensor(81.0200)),\n",
              "  (0.6638275551382867, tensor(80.8800)),\n",
              "  (0.6653634618492706, tensor(80.7600)),\n",
              "  (0.6632213390915745, tensor(80.9600)),\n",
              "  (0.6634252847045046, tensor(81.0700)),\n",
              "  (0.662650064672291, tensor(80.8400)),\n",
              "  (0.6630571437224677, tensor(80.9200)),\n",
              "  (0.6668992550192215, tensor(80.8700)),\n",
              "  (0.6708744460943071, tensor(80.5100)),\n",
              "  (0.6683726574782021, tensor(80.9900)),\n",
              "  (0.6733793527886078, tensor(80.6700)),\n",
              "  (0.6718268407456166, tensor(80.8100)),\n",
              "  (0.6727998974163413, tensor(80.8700)),\n",
              "  (0.67225201347, tensor(80.9300)),\n",
              "  (0.6718666979360028, tensor(80.8500)),\n",
              "  (0.6807758413926793, tensor(80.7300)),\n",
              "  (0.6764002102705224, tensor(80.8800)),\n",
              "  (0.683235371331952, tensor(80.5000)),\n",
              "  (0.6780109713067417, tensor(80.8700)),\n",
              "  (0.6799430360904855, tensor(80.8300)),\n",
              "  (0.6808034387339945, tensor(80.7900)),\n",
              "  (0.6809470807219433, tensor(80.8700)),\n",
              "  (0.683159881896407, tensor(80.7800)),\n",
              "  (0.6838313174670669, tensor(80.6600)),\n",
              "  (0.6864890106435276, tensor(80.7700)),\n",
              "  (0.6903666237869945, tensor(80.6000)),\n",
              "  (0.6932236440546121, tensor(80.3500)),\n",
              "  (0.6903421402402949, tensor(80.7400)),\n",
              "  (0.6882358738130957, tensor(80.7600)),\n",
              "  (0.6908499172315432, tensor(80.4700)),\n",
              "  (0.6902932250424199, tensor(80.7300)),\n",
              "  (0.6973988690738583, tensor(80.4300)),\n",
              "  (0.6963526222121431, tensor(80.4000)),\n",
              "  (0.69520333090594, tensor(80.7000)),\n",
              "  (0.6944429423386551, tensor(80.9300)),\n",
              "  (0.6956166840090942, tensor(80.7700)),\n",
              "  (0.6971585081105651, tensor(80.5800)),\n",
              "  (0.7015572789684832, tensor(80.5900)),\n",
              "  (0.7009982696251149, tensor(80.6800)),\n",
              "  (0.6995661949026869, tensor(80.8100)),\n",
              "  (0.7057817820751014, tensor(80.4800)),\n",
              "  (0.7076551760072435, tensor(80.4700)),\n",
              "  (0.7055146946800086, tensor(80.6600)),\n",
              "  (0.7097124049407912, tensor(80.7300)),\n",
              "  (0.7083500165464653, tensor(80.7200)),\n",
              "  (0.7106732957468356, tensor(80.6200)),\n",
              "  (0.7145917258610169, tensor(80.5400)),\n",
              "  (0.7136180992346983, tensor(80.4500)),\n",
              "  (0.7192452354942663, tensor(80.2600)),\n",
              "  (0.7166119320478284, tensor(80.4800)),\n",
              "  (0.7148839747754376, tensor(80.5600)),\n",
              "  (0.7160697945173604, tensor(80.6800)),\n",
              "  (0.7191833536996557, tensor(80.7400)),\n",
              "  (0.7208426479450796, tensor(80.6900)),\n",
              "  (0.7203822597986578, tensor(80.5200)),\n",
              "  (0.7253540857404178, tensor(80.3400)),\n",
              "  (0.7345440310133361, tensor(80.2200)),\n",
              "  (0.7250871096758672, tensor(80.7100)),\n",
              "  (0.7256325531854126, tensor(80.6400)),\n",
              "  (0.7300116080212401, tensor(80.4000)),\n",
              "  (0.7290520074609232, tensor(80.7700)),\n",
              "  (0.7306759697691898, tensor(80.4400)),\n",
              "  (0.7312501358316755, tensor(80.4300)),\n",
              "  (0.7315933367151801, tensor(80.6700)),\n",
              "  (0.7360910045107858, tensor(80.4000)),\n",
              "  (0.7362397827418896, tensor(80.5700)),\n",
              "  (0.7369566987380853, tensor(80.4800)),\n",
              "  (0.7416646506491416, tensor(80.4500)),\n",
              "  (0.7432149059882378, tensor(80.2500)),\n",
              "  (0.7481060337170439, tensor(80.2700)),\n",
              "  (0.7455022363596973, tensor(80.3200)),\n",
              "  (0.7471195738267024, tensor(80.3600)),\n",
              "  (0.7463331998443399, tensor(80.5200)),\n",
              "  (0.746972390369682, tensor(80.5800)),\n",
              "  (0.7499616079097696, tensor(80.5000)),\n",
              "  (0.7569373392373574, tensor(80.2800)),\n",
              "  (0.7533197378089179, tensor(80.4300)),\n",
              "  (0.7514561315853197, tensor(80.6700)),\n",
              "  (0.7539214745875975, tensor(80.5200)),\n",
              "  (0.753692612974585, tensor(80.7500)),\n",
              "  (0.7569246891592077, tensor(80.3800)),\n",
              "  (0.7576983410940376, tensor(80.5200)),\n",
              "  (0.7597122170894479, tensor(80.4800)),\n",
              "  (0.7629433530505072, tensor(80.4400)),\n",
              "  (0.7615175334791379, tensor(80.4400)),\n",
              "  (0.7623337827556138, tensor(80.6000)),\n",
              "  (0.7648304740312547, tensor(80.6600)),\n",
              "  (0.772022846275262, tensor(80.2300)),\n",
              "  (0.7707303743404145, tensor(80.3800)),\n",
              "  (0.7707744977625435, tensor(80.4100)),\n",
              "  (0.7727521080411884, tensor(80.4400)),\n",
              "  (0.777636389797705, tensor(80.2200)),\n",
              "  (0.7800926456540754, tensor(80.3500)),\n",
              "  (0.7783365782442828, tensor(80.4100)),\n",
              "  (0.780136268354042, tensor(80.2900)),\n",
              "  (0.7778135446611499, tensor(80.5300)),\n",
              "  (0.7829151317486757, tensor(80.3800)),\n",
              "  (0.7824045032928283, tensor(80.4300)),\n",
              "  (0.7838363967159643, tensor(80.5200)),\n",
              "  (0.7827920114958454, tensor(80.5500)),\n",
              "  (0.7864484000914355, tensor(80.5000)),\n",
              "  (0.7885609538496087, tensor(80.4400)),\n",
              "  (0.7899483057871323, tensor(80.5600)),\n",
              "  (0.7900933363033257, tensor(80.4700)),\n",
              "  (0.7947767805108421, tensor(80.3100)),\n",
              "  (0.801861510899102, tensor(80.2400)),\n",
              "  (0.7954548737237367, tensor(80.5000)),\n",
              "  (0.7980412456896373, tensor(80.4300)),\n",
              "  (0.7984257818946038, tensor(80.5700)),\n",
              "  (0.8026391811855851, tensor(80.3200)),\n",
              "  (0.8021169280785785, tensor(80.3700)),\n",
              "  (0.8012935760766166, tensor(80.4400)),\n",
              "  (0.8041360728175783, tensor(80.4900)),\n",
              "  (0.8050837435862361, tensor(80.4300)),\n",
              "  (0.8087940217538495, tensor(80.3700)),\n",
              "  (0.8101876322856535, tensor(80.4600)),\n",
              "  (0.8117319773916328, tensor(80.3900)),\n",
              "  (0.8131510278226357, tensor(80.3700)),\n",
              "  (0.8143613064345587, tensor(80.3600)),\n",
              "  (0.8127407115278623, tensor(80.6000)),\n",
              "  (0.8166926493941103, tensor(80.4500)),\n",
              "  (0.8175362653789888, tensor(80.4500)),\n",
              "  (0.8202790337493959, tensor(80.3900)),\n",
              "  (0.8221227564313769, tensor(80.3600)),\n",
              "  (0.8219058167548837, tensor(80.4300)),\n",
              "  (0.8240536129083929, tensor(80.4000)),\n",
              "  (0.8262704648094606, tensor(80.3300)),\n",
              "  (0.8236216803535786, tensor(80.4000)),\n",
              "  (0.8275672074684491, tensor(80.3900)),\n",
              "  (0.8275946263972203, tensor(80.3700)),\n",
              "  (0.8280243953401752, tensor(80.5700)),\n",
              "  (0.8306010951120919, tensor(80.4000)),\n",
              "  (0.8330659773708108, tensor(80.4100)),\n",
              "  (0.8338805665597425, tensor(80.4900)),\n",
              "  (0.8334143010280389, tensor(80.4400)),\n",
              "  (0.8349797654409489, tensor(80.5000)),\n",
              "  (0.835441789551844, tensor(80.5200)),\n",
              "  (0.8376637576940448, tensor(80.4700)),\n",
              "  (0.8399607915422242, tensor(80.5100)),\n",
              "  (0.8407637345640673, tensor(80.4900)),\n",
              "  (0.8432482951503274, tensor(80.3700)),\n",
              "  (0.8485505045087899, tensor(80.3000)),\n",
              "  (0.8464112546589997, tensor(80.5800)),\n",
              "  (0.8460359053109939, tensor(80.5100)),\n",
              "  (0.8480482494450368, tensor(80.4900)),\n",
              "  (0.8489813407893434, tensor(80.5400)),\n",
              "  (0.8531152794680875, tensor(80.3900)),\n",
              "  (0.8511118695797959, tensor(80.5500)),\n",
              "  (0.8528817537941914, tensor(80.4900)),\n",
              "  (0.8545107085322943, tensor(80.5200)),\n",
              "  (0.8561572006389573, tensor(80.4100)),\n",
              "  (0.8557522782923741, tensor(80.5600)),\n",
              "  (0.8615377598744318, tensor(80.3400)),\n",
              "  (0.8608305515769773, tensor(80.4400)),\n",
              "  (0.8622519577682111, tensor(80.4400)),\n",
              "  (0.8673322265945037, tensor(80.3000)),\n",
              "  (0.8652886514974237, tensor(80.4600)),\n",
              "  (0.8677245519549776, tensor(80.3900)),\n",
              "  (0.8662073432695626, tensor(80.6100)),\n",
              "  (0.8689774809548406, tensor(80.4300)),\n",
              "  (0.8691463907140397, tensor(80.4300)),\n",
              "  (0.870157164303965, tensor(80.5100)),\n",
              "  (0.8715902433269787, tensor(80.5500)),\n",
              "  (0.8737891994903139, tensor(80.4500)),\n",
              "  (0.8730057358518992, tensor(80.4800)),\n",
              "  (0.8744907854409152, tensor(80.5500)),\n",
              "  (0.8770702428606599, tensor(80.4300)),\n",
              "  (0.8779281188180211, tensor(80.5100)),\n",
              "  (0.8786796603116269, tensor(80.4600)),\n",
              "  (0.8807729524019815, tensor(80.4500)),\n",
              "  (0.8826815993475827, tensor(80.4500)),\n",
              "  (0.8841471172109665, tensor(80.4400)),\n",
              "  (0.8853064444858306, tensor(80.4900)),\n",
              "  (0.88551423362435, tensor(80.4300)),\n",
              "  (0.8857295708182317, tensor(80.4500)),\n",
              "  (0.8878804279849355, tensor(80.4900)),\n",
              "  (0.8893799124964249, tensor(80.5300)),\n",
              "  (0.8914674008630127, tensor(80.3400)),\n",
              "  (0.8918525810336665, tensor(80.4600)),\n",
              "  (0.8920614806064706, tensor(80.4800)),\n",
              "  (0.8934404857920998, tensor(80.5600)),\n",
              "  (0.8941199964145461, tensor(80.6400)),\n",
              "  (0.8973416711966961, tensor(80.4400)),\n",
              "  (0.8982017007734814, tensor(80.5400)),\n",
              "  (0.8988863628627969, tensor(80.6300)),\n",
              "  (0.9012446887532342, tensor(80.4700)),\n",
              "  (0.9014405130451031, tensor(80.4900))]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    }
  ]
}