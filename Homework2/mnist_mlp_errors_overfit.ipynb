{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "mnist_mlp.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "12d955c72ab0446fb3d9ed6c23095cfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4e1bdc451cf04ba2906520850496ac31",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7dd872c094d84fe9a6464eab9cec84ca",
              "IPY_MODEL_49fb7fa74fe8455aad7ed19573b0403e"
            ]
          }
        },
        "4e1bdc451cf04ba2906520850496ac31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7dd872c094d84fe9a6464eab9cec84ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a027dff7cb3e4a1ca0d9d84c52db3e97",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_749aa13a19fb45d2bb43fc51fbe003c9"
          }
        },
        "49fb7fa74fe8455aad7ed19573b0403e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0b6b33edd132427eb45f836086808be8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 9920512/? [00:20&lt;00:00, 1154046.23it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f35242bb51b445ac9c464cef822c6436"
          }
        },
        "a027dff7cb3e4a1ca0d9d84c52db3e97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "749aa13a19fb45d2bb43fc51fbe003c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0b6b33edd132427eb45f836086808be8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f35242bb51b445ac9c464cef822c6436": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "777bebec95624f62ac2c575f191cd293": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_372504627a16414782a329011f0cf2cb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b9fc8f6b0cbf4a3b84420415b06d0ca8",
              "IPY_MODEL_86957f5f9c684fada8057b666f7f3d04"
            ]
          }
        },
        "372504627a16414782a329011f0cf2cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b9fc8f6b0cbf4a3b84420415b06d0ca8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d8e9f9247e4f471cbb8a1ce4c728a032",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_18f7b11c7ffe495ca8eb36ce95176c6c"
          }
        },
        "86957f5f9c684fada8057b666f7f3d04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_dff22b5a71d34b7ca981b360048b24cb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 32768/? [00:01&lt;00:00, 27742.80it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0d45989f94e545cf93c0cb24fee8de6f"
          }
        },
        "d8e9f9247e4f471cbb8a1ce4c728a032": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "18f7b11c7ffe495ca8eb36ce95176c6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dff22b5a71d34b7ca981b360048b24cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0d45989f94e545cf93c0cb24fee8de6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d189a545fc16473dbd50ce7c38f0cafa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_04e891f7af284441b5a77b2348c82c8f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bb219c74ec884628a1ed82f4eba69e71",
              "IPY_MODEL_883d7c344b794438ad5f843c7156bce2"
            ]
          }
        },
        "04e891f7af284441b5a77b2348c82c8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bb219c74ec884628a1ed82f4eba69e71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9f016105b0dc485b977bf64f0b593782",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cc51b2b01cc34fa59e32d01781e3f415"
          }
        },
        "883d7c344b794438ad5f843c7156bce2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_298a0f0ce63a472cb5eb239ac0b048e2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1654784/? [00:00&lt;00:00, 1796617.39it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_345dbf4949a34d9b8e9a68a1261abe54"
          }
        },
        "9f016105b0dc485b977bf64f0b593782": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cc51b2b01cc34fa59e32d01781e3f415": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "298a0f0ce63a472cb5eb239ac0b048e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "345dbf4949a34d9b8e9a68a1261abe54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6cac0ecaca134f0bba51113261c5b58a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_51a9370aa95249c5b9cafaafb6ddf7ba",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_879a7d7fd1cf487d9898cf128bc2f493",
              "IPY_MODEL_2c93b1f83577404891c2cd8286ecf200"
            ]
          }
        },
        "51a9370aa95249c5b9cafaafb6ddf7ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "879a7d7fd1cf487d9898cf128bc2f493": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_58827964d9064efda6b67a6cac4cb8c7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e1858f7292af414ba179aa3a86a57afb"
          }
        },
        "2c93b1f83577404891c2cd8286ecf200": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cdfaacdc03044cc58a82d32314c0b552",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 8192/? [00:00&lt;00:00, 29494.98it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d7a395d94ab84b8b8eb533b9f59bdd71"
          }
        },
        "58827964d9064efda6b67a6cac4cb8c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e1858f7292af414ba179aa3a86a57afb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cdfaacdc03044cc58a82d32314c0b552": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d7a395d94ab84b8b8eb533b9f59bdd71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw7SPqRCbLXf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qz6iMBeybLXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.1307,), (0.3081,)),\n",
        "           ])\n",
        "\n",
        "def mnist(batch_size=50, valid=0, shuffle=True, transform=mnist_transform, path='./MNIST_data'):\n",
        "    test_data = datasets.MNIST(path, train=False, download=True, transform=transform)\n",
        "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "    \n",
        "    train_data = datasets.MNIST(path, train=True, download=True, transform=transform)\n",
        "    train_data.data = train_data.data[:1000,:,:]\n",
        "    if valid > 0:\n",
        "        num_train = len(train_data)\n",
        "        indices = list(range(num_train))\n",
        "        split = num_train-valid\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        train_idx, valid_idx = indices[:split], indices[split:]\n",
        "        train_sampler = SubsetRandomSampler(train_idx)\n",
        "        valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "        train_loader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n",
        "        valid_loader = DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler)\n",
        "    \n",
        "        return train_loader, valid_loader, test_loader\n",
        "    else:\n",
        "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=shuffle)\n",
        "        return train_loader, test_loader\n",
        "\n",
        "\n",
        "def plot_mnist(images, shape):\n",
        "    fig = plt.figure(figsize=shape[::-1], dpi=80)\n",
        "    for j in range(1, len(images) + 1):\n",
        "        ax = fig.add_subplot(shape[0], shape[1], j)\n",
        "        ax.matshow(images[j - 1, 0, :, :], cmap = matplotlib.cm.binary)\n",
        "        plt.xticks(np.array([]))\n",
        "        plt.yticks(np.array([]))\n",
        "    plt.show()\n",
        "    \n",
        "def plot_graphs(log, tpe='loss'):\n",
        "    keys = log.keys()\n",
        "    logs = {k:[z for z in zip(*log[k])] for k in keys}\n",
        "    epochs = {k:range(len(log[k])) for k in keys}\n",
        "    \n",
        "    if tpe == 'loss':\n",
        "        handlers, = zip(*[plt.plot(epochs[k], logs[k][0], label=k) for k in keys])\n",
        "        plt.title('errors')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.ylabel('error')\n",
        "        plt.legend(handles=handlers)\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "    elif tpe == 'accuracy':\n",
        "        handlers, = zip(*[plt.plot(epochs[k], logs[k][1], label=k) for k in log.keys()])\n",
        "        plt.title('accuracy')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.ylabel('accuracy')\n",
        "        plt.legend(handles=handlers)\n",
        "        plt.grid()\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DhDm90RNw8_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403,
          "referenced_widgets": [
            "12d955c72ab0446fb3d9ed6c23095cfc",
            "4e1bdc451cf04ba2906520850496ac31",
            "7dd872c094d84fe9a6464eab9cec84ca",
            "49fb7fa74fe8455aad7ed19573b0403e",
            "a027dff7cb3e4a1ca0d9d84c52db3e97",
            "749aa13a19fb45d2bb43fc51fbe003c9",
            "0b6b33edd132427eb45f836086808be8",
            "f35242bb51b445ac9c464cef822c6436",
            "777bebec95624f62ac2c575f191cd293",
            "372504627a16414782a329011f0cf2cb",
            "b9fc8f6b0cbf4a3b84420415b06d0ca8",
            "86957f5f9c684fada8057b666f7f3d04",
            "d8e9f9247e4f471cbb8a1ce4c728a032",
            "18f7b11c7ffe495ca8eb36ce95176c6c",
            "dff22b5a71d34b7ca981b360048b24cb",
            "0d45989f94e545cf93c0cb24fee8de6f",
            "d189a545fc16473dbd50ce7c38f0cafa",
            "04e891f7af284441b5a77b2348c82c8f",
            "bb219c74ec884628a1ed82f4eba69e71",
            "883d7c344b794438ad5f843c7156bce2",
            "9f016105b0dc485b977bf64f0b593782",
            "cc51b2b01cc34fa59e32d01781e3f415",
            "298a0f0ce63a472cb5eb239ac0b048e2",
            "345dbf4949a34d9b8e9a68a1261abe54",
            "6cac0ecaca134f0bba51113261c5b58a",
            "51a9370aa95249c5b9cafaafb6ddf7ba",
            "879a7d7fd1cf487d9898cf128bc2f493",
            "2c93b1f83577404891c2cd8286ecf200",
            "58827964d9064efda6b67a6cac4cb8c7",
            "e1858f7292af414ba179aa3a86a57afb",
            "cdfaacdc03044cc58a82d32314c0b552",
            "d7a395d94ab84b8b8eb533b9f59bdd71"
          ]
        },
        "outputId": "15c96f54-b5f7-4697-8d79-c125a3f3c3e3"
      },
      "source": [
        "train_loader, test_loader = mnist(1)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12d955c72ab0446fb3d9ed6c23095cfc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST_data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "777bebec95624f62ac2c575f191cd293",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST_data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d189a545fc16473dbd50ce7c38f0cafa",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST_data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6cac0ecaca134f0bba51113261c5b58a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST_data/MNIST/raw\n",
            "Processing...\n",
            "\n",
            "\n",
            "\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0jvvDrjbLXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, log_softmax=False):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 128)\n",
        "        self.fc2 = nn.Linear(128, 256)\n",
        "        self.fc3 = nn.Linear(256, 512)\n",
        "        self.fc4 = nn.Linear(512, 1024)\n",
        "        self.fc5 = nn.Linear(1024, 10)\n",
        "        self.log_softmax = log_softmax\n",
        "        self.optim = optim.SGD(self.parameters(), lr=1e-4)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = torch.sigmoid(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = torch.relu(self.fc4(x))\n",
        "        x = self.fc5(x)\n",
        "        if self.log_softmax:\n",
        "            x = F.log_softmax(x, dim=1)\n",
        "        else:\n",
        "            x = torch.log(F.softmax(x, dim=1))\n",
        "        return x\n",
        "    \n",
        "    def loss(self, output, target, **kwargs):\n",
        "        self._loss = F.nll_loss(output, target, **kwargs)\n",
        "        return self._loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANxuIpOkbLXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epoch, models):\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        for name, model in models.items():\n",
        "            model.optim.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = model.loss(output, target)\n",
        "            loss.backward()\n",
        "            model.optim.step()\n",
        "            \n",
        "        if batch_idx % 200 == 0:\n",
        "            line = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLosses '.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader))\n",
        "            losses = ' '.join(['{}: {:.6f}'.format(k, m._loss.item()) for k, m in models.items()])\n",
        "            print(line + losses)\n",
        "            \n",
        "    else:\n",
        "        batch_idx += 1\n",
        "        line = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLosses '.format(\n",
        "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "            100. * batch_idx / len(train_loader))\n",
        "        losses = ' '.join(['{}: {:.6f}'.format(k, m._loss.item()) for k, m in models.items()])\n",
        "        print(line + losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbrbWpJNbLXy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models = {'F.softmax': Net(), 'log_softmax': Net(True)}\n",
        "test_log = {k: [] for k in models}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-2EpiW9bLX0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "avg_lambda = lambda l: 'Loss: {:.4f}'.format(l)\n",
        "acc_lambda = lambda c, p: 'Accuracy: {}/{} ({:.0f}%)'.format(c, len(test_loader.dataset), p)\n",
        "line = lambda i, l, c, p: '{}: '.format(i) + avg_lambda(l) + '\\t' + acc_lambda(c, p)\n",
        "\n",
        "def test(models, log=None):\n",
        "    test_size = len(test_loader.sampler)\n",
        "    test_loss = {k: 0. for k in models}\n",
        "    correct = {k: 0. for k in models}\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            for k, m in models.items():\n",
        "                output = m(data)\n",
        "                test_loss[k] += m.loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "                pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "                correct[k] += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "    \n",
        "    for k in models.keys():\n",
        "        test_loss[k] /= test_size\n",
        "    correct_pct = {k: 100. * correct[k] / test_size for k in correct}\n",
        "    lines = '\\n'.join([line(k, test_loss[k], correct[k], correct_pct[k]) for k in models]) + '\\n'\n",
        "    report = 'Test set:\\n' + lines\n",
        "    if log is not None:\n",
        "        for k in models.keys():\n",
        "            log[k].append((test_loss[k], correct_pct[k]))\n",
        "    print(report)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hp2nSaCjbLX2",
        "colab_type": "code",
        "outputId": "1324198e-eca1-4e39-f1d5-7cf89eb87f52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(1, 501):\n",
        "    train(epoch, models)\n",
        "    test(models, test_log)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Train Epoch: 1 [200/1000 (20%)]\tLosses F.softmax: 2.244390 log_softmax: 2.294800\n",
            "Train Epoch: 1 [400/1000 (40%)]\tLosses F.softmax: 2.301178 log_softmax: 2.296190\n",
            "Train Epoch: 1 [600/1000 (60%)]\tLosses F.softmax: 2.242859 log_softmax: 2.290487\n",
            "Train Epoch: 1 [800/1000 (80%)]\tLosses F.softmax: 2.328349 log_softmax: 2.315445\n",
            "Train Epoch: 1 [1000/1000 (100%)]\tLosses F.softmax: 2.323900 log_softmax: 2.292133\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3046\tAccuracy: 892.0/10000 (9%)\n",
            "log_softmax: Loss: 2.3031\tAccuracy: 1009.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 2 [0/1000 (0%)]\tLosses F.softmax: 2.317962 log_softmax: 2.287045\n",
            "Train Epoch: 2 [200/1000 (20%)]\tLosses F.softmax: 2.314420 log_softmax: 2.286048\n",
            "Train Epoch: 2 [400/1000 (40%)]\tLosses F.softmax: 2.306148 log_softmax: 2.300869\n",
            "Train Epoch: 2 [600/1000 (60%)]\tLosses F.softmax: 2.327732 log_softmax: 2.313975\n",
            "Train Epoch: 2 [800/1000 (80%)]\tLosses F.softmax: 2.316226 log_softmax: 2.323819\n",
            "Train Epoch: 2 [1000/1000 (100%)]\tLosses F.softmax: 2.332416 log_softmax: 2.306520\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3038\tAccuracy: 892.0/10000 (9%)\n",
            "log_softmax: Loss: 2.3025\tAccuracy: 1009.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 3 [0/1000 (0%)]\tLosses F.softmax: 2.312976 log_softmax: 2.283109\n",
            "Train Epoch: 3 [200/1000 (20%)]\tLosses F.softmax: 2.294184 log_softmax: 2.346964\n",
            "Train Epoch: 3 [400/1000 (40%)]\tLosses F.softmax: 2.251545 log_softmax: 2.310026\n",
            "Train Epoch: 3 [600/1000 (60%)]\tLosses F.softmax: 2.321344 log_softmax: 2.287028\n",
            "Train Epoch: 3 [800/1000 (80%)]\tLosses F.softmax: 2.323248 log_softmax: 2.287558\n",
            "Train Epoch: 3 [1000/1000 (100%)]\tLosses F.softmax: 2.277838 log_softmax: 2.259617\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3032\tAccuracy: 892.0/10000 (9%)\n",
            "log_softmax: Loss: 2.3019\tAccuracy: 1010.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 4 [0/1000 (0%)]\tLosses F.softmax: 2.298520 log_softmax: 2.311781\n",
            "Train Epoch: 4 [200/1000 (20%)]\tLosses F.softmax: 2.260877 log_softmax: 2.311790\n",
            "Train Epoch: 4 [400/1000 (40%)]\tLosses F.softmax: 2.295383 log_softmax: 2.307673\n",
            "Train Epoch: 4 [600/1000 (60%)]\tLosses F.softmax: 2.301252 log_softmax: 2.273649\n",
            "Train Epoch: 4 [800/1000 (80%)]\tLosses F.softmax: 2.317710 log_softmax: 2.312706\n",
            "Train Epoch: 4 [1000/1000 (100%)]\tLosses F.softmax: 2.330178 log_softmax: 2.312877\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3026\tAccuracy: 892.0/10000 (9%)\n",
            "log_softmax: Loss: 2.3013\tAccuracy: 998.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 5 [0/1000 (0%)]\tLosses F.softmax: 2.318710 log_softmax: 2.282784\n",
            "Train Epoch: 5 [200/1000 (20%)]\tLosses F.softmax: 2.277746 log_softmax: 2.262362\n",
            "Train Epoch: 5 [400/1000 (40%)]\tLosses F.softmax: 2.320323 log_softmax: 2.319176\n",
            "Train Epoch: 5 [600/1000 (60%)]\tLosses F.softmax: 2.319450 log_softmax: 2.289811\n",
            "Train Epoch: 5 [800/1000 (80%)]\tLosses F.softmax: 2.319337 log_softmax: 2.282466\n",
            "Train Epoch: 5 [1000/1000 (100%)]\tLosses F.softmax: 2.265885 log_softmax: 2.312032\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3019\tAccuracy: 889.0/10000 (9%)\n",
            "log_softmax: Loss: 2.3007\tAccuracy: 1227.0/10000 (12%)\n",
            "\n",
            "Train Epoch: 6 [0/1000 (0%)]\tLosses F.softmax: 2.271150 log_softmax: 2.311701\n",
            "Train Epoch: 6 [200/1000 (20%)]\tLosses F.softmax: 2.311624 log_softmax: 2.278146\n",
            "Train Epoch: 6 [400/1000 (40%)]\tLosses F.softmax: 2.275919 log_softmax: 2.262062\n",
            "Train Epoch: 6 [600/1000 (60%)]\tLosses F.softmax: 2.311905 log_softmax: 2.284477\n",
            "Train Epoch: 6 [800/1000 (80%)]\tLosses F.softmax: 2.278854 log_softmax: 2.264911\n",
            "Train Epoch: 6 [1000/1000 (100%)]\tLosses F.softmax: 2.325314 log_softmax: 2.318405\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3014\tAccuracy: 886.0/10000 (9%)\n",
            "log_softmax: Loss: 2.3001\tAccuracy: 1309.0/10000 (13%)\n",
            "\n",
            "Train Epoch: 7 [0/1000 (0%)]\tLosses F.softmax: 2.300036 log_softmax: 2.337974\n",
            "Train Epoch: 7 [200/1000 (20%)]\tLosses F.softmax: 2.283494 log_softmax: 2.265506\n",
            "Train Epoch: 7 [400/1000 (40%)]\tLosses F.softmax: 2.332707 log_softmax: 2.322757\n",
            "Train Epoch: 7 [600/1000 (60%)]\tLosses F.softmax: 2.326526 log_softmax: 2.326869\n",
            "Train Epoch: 7 [800/1000 (80%)]\tLosses F.softmax: 2.322790 log_softmax: 2.315212\n",
            "Train Epoch: 7 [1000/1000 (100%)]\tLosses F.softmax: 2.300216 log_softmax: 2.340274\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3008\tAccuracy: 1138.0/10000 (11%)\n",
            "log_softmax: Loss: 2.2995\tAccuracy: 1039.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 8 [0/1000 (0%)]\tLosses F.softmax: 2.281748 log_softmax: 2.264824\n",
            "Train Epoch: 8 [200/1000 (20%)]\tLosses F.softmax: 2.270679 log_softmax: 2.283959\n",
            "Train Epoch: 8 [400/1000 (40%)]\tLosses F.softmax: 2.308700 log_softmax: 2.282812\n",
            "Train Epoch: 8 [600/1000 (60%)]\tLosses F.softmax: 2.307143 log_softmax: 2.278844\n",
            "Train Epoch: 8 [800/1000 (80%)]\tLosses F.softmax: 2.262000 log_softmax: 2.279680\n",
            "Train Epoch: 8 [1000/1000 (100%)]\tLosses F.softmax: 2.290400 log_softmax: 2.270184\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3003\tAccuracy: 1430.0/10000 (14%)\n",
            "log_softmax: Loss: 2.2990\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 9 [0/1000 (0%)]\tLosses F.softmax: 2.320248 log_softmax: 2.306607\n",
            "Train Epoch: 9 [200/1000 (20%)]\tLosses F.softmax: 2.324071 log_softmax: 2.313793\n",
            "Train Epoch: 9 [400/1000 (40%)]\tLosses F.softmax: 2.302471 log_softmax: 2.337239\n",
            "Train Epoch: 9 [600/1000 (60%)]\tLosses F.softmax: 2.319523 log_softmax: 2.308844\n",
            "Train Epoch: 9 [800/1000 (80%)]\tLosses F.softmax: 2.319015 log_softmax: 2.304640\n",
            "Train Epoch: 9 [1000/1000 (100%)]\tLosses F.softmax: 2.277066 log_softmax: 2.264357\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2997\tAccuracy: 1516.0/10000 (15%)\n",
            "log_softmax: Loss: 2.2984\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 10 [0/1000 (0%)]\tLosses F.softmax: 2.303993 log_softmax: 2.280987\n",
            "Train Epoch: 10 [200/1000 (20%)]\tLosses F.softmax: 2.281836 log_softmax: 2.327213\n",
            "Train Epoch: 10 [400/1000 (40%)]\tLosses F.softmax: 2.256327 log_softmax: 2.250520\n",
            "Train Epoch: 10 [600/1000 (60%)]\tLosses F.softmax: 2.303585 log_softmax: 2.332176\n",
            "Train Epoch: 10 [800/1000 (80%)]\tLosses F.softmax: 2.308767 log_softmax: 2.274573\n",
            "Train Epoch: 10 [1000/1000 (100%)]\tLosses F.softmax: 2.255319 log_softmax: 2.268564\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2992\tAccuracy: 1654.0/10000 (17%)\n",
            "log_softmax: Loss: 2.2979\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 11 [0/1000 (0%)]\tLosses F.softmax: 2.278373 log_softmax: 2.264684\n",
            "Train Epoch: 11 [200/1000 (20%)]\tLosses F.softmax: 2.255093 log_softmax: 2.268148\n",
            "Train Epoch: 11 [400/1000 (40%)]\tLosses F.softmax: 2.246311 log_softmax: 2.266832\n",
            "Train Epoch: 11 [600/1000 (60%)]\tLosses F.softmax: 2.322089 log_softmax: 2.320012\n",
            "Train Epoch: 11 [800/1000 (80%)]\tLosses F.softmax: 2.279466 log_softmax: 2.265327\n",
            "Train Epoch: 11 [1000/1000 (100%)]\tLosses F.softmax: 2.320490 log_softmax: 2.311048\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2987\tAccuracy: 1781.0/10000 (18%)\n",
            "log_softmax: Loss: 2.2974\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 12 [0/1000 (0%)]\tLosses F.softmax: 2.243363 log_softmax: 2.262015\n",
            "Train Epoch: 12 [200/1000 (20%)]\tLosses F.softmax: 2.322676 log_softmax: 2.314720\n",
            "Train Epoch: 12 [400/1000 (40%)]\tLosses F.softmax: 2.325952 log_softmax: 2.309331\n",
            "Train Epoch: 12 [600/1000 (60%)]\tLosses F.softmax: 2.307102 log_softmax: 2.327244\n",
            "Train Epoch: 12 [800/1000 (80%)]\tLosses F.softmax: 2.330738 log_softmax: 2.327307\n",
            "Train Epoch: 12 [1000/1000 (100%)]\tLosses F.softmax: 2.328025 log_softmax: 2.306664\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2982\tAccuracy: 1872.0/10000 (19%)\n",
            "log_softmax: Loss: 2.2968\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 13 [0/1000 (0%)]\tLosses F.softmax: 2.290437 log_softmax: 2.328651\n",
            "Train Epoch: 13 [200/1000 (20%)]\tLosses F.softmax: 2.296702 log_softmax: 2.266648\n",
            "Train Epoch: 13 [400/1000 (40%)]\tLosses F.softmax: 2.324655 log_softmax: 2.305959\n",
            "Train Epoch: 13 [600/1000 (60%)]\tLosses F.softmax: 2.285873 log_softmax: 2.266665\n",
            "Train Epoch: 13 [800/1000 (80%)]\tLosses F.softmax: 2.293291 log_softmax: 2.273565\n",
            "Train Epoch: 13 [1000/1000 (100%)]\tLosses F.softmax: 2.322742 log_softmax: 2.314251\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2977\tAccuracy: 1921.0/10000 (19%)\n",
            "log_softmax: Loss: 2.2963\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 14 [0/1000 (0%)]\tLosses F.softmax: 2.295566 log_softmax: 2.260620\n",
            "Train Epoch: 14 [200/1000 (20%)]\tLosses F.softmax: 2.235881 log_softmax: 2.252520\n",
            "Train Epoch: 14 [400/1000 (40%)]\tLosses F.softmax: 2.304147 log_softmax: 2.332111\n",
            "Train Epoch: 14 [600/1000 (60%)]\tLosses F.softmax: 2.289822 log_softmax: 2.262515\n",
            "Train Epoch: 14 [800/1000 (80%)]\tLosses F.softmax: 2.304508 log_softmax: 2.321069\n",
            "Train Epoch: 14 [1000/1000 (100%)]\tLosses F.softmax: 2.233424 log_softmax: 2.247360\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2972\tAccuracy: 1936.0/10000 (19%)\n",
            "log_softmax: Loss: 2.2958\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 15 [0/1000 (0%)]\tLosses F.softmax: 2.240805 log_softmax: 2.221867\n",
            "Train Epoch: 15 [200/1000 (20%)]\tLosses F.softmax: 2.236904 log_softmax: 2.210716\n",
            "Train Epoch: 15 [400/1000 (40%)]\tLosses F.softmax: 2.350677 log_softmax: 2.357052\n",
            "Train Epoch: 15 [600/1000 (60%)]\tLosses F.softmax: 2.234718 log_softmax: 2.221984\n",
            "Train Epoch: 15 [800/1000 (80%)]\tLosses F.softmax: 2.317933 log_softmax: 2.305307\n",
            "Train Epoch: 15 [1000/1000 (100%)]\tLosses F.softmax: 2.353261 log_softmax: 2.358546\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2968\tAccuracy: 1948.0/10000 (19%)\n",
            "log_softmax: Loss: 2.2953\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 16 [0/1000 (0%)]\tLosses F.softmax: 2.289167 log_softmax: 2.327934\n",
            "Train Epoch: 16 [200/1000 (20%)]\tLosses F.softmax: 2.282804 log_softmax: 2.281606\n",
            "Train Epoch: 16 [400/1000 (40%)]\tLosses F.softmax: 2.223168 log_softmax: 2.240942\n",
            "Train Epoch: 16 [600/1000 (60%)]\tLosses F.softmax: 2.309486 log_softmax: 2.332107\n",
            "Train Epoch: 16 [800/1000 (80%)]\tLosses F.softmax: 2.326227 log_softmax: 2.310011\n",
            "Train Epoch: 16 [1000/1000 (100%)]\tLosses F.softmax: 2.291273 log_softmax: 2.274609\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2963\tAccuracy: 1959.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2948\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 17 [0/1000 (0%)]\tLosses F.softmax: 2.207970 log_softmax: 2.241510\n",
            "Train Epoch: 17 [200/1000 (20%)]\tLosses F.softmax: 2.286101 log_softmax: 2.258561\n",
            "Train Epoch: 17 [400/1000 (40%)]\tLosses F.softmax: 2.291270 log_softmax: 2.321733\n",
            "Train Epoch: 17 [600/1000 (60%)]\tLosses F.softmax: 2.311036 log_softmax: 2.318629\n",
            "Train Epoch: 17 [800/1000 (80%)]\tLosses F.softmax: 2.319581 log_softmax: 2.317957\n",
            "Train Epoch: 17 [1000/1000 (100%)]\tLosses F.softmax: 2.288992 log_softmax: 2.261180\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2958\tAccuracy: 1945.0/10000 (19%)\n",
            "log_softmax: Loss: 2.2943\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 18 [0/1000 (0%)]\tLosses F.softmax: 2.332789 log_softmax: 2.310437\n",
            "Train Epoch: 18 [200/1000 (20%)]\tLosses F.softmax: 2.225320 log_softmax: 2.202756\n",
            "Train Epoch: 18 [400/1000 (40%)]\tLosses F.softmax: 2.212519 log_softmax: 2.234659\n",
            "Train Epoch: 18 [600/1000 (60%)]\tLosses F.softmax: 2.204249 log_softmax: 2.233496\n",
            "Train Epoch: 18 [800/1000 (80%)]\tLosses F.softmax: 2.226297 log_softmax: 2.212095\n",
            "Train Epoch: 18 [1000/1000 (100%)]\tLosses F.softmax: 2.338503 log_softmax: 2.327561\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2953\tAccuracy: 1946.0/10000 (19%)\n",
            "log_softmax: Loss: 2.2937\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 19 [0/1000 (0%)]\tLosses F.softmax: 2.338202 log_softmax: 2.344044\n",
            "Train Epoch: 19 [200/1000 (20%)]\tLosses F.softmax: 2.365786 log_softmax: 2.368502\n",
            "Train Epoch: 19 [400/1000 (40%)]\tLosses F.softmax: 2.211501 log_softmax: 2.234994\n",
            "Train Epoch: 19 [600/1000 (60%)]\tLosses F.softmax: 2.283453 log_softmax: 2.273695\n",
            "Train Epoch: 19 [800/1000 (80%)]\tLosses F.softmax: 2.280658 log_softmax: 2.272685\n",
            "Train Epoch: 19 [1000/1000 (100%)]\tLosses F.softmax: 2.330503 log_softmax: 2.328217\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2949\tAccuracy: 1951.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2932\tAccuracy: 1028.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 20 [0/1000 (0%)]\tLosses F.softmax: 2.214915 log_softmax: 2.184463\n",
            "Train Epoch: 20 [200/1000 (20%)]\tLosses F.softmax: 2.323876 log_softmax: 2.332049\n",
            "Train Epoch: 20 [400/1000 (40%)]\tLosses F.softmax: 2.285727 log_softmax: 2.252816\n",
            "Train Epoch: 20 [600/1000 (60%)]\tLosses F.softmax: 2.320631 log_softmax: 2.311211\n",
            "Train Epoch: 20 [800/1000 (80%)]\tLosses F.softmax: 2.218614 log_softmax: 2.198328\n",
            "Train Epoch: 20 [1000/1000 (100%)]\tLosses F.softmax: 2.221701 log_softmax: 2.209834\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2944\tAccuracy: 1948.0/10000 (19%)\n",
            "log_softmax: Loss: 2.2927\tAccuracy: 1029.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 21 [0/1000 (0%)]\tLosses F.softmax: 2.281740 log_softmax: 2.258598\n",
            "Train Epoch: 21 [200/1000 (20%)]\tLosses F.softmax: 2.312928 log_softmax: 2.336586\n",
            "Train Epoch: 21 [400/1000 (40%)]\tLosses F.softmax: 2.345852 log_softmax: 2.339361\n",
            "Train Epoch: 21 [600/1000 (60%)]\tLosses F.softmax: 2.308612 log_softmax: 2.322551\n",
            "Train Epoch: 21 [800/1000 (80%)]\tLosses F.softmax: 2.271535 log_softmax: 2.275190\n",
            "Train Epoch: 21 [1000/1000 (100%)]\tLosses F.softmax: 2.286443 log_softmax: 2.276856\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2939\tAccuracy: 1948.0/10000 (19%)\n",
            "log_softmax: Loss: 2.2921\tAccuracy: 1031.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 22 [0/1000 (0%)]\tLosses F.softmax: 2.310022 log_softmax: 2.307989\n",
            "Train Epoch: 22 [200/1000 (20%)]\tLosses F.softmax: 2.313471 log_softmax: 2.344760\n",
            "Train Epoch: 22 [400/1000 (40%)]\tLosses F.softmax: 2.337298 log_softmax: 2.316178\n",
            "Train Epoch: 22 [600/1000 (60%)]\tLosses F.softmax: 2.312705 log_softmax: 2.331877\n",
            "Train Epoch: 22 [800/1000 (80%)]\tLosses F.softmax: 2.292264 log_softmax: 2.281610\n",
            "Train Epoch: 22 [1000/1000 (100%)]\tLosses F.softmax: 2.287395 log_softmax: 2.279101\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2934\tAccuracy: 1945.0/10000 (19%)\n",
            "log_softmax: Loss: 2.2916\tAccuracy: 1038.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 23 [0/1000 (0%)]\tLosses F.softmax: 2.320272 log_softmax: 2.312171\n",
            "Train Epoch: 23 [200/1000 (20%)]\tLosses F.softmax: 2.376314 log_softmax: 2.374013\n",
            "Train Epoch: 23 [400/1000 (40%)]\tLosses F.softmax: 2.318010 log_softmax: 2.344880\n",
            "Train Epoch: 23 [600/1000 (60%)]\tLosses F.softmax: 2.195612 log_softmax: 2.215135\n",
            "Train Epoch: 23 [800/1000 (80%)]\tLosses F.softmax: 2.202170 log_softmax: 2.175958\n",
            "Train Epoch: 23 [1000/1000 (100%)]\tLosses F.softmax: 2.317116 log_softmax: 2.336776\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2929\tAccuracy: 1950.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2910\tAccuracy: 1044.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 24 [0/1000 (0%)]\tLosses F.softmax: 2.199814 log_softmax: 2.172024\n",
            "Train Epoch: 24 [200/1000 (20%)]\tLosses F.softmax: 2.340503 log_softmax: 2.326544\n",
            "Train Epoch: 24 [400/1000 (40%)]\tLosses F.softmax: 2.186299 log_softmax: 2.218720\n",
            "Train Epoch: 24 [600/1000 (60%)]\tLosses F.softmax: 2.279871 log_softmax: 2.249427\n",
            "Train Epoch: 24 [800/1000 (80%)]\tLosses F.softmax: 2.316214 log_softmax: 2.306827\n",
            "Train Epoch: 24 [1000/1000 (100%)]\tLosses F.softmax: 2.337802 log_softmax: 2.332100\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2924\tAccuracy: 1944.0/10000 (19%)\n",
            "log_softmax: Loss: 2.2904\tAccuracy: 1053.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 25 [0/1000 (0%)]\tLosses F.softmax: 2.334618 log_softmax: 2.312071\n",
            "Train Epoch: 25 [200/1000 (20%)]\tLosses F.softmax: 2.328582 log_softmax: 2.322863\n",
            "Train Epoch: 25 [400/1000 (40%)]\tLosses F.softmax: 2.314988 log_softmax: 2.310419\n",
            "Train Epoch: 25 [600/1000 (60%)]\tLosses F.softmax: 2.286859 log_softmax: 2.274928\n",
            "Train Epoch: 25 [800/1000 (80%)]\tLosses F.softmax: 2.306725 log_softmax: 2.307853\n",
            "Train Epoch: 25 [1000/1000 (100%)]\tLosses F.softmax: 2.287139 log_softmax: 2.274197\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2919\tAccuracy: 1956.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2898\tAccuracy: 1068.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 26 [0/1000 (0%)]\tLosses F.softmax: 2.314559 log_softmax: 2.326091\n",
            "Train Epoch: 26 [200/1000 (20%)]\tLosses F.softmax: 2.334172 log_softmax: 2.316958\n",
            "Train Epoch: 26 [400/1000 (40%)]\tLosses F.softmax: 2.174323 log_softmax: 2.206931\n",
            "Train Epoch: 26 [600/1000 (60%)]\tLosses F.softmax: 2.329920 log_softmax: 2.354733\n",
            "Train Epoch: 26 [800/1000 (80%)]\tLosses F.softmax: 2.192257 log_softmax: 2.157186\n",
            "Train Epoch: 26 [1000/1000 (100%)]\tLosses F.softmax: 2.322135 log_softmax: 2.315959\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2913\tAccuracy: 1955.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2892\tAccuracy: 1086.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 27 [0/1000 (0%)]\tLosses F.softmax: 2.381145 log_softmax: 2.381448\n",
            "Train Epoch: 27 [200/1000 (20%)]\tLosses F.softmax: 2.344932 log_softmax: 2.345588\n",
            "Train Epoch: 27 [400/1000 (40%)]\tLosses F.softmax: 2.388395 log_softmax: 2.393570\n",
            "Train Epoch: 27 [600/1000 (60%)]\tLosses F.softmax: 2.307947 log_softmax: 2.291648\n",
            "Train Epoch: 27 [800/1000 (80%)]\tLosses F.softmax: 2.263589 log_softmax: 2.237698\n",
            "Train Epoch: 27 [1000/1000 (100%)]\tLosses F.softmax: 2.327198 log_softmax: 2.331218\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2908\tAccuracy: 1957.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2886\tAccuracy: 1117.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 28 [0/1000 (0%)]\tLosses F.softmax: 2.162890 log_softmax: 2.192348\n",
            "Train Epoch: 28 [200/1000 (20%)]\tLosses F.softmax: 2.341940 log_softmax: 2.344967\n",
            "Train Epoch: 28 [400/1000 (40%)]\tLosses F.softmax: 2.374600 log_softmax: 2.385601\n",
            "Train Epoch: 28 [600/1000 (60%)]\tLosses F.softmax: 2.259494 log_softmax: 2.233372\n",
            "Train Epoch: 28 [800/1000 (80%)]\tLosses F.softmax: 2.387838 log_softmax: 2.396894\n",
            "Train Epoch: 28 [1000/1000 (100%)]\tLosses F.softmax: 2.267402 log_softmax: 2.237306\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2902\tAccuracy: 1963.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2879\tAccuracy: 1150.0/10000 (12%)\n",
            "\n",
            "Train Epoch: 29 [0/1000 (0%)]\tLosses F.softmax: 2.277313 log_softmax: 2.274870\n",
            "Train Epoch: 29 [200/1000 (20%)]\tLosses F.softmax: 2.302277 log_softmax: 2.295794\n",
            "Train Epoch: 29 [400/1000 (40%)]\tLosses F.softmax: 2.177886 log_softmax: 2.159476\n",
            "Train Epoch: 29 [600/1000 (60%)]\tLosses F.softmax: 2.302004 log_softmax: 2.288088\n",
            "Train Epoch: 29 [800/1000 (80%)]\tLosses F.softmax: 2.340036 log_softmax: 2.344220\n",
            "Train Epoch: 29 [1000/1000 (100%)]\tLosses F.softmax: 2.180306 log_softmax: 2.151219\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2896\tAccuracy: 1963.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2873\tAccuracy: 1188.0/10000 (12%)\n",
            "\n",
            "Train Epoch: 30 [0/1000 (0%)]\tLosses F.softmax: 2.304732 log_softmax: 2.290392\n",
            "Train Epoch: 30 [200/1000 (20%)]\tLosses F.softmax: 2.325144 log_softmax: 2.326623\n",
            "Train Epoch: 30 [400/1000 (40%)]\tLosses F.softmax: 2.264683 log_softmax: 2.238630\n",
            "Train Epoch: 30 [600/1000 (60%)]\tLosses F.softmax: 2.386921 log_softmax: 2.389689\n",
            "Train Epoch: 30 [800/1000 (80%)]\tLosses F.softmax: 2.337419 log_softmax: 2.328135\n",
            "Train Epoch: 30 [1000/1000 (100%)]\tLosses F.softmax: 2.339012 log_softmax: 2.355398\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2890\tAccuracy: 1968.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2866\tAccuracy: 1224.0/10000 (12%)\n",
            "\n",
            "Train Epoch: 31 [0/1000 (0%)]\tLosses F.softmax: 2.148459 log_softmax: 2.185683\n",
            "Train Epoch: 31 [200/1000 (20%)]\tLosses F.softmax: 2.138295 log_softmax: 2.181388\n",
            "Train Epoch: 31 [400/1000 (40%)]\tLosses F.softmax: 2.271204 log_softmax: 2.269402\n",
            "Train Epoch: 31 [600/1000 (60%)]\tLosses F.softmax: 2.186509 log_softmax: 2.162759\n",
            "Train Epoch: 31 [800/1000 (80%)]\tLosses F.softmax: 2.189020 log_softmax: 2.172866\n",
            "Train Epoch: 31 [1000/1000 (100%)]\tLosses F.softmax: 2.287174 log_softmax: 2.271671\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2884\tAccuracy: 1976.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2858\tAccuracy: 1254.0/10000 (13%)\n",
            "\n",
            "Train Epoch: 32 [0/1000 (0%)]\tLosses F.softmax: 2.154624 log_softmax: 2.191281\n",
            "Train Epoch: 32 [200/1000 (20%)]\tLosses F.softmax: 2.338985 log_softmax: 2.349884\n",
            "Train Epoch: 32 [400/1000 (40%)]\tLosses F.softmax: 2.172101 log_softmax: 2.144122\n",
            "Train Epoch: 32 [600/1000 (60%)]\tLosses F.softmax: 2.389134 log_softmax: 2.387361\n",
            "Train Epoch: 32 [800/1000 (80%)]\tLosses F.softmax: 2.286233 log_softmax: 2.289896\n",
            "Train Epoch: 32 [1000/1000 (100%)]\tLosses F.softmax: 2.270551 log_softmax: 2.273188\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2877\tAccuracy: 1981.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2851\tAccuracy: 1297.0/10000 (13%)\n",
            "\n",
            "Train Epoch: 33 [0/1000 (0%)]\tLosses F.softmax: 2.257681 log_softmax: 2.232459\n",
            "Train Epoch: 33 [200/1000 (20%)]\tLosses F.softmax: 2.346159 log_softmax: 2.331422\n",
            "Train Epoch: 33 [400/1000 (40%)]\tLosses F.softmax: 2.320313 log_softmax: 2.282940\n",
            "Train Epoch: 33 [600/1000 (60%)]\tLosses F.softmax: 2.263907 log_softmax: 2.240747\n",
            "Train Epoch: 33 [800/1000 (80%)]\tLosses F.softmax: 2.162616 log_softmax: 2.162344\n",
            "Train Epoch: 33 [1000/1000 (100%)]\tLosses F.softmax: 2.270794 log_softmax: 2.249189\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2871\tAccuracy: 1982.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2843\tAccuracy: 1339.0/10000 (13%)\n",
            "\n",
            "Train Epoch: 34 [0/1000 (0%)]\tLosses F.softmax: 2.284849 log_softmax: 2.274254\n",
            "Train Epoch: 34 [200/1000 (20%)]\tLosses F.softmax: 2.381887 log_softmax: 2.388488\n",
            "Train Epoch: 34 [400/1000 (40%)]\tLosses F.softmax: 2.313987 log_softmax: 2.294770\n",
            "Train Epoch: 34 [600/1000 (60%)]\tLosses F.softmax: 2.272573 log_softmax: 2.240854\n",
            "Train Epoch: 34 [800/1000 (80%)]\tLosses F.softmax: 2.396354 log_softmax: 2.397675\n",
            "Train Epoch: 34 [1000/1000 (100%)]\tLosses F.softmax: 2.130023 log_softmax: 2.168101\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2864\tAccuracy: 1987.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2835\tAccuracy: 1388.0/10000 (14%)\n",
            "\n",
            "Train Epoch: 35 [0/1000 (0%)]\tLosses F.softmax: 2.341242 log_softmax: 2.362425\n",
            "Train Epoch: 35 [200/1000 (20%)]\tLosses F.softmax: 2.399735 log_softmax: 2.395954\n",
            "Train Epoch: 35 [400/1000 (40%)]\tLosses F.softmax: 2.123434 log_softmax: 2.162120\n",
            "Train Epoch: 35 [600/1000 (60%)]\tLosses F.softmax: 2.345210 log_softmax: 2.354438\n",
            "Train Epoch: 35 [800/1000 (80%)]\tLosses F.softmax: 2.341173 log_softmax: 2.353701\n",
            "Train Epoch: 35 [1000/1000 (100%)]\tLosses F.softmax: 2.154279 log_softmax: 2.155184\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2857\tAccuracy: 1993.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2827\tAccuracy: 1434.0/10000 (14%)\n",
            "\n",
            "Train Epoch: 36 [0/1000 (0%)]\tLosses F.softmax: 2.339176 log_softmax: 2.309101\n",
            "Train Epoch: 36 [200/1000 (20%)]\tLosses F.softmax: 2.338727 log_softmax: 2.330856\n",
            "Train Epoch: 36 [400/1000 (40%)]\tLosses F.softmax: 2.352791 log_softmax: 2.336910\n",
            "Train Epoch: 36 [600/1000 (60%)]\tLosses F.softmax: 2.315758 log_softmax: 2.299893\n",
            "Train Epoch: 36 [800/1000 (80%)]\tLosses F.softmax: 2.159221 log_softmax: 2.140134\n",
            "Train Epoch: 36 [1000/1000 (100%)]\tLosses F.softmax: 2.285545 log_softmax: 2.284301\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2849\tAccuracy: 1999.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2818\tAccuracy: 1490.0/10000 (15%)\n",
            "\n",
            "Train Epoch: 37 [0/1000 (0%)]\tLosses F.softmax: 2.144291 log_softmax: 2.180666\n",
            "Train Epoch: 37 [200/1000 (20%)]\tLosses F.softmax: 2.339191 log_softmax: 2.347696\n",
            "Train Epoch: 37 [400/1000 (40%)]\tLosses F.softmax: 2.271514 log_softmax: 2.257553\n",
            "Train Epoch: 37 [600/1000 (60%)]\tLosses F.softmax: 2.263016 log_softmax: 2.244179\n",
            "Train Epoch: 37 [800/1000 (80%)]\tLosses F.softmax: 2.121713 log_softmax: 2.158054\n",
            "Train Epoch: 37 [1000/1000 (100%)]\tLosses F.softmax: 2.310823 log_softmax: 2.301308\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2841\tAccuracy: 1997.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2809\tAccuracy: 1559.0/10000 (16%)\n",
            "\n",
            "Train Epoch: 38 [0/1000 (0%)]\tLosses F.softmax: 2.355430 log_softmax: 2.362264\n",
            "Train Epoch: 38 [200/1000 (20%)]\tLosses F.softmax: 2.293801 log_softmax: 2.283556\n",
            "Train Epoch: 38 [400/1000 (40%)]\tLosses F.softmax: 2.149216 log_softmax: 2.128553\n",
            "Train Epoch: 38 [600/1000 (60%)]\tLosses F.softmax: 2.354859 log_softmax: 2.344882\n",
            "Train Epoch: 38 [800/1000 (80%)]\tLosses F.softmax: 2.145569 log_softmax: 2.168525\n",
            "Train Epoch: 38 [1000/1000 (100%)]\tLosses F.softmax: 2.406581 log_softmax: 2.403796\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2833\tAccuracy: 1996.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2799\tAccuracy: 1620.0/10000 (16%)\n",
            "\n",
            "Train Epoch: 39 [0/1000 (0%)]\tLosses F.softmax: 2.175102 log_softmax: 2.146187\n",
            "Train Epoch: 39 [200/1000 (20%)]\tLosses F.softmax: 2.355128 log_softmax: 2.361621\n",
            "Train Epoch: 39 [400/1000 (40%)]\tLosses F.softmax: 2.271395 log_softmax: 2.269775\n",
            "Train Epoch: 39 [600/1000 (60%)]\tLosses F.softmax: 2.322475 log_softmax: 2.321978\n",
            "Train Epoch: 39 [800/1000 (80%)]\tLosses F.softmax: 2.401143 log_softmax: 2.411930\n",
            "Train Epoch: 39 [1000/1000 (100%)]\tLosses F.softmax: 2.243788 log_softmax: 2.219060\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2825\tAccuracy: 2001.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2789\tAccuracy: 1679.0/10000 (17%)\n",
            "\n",
            "Train Epoch: 40 [0/1000 (0%)]\tLosses F.softmax: 2.281845 log_softmax: 2.284068\n",
            "Train Epoch: 40 [200/1000 (20%)]\tLosses F.softmax: 2.148234 log_softmax: 2.138587\n",
            "Train Epoch: 40 [400/1000 (40%)]\tLosses F.softmax: 2.143640 log_softmax: 2.122961\n",
            "Train Epoch: 40 [600/1000 (60%)]\tLosses F.softmax: 2.327624 log_softmax: 2.350269\n",
            "Train Epoch: 40 [800/1000 (80%)]\tLosses F.softmax: 2.151783 log_softmax: 2.132398\n",
            "Train Epoch: 40 [1000/1000 (100%)]\tLosses F.softmax: 2.158838 log_softmax: 2.148487\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2816\tAccuracy: 2008.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2779\tAccuracy: 1704.0/10000 (17%)\n",
            "\n",
            "Train Epoch: 41 [0/1000 (0%)]\tLosses F.softmax: 2.146831 log_softmax: 2.117362\n",
            "Train Epoch: 41 [200/1000 (20%)]\tLosses F.softmax: 2.257580 log_softmax: 2.228957\n",
            "Train Epoch: 41 [400/1000 (40%)]\tLosses F.softmax: 2.305847 log_softmax: 2.285763\n",
            "Train Epoch: 41 [600/1000 (60%)]\tLosses F.softmax: 2.340347 log_softmax: 2.340657\n",
            "Train Epoch: 41 [800/1000 (80%)]\tLosses F.softmax: 2.113019 log_softmax: 2.148896\n",
            "Train Epoch: 41 [1000/1000 (100%)]\tLosses F.softmax: 2.098904 log_softmax: 2.134608\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2807\tAccuracy: 2010.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2768\tAccuracy: 1750.0/10000 (18%)\n",
            "\n",
            "Train Epoch: 42 [0/1000 (0%)]\tLosses F.softmax: 2.242791 log_softmax: 2.218257\n",
            "Train Epoch: 42 [200/1000 (20%)]\tLosses F.softmax: 2.357726 log_softmax: 2.340548\n",
            "Train Epoch: 42 [400/1000 (40%)]\tLosses F.softmax: 2.295562 log_softmax: 2.282046\n",
            "Train Epoch: 42 [600/1000 (60%)]\tLosses F.softmax: 2.326418 log_softmax: 2.308060\n",
            "Train Epoch: 42 [800/1000 (80%)]\tLosses F.softmax: 2.353572 log_softmax: 2.339026\n",
            "Train Epoch: 42 [1000/1000 (100%)]\tLosses F.softmax: 2.246508 log_softmax: 2.220238\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2798\tAccuracy: 2015.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2757\tAccuracy: 1793.0/10000 (18%)\n",
            "\n",
            "Train Epoch: 43 [0/1000 (0%)]\tLosses F.softmax: 2.361851 log_softmax: 2.347606\n",
            "Train Epoch: 43 [200/1000 (20%)]\tLosses F.softmax: 2.331263 log_softmax: 2.348385\n",
            "Train Epoch: 43 [400/1000 (40%)]\tLosses F.softmax: 2.322567 log_softmax: 2.263609\n",
            "Train Epoch: 43 [600/1000 (60%)]\tLosses F.softmax: 2.392521 log_softmax: 2.390181\n",
            "Train Epoch: 43 [800/1000 (80%)]\tLosses F.softmax: 2.343547 log_softmax: 2.338203\n",
            "Train Epoch: 43 [1000/1000 (100%)]\tLosses F.softmax: 2.306560 log_softmax: 2.263751\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2788\tAccuracy: 2020.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2745\tAccuracy: 1834.0/10000 (18%)\n",
            "\n",
            "Train Epoch: 44 [0/1000 (0%)]\tLosses F.softmax: 2.136896 log_softmax: 2.103820\n",
            "Train Epoch: 44 [200/1000 (20%)]\tLosses F.softmax: 2.327837 log_softmax: 2.306227\n",
            "Train Epoch: 44 [400/1000 (40%)]\tLosses F.softmax: 2.275588 log_softmax: 2.267781\n",
            "Train Epoch: 44 [600/1000 (60%)]\tLosses F.softmax: 2.330912 log_softmax: 2.319380\n",
            "Train Epoch: 44 [800/1000 (80%)]\tLosses F.softmax: 2.407968 log_softmax: 2.410853\n",
            "Train Epoch: 44 [1000/1000 (100%)]\tLosses F.softmax: 2.330538 log_softmax: 2.335329\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2778\tAccuracy: 2021.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2733\tAccuracy: 1875.0/10000 (19%)\n",
            "\n",
            "Train Epoch: 45 [0/1000 (0%)]\tLosses F.softmax: 2.257046 log_softmax: 2.264752\n",
            "Train Epoch: 45 [200/1000 (20%)]\tLosses F.softmax: 2.264417 log_softmax: 2.239185\n",
            "Train Epoch: 45 [400/1000 (40%)]\tLosses F.softmax: 2.257148 log_softmax: 2.258663\n",
            "Train Epoch: 45 [600/1000 (60%)]\tLosses F.softmax: 2.332687 log_softmax: 2.341195\n",
            "Train Epoch: 45 [800/1000 (80%)]\tLosses F.softmax: 2.123888 log_softmax: 2.090189\n",
            "Train Epoch: 45 [1000/1000 (100%)]\tLosses F.softmax: 2.310864 log_softmax: 2.287850\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2767\tAccuracy: 2025.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2720\tAccuracy: 1909.0/10000 (19%)\n",
            "\n",
            "Train Epoch: 46 [0/1000 (0%)]\tLosses F.softmax: 2.264353 log_softmax: 2.242331\n",
            "Train Epoch: 46 [200/1000 (20%)]\tLosses F.softmax: 2.275199 log_softmax: 2.267056\n",
            "Train Epoch: 46 [400/1000 (40%)]\tLosses F.softmax: 2.355359 log_softmax: 2.368981\n",
            "Train Epoch: 46 [600/1000 (60%)]\tLosses F.softmax: 2.402112 log_softmax: 2.403054\n",
            "Train Epoch: 46 [800/1000 (80%)]\tLosses F.softmax: 2.356239 log_softmax: 2.337990\n",
            "Train Epoch: 46 [1000/1000 (100%)]\tLosses F.softmax: 2.289510 log_softmax: 2.273245\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2756\tAccuracy: 2027.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2707\tAccuracy: 1944.0/10000 (19%)\n",
            "\n",
            "Train Epoch: 47 [0/1000 (0%)]\tLosses F.softmax: 2.282935 log_softmax: 2.273045\n",
            "Train Epoch: 47 [200/1000 (20%)]\tLosses F.softmax: 2.330288 log_softmax: 2.338779\n",
            "Train Epoch: 47 [400/1000 (40%)]\tLosses F.softmax: 2.330719 log_softmax: 2.344558\n",
            "Train Epoch: 47 [600/1000 (60%)]\tLosses F.softmax: 2.314701 log_softmax: 2.298771\n",
            "Train Epoch: 47 [800/1000 (80%)]\tLosses F.softmax: 2.114414 log_softmax: 2.085145\n",
            "Train Epoch: 47 [1000/1000 (100%)]\tLosses F.softmax: 2.307678 log_softmax: 2.320827\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2745\tAccuracy: 2027.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2692\tAccuracy: 1970.0/10000 (20%)\n",
            "\n",
            "Train Epoch: 48 [0/1000 (0%)]\tLosses F.softmax: 2.237061 log_softmax: 2.208879\n",
            "Train Epoch: 48 [200/1000 (20%)]\tLosses F.softmax: 2.333622 log_softmax: 2.303862\n",
            "Train Epoch: 48 [400/1000 (40%)]\tLosses F.softmax: 2.235856 log_softmax: 2.209222\n",
            "Train Epoch: 48 [600/1000 (60%)]\tLosses F.softmax: 2.354093 log_softmax: 2.361978\n",
            "Train Epoch: 48 [800/1000 (80%)]\tLosses F.softmax: 2.252523 log_softmax: 2.257241\n",
            "Train Epoch: 48 [1000/1000 (100%)]\tLosses F.softmax: 2.310026 log_softmax: 2.293501\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2732\tAccuracy: 2030.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2678\tAccuracy: 2002.0/10000 (20%)\n",
            "\n",
            "Train Epoch: 49 [0/1000 (0%)]\tLosses F.softmax: 2.077301 log_softmax: 2.113414\n",
            "Train Epoch: 49 [200/1000 (20%)]\tLosses F.softmax: 2.323111 log_softmax: 2.321658\n",
            "Train Epoch: 49 [400/1000 (40%)]\tLosses F.softmax: 2.327809 log_softmax: 2.302801\n",
            "Train Epoch: 49 [600/1000 (60%)]\tLosses F.softmax: 2.282777 log_softmax: 2.274599\n",
            "Train Epoch: 49 [800/1000 (80%)]\tLosses F.softmax: 2.329852 log_softmax: 2.310559\n",
            "Train Epoch: 49 [1000/1000 (100%)]\tLosses F.softmax: 2.237279 log_softmax: 2.215978\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2720\tAccuracy: 2030.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2662\tAccuracy: 2025.0/10000 (20%)\n",
            "\n",
            "Train Epoch: 50 [0/1000 (0%)]\tLosses F.softmax: 2.348536 log_softmax: 2.359102\n",
            "Train Epoch: 50 [200/1000 (20%)]\tLosses F.softmax: 2.355072 log_softmax: 2.373431\n",
            "Train Epoch: 50 [400/1000 (40%)]\tLosses F.softmax: 2.107858 log_softmax: 2.085141\n",
            "Train Epoch: 50 [600/1000 (60%)]\tLosses F.softmax: 2.227928 log_softmax: 2.203856\n",
            "Train Epoch: 50 [800/1000 (80%)]\tLosses F.softmax: 2.343419 log_softmax: 2.323244\n",
            "Train Epoch: 50 [1000/1000 (100%)]\tLosses F.softmax: 2.416038 log_softmax: 2.419859\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2706\tAccuracy: 2032.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2646\tAccuracy: 2039.0/10000 (20%)\n",
            "\n",
            "Train Epoch: 51 [0/1000 (0%)]\tLosses F.softmax: 2.320672 log_softmax: 2.323857\n",
            "Train Epoch: 51 [200/1000 (20%)]\tLosses F.softmax: 2.051085 log_softmax: 2.090338\n",
            "Train Epoch: 51 [400/1000 (40%)]\tLosses F.softmax: 2.325981 log_softmax: 2.335409\n",
            "Train Epoch: 51 [600/1000 (60%)]\tLosses F.softmax: 2.060426 log_softmax: 2.089527\n",
            "Train Epoch: 51 [800/1000 (80%)]\tLosses F.softmax: 2.092153 log_softmax: 2.115060\n",
            "Train Epoch: 51 [1000/1000 (100%)]\tLosses F.softmax: 2.394685 log_softmax: 2.401597\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2692\tAccuracy: 2034.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2629\tAccuracy: 2053.0/10000 (21%)\n",
            "\n",
            "Train Epoch: 52 [0/1000 (0%)]\tLosses F.softmax: 2.264857 log_softmax: 2.237510\n",
            "Train Epoch: 52 [200/1000 (20%)]\tLosses F.softmax: 2.225355 log_softmax: 2.196734\n",
            "Train Epoch: 52 [400/1000 (40%)]\tLosses F.softmax: 2.272896 log_softmax: 2.237598\n",
            "Train Epoch: 52 [600/1000 (60%)]\tLosses F.softmax: 2.379375 log_softmax: 2.342110\n",
            "Train Epoch: 52 [800/1000 (80%)]\tLosses F.softmax: 2.034302 log_softmax: 2.078733\n",
            "Train Epoch: 52 [1000/1000 (100%)]\tLosses F.softmax: 2.353004 log_softmax: 2.366966\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2678\tAccuracy: 2035.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2611\tAccuracy: 2063.0/10000 (21%)\n",
            "\n",
            "Train Epoch: 53 [0/1000 (0%)]\tLosses F.softmax: 2.286262 log_softmax: 2.284482\n",
            "Train Epoch: 53 [200/1000 (20%)]\tLosses F.softmax: 2.338971 log_softmax: 2.346593\n",
            "Train Epoch: 53 [400/1000 (40%)]\tLosses F.softmax: 2.309794 log_softmax: 2.328068\n",
            "Train Epoch: 53 [600/1000 (60%)]\tLosses F.softmax: 2.329801 log_softmax: 2.295635\n",
            "Train Epoch: 53 [800/1000 (80%)]\tLosses F.softmax: 2.221647 log_softmax: 2.186421\n",
            "Train Epoch: 53 [1000/1000 (100%)]\tLosses F.softmax: 2.407889 log_softmax: 2.412599\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2662\tAccuracy: 2034.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2592\tAccuracy: 2082.0/10000 (21%)\n",
            "\n",
            "Train Epoch: 54 [0/1000 (0%)]\tLosses F.softmax: 2.269356 log_softmax: 2.263762\n",
            "Train Epoch: 54 [200/1000 (20%)]\tLosses F.softmax: 2.270489 log_softmax: 2.242331\n",
            "Train Epoch: 54 [400/1000 (40%)]\tLosses F.softmax: 2.251164 log_softmax: 2.215273\n",
            "Train Epoch: 54 [600/1000 (60%)]\tLosses F.softmax: 2.287075 log_softmax: 2.232944\n",
            "Train Epoch: 54 [800/1000 (80%)]\tLosses F.softmax: 2.123781 log_softmax: 2.114369\n",
            "Train Epoch: 54 [1000/1000 (100%)]\tLosses F.softmax: 2.303442 log_softmax: 2.324180\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2646\tAccuracy: 2034.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2572\tAccuracy: 2100.0/10000 (21%)\n",
            "\n",
            "Train Epoch: 55 [0/1000 (0%)]\tLosses F.softmax: 2.094240 log_softmax: 2.060932\n",
            "Train Epoch: 55 [200/1000 (20%)]\tLosses F.softmax: 2.234817 log_softmax: 2.217594\n",
            "Train Epoch: 55 [400/1000 (40%)]\tLosses F.softmax: 2.428043 log_softmax: 2.426596\n",
            "Train Epoch: 55 [600/1000 (60%)]\tLosses F.softmax: 2.321134 log_softmax: 2.293146\n",
            "Train Epoch: 55 [800/1000 (80%)]\tLosses F.softmax: 2.362704 log_softmax: 2.380224\n",
            "Train Epoch: 55 [1000/1000 (100%)]\tLosses F.softmax: 2.219813 log_softmax: 2.186182\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2629\tAccuracy: 2034.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2551\tAccuracy: 2130.0/10000 (21%)\n",
            "\n",
            "Train Epoch: 56 [0/1000 (0%)]\tLosses F.softmax: 2.256605 log_softmax: 2.266739\n",
            "Train Epoch: 56 [200/1000 (20%)]\tLosses F.softmax: 2.311206 log_softmax: 2.291111\n",
            "Train Epoch: 56 [400/1000 (40%)]\tLosses F.softmax: 2.253273 log_softmax: 2.222319\n",
            "Train Epoch: 56 [600/1000 (60%)]\tLosses F.softmax: 2.072098 log_softmax: 2.060678\n",
            "Train Epoch: 56 [800/1000 (80%)]\tLosses F.softmax: 2.083483 log_softmax: 2.045062\n",
            "Train Epoch: 56 [1000/1000 (100%)]\tLosses F.softmax: 2.246993 log_softmax: 2.205454\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2611\tAccuracy: 2035.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2529\tAccuracy: 2170.0/10000 (22%)\n",
            "\n",
            "Train Epoch: 57 [0/1000 (0%)]\tLosses F.softmax: 2.279306 log_softmax: 2.255796\n",
            "Train Epoch: 57 [200/1000 (20%)]\tLosses F.softmax: 2.333426 log_softmax: 2.301205\n",
            "Train Epoch: 57 [400/1000 (40%)]\tLosses F.softmax: 2.313529 log_softmax: 2.269676\n",
            "Train Epoch: 57 [600/1000 (60%)]\tLosses F.softmax: 2.400158 log_softmax: 2.400010\n",
            "Train Epoch: 57 [800/1000 (80%)]\tLosses F.softmax: 2.254491 log_softmax: 2.248322\n",
            "Train Epoch: 57 [1000/1000 (100%)]\tLosses F.softmax: 2.400997 log_softmax: 2.391495\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2593\tAccuracy: 2037.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2506\tAccuracy: 2204.0/10000 (22%)\n",
            "\n",
            "Train Epoch: 58 [0/1000 (0%)]\tLosses F.softmax: 2.297662 log_softmax: 2.294964\n",
            "Train Epoch: 58 [200/1000 (20%)]\tLosses F.softmax: 2.311769 log_softmax: 2.270983\n",
            "Train Epoch: 58 [400/1000 (40%)]\tLosses F.softmax: 2.346109 log_softmax: 2.303304\n",
            "Train Epoch: 58 [600/1000 (60%)]\tLosses F.softmax: 2.212937 log_softmax: 2.187142\n",
            "Train Epoch: 58 [800/1000 (80%)]\tLosses F.softmax: 2.240026 log_softmax: 2.240450\n",
            "Train Epoch: 58 [1000/1000 (100%)]\tLosses F.softmax: 2.261602 log_softmax: 2.176235\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2573\tAccuracy: 2046.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2482\tAccuracy: 2261.0/10000 (23%)\n",
            "\n",
            "Train Epoch: 59 [0/1000 (0%)]\tLosses F.softmax: 2.291381 log_softmax: 2.277855\n",
            "Train Epoch: 59 [200/1000 (20%)]\tLosses F.softmax: 2.322312 log_softmax: 2.298512\n",
            "Train Epoch: 59 [400/1000 (40%)]\tLosses F.softmax: 2.394389 log_softmax: 2.405005\n",
            "Train Epoch: 59 [600/1000 (60%)]\tLosses F.softmax: 2.075492 log_softmax: 2.087195\n",
            "Train Epoch: 59 [800/1000 (80%)]\tLosses F.softmax: 2.249009 log_softmax: 2.228391\n",
            "Train Epoch: 59 [1000/1000 (100%)]\tLosses F.softmax: 2.090301 log_softmax: 2.063046\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2552\tAccuracy: 2062.0/10000 (21%)\n",
            "log_softmax: Loss: 2.2456\tAccuracy: 2313.0/10000 (23%)\n",
            "\n",
            "Train Epoch: 60 [0/1000 (0%)]\tLosses F.softmax: 2.310592 log_softmax: 2.264141\n",
            "Train Epoch: 60 [200/1000 (20%)]\tLosses F.softmax: 2.300683 log_softmax: 2.317634\n",
            "Train Epoch: 60 [400/1000 (40%)]\tLosses F.softmax: 2.233238 log_softmax: 2.248567\n",
            "Train Epoch: 60 [600/1000 (60%)]\tLosses F.softmax: 2.272104 log_softmax: 2.248435\n",
            "Train Epoch: 60 [800/1000 (80%)]\tLosses F.softmax: 2.218680 log_softmax: 2.166429\n",
            "Train Epoch: 60 [1000/1000 (100%)]\tLosses F.softmax: 2.049384 log_softmax: 2.095670\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2530\tAccuracy: 2090.0/10000 (21%)\n",
            "log_softmax: Loss: 2.2429\tAccuracy: 2371.0/10000 (24%)\n",
            "\n",
            "Train Epoch: 61 [0/1000 (0%)]\tLosses F.softmax: 1.970226 log_softmax: 2.016113\n",
            "Train Epoch: 61 [200/1000 (20%)]\tLosses F.softmax: 2.050390 log_softmax: 2.070673\n",
            "Train Epoch: 61 [400/1000 (40%)]\tLosses F.softmax: 1.992866 log_softmax: 2.038416\n",
            "Train Epoch: 61 [600/1000 (60%)]\tLosses F.softmax: 2.395125 log_softmax: 2.394907\n",
            "Train Epoch: 61 [800/1000 (80%)]\tLosses F.softmax: 2.443173 log_softmax: 2.426940\n",
            "Train Epoch: 61 [1000/1000 (100%)]\tLosses F.softmax: 2.270014 log_softmax: 2.210807\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2507\tAccuracy: 2118.0/10000 (21%)\n",
            "log_softmax: Loss: 2.2400\tAccuracy: 2417.0/10000 (24%)\n",
            "\n",
            "Train Epoch: 62 [0/1000 (0%)]\tLosses F.softmax: 2.201185 log_softmax: 2.157331\n",
            "Train Epoch: 62 [200/1000 (20%)]\tLosses F.softmax: 2.319925 log_softmax: 2.279550\n",
            "Train Epoch: 62 [400/1000 (40%)]\tLosses F.softmax: 2.329272 log_softmax: 2.310739\n",
            "Train Epoch: 62 [600/1000 (60%)]\tLosses F.softmax: 2.267743 log_softmax: 2.263391\n",
            "Train Epoch: 62 [800/1000 (80%)]\tLosses F.softmax: 2.327799 log_softmax: 2.315817\n",
            "Train Epoch: 62 [1000/1000 (100%)]\tLosses F.softmax: 2.242720 log_softmax: 2.268471\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2482\tAccuracy: 2156.0/10000 (22%)\n",
            "log_softmax: Loss: 2.2369\tAccuracy: 2484.0/10000 (25%)\n",
            "\n",
            "Train Epoch: 63 [0/1000 (0%)]\tLosses F.softmax: 2.300599 log_softmax: 2.202060\n",
            "Train Epoch: 63 [200/1000 (20%)]\tLosses F.softmax: 2.367844 log_softmax: 2.385614\n",
            "Train Epoch: 63 [400/1000 (40%)]\tLosses F.softmax: 2.385581 log_softmax: 2.381470\n",
            "Train Epoch: 63 [600/1000 (60%)]\tLosses F.softmax: 2.232056 log_softmax: 2.234393\n",
            "Train Epoch: 63 [800/1000 (80%)]\tLosses F.softmax: 2.275898 log_softmax: 2.271596\n",
            "Train Epoch: 63 [1000/1000 (100%)]\tLosses F.softmax: 2.325091 log_softmax: 2.300876\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2456\tAccuracy: 2199.0/10000 (22%)\n",
            "log_softmax: Loss: 2.2337\tAccuracy: 2532.0/10000 (25%)\n",
            "\n",
            "Train Epoch: 64 [0/1000 (0%)]\tLosses F.softmax: 2.426167 log_softmax: 2.416583\n",
            "Train Epoch: 64 [200/1000 (20%)]\tLosses F.softmax: 2.418331 log_softmax: 2.422864\n",
            "Train Epoch: 64 [400/1000 (40%)]\tLosses F.softmax: 2.317252 log_softmax: 2.314277\n",
            "Train Epoch: 64 [600/1000 (60%)]\tLosses F.softmax: 2.262439 log_softmax: 2.250877\n",
            "Train Epoch: 64 [800/1000 (80%)]\tLosses F.softmax: 2.278711 log_softmax: 2.253479\n",
            "Train Epoch: 64 [1000/1000 (100%)]\tLosses F.softmax: 2.199049 log_softmax: 2.155016\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2428\tAccuracy: 2245.0/10000 (22%)\n",
            "log_softmax: Loss: 2.2303\tAccuracy: 2578.0/10000 (26%)\n",
            "\n",
            "Train Epoch: 65 [0/1000 (0%)]\tLosses F.softmax: 2.337969 log_softmax: 2.349744\n",
            "Train Epoch: 65 [200/1000 (20%)]\tLosses F.softmax: 2.253141 log_softmax: 2.199330\n",
            "Train Epoch: 65 [400/1000 (40%)]\tLosses F.softmax: 2.330009 log_softmax: 2.301451\n",
            "Train Epoch: 65 [600/1000 (60%)]\tLosses F.softmax: 2.294744 log_softmax: 2.300698\n",
            "Train Epoch: 65 [800/1000 (80%)]\tLosses F.softmax: 2.198233 log_softmax: 2.214415\n",
            "Train Epoch: 65 [1000/1000 (100%)]\tLosses F.softmax: 2.293547 log_softmax: 2.233894\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2399\tAccuracy: 2276.0/10000 (23%)\n",
            "log_softmax: Loss: 2.2267\tAccuracy: 2610.0/10000 (26%)\n",
            "\n",
            "Train Epoch: 66 [0/1000 (0%)]\tLosses F.softmax: 2.350616 log_softmax: 2.377487\n",
            "Train Epoch: 66 [200/1000 (20%)]\tLosses F.softmax: 1.975537 log_softmax: 2.024645\n",
            "Train Epoch: 66 [400/1000 (40%)]\tLosses F.softmax: 2.204954 log_softmax: 2.212856\n",
            "Train Epoch: 66 [600/1000 (60%)]\tLosses F.softmax: 2.232513 log_softmax: 2.239758\n",
            "Train Epoch: 66 [800/1000 (80%)]\tLosses F.softmax: 2.307026 log_softmax: 2.290060\n",
            "Train Epoch: 66 [1000/1000 (100%)]\tLosses F.softmax: 1.917568 log_softmax: 1.945889\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2368\tAccuracy: 2331.0/10000 (23%)\n",
            "log_softmax: Loss: 2.2229\tAccuracy: 2651.0/10000 (27%)\n",
            "\n",
            "Train Epoch: 67 [0/1000 (0%)]\tLosses F.softmax: 2.283852 log_softmax: 2.244917\n",
            "Train Epoch: 67 [200/1000 (20%)]\tLosses F.softmax: 2.339404 log_softmax: 2.350006\n",
            "Train Epoch: 67 [400/1000 (40%)]\tLosses F.softmax: 2.356712 log_softmax: 2.359006\n",
            "Train Epoch: 67 [600/1000 (60%)]\tLosses F.softmax: 1.932790 log_softmax: 1.978950\n",
            "Train Epoch: 67 [800/1000 (80%)]\tLosses F.softmax: 2.376028 log_softmax: 2.381138\n",
            "Train Epoch: 67 [1000/1000 (100%)]\tLosses F.softmax: 2.281090 log_softmax: 2.161155\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2335\tAccuracy: 2371.0/10000 (24%)\n",
            "log_softmax: Loss: 2.2188\tAccuracy: 2682.0/10000 (27%)\n",
            "\n",
            "Train Epoch: 68 [0/1000 (0%)]\tLosses F.softmax: 2.037985 log_softmax: 2.008512\n",
            "Train Epoch: 68 [200/1000 (20%)]\tLosses F.softmax: 2.056964 log_softmax: 2.043308\n",
            "Train Epoch: 68 [400/1000 (40%)]\tLosses F.softmax: 1.996340 log_softmax: 2.047477\n",
            "Train Epoch: 68 [600/1000 (60%)]\tLosses F.softmax: 2.254053 log_softmax: 2.222912\n",
            "Train Epoch: 68 [800/1000 (80%)]\tLosses F.softmax: 2.298702 log_softmax: 2.291816\n",
            "Train Epoch: 68 [1000/1000 (100%)]\tLosses F.softmax: 2.426600 log_softmax: 2.405832\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2300\tAccuracy: 2423.0/10000 (24%)\n",
            "log_softmax: Loss: 2.2145\tAccuracy: 2710.0/10000 (27%)\n",
            "\n",
            "Train Epoch: 69 [0/1000 (0%)]\tLosses F.softmax: 2.301006 log_softmax: 2.302192\n",
            "Train Epoch: 69 [200/1000 (20%)]\tLosses F.softmax: 2.024018 log_softmax: 2.002071\n",
            "Train Epoch: 69 [400/1000 (40%)]\tLosses F.softmax: 2.244848 log_softmax: 2.213925\n",
            "Train Epoch: 69 [600/1000 (60%)]\tLosses F.softmax: 2.038700 log_softmax: 2.038214\n",
            "Train Epoch: 69 [800/1000 (80%)]\tLosses F.softmax: 2.348463 log_softmax: 2.345084\n",
            "Train Epoch: 69 [1000/1000 (100%)]\tLosses F.softmax: 2.356531 log_softmax: 2.369924\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2263\tAccuracy: 2467.0/10000 (25%)\n",
            "log_softmax: Loss: 2.2099\tAccuracy: 2737.0/10000 (27%)\n",
            "\n",
            "Train Epoch: 70 [0/1000 (0%)]\tLosses F.softmax: 2.214677 log_softmax: 2.238419\n",
            "Train Epoch: 70 [200/1000 (20%)]\tLosses F.softmax: 2.237227 log_softmax: 2.129090\n",
            "Train Epoch: 70 [400/1000 (40%)]\tLosses F.softmax: 2.076517 log_softmax: 2.092967\n",
            "Train Epoch: 70 [600/1000 (60%)]\tLosses F.softmax: 2.255244 log_softmax: 2.261050\n",
            "Train Epoch: 70 [800/1000 (80%)]\tLosses F.softmax: 2.193269 log_softmax: 2.100716\n",
            "Train Epoch: 70 [1000/1000 (100%)]\tLosses F.softmax: 2.166520 log_softmax: 2.121669\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2223\tAccuracy: 2516.0/10000 (25%)\n",
            "log_softmax: Loss: 2.2050\tAccuracy: 2774.0/10000 (28%)\n",
            "\n",
            "Train Epoch: 71 [0/1000 (0%)]\tLosses F.softmax: 2.428395 log_softmax: 2.446755\n",
            "Train Epoch: 71 [200/1000 (20%)]\tLosses F.softmax: 2.229709 log_softmax: 2.201531\n",
            "Train Epoch: 71 [400/1000 (40%)]\tLosses F.softmax: 2.031675 log_softmax: 2.056434\n",
            "Train Epoch: 71 [600/1000 (60%)]\tLosses F.softmax: 2.261900 log_softmax: 2.235194\n",
            "Train Epoch: 71 [800/1000 (80%)]\tLosses F.softmax: 2.337539 log_softmax: 2.303601\n",
            "Train Epoch: 71 [1000/1000 (100%)]\tLosses F.softmax: 2.358716 log_softmax: 2.304768\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2181\tAccuracy: 2557.0/10000 (26%)\n",
            "log_softmax: Loss: 2.1997\tAccuracy: 2811.0/10000 (28%)\n",
            "\n",
            "Train Epoch: 72 [0/1000 (0%)]\tLosses F.softmax: 2.282759 log_softmax: 2.223448\n",
            "Train Epoch: 72 [200/1000 (20%)]\tLosses F.softmax: 1.893302 log_softmax: 1.939308\n",
            "Train Epoch: 72 [400/1000 (40%)]\tLosses F.softmax: 1.984123 log_softmax: 1.945612\n",
            "Train Epoch: 72 [600/1000 (60%)]\tLosses F.softmax: 2.194935 log_softmax: 2.206230\n",
            "Train Epoch: 72 [800/1000 (80%)]\tLosses F.softmax: 2.431592 log_softmax: 2.452938\n",
            "Train Epoch: 72 [1000/1000 (100%)]\tLosses F.softmax: 1.874504 log_softmax: 1.881287\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2136\tAccuracy: 2593.0/10000 (26%)\n",
            "log_softmax: Loss: 2.1941\tAccuracy: 2856.0/10000 (29%)\n",
            "\n",
            "Train Epoch: 73 [0/1000 (0%)]\tLosses F.softmax: 2.138342 log_softmax: 2.085149\n",
            "Train Epoch: 73 [200/1000 (20%)]\tLosses F.softmax: 2.204872 log_softmax: 2.064851\n",
            "Train Epoch: 73 [400/1000 (40%)]\tLosses F.softmax: 2.288100 log_softmax: 2.282650\n",
            "Train Epoch: 73 [600/1000 (60%)]\tLosses F.softmax: 2.381887 log_softmax: 2.380556\n",
            "Train Epoch: 73 [800/1000 (80%)]\tLosses F.softmax: 1.970653 log_softmax: 1.946344\n",
            "Train Epoch: 73 [1000/1000 (100%)]\tLosses F.softmax: 2.358463 log_softmax: 2.382402\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2088\tAccuracy: 2650.0/10000 (26%)\n",
            "log_softmax: Loss: 2.1882\tAccuracy: 2894.0/10000 (29%)\n",
            "\n",
            "Train Epoch: 74 [0/1000 (0%)]\tLosses F.softmax: 2.385841 log_softmax: 2.404381\n",
            "Train Epoch: 74 [200/1000 (20%)]\tLosses F.softmax: 2.436285 log_softmax: 2.320965\n",
            "Train Epoch: 74 [400/1000 (40%)]\tLosses F.softmax: 2.037144 log_softmax: 2.008205\n",
            "Train Epoch: 74 [600/1000 (60%)]\tLosses F.softmax: 2.390774 log_softmax: 2.374674\n",
            "Train Epoch: 74 [800/1000 (80%)]\tLosses F.softmax: 1.965388 log_softmax: 1.987580\n",
            "Train Epoch: 74 [1000/1000 (100%)]\tLosses F.softmax: 2.390867 log_softmax: 2.394384\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2037\tAccuracy: 2705.0/10000 (27%)\n",
            "log_softmax: Loss: 2.1819\tAccuracy: 2927.0/10000 (29%)\n",
            "\n",
            "Train Epoch: 75 [0/1000 (0%)]\tLosses F.softmax: 2.293632 log_softmax: 2.288045\n",
            "Train Epoch: 75 [200/1000 (20%)]\tLosses F.softmax: 2.390422 log_softmax: 2.392080\n",
            "Train Epoch: 75 [400/1000 (40%)]\tLosses F.softmax: 2.386689 log_softmax: 2.366684\n",
            "Train Epoch: 75 [600/1000 (60%)]\tLosses F.softmax: 2.347479 log_softmax: 2.351681\n",
            "Train Epoch: 75 [800/1000 (80%)]\tLosses F.softmax: 2.137559 log_softmax: 2.075022\n",
            "Train Epoch: 75 [1000/1000 (100%)]\tLosses F.softmax: 2.437491 log_softmax: 2.462787\n",
            "Test set:\n",
            "F.softmax: Loss: 2.1982\tAccuracy: 2752.0/10000 (28%)\n",
            "log_softmax: Loss: 2.1751\tAccuracy: 2952.0/10000 (30%)\n",
            "\n",
            "Train Epoch: 76 [0/1000 (0%)]\tLosses F.softmax: 1.806523 log_softmax: 1.818054\n",
            "Train Epoch: 76 [200/1000 (20%)]\tLosses F.softmax: 1.762883 log_softmax: 1.785748\n",
            "Train Epoch: 76 [400/1000 (40%)]\tLosses F.softmax: 2.289725 log_softmax: 2.302465\n",
            "Train Epoch: 76 [600/1000 (60%)]\tLosses F.softmax: 1.922117 log_softmax: 1.938417\n",
            "Train Epoch: 76 [800/1000 (80%)]\tLosses F.softmax: 2.182422 log_softmax: 2.186235\n",
            "Train Epoch: 76 [1000/1000 (100%)]\tLosses F.softmax: 2.366290 log_softmax: 2.392021\n",
            "Test set:\n",
            "F.softmax: Loss: 2.1924\tAccuracy: 2796.0/10000 (28%)\n",
            "log_softmax: Loss: 2.1679\tAccuracy: 2982.0/10000 (30%)\n",
            "\n",
            "Train Epoch: 77 [0/1000 (0%)]\tLosses F.softmax: 2.316280 log_softmax: 2.270376\n",
            "Train Epoch: 77 [200/1000 (20%)]\tLosses F.softmax: 2.381628 log_softmax: 2.367674\n",
            "Train Epoch: 77 [400/1000 (40%)]\tLosses F.softmax: 2.156801 log_softmax: 1.987227\n",
            "Train Epoch: 77 [600/1000 (60%)]\tLosses F.softmax: 2.176511 log_softmax: 2.114869\n",
            "Train Epoch: 77 [800/1000 (80%)]\tLosses F.softmax: 1.895281 log_softmax: 1.926646\n",
            "Train Epoch: 77 [1000/1000 (100%)]\tLosses F.softmax: 2.274840 log_softmax: 2.275239\n",
            "Test set:\n",
            "F.softmax: Loss: 2.1861\tAccuracy: 2839.0/10000 (28%)\n",
            "log_softmax: Loss: 2.1602\tAccuracy: 3021.0/10000 (30%)\n",
            "\n",
            "Train Epoch: 78 [0/1000 (0%)]\tLosses F.softmax: 2.384774 log_softmax: 2.370539\n",
            "Train Epoch: 78 [200/1000 (20%)]\tLosses F.softmax: 2.309944 log_softmax: 2.306601\n",
            "Train Epoch: 78 [400/1000 (40%)]\tLosses F.softmax: 2.092744 log_softmax: 2.025850\n",
            "Train Epoch: 78 [600/1000 (60%)]\tLosses F.softmax: 2.089161 log_softmax: 2.021067\n",
            "Train Epoch: 78 [800/1000 (80%)]\tLosses F.softmax: 2.231107 log_softmax: 2.265353\n",
            "Train Epoch: 78 [1000/1000 (100%)]\tLosses F.softmax: 2.321140 log_softmax: 2.321701\n",
            "Test set:\n",
            "F.softmax: Loss: 2.1795\tAccuracy: 2873.0/10000 (29%)\n",
            "log_softmax: Loss: 2.1519\tAccuracy: 3060.0/10000 (31%)\n",
            "\n",
            "Train Epoch: 79 [0/1000 (0%)]\tLosses F.softmax: 1.920517 log_softmax: 1.906418\n",
            "Train Epoch: 79 [200/1000 (20%)]\tLosses F.softmax: 2.281801 log_softmax: 2.154330\n",
            "Train Epoch: 79 [400/1000 (40%)]\tLosses F.softmax: 2.254679 log_softmax: 2.138674\n",
            "Train Epoch: 79 [600/1000 (60%)]\tLosses F.softmax: 1.877976 log_softmax: 1.885394\n",
            "Train Epoch: 79 [800/1000 (80%)]\tLosses F.softmax: 2.420464 log_softmax: 2.391910\n",
            "Train Epoch: 79 [1000/1000 (100%)]\tLosses F.softmax: 1.766534 log_softmax: 1.820777\n",
            "Test set:\n",
            "F.softmax: Loss: 2.1723\tAccuracy: 2907.0/10000 (29%)\n",
            "log_softmax: Loss: 2.1431\tAccuracy: 3093.0/10000 (31%)\n",
            "\n",
            "Train Epoch: 80 [0/1000 (0%)]\tLosses F.softmax: 2.118160 log_softmax: 1.999919\n",
            "Train Epoch: 80 [200/1000 (20%)]\tLosses F.softmax: 2.236760 log_softmax: 2.201861\n",
            "Train Epoch: 80 [400/1000 (40%)]\tLosses F.softmax: 1.934919 log_softmax: 1.946117\n",
            "Train Epoch: 80 [600/1000 (60%)]\tLosses F.softmax: 2.087221 log_softmax: 1.997941\n",
            "Train Epoch: 80 [800/1000 (80%)]\tLosses F.softmax: 1.663395 log_softmax: 1.704467\n",
            "Train Epoch: 80 [1000/1000 (100%)]\tLosses F.softmax: 2.105151 log_softmax: 2.149331\n",
            "Test set:\n",
            "F.softmax: Loss: 2.1646\tAccuracy: 2929.0/10000 (29%)\n",
            "log_softmax: Loss: 2.1337\tAccuracy: 3115.0/10000 (31%)\n",
            "\n",
            "Train Epoch: 81 [0/1000 (0%)]\tLosses F.softmax: 2.174687 log_softmax: 2.198036\n",
            "Train Epoch: 81 [200/1000 (20%)]\tLosses F.softmax: 2.101570 log_softmax: 1.972563\n",
            "Train Epoch: 81 [400/1000 (40%)]\tLosses F.softmax: 2.198094 log_softmax: 2.158689\n",
            "Train Epoch: 81 [600/1000 (60%)]\tLosses F.softmax: 2.394891 log_softmax: 2.424802\n",
            "Train Epoch: 81 [800/1000 (80%)]\tLosses F.softmax: 2.215911 log_softmax: 2.201979\n",
            "Train Epoch: 81 [1000/1000 (100%)]\tLosses F.softmax: 2.312164 log_softmax: 2.247286\n",
            "Test set:\n",
            "F.softmax: Loss: 2.1565\tAccuracy: 2959.0/10000 (30%)\n",
            "log_softmax: Loss: 2.1236\tAccuracy: 3149.0/10000 (31%)\n",
            "\n",
            "Train Epoch: 82 [0/1000 (0%)]\tLosses F.softmax: 2.092015 log_softmax: 2.000450\n",
            "Train Epoch: 82 [200/1000 (20%)]\tLosses F.softmax: 2.081995 log_softmax: 1.865471\n",
            "Train Epoch: 82 [400/1000 (40%)]\tLosses F.softmax: 2.394720 log_softmax: 2.429517\n",
            "Train Epoch: 82 [600/1000 (60%)]\tLosses F.softmax: 1.688745 log_softmax: 1.705818\n",
            "Train Epoch: 82 [800/1000 (80%)]\tLosses F.softmax: 2.052199 log_softmax: 1.814415\n",
            "Train Epoch: 82 [1000/1000 (100%)]\tLosses F.softmax: 2.128086 log_softmax: 2.198663\n",
            "Test set:\n",
            "F.softmax: Loss: 2.1476\tAccuracy: 3009.0/10000 (30%)\n",
            "log_softmax: Loss: 2.1128\tAccuracy: 3176.0/10000 (32%)\n",
            "\n",
            "Train Epoch: 83 [0/1000 (0%)]\tLosses F.softmax: 1.864618 log_softmax: 1.850705\n",
            "Train Epoch: 83 [200/1000 (20%)]\tLosses F.softmax: 2.309003 log_softmax: 2.309944\n",
            "Train Epoch: 83 [400/1000 (40%)]\tLosses F.softmax: 2.219692 log_softmax: 2.013976\n",
            "Train Epoch: 83 [600/1000 (60%)]\tLosses F.softmax: 2.281795 log_softmax: 2.226624\n",
            "Train Epoch: 83 [800/1000 (80%)]\tLosses F.softmax: 2.227554 log_softmax: 2.096626\n",
            "Train Epoch: 83 [1000/1000 (100%)]\tLosses F.softmax: 1.630457 log_softmax: 1.662349\n",
            "Test set:\n",
            "F.softmax: Loss: 2.1382\tAccuracy: 3024.0/10000 (30%)\n",
            "log_softmax: Loss: 2.1013\tAccuracy: 3186.0/10000 (32%)\n",
            "\n",
            "Train Epoch: 84 [0/1000 (0%)]\tLosses F.softmax: 2.381457 log_softmax: 2.351563\n",
            "Train Epoch: 84 [200/1000 (20%)]\tLosses F.softmax: 2.330710 log_softmax: 2.337639\n",
            "Train Epoch: 84 [400/1000 (40%)]\tLosses F.softmax: 1.822927 log_softmax: 1.803175\n",
            "Train Epoch: 84 [600/1000 (60%)]\tLosses F.softmax: 2.273846 log_softmax: 2.275128\n",
            "Train Epoch: 84 [800/1000 (80%)]\tLosses F.softmax: 2.385442 log_softmax: 2.175766\n",
            "Train Epoch: 84 [1000/1000 (100%)]\tLosses F.softmax: 2.257506 log_softmax: 2.244056\n",
            "Test set:\n",
            "F.softmax: Loss: 2.1281\tAccuracy: 3055.0/10000 (31%)\n",
            "log_softmax: Loss: 2.0889\tAccuracy: 3211.0/10000 (32%)\n",
            "\n",
            "Train Epoch: 85 [0/1000 (0%)]\tLosses F.softmax: 1.888044 log_softmax: 1.888590\n",
            "Train Epoch: 85 [200/1000 (20%)]\tLosses F.softmax: 2.283599 log_softmax: 2.254162\n",
            "Train Epoch: 85 [400/1000 (40%)]\tLosses F.softmax: 2.149560 log_softmax: 2.172255\n",
            "Train Epoch: 85 [600/1000 (60%)]\tLosses F.softmax: 2.093925 log_softmax: 2.100303\n",
            "Train Epoch: 85 [800/1000 (80%)]\tLosses F.softmax: 2.316428 log_softmax: 2.240701\n",
            "Train Epoch: 85 [1000/1000 (100%)]\tLosses F.softmax: 1.499762 log_softmax: 1.533469\n",
            "Test set:\n",
            "F.softmax: Loss: 2.1174\tAccuracy: 3089.0/10000 (31%)\n",
            "log_softmax: Loss: 2.0758\tAccuracy: 3257.0/10000 (33%)\n",
            "\n",
            "Train Epoch: 86 [0/1000 (0%)]\tLosses F.softmax: 1.481467 log_softmax: 1.516676\n",
            "Train Epoch: 86 [200/1000 (20%)]\tLosses F.softmax: 2.161711 log_softmax: 2.150674\n",
            "Train Epoch: 86 [400/1000 (40%)]\tLosses F.softmax: 1.982229 log_softmax: 1.720223\n",
            "Train Epoch: 86 [600/1000 (60%)]\tLosses F.softmax: 2.190585 log_softmax: 2.040104\n",
            "Train Epoch: 86 [800/1000 (80%)]\tLosses F.softmax: 2.105859 log_softmax: 2.033806\n",
            "Train Epoch: 86 [1000/1000 (100%)]\tLosses F.softmax: 2.297905 log_softmax: 2.321131\n",
            "Test set:\n",
            "F.softmax: Loss: 2.1057\tAccuracy: 3116.0/10000 (31%)\n",
            "log_softmax: Loss: 2.0617\tAccuracy: 3292.0/10000 (33%)\n",
            "\n",
            "Train Epoch: 87 [0/1000 (0%)]\tLosses F.softmax: 2.362939 log_softmax: 2.367624\n",
            "Train Epoch: 87 [200/1000 (20%)]\tLosses F.softmax: 1.722291 log_softmax: 1.686206\n",
            "Train Epoch: 87 [400/1000 (40%)]\tLosses F.softmax: 1.742807 log_softmax: 1.728547\n",
            "Train Epoch: 87 [600/1000 (60%)]\tLosses F.softmax: 2.303779 log_softmax: 2.258426\n",
            "Train Epoch: 87 [800/1000 (80%)]\tLosses F.softmax: 1.538142 log_softmax: 1.484332\n",
            "Train Epoch: 87 [1000/1000 (100%)]\tLosses F.softmax: 2.435361 log_softmax: 2.448038\n",
            "Test set:\n",
            "F.softmax: Loss: 2.0933\tAccuracy: 3147.0/10000 (31%)\n",
            "log_softmax: Loss: 2.0466\tAccuracy: 3304.0/10000 (33%)\n",
            "\n",
            "Train Epoch: 88 [0/1000 (0%)]\tLosses F.softmax: 1.978127 log_softmax: 1.705496\n",
            "Train Epoch: 88 [200/1000 (20%)]\tLosses F.softmax: 2.263921 log_softmax: 2.118410\n",
            "Train Epoch: 88 [400/1000 (40%)]\tLosses F.softmax: 2.333982 log_softmax: 2.285833\n",
            "Train Epoch: 88 [600/1000 (60%)]\tLosses F.softmax: 2.248889 log_softmax: 2.126220\n",
            "Train Epoch: 88 [800/1000 (80%)]\tLosses F.softmax: 1.747437 log_softmax: 1.714415\n",
            "Train Epoch: 88 [1000/1000 (100%)]\tLosses F.softmax: 1.634604 log_softmax: 1.590953\n",
            "Test set:\n",
            "F.softmax: Loss: 2.0800\tAccuracy: 3186.0/10000 (32%)\n",
            "log_softmax: Loss: 2.0306\tAccuracy: 3348.0/10000 (33%)\n",
            "\n",
            "Train Epoch: 89 [0/1000 (0%)]\tLosses F.softmax: 1.962842 log_softmax: 2.003840\n",
            "Train Epoch: 89 [200/1000 (20%)]\tLosses F.softmax: 2.279238 log_softmax: 2.180724\n",
            "Train Epoch: 89 [400/1000 (40%)]\tLosses F.softmax: 1.632985 log_softmax: 1.583030\n",
            "Train Epoch: 89 [600/1000 (60%)]\tLosses F.softmax: 2.083097 log_softmax: 1.986287\n",
            "Train Epoch: 89 [800/1000 (80%)]\tLosses F.softmax: 2.342924 log_softmax: 2.369924\n",
            "Train Epoch: 89 [1000/1000 (100%)]\tLosses F.softmax: 1.954494 log_softmax: 1.805714\n",
            "Test set:\n",
            "F.softmax: Loss: 2.0657\tAccuracy: 3220.0/10000 (32%)\n",
            "log_softmax: Loss: 2.0136\tAccuracy: 3385.0/10000 (34%)\n",
            "\n",
            "Train Epoch: 90 [0/1000 (0%)]\tLosses F.softmax: 2.433234 log_softmax: 2.403799\n",
            "Train Epoch: 90 [200/1000 (20%)]\tLosses F.softmax: 2.011433 log_softmax: 2.129227\n",
            "Train Epoch: 90 [400/1000 (40%)]\tLosses F.softmax: 2.038188 log_softmax: 2.110041\n",
            "Train Epoch: 90 [600/1000 (60%)]\tLosses F.softmax: 2.119391 log_softmax: 2.143176\n",
            "Train Epoch: 90 [800/1000 (80%)]\tLosses F.softmax: 1.586794 log_softmax: 1.554240\n",
            "Train Epoch: 90 [1000/1000 (100%)]\tLosses F.softmax: 2.231756 log_softmax: 2.086441\n",
            "Test set:\n",
            "F.softmax: Loss: 2.0506\tAccuracy: 3260.0/10000 (33%)\n",
            "log_softmax: Loss: 1.9957\tAccuracy: 3436.0/10000 (34%)\n",
            "\n",
            "Train Epoch: 91 [0/1000 (0%)]\tLosses F.softmax: 1.305850 log_softmax: 1.284174\n",
            "Train Epoch: 91 [200/1000 (20%)]\tLosses F.softmax: 2.124357 log_softmax: 2.143909\n",
            "Train Epoch: 91 [400/1000 (40%)]\tLosses F.softmax: 2.301513 log_softmax: 2.176357\n",
            "Train Epoch: 91 [600/1000 (60%)]\tLosses F.softmax: 2.519532 log_softmax: 2.520413\n",
            "Train Epoch: 91 [800/1000 (80%)]\tLosses F.softmax: 2.127339 log_softmax: 2.077101\n",
            "Train Epoch: 91 [1000/1000 (100%)]\tLosses F.softmax: 2.156526 log_softmax: 2.157903\n",
            "Test set:\n",
            "F.softmax: Loss: 2.0347\tAccuracy: 3285.0/10000 (33%)\n",
            "log_softmax: Loss: 1.9770\tAccuracy: 3457.0/10000 (35%)\n",
            "\n",
            "Train Epoch: 92 [0/1000 (0%)]\tLosses F.softmax: 1.632275 log_softmax: 1.675734\n",
            "Train Epoch: 92 [200/1000 (20%)]\tLosses F.softmax: 1.242191 log_softmax: 1.231908\n",
            "Train Epoch: 92 [400/1000 (40%)]\tLosses F.softmax: 2.303426 log_softmax: 2.211692\n",
            "Train Epoch: 92 [600/1000 (60%)]\tLosses F.softmax: 2.267699 log_softmax: 2.084328\n",
            "Train Epoch: 92 [800/1000 (80%)]\tLosses F.softmax: 2.318138 log_softmax: 2.293550\n",
            "Train Epoch: 92 [1000/1000 (100%)]\tLosses F.softmax: 2.297083 log_softmax: 2.274004\n",
            "Test set:\n",
            "F.softmax: Loss: 2.0173\tAccuracy: 3309.0/10000 (33%)\n",
            "log_softmax: Loss: 1.9566\tAccuracy: 3480.0/10000 (35%)\n",
            "\n",
            "Train Epoch: 93 [0/1000 (0%)]\tLosses F.softmax: 2.014560 log_softmax: 1.735983\n",
            "Train Epoch: 93 [200/1000 (20%)]\tLosses F.softmax: 1.958361 log_softmax: 2.010063\n",
            "Train Epoch: 93 [400/1000 (40%)]\tLosses F.softmax: 1.604783 log_softmax: 1.567573\n",
            "Train Epoch: 93 [600/1000 (60%)]\tLosses F.softmax: 1.183883 log_softmax: 1.227366\n",
            "Train Epoch: 93 [800/1000 (80%)]\tLosses F.softmax: 1.284128 log_softmax: 1.216993\n",
            "Train Epoch: 93 [1000/1000 (100%)]\tLosses F.softmax: 1.879046 log_softmax: 1.521577\n",
            "Test set:\n",
            "F.softmax: Loss: 1.9991\tAccuracy: 3342.0/10000 (33%)\n",
            "log_softmax: Loss: 1.9355\tAccuracy: 3530.0/10000 (35%)\n",
            "\n",
            "Train Epoch: 94 [0/1000 (0%)]\tLosses F.softmax: 2.090248 log_softmax: 2.024563\n",
            "Train Epoch: 94 [200/1000 (20%)]\tLosses F.softmax: 1.757311 log_softmax: 1.404460\n",
            "Train Epoch: 94 [400/1000 (40%)]\tLosses F.softmax: 2.354898 log_softmax: 2.352982\n",
            "Train Epoch: 94 [600/1000 (60%)]\tLosses F.softmax: 2.223245 log_softmax: 2.068720\n",
            "Train Epoch: 94 [800/1000 (80%)]\tLosses F.softmax: 2.245868 log_softmax: 2.035180\n",
            "Train Epoch: 94 [1000/1000 (100%)]\tLosses F.softmax: 1.719195 log_softmax: 1.334679\n",
            "Test set:\n",
            "F.softmax: Loss: 1.9800\tAccuracy: 3344.0/10000 (33%)\n",
            "log_softmax: Loss: 1.9136\tAccuracy: 3569.0/10000 (36%)\n",
            "\n",
            "Train Epoch: 95 [0/1000 (0%)]\tLosses F.softmax: 1.785847 log_softmax: 1.798590\n",
            "Train Epoch: 95 [200/1000 (20%)]\tLosses F.softmax: 1.825053 log_softmax: 1.710456\n",
            "Train Epoch: 95 [400/1000 (40%)]\tLosses F.softmax: 2.353016 log_softmax: 2.451076\n",
            "Train Epoch: 95 [600/1000 (60%)]\tLosses F.softmax: 1.415807 log_softmax: 1.401672\n",
            "Train Epoch: 95 [800/1000 (80%)]\tLosses F.softmax: 1.914061 log_softmax: 1.756245\n",
            "Train Epoch: 95 [1000/1000 (100%)]\tLosses F.softmax: 2.250807 log_softmax: 2.312967\n",
            "Test set:\n",
            "F.softmax: Loss: 1.9599\tAccuracy: 3398.0/10000 (34%)\n",
            "log_softmax: Loss: 1.8908\tAccuracy: 3622.0/10000 (36%)\n",
            "\n",
            "Train Epoch: 96 [0/1000 (0%)]\tLosses F.softmax: 1.709784 log_softmax: 1.311010\n",
            "Train Epoch: 96 [200/1000 (20%)]\tLosses F.softmax: 1.535081 log_softmax: 1.470266\n",
            "Train Epoch: 96 [400/1000 (40%)]\tLosses F.softmax: 1.364864 log_softmax: 1.328933\n",
            "Train Epoch: 96 [600/1000 (60%)]\tLosses F.softmax: 2.124993 log_softmax: 1.955255\n",
            "Train Epoch: 96 [800/1000 (80%)]\tLosses F.softmax: 1.437420 log_softmax: 1.384539\n",
            "Train Epoch: 96 [1000/1000 (100%)]\tLosses F.softmax: 2.058053 log_softmax: 1.828478\n",
            "Test set:\n",
            "F.softmax: Loss: 1.9385\tAccuracy: 3437.0/10000 (34%)\n",
            "log_softmax: Loss: 1.8667\tAccuracy: 3687.0/10000 (37%)\n",
            "\n",
            "Train Epoch: 97 [0/1000 (0%)]\tLosses F.softmax: 2.431745 log_softmax: 2.464146\n",
            "Train Epoch: 97 [200/1000 (20%)]\tLosses F.softmax: 1.369888 log_softmax: 1.338729\n",
            "Train Epoch: 97 [400/1000 (40%)]\tLosses F.softmax: 1.604102 log_softmax: 1.613253\n",
            "Train Epoch: 97 [600/1000 (60%)]\tLosses F.softmax: 2.375868 log_softmax: 2.351080\n",
            "Train Epoch: 97 [800/1000 (80%)]\tLosses F.softmax: 2.402354 log_softmax: 2.353493\n",
            "Train Epoch: 97 [1000/1000 (100%)]\tLosses F.softmax: 1.602042 log_softmax: 1.586389\n",
            "Test set:\n",
            "F.softmax: Loss: 1.9165\tAccuracy: 3483.0/10000 (35%)\n",
            "log_softmax: Loss: 1.8422\tAccuracy: 3763.0/10000 (38%)\n",
            "\n",
            "Train Epoch: 98 [0/1000 (0%)]\tLosses F.softmax: 2.700876 log_softmax: 2.909238\n",
            "Train Epoch: 98 [200/1000 (20%)]\tLosses F.softmax: 1.808753 log_softmax: 1.679687\n",
            "Train Epoch: 98 [400/1000 (40%)]\tLosses F.softmax: 1.679991 log_softmax: 1.503966\n",
            "Train Epoch: 98 [600/1000 (60%)]\tLosses F.softmax: 2.347959 log_softmax: 2.227154\n",
            "Train Epoch: 98 [800/1000 (80%)]\tLosses F.softmax: 1.720240 log_softmax: 1.541431\n",
            "Train Epoch: 98 [1000/1000 (100%)]\tLosses F.softmax: 1.887145 log_softmax: 1.860021\n",
            "Test set:\n",
            "F.softmax: Loss: 1.8938\tAccuracy: 3500.0/10000 (35%)\n",
            "log_softmax: Loss: 1.8171\tAccuracy: 3828.0/10000 (38%)\n",
            "\n",
            "Train Epoch: 99 [0/1000 (0%)]\tLosses F.softmax: 1.980903 log_softmax: 1.983454\n",
            "Train Epoch: 99 [200/1000 (20%)]\tLosses F.softmax: 2.219854 log_softmax: 2.121480\n",
            "Train Epoch: 99 [400/1000 (40%)]\tLosses F.softmax: 2.252556 log_softmax: 2.138873\n",
            "Train Epoch: 99 [600/1000 (60%)]\tLosses F.softmax: 2.063362 log_softmax: 1.768582\n",
            "Train Epoch: 99 [800/1000 (80%)]\tLosses F.softmax: 2.303279 log_softmax: 2.273771\n",
            "Train Epoch: 99 [1000/1000 (100%)]\tLosses F.softmax: 1.704854 log_softmax: 1.588544\n",
            "Test set:\n",
            "F.softmax: Loss: 1.8700\tAccuracy: 3570.0/10000 (36%)\n",
            "log_softmax: Loss: 1.7913\tAccuracy: 3915.0/10000 (39%)\n",
            "\n",
            "Train Epoch: 100 [0/1000 (0%)]\tLosses F.softmax: 2.028052 log_softmax: 1.717278\n",
            "Train Epoch: 100 [200/1000 (20%)]\tLosses F.softmax: 1.263780 log_softmax: 1.251423\n",
            "Train Epoch: 100 [400/1000 (40%)]\tLosses F.softmax: 1.618692 log_softmax: 1.466392\n",
            "Train Epoch: 100 [600/1000 (60%)]\tLosses F.softmax: 1.466021 log_softmax: 0.998541\n",
            "Train Epoch: 100 [800/1000 (80%)]\tLosses F.softmax: 2.041174 log_softmax: 1.742352\n",
            "Train Epoch: 100 [1000/1000 (100%)]\tLosses F.softmax: 2.234311 log_softmax: 2.133552\n",
            "Test set:\n",
            "F.softmax: Loss: 1.8456\tAccuracy: 3605.0/10000 (36%)\n",
            "log_softmax: Loss: 1.7648\tAccuracy: 4021.0/10000 (40%)\n",
            "\n",
            "Train Epoch: 101 [0/1000 (0%)]\tLosses F.softmax: 0.711212 log_softmax: 0.767269\n",
            "Train Epoch: 101 [200/1000 (20%)]\tLosses F.softmax: 0.776802 log_softmax: 0.794421\n",
            "Train Epoch: 101 [400/1000 (40%)]\tLosses F.softmax: 2.359605 log_softmax: 2.349426\n",
            "Train Epoch: 101 [600/1000 (60%)]\tLosses F.softmax: 2.223947 log_softmax: 2.153131\n",
            "Train Epoch: 101 [800/1000 (80%)]\tLosses F.softmax: 1.807577 log_softmax: 1.629995\n",
            "Train Epoch: 101 [1000/1000 (100%)]\tLosses F.softmax: 2.358255 log_softmax: 2.279057\n",
            "Test set:\n",
            "F.softmax: Loss: 1.8212\tAccuracy: 3634.0/10000 (36%)\n",
            "log_softmax: Loss: 1.7384\tAccuracy: 4091.0/10000 (41%)\n",
            "\n",
            "Train Epoch: 102 [0/1000 (0%)]\tLosses F.softmax: 1.848882 log_softmax: 1.435278\n",
            "Train Epoch: 102 [200/1000 (20%)]\tLosses F.softmax: 2.202921 log_softmax: 2.122399\n",
            "Train Epoch: 102 [400/1000 (40%)]\tLosses F.softmax: 1.844127 log_softmax: 1.887212\n",
            "Train Epoch: 102 [600/1000 (60%)]\tLosses F.softmax: 1.194597 log_softmax: 1.137708\n",
            "Train Epoch: 102 [800/1000 (80%)]\tLosses F.softmax: 1.885858 log_softmax: 1.894093\n",
            "Train Epoch: 102 [1000/1000 (100%)]\tLosses F.softmax: 1.534670 log_softmax: 1.375968\n",
            "Test set:\n",
            "F.softmax: Loss: 1.7960\tAccuracy: 3693.0/10000 (37%)\n",
            "log_softmax: Loss: 1.7111\tAccuracy: 4219.0/10000 (42%)\n",
            "\n",
            "Train Epoch: 103 [0/1000 (0%)]\tLosses F.softmax: 1.977754 log_softmax: 2.015753\n",
            "Train Epoch: 103 [200/1000 (20%)]\tLosses F.softmax: 2.062838 log_softmax: 1.823775\n",
            "Train Epoch: 103 [400/1000 (40%)]\tLosses F.softmax: 1.718591 log_softmax: 1.690838\n",
            "Train Epoch: 103 [600/1000 (60%)]\tLosses F.softmax: 2.112760 log_softmax: 1.945372\n",
            "Train Epoch: 103 [800/1000 (80%)]\tLosses F.softmax: 1.865608 log_softmax: 1.745446\n",
            "Train Epoch: 103 [1000/1000 (100%)]\tLosses F.softmax: 1.948236 log_softmax: 1.867255\n",
            "Test set:\n",
            "F.softmax: Loss: 1.7708\tAccuracy: 3735.0/10000 (37%)\n",
            "log_softmax: Loss: 1.6841\tAccuracy: 4297.0/10000 (43%)\n",
            "\n",
            "Train Epoch: 104 [0/1000 (0%)]\tLosses F.softmax: 1.918267 log_softmax: 1.871149\n",
            "Train Epoch: 104 [200/1000 (20%)]\tLosses F.softmax: 1.782163 log_softmax: 1.824744\n",
            "Train Epoch: 104 [400/1000 (40%)]\tLosses F.softmax: 2.257783 log_softmax: 2.240929\n",
            "Train Epoch: 104 [600/1000 (60%)]\tLosses F.softmax: 1.084829 log_softmax: 1.106084\n",
            "Train Epoch: 104 [800/1000 (80%)]\tLosses F.softmax: 2.348728 log_softmax: 2.354852\n",
            "Train Epoch: 104 [1000/1000 (100%)]\tLosses F.softmax: 2.135399 log_softmax: 2.221336\n",
            "Test set:\n",
            "F.softmax: Loss: 1.7465\tAccuracy: 3789.0/10000 (38%)\n",
            "log_softmax: Loss: 1.6579\tAccuracy: 4383.0/10000 (44%)\n",
            "\n",
            "Train Epoch: 105 [0/1000 (0%)]\tLosses F.softmax: 2.155880 log_softmax: 2.127849\n",
            "Train Epoch: 105 [200/1000 (20%)]\tLosses F.softmax: 1.960291 log_softmax: 1.791129\n",
            "Train Epoch: 105 [400/1000 (40%)]\tLosses F.softmax: 1.769141 log_softmax: 1.724943\n",
            "Train Epoch: 105 [600/1000 (60%)]\tLosses F.softmax: 1.898501 log_softmax: 1.734696\n",
            "Train Epoch: 105 [800/1000 (80%)]\tLosses F.softmax: 1.017447 log_softmax: 0.955118\n",
            "Train Epoch: 105 [1000/1000 (100%)]\tLosses F.softmax: 1.903312 log_softmax: 1.512824\n",
            "Test set:\n",
            "F.softmax: Loss: 1.7215\tAccuracy: 3865.0/10000 (39%)\n",
            "log_softmax: Loss: 1.6311\tAccuracy: 4489.0/10000 (45%)\n",
            "\n",
            "Train Epoch: 106 [0/1000 (0%)]\tLosses F.softmax: 1.433214 log_softmax: 1.207747\n",
            "Train Epoch: 106 [200/1000 (20%)]\tLosses F.softmax: 0.970755 log_softmax: 0.972232\n",
            "Train Epoch: 106 [400/1000 (40%)]\tLosses F.softmax: 2.199734 log_softmax: 1.981243\n",
            "Train Epoch: 106 [600/1000 (60%)]\tLosses F.softmax: 1.611246 log_softmax: 1.621097\n",
            "Train Epoch: 106 [800/1000 (80%)]\tLosses F.softmax: 0.669961 log_softmax: 0.659939\n",
            "Train Epoch: 106 [1000/1000 (100%)]\tLosses F.softmax: 1.340959 log_softmax: 1.144015\n",
            "Test set:\n",
            "F.softmax: Loss: 1.6971\tAccuracy: 3948.0/10000 (39%)\n",
            "log_softmax: Loss: 1.6045\tAccuracy: 4593.0/10000 (46%)\n",
            "\n",
            "Train Epoch: 107 [0/1000 (0%)]\tLosses F.softmax: 1.919889 log_softmax: 1.723413\n",
            "Train Epoch: 107 [200/1000 (20%)]\tLosses F.softmax: 0.649791 log_softmax: 0.734389\n",
            "Train Epoch: 107 [400/1000 (40%)]\tLosses F.softmax: 1.952377 log_softmax: 1.540876\n",
            "Train Epoch: 107 [600/1000 (60%)]\tLosses F.softmax: 0.935127 log_softmax: 0.930760\n",
            "Train Epoch: 107 [800/1000 (80%)]\tLosses F.softmax: 1.085262 log_softmax: 0.708746\n",
            "Train Epoch: 107 [1000/1000 (100%)]\tLosses F.softmax: 1.720618 log_softmax: 1.480851\n",
            "Test set:\n",
            "F.softmax: Loss: 1.6725\tAccuracy: 3993.0/10000 (40%)\n",
            "log_softmax: Loss: 1.5776\tAccuracy: 4681.0/10000 (47%)\n",
            "\n",
            "Train Epoch: 108 [0/1000 (0%)]\tLosses F.softmax: 1.097575 log_softmax: 1.098928\n",
            "Train Epoch: 108 [200/1000 (20%)]\tLosses F.softmax: 1.831030 log_softmax: 1.592798\n",
            "Train Epoch: 108 [400/1000 (40%)]\tLosses F.softmax: 1.409304 log_softmax: 1.230443\n",
            "Train Epoch: 108 [600/1000 (60%)]\tLosses F.softmax: 2.267446 log_softmax: 2.090617\n",
            "Train Epoch: 108 [800/1000 (80%)]\tLosses F.softmax: 1.905631 log_softmax: 1.802377\n",
            "Train Epoch: 108 [1000/1000 (100%)]\tLosses F.softmax: 0.854073 log_softmax: 0.891509\n",
            "Test set:\n",
            "F.softmax: Loss: 1.6487\tAccuracy: 4085.0/10000 (41%)\n",
            "log_softmax: Loss: 1.5516\tAccuracy: 4789.0/10000 (48%)\n",
            "\n",
            "Train Epoch: 109 [0/1000 (0%)]\tLosses F.softmax: 2.260503 log_softmax: 2.271617\n",
            "Train Epoch: 109 [200/1000 (20%)]\tLosses F.softmax: 2.174579 log_softmax: 2.073818\n",
            "Train Epoch: 109 [400/1000 (40%)]\tLosses F.softmax: 1.560032 log_softmax: 1.315250\n",
            "Train Epoch: 109 [600/1000 (60%)]\tLosses F.softmax: 1.048127 log_softmax: 0.699855\n",
            "Train Epoch: 109 [800/1000 (80%)]\tLosses F.softmax: 2.113263 log_softmax: 1.820306\n",
            "Train Epoch: 109 [1000/1000 (100%)]\tLosses F.softmax: 1.852497 log_softmax: 1.682632\n",
            "Test set:\n",
            "F.softmax: Loss: 1.6250\tAccuracy: 4187.0/10000 (42%)\n",
            "log_softmax: Loss: 1.5258\tAccuracy: 4892.0/10000 (49%)\n",
            "\n",
            "Train Epoch: 110 [0/1000 (0%)]\tLosses F.softmax: 2.302883 log_softmax: 2.234656\n",
            "Train Epoch: 110 [200/1000 (20%)]\tLosses F.softmax: 1.233223 log_softmax: 1.182217\n",
            "Train Epoch: 110 [400/1000 (40%)]\tLosses F.softmax: 0.432090 log_softmax: 0.472557\n",
            "Train Epoch: 110 [600/1000 (60%)]\tLosses F.softmax: 1.379473 log_softmax: 1.441615\n",
            "Train Epoch: 110 [800/1000 (80%)]\tLosses F.softmax: 2.350868 log_softmax: 2.177697\n",
            "Train Epoch: 110 [1000/1000 (100%)]\tLosses F.softmax: 0.949306 log_softmax: 0.974964\n",
            "Test set:\n",
            "F.softmax: Loss: 1.6037\tAccuracy: 4296.0/10000 (43%)\n",
            "log_softmax: Loss: 1.5026\tAccuracy: 5032.0/10000 (50%)\n",
            "\n",
            "Train Epoch: 111 [0/1000 (0%)]\tLosses F.softmax: 0.815324 log_softmax: 0.877750\n",
            "Train Epoch: 111 [200/1000 (20%)]\tLosses F.softmax: 1.235628 log_softmax: 1.268695\n",
            "Train Epoch: 111 [400/1000 (40%)]\tLosses F.softmax: 1.649259 log_softmax: 1.531242\n",
            "Train Epoch: 111 [600/1000 (60%)]\tLosses F.softmax: 1.982140 log_softmax: 1.869804\n",
            "Train Epoch: 111 [800/1000 (80%)]\tLosses F.softmax: 1.627145 log_softmax: 1.361290\n",
            "Train Epoch: 111 [1000/1000 (100%)]\tLosses F.softmax: 2.101181 log_softmax: 2.008792\n",
            "Test set:\n",
            "F.softmax: Loss: 1.5815\tAccuracy: 4391.0/10000 (44%)\n",
            "log_softmax: Loss: 1.4778\tAccuracy: 5103.0/10000 (51%)\n",
            "\n",
            "Train Epoch: 112 [0/1000 (0%)]\tLosses F.softmax: 0.425756 log_softmax: 0.539144\n",
            "Train Epoch: 112 [200/1000 (20%)]\tLosses F.softmax: 0.739656 log_softmax: 0.770383\n",
            "Train Epoch: 112 [400/1000 (40%)]\tLosses F.softmax: 1.734546 log_softmax: 1.286299\n",
            "Train Epoch: 112 [600/1000 (60%)]\tLosses F.softmax: 1.771383 log_softmax: 1.515303\n",
            "Train Epoch: 112 [800/1000 (80%)]\tLosses F.softmax: 1.335300 log_softmax: 1.367758\n",
            "Train Epoch: 112 [1000/1000 (100%)]\tLosses F.softmax: 1.858158 log_softmax: 1.640835\n",
            "Test set:\n",
            "F.softmax: Loss: 1.5607\tAccuracy: 4484.0/10000 (45%)\n",
            "log_softmax: Loss: 1.4549\tAccuracy: 5213.0/10000 (52%)\n",
            "\n",
            "Train Epoch: 113 [0/1000 (0%)]\tLosses F.softmax: 3.167831 log_softmax: 2.886193\n",
            "Train Epoch: 113 [200/1000 (20%)]\tLosses F.softmax: 2.050640 log_softmax: 2.063388\n",
            "Train Epoch: 113 [400/1000 (40%)]\tLosses F.softmax: 1.553716 log_softmax: 1.413900\n",
            "Train Epoch: 113 [600/1000 (60%)]\tLosses F.softmax: 0.465054 log_softmax: 0.465018\n",
            "Train Epoch: 113 [800/1000 (80%)]\tLosses F.softmax: 1.678202 log_softmax: 1.702878\n",
            "Train Epoch: 113 [1000/1000 (100%)]\tLosses F.softmax: 2.367065 log_softmax: 2.445731\n",
            "Test set:\n",
            "F.softmax: Loss: 1.5399\tAccuracy: 4592.0/10000 (46%)\n",
            "log_softmax: Loss: 1.4318\tAccuracy: 5310.0/10000 (53%)\n",
            "\n",
            "Train Epoch: 114 [0/1000 (0%)]\tLosses F.softmax: 2.036857 log_softmax: 1.872323\n",
            "Train Epoch: 114 [200/1000 (20%)]\tLosses F.softmax: 0.950908 log_softmax: 0.605751\n",
            "Train Epoch: 114 [400/1000 (40%)]\tLosses F.softmax: 1.178628 log_softmax: 1.057961\n",
            "Train Epoch: 114 [600/1000 (60%)]\tLosses F.softmax: 2.080872 log_softmax: 1.965577\n",
            "Train Epoch: 114 [800/1000 (80%)]\tLosses F.softmax: 2.556917 log_softmax: 2.647019\n",
            "Train Epoch: 114 [1000/1000 (100%)]\tLosses F.softmax: 1.803718 log_softmax: 1.429268\n",
            "Test set:\n",
            "F.softmax: Loss: 1.5210\tAccuracy: 4687.0/10000 (47%)\n",
            "log_softmax: Loss: 1.4112\tAccuracy: 5382.0/10000 (54%)\n",
            "\n",
            "Train Epoch: 115 [0/1000 (0%)]\tLosses F.softmax: 1.810376 log_softmax: 1.524482\n",
            "Train Epoch: 115 [200/1000 (20%)]\tLosses F.softmax: 1.775171 log_softmax: 1.286913\n",
            "Train Epoch: 115 [400/1000 (40%)]\tLosses F.softmax: 1.826336 log_softmax: 1.582316\n",
            "Train Epoch: 115 [600/1000 (60%)]\tLosses F.softmax: 0.826750 log_softmax: 0.887759\n",
            "Train Epoch: 115 [800/1000 (80%)]\tLosses F.softmax: 0.384280 log_softmax: 0.467180\n",
            "Train Epoch: 115 [1000/1000 (100%)]\tLosses F.softmax: 2.131022 log_softmax: 2.145879\n",
            "Test set:\n",
            "F.softmax: Loss: 1.5017\tAccuracy: 4791.0/10000 (48%)\n",
            "log_softmax: Loss: 1.3896\tAccuracy: 5448.0/10000 (54%)\n",
            "\n",
            "Train Epoch: 116 [0/1000 (0%)]\tLosses F.softmax: 1.890398 log_softmax: 1.771920\n",
            "Train Epoch: 116 [200/1000 (20%)]\tLosses F.softmax: 2.490375 log_softmax: 2.249669\n",
            "Train Epoch: 116 [400/1000 (40%)]\tLosses F.softmax: 0.331881 log_softmax: 0.464134\n",
            "Train Epoch: 116 [600/1000 (60%)]\tLosses F.softmax: 1.304541 log_softmax: 1.282850\n",
            "Train Epoch: 116 [800/1000 (80%)]\tLosses F.softmax: 1.298967 log_softmax: 1.283555\n",
            "Train Epoch: 116 [1000/1000 (100%)]\tLosses F.softmax: 1.582432 log_softmax: 1.090250\n",
            "Test set:\n",
            "F.softmax: Loss: 1.4827\tAccuracy: 4886.0/10000 (49%)\n",
            "log_softmax: Loss: 1.3686\tAccuracy: 5519.0/10000 (55%)\n",
            "\n",
            "Train Epoch: 117 [0/1000 (0%)]\tLosses F.softmax: 1.700814 log_softmax: 1.460134\n",
            "Train Epoch: 117 [200/1000 (20%)]\tLosses F.softmax: 2.239367 log_softmax: 2.199075\n",
            "Train Epoch: 117 [400/1000 (40%)]\tLosses F.softmax: 1.206122 log_softmax: 0.957205\n",
            "Train Epoch: 117 [600/1000 (60%)]\tLosses F.softmax: 0.927548 log_softmax: 0.810320\n",
            "Train Epoch: 117 [800/1000 (80%)]\tLosses F.softmax: 2.023270 log_softmax: 1.832901\n",
            "Train Epoch: 117 [1000/1000 (100%)]\tLosses F.softmax: 0.880560 log_softmax: 0.592631\n",
            "Test set:\n",
            "F.softmax: Loss: 1.4660\tAccuracy: 4917.0/10000 (49%)\n",
            "log_softmax: Loss: 1.3505\tAccuracy: 5539.0/10000 (55%)\n",
            "\n",
            "Train Epoch: 118 [0/1000 (0%)]\tLosses F.softmax: 2.128957 log_softmax: 2.031023\n",
            "Train Epoch: 118 [200/1000 (20%)]\tLosses F.softmax: 0.439999 log_softmax: 0.475206\n",
            "Train Epoch: 118 [400/1000 (40%)]\tLosses F.softmax: 1.734965 log_softmax: 1.440522\n",
            "Train Epoch: 118 [600/1000 (60%)]\tLosses F.softmax: 1.375529 log_softmax: 0.866268\n",
            "Train Epoch: 118 [800/1000 (80%)]\tLosses F.softmax: 1.554170 log_softmax: 1.441897\n",
            "Train Epoch: 118 [1000/1000 (100%)]\tLosses F.softmax: 0.398909 log_softmax: 0.467452\n",
            "Test set:\n",
            "F.softmax: Loss: 1.4475\tAccuracy: 4975.0/10000 (50%)\n",
            "log_softmax: Loss: 1.3306\tAccuracy: 5592.0/10000 (56%)\n",
            "\n",
            "Train Epoch: 119 [0/1000 (0%)]\tLosses F.softmax: 0.884825 log_softmax: 0.647126\n",
            "Train Epoch: 119 [200/1000 (20%)]\tLosses F.softmax: 1.384683 log_softmax: 1.190902\n",
            "Train Epoch: 119 [400/1000 (40%)]\tLosses F.softmax: 0.280124 log_softmax: 0.343997\n",
            "Train Epoch: 119 [600/1000 (60%)]\tLosses F.softmax: 1.369517 log_softmax: 1.509748\n",
            "Train Epoch: 119 [800/1000 (80%)]\tLosses F.softmax: 1.168018 log_softmax: 0.980975\n",
            "Train Epoch: 119 [1000/1000 (100%)]\tLosses F.softmax: 2.017484 log_softmax: 1.975325\n",
            "Test set:\n",
            "F.softmax: Loss: 1.4310\tAccuracy: 5043.0/10000 (50%)\n",
            "log_softmax: Loss: 1.3130\tAccuracy: 5667.0/10000 (57%)\n",
            "\n",
            "Train Epoch: 120 [0/1000 (0%)]\tLosses F.softmax: 1.987398 log_softmax: 1.939756\n",
            "Train Epoch: 120 [200/1000 (20%)]\tLosses F.softmax: 1.095663 log_softmax: 0.850207\n",
            "Train Epoch: 120 [400/1000 (40%)]\tLosses F.softmax: 0.653490 log_softmax: 0.647352\n",
            "Train Epoch: 120 [600/1000 (60%)]\tLosses F.softmax: 1.335925 log_softmax: 1.433291\n",
            "Train Epoch: 120 [800/1000 (80%)]\tLosses F.softmax: 0.721469 log_softmax: 0.440012\n",
            "Train Epoch: 120 [1000/1000 (100%)]\tLosses F.softmax: 1.418594 log_softmax: 1.486018\n",
            "Test set:\n",
            "F.softmax: Loss: 1.4143\tAccuracy: 5091.0/10000 (51%)\n",
            "log_softmax: Loss: 1.2943\tAccuracy: 5720.0/10000 (57%)\n",
            "\n",
            "Train Epoch: 121 [0/1000 (0%)]\tLosses F.softmax: 1.831641 log_softmax: 1.659631\n",
            "Train Epoch: 121 [200/1000 (20%)]\tLosses F.softmax: 1.055077 log_softmax: 0.785584\n",
            "Train Epoch: 121 [400/1000 (40%)]\tLosses F.softmax: 1.130748 log_softmax: 0.847889\n",
            "Train Epoch: 121 [600/1000 (60%)]\tLosses F.softmax: 1.688214 log_softmax: 1.531047\n",
            "Train Epoch: 121 [800/1000 (80%)]\tLosses F.softmax: 0.651317 log_softmax: 0.637962\n",
            "Train Epoch: 121 [1000/1000 (100%)]\tLosses F.softmax: 0.927338 log_softmax: 0.903033\n",
            "Test set:\n",
            "F.softmax: Loss: 1.3984\tAccuracy: 5128.0/10000 (51%)\n",
            "log_softmax: Loss: 1.2772\tAccuracy: 5743.0/10000 (57%)\n",
            "\n",
            "Train Epoch: 122 [0/1000 (0%)]\tLosses F.softmax: 0.529587 log_softmax: 0.544186\n",
            "Train Epoch: 122 [200/1000 (20%)]\tLosses F.softmax: 0.701229 log_softmax: 0.399474\n",
            "Train Epoch: 122 [400/1000 (40%)]\tLosses F.softmax: 1.566687 log_softmax: 1.306002\n",
            "Train Epoch: 122 [600/1000 (60%)]\tLosses F.softmax: 0.923157 log_softmax: 1.022040\n",
            "Train Epoch: 122 [800/1000 (80%)]\tLosses F.softmax: 1.299667 log_softmax: 1.190189\n",
            "Train Epoch: 122 [1000/1000 (100%)]\tLosses F.softmax: 1.569120 log_softmax: 1.458063\n",
            "Test set:\n",
            "F.softmax: Loss: 1.3825\tAccuracy: 5197.0/10000 (52%)\n",
            "log_softmax: Loss: 1.2604\tAccuracy: 5793.0/10000 (58%)\n",
            "\n",
            "Train Epoch: 123 [0/1000 (0%)]\tLosses F.softmax: 0.290563 log_softmax: 0.414477\n",
            "Train Epoch: 123 [200/1000 (20%)]\tLosses F.softmax: 1.726314 log_softmax: 1.630761\n",
            "Train Epoch: 123 [400/1000 (40%)]\tLosses F.softmax: 2.033550 log_softmax: 1.902578\n",
            "Train Epoch: 123 [600/1000 (60%)]\tLosses F.softmax: 2.134011 log_softmax: 1.816525\n",
            "Train Epoch: 123 [800/1000 (80%)]\tLosses F.softmax: 1.268390 log_softmax: 1.051163\n",
            "Train Epoch: 123 [1000/1000 (100%)]\tLosses F.softmax: 0.323828 log_softmax: 0.344433\n",
            "Test set:\n",
            "F.softmax: Loss: 1.3682\tAccuracy: 5266.0/10000 (53%)\n",
            "log_softmax: Loss: 1.2451\tAccuracy: 5885.0/10000 (59%)\n",
            "\n",
            "Train Epoch: 124 [0/1000 (0%)]\tLosses F.softmax: 1.220629 log_softmax: 1.279053\n",
            "Train Epoch: 124 [200/1000 (20%)]\tLosses F.softmax: 1.264698 log_softmax: 0.826561\n",
            "Train Epoch: 124 [400/1000 (40%)]\tLosses F.softmax: 1.887193 log_softmax: 1.698515\n",
            "Train Epoch: 124 [600/1000 (60%)]\tLosses F.softmax: 1.173743 log_softmax: 1.151444\n",
            "Train Epoch: 124 [800/1000 (80%)]\tLosses F.softmax: 1.427128 log_softmax: 1.423494\n",
            "Train Epoch: 124 [1000/1000 (100%)]\tLosses F.softmax: 1.485454 log_softmax: 1.033986\n",
            "Test set:\n",
            "F.softmax: Loss: 1.3560\tAccuracy: 5285.0/10000 (53%)\n",
            "log_softmax: Loss: 1.2318\tAccuracy: 5883.0/10000 (59%)\n",
            "\n",
            "Train Epoch: 125 [0/1000 (0%)]\tLosses F.softmax: 0.504979 log_softmax: 0.505856\n",
            "Train Epoch: 125 [200/1000 (20%)]\tLosses F.softmax: 1.263487 log_softmax: 1.153524\n",
            "Train Epoch: 125 [400/1000 (40%)]\tLosses F.softmax: 0.703642 log_softmax: 0.409996\n",
            "Train Epoch: 125 [600/1000 (60%)]\tLosses F.softmax: 1.156134 log_softmax: 1.105734\n",
            "Train Epoch: 125 [800/1000 (80%)]\tLosses F.softmax: 0.280629 log_softmax: 0.343155\n",
            "Train Epoch: 125 [1000/1000 (100%)]\tLosses F.softmax: 1.493522 log_softmax: 1.158185\n",
            "Test set:\n",
            "F.softmax: Loss: 1.3392\tAccuracy: 5361.0/10000 (54%)\n",
            "log_softmax: Loss: 1.2140\tAccuracy: 6017.0/10000 (60%)\n",
            "\n",
            "Train Epoch: 126 [0/1000 (0%)]\tLosses F.softmax: 1.089480 log_softmax: 0.854525\n",
            "Train Epoch: 126 [200/1000 (20%)]\tLosses F.softmax: 0.683406 log_softmax: 0.404769\n",
            "Train Epoch: 126 [400/1000 (40%)]\tLosses F.softmax: 1.073663 log_softmax: 0.713926\n",
            "Train Epoch: 126 [600/1000 (60%)]\tLosses F.softmax: 1.389255 log_softmax: 1.295370\n",
            "Train Epoch: 126 [800/1000 (80%)]\tLosses F.softmax: 0.624666 log_softmax: 0.583961\n",
            "Train Epoch: 126 [1000/1000 (100%)]\tLosses F.softmax: 1.457539 log_softmax: 1.183179\n",
            "Test set:\n",
            "F.softmax: Loss: 1.3249\tAccuracy: 5386.0/10000 (54%)\n",
            "log_softmax: Loss: 1.1992\tAccuracy: 5989.0/10000 (60%)\n",
            "\n",
            "Train Epoch: 127 [0/1000 (0%)]\tLosses F.softmax: 0.266083 log_softmax: 0.357103\n",
            "Train Epoch: 127 [200/1000 (20%)]\tLosses F.softmax: 1.802499 log_softmax: 1.754863\n",
            "Train Epoch: 127 [400/1000 (40%)]\tLosses F.softmax: 0.524944 log_softmax: 0.378803\n",
            "Train Epoch: 127 [600/1000 (60%)]\tLosses F.softmax: 0.806986 log_softmax: 0.592043\n",
            "Train Epoch: 127 [800/1000 (80%)]\tLosses F.softmax: 1.025941 log_softmax: 1.157526\n",
            "Train Epoch: 127 [1000/1000 (100%)]\tLosses F.softmax: 0.848029 log_softmax: 0.801068\n",
            "Test set:\n",
            "F.softmax: Loss: 1.3130\tAccuracy: 5446.0/10000 (54%)\n",
            "log_softmax: Loss: 1.1863\tAccuracy: 6101.0/10000 (61%)\n",
            "\n",
            "Train Epoch: 128 [0/1000 (0%)]\tLosses F.softmax: 1.669250 log_softmax: 1.449418\n",
            "Train Epoch: 128 [200/1000 (20%)]\tLosses F.softmax: 1.634916 log_softmax: 1.602258\n",
            "Train Epoch: 128 [400/1000 (40%)]\tLosses F.softmax: 1.793873 log_softmax: 1.728238\n",
            "Train Epoch: 128 [600/1000 (60%)]\tLosses F.softmax: 0.878867 log_softmax: 0.667889\n",
            "Train Epoch: 128 [800/1000 (80%)]\tLosses F.softmax: 0.603834 log_softmax: 0.387001\n",
            "Train Epoch: 128 [1000/1000 (100%)]\tLosses F.softmax: 0.532061 log_softmax: 0.448381\n",
            "Test set:\n",
            "F.softmax: Loss: 1.3000\tAccuracy: 5507.0/10000 (55%)\n",
            "log_softmax: Loss: 1.1718\tAccuracy: 6163.0/10000 (62%)\n",
            "\n",
            "Train Epoch: 129 [0/1000 (0%)]\tLosses F.softmax: 1.390350 log_softmax: 1.252726\n",
            "Train Epoch: 129 [200/1000 (20%)]\tLosses F.softmax: 1.474596 log_softmax: 1.015393\n",
            "Train Epoch: 129 [400/1000 (40%)]\tLosses F.softmax: 1.118297 log_softmax: 0.929337\n",
            "Train Epoch: 129 [600/1000 (60%)]\tLosses F.softmax: 0.563406 log_softmax: 0.364958\n",
            "Train Epoch: 129 [800/1000 (80%)]\tLosses F.softmax: 1.480491 log_softmax: 1.059422\n",
            "Train Epoch: 129 [1000/1000 (100%)]\tLosses F.softmax: 1.090042 log_softmax: 0.753261\n",
            "Test set:\n",
            "F.softmax: Loss: 1.2893\tAccuracy: 5519.0/10000 (55%)\n",
            "log_softmax: Loss: 1.1605\tAccuracy: 6177.0/10000 (62%)\n",
            "\n",
            "Train Epoch: 130 [0/1000 (0%)]\tLosses F.softmax: 0.546322 log_softmax: 0.485673\n",
            "Train Epoch: 130 [200/1000 (20%)]\tLosses F.softmax: 0.820246 log_softmax: 0.799134\n",
            "Train Epoch: 130 [400/1000 (40%)]\tLosses F.softmax: 1.219867 log_softmax: 1.101759\n",
            "Train Epoch: 130 [600/1000 (60%)]\tLosses F.softmax: 1.479280 log_softmax: 1.051084\n",
            "Train Epoch: 130 [800/1000 (80%)]\tLosses F.softmax: 1.094767 log_softmax: 1.115583\n",
            "Train Epoch: 130 [1000/1000 (100%)]\tLosses F.softmax: 1.768274 log_softmax: 1.678970\n",
            "Test set:\n",
            "F.softmax: Loss: 1.2777\tAccuracy: 5546.0/10000 (55%)\n",
            "log_softmax: Loss: 1.1476\tAccuracy: 6194.0/10000 (62%)\n",
            "\n",
            "Train Epoch: 131 [0/1000 (0%)]\tLosses F.softmax: 1.412707 log_softmax: 1.465390\n",
            "Train Epoch: 131 [200/1000 (20%)]\tLosses F.softmax: 0.876273 log_softmax: 0.475219\n",
            "Train Epoch: 131 [400/1000 (40%)]\tLosses F.softmax: 0.494863 log_softmax: 0.458815\n",
            "Train Epoch: 131 [600/1000 (60%)]\tLosses F.softmax: 0.646346 log_softmax: 0.411546\n",
            "Train Epoch: 131 [800/1000 (80%)]\tLosses F.softmax: 1.585991 log_softmax: 1.388948\n",
            "Train Epoch: 131 [1000/1000 (100%)]\tLosses F.softmax: 0.318333 log_softmax: 0.403483\n",
            "Test set:\n",
            "F.softmax: Loss: 1.2648\tAccuracy: 5618.0/10000 (56%)\n",
            "log_softmax: Loss: 1.1334\tAccuracy: 6271.0/10000 (63%)\n",
            "\n",
            "Train Epoch: 132 [0/1000 (0%)]\tLosses F.softmax: 1.181207 log_softmax: 1.177705\n",
            "Train Epoch: 132 [200/1000 (20%)]\tLosses F.softmax: 0.909837 log_softmax: 0.568156\n",
            "Train Epoch: 132 [400/1000 (40%)]\tLosses F.softmax: 1.248859 log_softmax: 1.182489\n",
            "Train Epoch: 132 [600/1000 (60%)]\tLosses F.softmax: 0.668938 log_softmax: 0.809457\n",
            "Train Epoch: 132 [800/1000 (80%)]\tLosses F.softmax: 0.492298 log_softmax: 0.304341\n",
            "Train Epoch: 132 [1000/1000 (100%)]\tLosses F.softmax: 1.758265 log_softmax: 1.965457\n",
            "Test set:\n",
            "F.softmax: Loss: 1.2525\tAccuracy: 5669.0/10000 (57%)\n",
            "log_softmax: Loss: 1.1192\tAccuracy: 6348.0/10000 (63%)\n",
            "\n",
            "Train Epoch: 133 [0/1000 (0%)]\tLosses F.softmax: 0.400896 log_softmax: 0.349296\n",
            "Train Epoch: 133 [200/1000 (20%)]\tLosses F.softmax: 0.575947 log_softmax: 0.496381\n",
            "Train Epoch: 133 [400/1000 (40%)]\tLosses F.softmax: 1.898797 log_softmax: 2.070918\n",
            "Train Epoch: 133 [600/1000 (60%)]\tLosses F.softmax: 1.240252 log_softmax: 0.924035\n",
            "Train Epoch: 133 [800/1000 (80%)]\tLosses F.softmax: 1.225384 log_softmax: 1.183586\n",
            "Train Epoch: 133 [1000/1000 (100%)]\tLosses F.softmax: 0.423577 log_softmax: 0.225583\n",
            "Test set:\n",
            "F.softmax: Loss: 1.2421\tAccuracy: 5727.0/10000 (57%)\n",
            "log_softmax: Loss: 1.1067\tAccuracy: 6416.0/10000 (64%)\n",
            "\n",
            "Train Epoch: 134 [0/1000 (0%)]\tLosses F.softmax: 1.510839 log_softmax: 1.380371\n",
            "Train Epoch: 134 [200/1000 (20%)]\tLosses F.softmax: 1.377509 log_softmax: 1.038024\n",
            "Train Epoch: 134 [400/1000 (40%)]\tLosses F.softmax: 0.458646 log_softmax: 0.441584\n",
            "Train Epoch: 134 [600/1000 (60%)]\tLosses F.softmax: 1.578365 log_softmax: 1.105698\n",
            "Train Epoch: 134 [800/1000 (80%)]\tLosses F.softmax: 1.053095 log_softmax: 0.973733\n",
            "Train Epoch: 134 [1000/1000 (100%)]\tLosses F.softmax: 0.786677 log_softmax: 1.012482\n",
            "Test set:\n",
            "F.softmax: Loss: 1.2310\tAccuracy: 5822.0/10000 (58%)\n",
            "log_softmax: Loss: 1.0940\tAccuracy: 6461.0/10000 (65%)\n",
            "\n",
            "Train Epoch: 135 [0/1000 (0%)]\tLosses F.softmax: 0.745136 log_softmax: 0.377822\n",
            "Train Epoch: 135 [200/1000 (20%)]\tLosses F.softmax: 0.215299 log_softmax: 0.173985\n",
            "Train Epoch: 135 [400/1000 (40%)]\tLosses F.softmax: 2.244351 log_softmax: 1.909277\n",
            "Train Epoch: 135 [600/1000 (60%)]\tLosses F.softmax: 1.358524 log_softmax: 1.389502\n",
            "Train Epoch: 135 [800/1000 (80%)]\tLosses F.softmax: 0.156291 log_softmax: 0.197900\n",
            "Train Epoch: 135 [1000/1000 (100%)]\tLosses F.softmax: 0.409047 log_softmax: 0.434233\n",
            "Test set:\n",
            "F.softmax: Loss: 1.2181\tAccuracy: 5872.0/10000 (59%)\n",
            "log_softmax: Loss: 1.0799\tAccuracy: 6555.0/10000 (66%)\n",
            "\n",
            "Train Epoch: 136 [0/1000 (0%)]\tLosses F.softmax: 0.168170 log_softmax: 0.198724\n",
            "Train Epoch: 136 [200/1000 (20%)]\tLosses F.softmax: 0.604045 log_softmax: 0.362563\n",
            "Train Epoch: 136 [400/1000 (40%)]\tLosses F.softmax: 1.535678 log_softmax: 1.867764\n",
            "Train Epoch: 136 [600/1000 (60%)]\tLosses F.softmax: 1.275214 log_softmax: 1.195860\n",
            "Train Epoch: 136 [800/1000 (80%)]\tLosses F.softmax: 1.406394 log_softmax: 1.513400\n",
            "Train Epoch: 136 [1000/1000 (100%)]\tLosses F.softmax: 1.241160 log_softmax: 1.052348\n",
            "Test set:\n",
            "F.softmax: Loss: 1.2099\tAccuracy: 5862.0/10000 (59%)\n",
            "log_softmax: Loss: 1.0708\tAccuracy: 6507.0/10000 (65%)\n",
            "\n",
            "Train Epoch: 137 [0/1000 (0%)]\tLosses F.softmax: 0.341629 log_softmax: 0.215656\n",
            "Train Epoch: 137 [200/1000 (20%)]\tLosses F.softmax: 0.933883 log_softmax: 1.134347\n",
            "Train Epoch: 137 [400/1000 (40%)]\tLosses F.softmax: 1.224453 log_softmax: 0.975287\n",
            "Train Epoch: 137 [600/1000 (60%)]\tLosses F.softmax: 0.727004 log_softmax: 0.762935\n",
            "Train Epoch: 137 [800/1000 (80%)]\tLosses F.softmax: 0.586270 log_softmax: 0.440375\n",
            "Train Epoch: 137 [1000/1000 (100%)]\tLosses F.softmax: 1.871468 log_softmax: 2.215251\n",
            "Test set:\n",
            "F.softmax: Loss: 1.2005\tAccuracy: 5875.0/10000 (59%)\n",
            "log_softmax: Loss: 1.0587\tAccuracy: 6565.0/10000 (66%)\n",
            "\n",
            "Train Epoch: 138 [0/1000 (0%)]\tLosses F.softmax: 1.033680 log_softmax: 1.000154\n",
            "Train Epoch: 138 [200/1000 (20%)]\tLosses F.softmax: 1.109783 log_softmax: 1.001474\n",
            "Train Epoch: 138 [400/1000 (40%)]\tLosses F.softmax: 0.271410 log_softmax: 0.219804\n",
            "Train Epoch: 138 [600/1000 (60%)]\tLosses F.softmax: 0.925417 log_softmax: 0.875235\n",
            "Train Epoch: 138 [800/1000 (80%)]\tLosses F.softmax: 3.789215 log_softmax: 3.317767\n",
            "Train Epoch: 138 [1000/1000 (100%)]\tLosses F.softmax: 1.290617 log_softmax: 1.072105\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1905\tAccuracy: 5950.0/10000 (60%)\n",
            "log_softmax: Loss: 1.0467\tAccuracy: 6623.0/10000 (66%)\n",
            "\n",
            "Train Epoch: 139 [0/1000 (0%)]\tLosses F.softmax: 0.464667 log_softmax: 0.336685\n",
            "Train Epoch: 139 [200/1000 (20%)]\tLosses F.softmax: 1.789781 log_softmax: 2.174505\n",
            "Train Epoch: 139 [400/1000 (40%)]\tLosses F.softmax: 0.350721 log_softmax: 0.293531\n",
            "Train Epoch: 139 [600/1000 (60%)]\tLosses F.softmax: 1.425994 log_softmax: 0.913307\n",
            "Train Epoch: 139 [800/1000 (80%)]\tLosses F.softmax: 0.170500 log_softmax: 0.177347\n",
            "Train Epoch: 139 [1000/1000 (100%)]\tLosses F.softmax: 1.643554 log_softmax: 1.498292\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1805\tAccuracy: 5995.0/10000 (60%)\n",
            "log_softmax: Loss: 1.0352\tAccuracy: 6677.0/10000 (67%)\n",
            "\n",
            "Train Epoch: 140 [0/1000 (0%)]\tLosses F.softmax: 0.452205 log_softmax: 0.411081\n",
            "Train Epoch: 140 [200/1000 (20%)]\tLosses F.softmax: 1.184182 log_softmax: 1.010164\n",
            "Train Epoch: 140 [400/1000 (40%)]\tLosses F.softmax: 0.644144 log_softmax: 0.464615\n",
            "Train Epoch: 140 [600/1000 (60%)]\tLosses F.softmax: 0.228417 log_softmax: 0.290841\n",
            "Train Epoch: 140 [800/1000 (80%)]\tLosses F.softmax: 0.922346 log_softmax: 0.485989\n",
            "Train Epoch: 140 [1000/1000 (100%)]\tLosses F.softmax: 0.496831 log_softmax: 0.255442\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1726\tAccuracy: 6044.0/10000 (60%)\n",
            "log_softmax: Loss: 1.0252\tAccuracy: 6698.0/10000 (67%)\n",
            "\n",
            "Train Epoch: 141 [0/1000 (0%)]\tLosses F.softmax: 2.191046 log_softmax: 1.537556\n",
            "Train Epoch: 141 [200/1000 (20%)]\tLosses F.softmax: 0.992283 log_softmax: 1.072307\n",
            "Train Epoch: 141 [400/1000 (40%)]\tLosses F.softmax: 0.861620 log_softmax: 0.396826\n",
            "Train Epoch: 141 [600/1000 (60%)]\tLosses F.softmax: 0.610737 log_softmax: 0.434938\n",
            "Train Epoch: 141 [800/1000 (80%)]\tLosses F.softmax: 0.483924 log_softmax: 0.219003\n",
            "Train Epoch: 141 [1000/1000 (100%)]\tLosses F.softmax: 0.913873 log_softmax: 0.841822\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1663\tAccuracy: 6034.0/10000 (60%)\n",
            "log_softmax: Loss: 1.0169\tAccuracy: 6737.0/10000 (67%)\n",
            "\n",
            "Train Epoch: 142 [0/1000 (0%)]\tLosses F.softmax: 1.096640 log_softmax: 1.082910\n",
            "Train Epoch: 142 [200/1000 (20%)]\tLosses F.softmax: 1.032758 log_softmax: 0.904269\n",
            "Train Epoch: 142 [400/1000 (40%)]\tLosses F.softmax: 0.892526 log_softmax: 0.830894\n",
            "Train Epoch: 142 [600/1000 (60%)]\tLosses F.softmax: 0.950517 log_softmax: 0.673773\n",
            "Train Epoch: 142 [800/1000 (80%)]\tLosses F.softmax: 1.093013 log_softmax: 0.500149\n",
            "Train Epoch: 142 [1000/1000 (100%)]\tLosses F.softmax: 0.965514 log_softmax: 0.889763\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1541\tAccuracy: 6087.0/10000 (61%)\n",
            "log_softmax: Loss: 1.0026\tAccuracy: 6729.0/10000 (67%)\n",
            "\n",
            "Train Epoch: 143 [0/1000 (0%)]\tLosses F.softmax: 0.960963 log_softmax: 1.008467\n",
            "Train Epoch: 143 [200/1000 (20%)]\tLosses F.softmax: 0.254279 log_softmax: 0.241994\n",
            "Train Epoch: 143 [400/1000 (40%)]\tLosses F.softmax: 0.507254 log_softmax: 0.222451\n",
            "Train Epoch: 143 [600/1000 (60%)]\tLosses F.softmax: 0.556127 log_softmax: 0.245065\n",
            "Train Epoch: 143 [800/1000 (80%)]\tLosses F.softmax: 2.283094 log_softmax: 2.177789\n",
            "Train Epoch: 143 [1000/1000 (100%)]\tLosses F.softmax: 1.081843 log_softmax: 0.777566\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1472\tAccuracy: 6087.0/10000 (61%)\n",
            "log_softmax: Loss: 0.9929\tAccuracy: 6751.0/10000 (68%)\n",
            "\n",
            "Train Epoch: 144 [0/1000 (0%)]\tLosses F.softmax: 1.574438 log_softmax: 1.703471\n",
            "Train Epoch: 144 [200/1000 (20%)]\tLosses F.softmax: 0.362415 log_softmax: 0.234388\n",
            "Train Epoch: 144 [400/1000 (40%)]\tLosses F.softmax: 1.594447 log_softmax: 1.355730\n",
            "Train Epoch: 144 [600/1000 (60%)]\tLosses F.softmax: 1.188230 log_softmax: 0.417581\n",
            "Train Epoch: 144 [800/1000 (80%)]\tLosses F.softmax: 0.981860 log_softmax: 0.829204\n",
            "Train Epoch: 144 [1000/1000 (100%)]\tLosses F.softmax: 0.730699 log_softmax: 0.370284\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1399\tAccuracy: 6119.0/10000 (61%)\n",
            "log_softmax: Loss: 0.9844\tAccuracy: 6809.0/10000 (68%)\n",
            "\n",
            "Train Epoch: 145 [0/1000 (0%)]\tLosses F.softmax: 0.721143 log_softmax: 0.554935\n",
            "Train Epoch: 145 [200/1000 (20%)]\tLosses F.softmax: 0.430791 log_softmax: 0.493773\n",
            "Train Epoch: 145 [400/1000 (40%)]\tLosses F.softmax: 3.606955 log_softmax: 2.408201\n",
            "Train Epoch: 145 [600/1000 (60%)]\tLosses F.softmax: 0.425063 log_softmax: 0.163486\n",
            "Train Epoch: 145 [800/1000 (80%)]\tLosses F.softmax: 0.375058 log_softmax: 0.342762\n",
            "Train Epoch: 145 [1000/1000 (100%)]\tLosses F.softmax: 0.111721 log_softmax: 0.109162\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1305\tAccuracy: 6320.0/10000 (63%)\n",
            "log_softmax: Loss: 0.9727\tAccuracy: 6922.0/10000 (69%)\n",
            "\n",
            "Train Epoch: 146 [0/1000 (0%)]\tLosses F.softmax: 3.850960 log_softmax: 3.441035\n",
            "Train Epoch: 146 [200/1000 (20%)]\tLosses F.softmax: 1.008095 log_softmax: 0.891107\n",
            "Train Epoch: 146 [400/1000 (40%)]\tLosses F.softmax: 0.761319 log_softmax: 0.762635\n",
            "Train Epoch: 146 [600/1000 (60%)]\tLosses F.softmax: 2.079473 log_softmax: 1.732371\n",
            "Train Epoch: 146 [800/1000 (80%)]\tLosses F.softmax: 0.919378 log_softmax: 0.780692\n",
            "Train Epoch: 146 [1000/1000 (100%)]\tLosses F.softmax: 3.367784 log_softmax: 2.999572\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1242\tAccuracy: 6189.0/10000 (62%)\n",
            "log_softmax: Loss: 0.9628\tAccuracy: 6850.0/10000 (68%)\n",
            "\n",
            "Train Epoch: 147 [0/1000 (0%)]\tLosses F.softmax: 0.951732 log_softmax: 0.886921\n",
            "Train Epoch: 147 [200/1000 (20%)]\tLosses F.softmax: 0.995858 log_softmax: 0.375169\n",
            "Train Epoch: 147 [400/1000 (40%)]\tLosses F.softmax: 0.964839 log_softmax: 0.749123\n",
            "Train Epoch: 147 [600/1000 (60%)]\tLosses F.softmax: 0.386561 log_softmax: 0.131934\n",
            "Train Epoch: 147 [800/1000 (80%)]\tLosses F.softmax: 1.708288 log_softmax: 1.554698\n",
            "Train Epoch: 147 [1000/1000 (100%)]\tLosses F.softmax: 2.225723 log_softmax: 2.166552\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1170\tAccuracy: 6340.0/10000 (63%)\n",
            "log_softmax: Loss: 0.9560\tAccuracy: 6932.0/10000 (69%)\n",
            "\n",
            "Train Epoch: 148 [0/1000 (0%)]\tLosses F.softmax: 1.212174 log_softmax: 1.088751\n",
            "Train Epoch: 148 [200/1000 (20%)]\tLosses F.softmax: 0.272929 log_softmax: 0.200424\n",
            "Train Epoch: 148 [400/1000 (40%)]\tLosses F.softmax: 0.117706 log_softmax: 0.115823\n",
            "Train Epoch: 148 [600/1000 (60%)]\tLosses F.softmax: 0.334717 log_softmax: 0.261451\n",
            "Train Epoch: 148 [800/1000 (80%)]\tLosses F.softmax: 1.518494 log_softmax: 1.127854\n",
            "Train Epoch: 148 [1000/1000 (100%)]\tLosses F.softmax: 0.233006 log_softmax: 0.136161\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1088\tAccuracy: 6314.0/10000 (63%)\n",
            "log_softmax: Loss: 0.9444\tAccuracy: 6961.0/10000 (70%)\n",
            "\n",
            "Train Epoch: 149 [0/1000 (0%)]\tLosses F.softmax: 1.175668 log_softmax: 1.138947\n",
            "Train Epoch: 149 [200/1000 (20%)]\tLosses F.softmax: 1.153520 log_softmax: 0.534977\n",
            "Train Epoch: 149 [400/1000 (40%)]\tLosses F.softmax: 1.496345 log_softmax: 1.277625\n",
            "Train Epoch: 149 [600/1000 (60%)]\tLosses F.softmax: 1.967778 log_softmax: 1.950245\n",
            "Train Epoch: 149 [800/1000 (80%)]\tLosses F.softmax: 2.974954 log_softmax: 2.834641\n",
            "Train Epoch: 149 [1000/1000 (100%)]\tLosses F.softmax: 0.890180 log_softmax: 1.013918\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1010\tAccuracy: 6372.0/10000 (64%)\n",
            "log_softmax: Loss: 0.9358\tAccuracy: 6979.0/10000 (70%)\n",
            "\n",
            "Train Epoch: 150 [0/1000 (0%)]\tLosses F.softmax: 0.602555 log_softmax: 0.457054\n",
            "Train Epoch: 150 [200/1000 (20%)]\tLosses F.softmax: 3.214818 log_softmax: 2.021923\n",
            "Train Epoch: 150 [400/1000 (40%)]\tLosses F.softmax: 1.620385 log_softmax: 1.623721\n",
            "Train Epoch: 150 [600/1000 (60%)]\tLosses F.softmax: 0.672130 log_softmax: 0.589110\n",
            "Train Epoch: 150 [800/1000 (80%)]\tLosses F.softmax: 0.577105 log_softmax: 0.372101\n",
            "Train Epoch: 150 [1000/1000 (100%)]\tLosses F.softmax: 1.159170 log_softmax: 0.818093\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0939\tAccuracy: 6404.0/10000 (64%)\n",
            "log_softmax: Loss: 0.9257\tAccuracy: 7040.0/10000 (70%)\n",
            "\n",
            "Train Epoch: 151 [0/1000 (0%)]\tLosses F.softmax: 0.847302 log_softmax: 0.702133\n",
            "Train Epoch: 151 [200/1000 (20%)]\tLosses F.softmax: 1.813793 log_softmax: 1.322208\n",
            "Train Epoch: 151 [400/1000 (40%)]\tLosses F.softmax: 0.386792 log_softmax: 0.385038\n",
            "Train Epoch: 151 [600/1000 (60%)]\tLosses F.softmax: 1.477083 log_softmax: 1.250000\n",
            "Train Epoch: 151 [800/1000 (80%)]\tLosses F.softmax: 0.200341 log_softmax: 0.186616\n",
            "Train Epoch: 151 [1000/1000 (100%)]\tLosses F.softmax: 2.152610 log_softmax: 1.703694\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0851\tAccuracy: 6382.0/10000 (64%)\n",
            "log_softmax: Loss: 0.9169\tAccuracy: 7031.0/10000 (70%)\n",
            "\n",
            "Train Epoch: 152 [0/1000 (0%)]\tLosses F.softmax: 0.217712 log_softmax: 0.099509\n",
            "Train Epoch: 152 [200/1000 (20%)]\tLosses F.softmax: 0.160140 log_softmax: 0.054124\n",
            "Train Epoch: 152 [400/1000 (40%)]\tLosses F.softmax: 1.622070 log_softmax: 1.436187\n",
            "Train Epoch: 152 [600/1000 (60%)]\tLosses F.softmax: 1.084172 log_softmax: 0.303077\n",
            "Train Epoch: 152 [800/1000 (80%)]\tLosses F.softmax: 1.036259 log_softmax: 1.046505\n",
            "Train Epoch: 152 [1000/1000 (100%)]\tLosses F.softmax: 0.203550 log_softmax: 0.237892\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0819\tAccuracy: 6446.0/10000 (64%)\n",
            "log_softmax: Loss: 0.9106\tAccuracy: 7112.0/10000 (71%)\n",
            "\n",
            "Train Epoch: 153 [0/1000 (0%)]\tLosses F.softmax: 1.045295 log_softmax: 0.984286\n",
            "Train Epoch: 153 [200/1000 (20%)]\tLosses F.softmax: 0.173368 log_softmax: 0.125652\n",
            "Train Epoch: 153 [400/1000 (40%)]\tLosses F.softmax: 2.227281 log_softmax: 1.318246\n",
            "Train Epoch: 153 [600/1000 (60%)]\tLosses F.softmax: 0.699089 log_softmax: 0.654036\n",
            "Train Epoch: 153 [800/1000 (80%)]\tLosses F.softmax: 1.291438 log_softmax: 0.934053\n",
            "Train Epoch: 153 [1000/1000 (100%)]\tLosses F.softmax: 0.810352 log_softmax: 0.783490\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0726\tAccuracy: 6514.0/10000 (65%)\n",
            "log_softmax: Loss: 0.9004\tAccuracy: 7153.0/10000 (72%)\n",
            "\n",
            "Train Epoch: 154 [0/1000 (0%)]\tLosses F.softmax: 0.224885 log_softmax: 0.232504\n",
            "Train Epoch: 154 [200/1000 (20%)]\tLosses F.softmax: 0.672378 log_softmax: 0.636775\n",
            "Train Epoch: 154 [400/1000 (40%)]\tLosses F.softmax: 0.869811 log_softmax: 0.321079\n",
            "Train Epoch: 154 [600/1000 (60%)]\tLosses F.softmax: 0.161078 log_softmax: 0.069943\n",
            "Train Epoch: 154 [800/1000 (80%)]\tLosses F.softmax: 0.524906 log_softmax: 0.204535\n",
            "Train Epoch: 154 [1000/1000 (100%)]\tLosses F.softmax: 1.002549 log_softmax: 0.860825\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0667\tAccuracy: 6478.0/10000 (65%)\n",
            "log_softmax: Loss: 0.8932\tAccuracy: 7129.0/10000 (71%)\n",
            "\n",
            "Train Epoch: 155 [0/1000 (0%)]\tLosses F.softmax: 0.194167 log_softmax: 0.246801\n",
            "Train Epoch: 155 [200/1000 (20%)]\tLosses F.softmax: 0.198839 log_softmax: 0.111449\n",
            "Train Epoch: 155 [400/1000 (40%)]\tLosses F.softmax: 0.680335 log_softmax: 0.655704\n",
            "Train Epoch: 155 [600/1000 (60%)]\tLosses F.softmax: 2.210242 log_softmax: 2.365520\n",
            "Train Epoch: 155 [800/1000 (80%)]\tLosses F.softmax: 2.085236 log_softmax: 1.233339\n",
            "Train Epoch: 155 [1000/1000 (100%)]\tLosses F.softmax: 0.491570 log_softmax: 0.508042\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0582\tAccuracy: 6506.0/10000 (65%)\n",
            "log_softmax: Loss: 0.8833\tAccuracy: 7169.0/10000 (72%)\n",
            "\n",
            "Train Epoch: 156 [0/1000 (0%)]\tLosses F.softmax: 0.062899 log_softmax: 0.061244\n",
            "Train Epoch: 156 [200/1000 (20%)]\tLosses F.softmax: 1.365220 log_softmax: 1.296998\n",
            "Train Epoch: 156 [400/1000 (40%)]\tLosses F.softmax: 0.903417 log_softmax: 0.881710\n",
            "Train Epoch: 156 [600/1000 (60%)]\tLosses F.softmax: 1.232092 log_softmax: 0.979808\n",
            "Train Epoch: 156 [800/1000 (80%)]\tLosses F.softmax: 0.147910 log_softmax: 0.133447\n",
            "Train Epoch: 156 [1000/1000 (100%)]\tLosses F.softmax: 2.173498 log_softmax: 1.721462\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0535\tAccuracy: 6549.0/10000 (65%)\n",
            "log_softmax: Loss: 0.8782\tAccuracy: 7241.0/10000 (72%)\n",
            "\n",
            "Train Epoch: 157 [0/1000 (0%)]\tLosses F.softmax: 0.462153 log_softmax: 0.316543\n",
            "Train Epoch: 157 [200/1000 (20%)]\tLosses F.softmax: 0.565824 log_softmax: 0.357567\n",
            "Train Epoch: 157 [400/1000 (40%)]\tLosses F.softmax: 0.828734 log_softmax: 0.961356\n",
            "Train Epoch: 157 [600/1000 (60%)]\tLosses F.softmax: 0.162156 log_softmax: 0.120625\n",
            "Train Epoch: 157 [800/1000 (80%)]\tLosses F.softmax: 0.948806 log_softmax: 1.091411\n",
            "Train Epoch: 157 [1000/1000 (100%)]\tLosses F.softmax: 1.251805 log_softmax: 1.357392\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0449\tAccuracy: 6603.0/10000 (66%)\n",
            "log_softmax: Loss: 0.8689\tAccuracy: 7224.0/10000 (72%)\n",
            "\n",
            "Train Epoch: 158 [0/1000 (0%)]\tLosses F.softmax: 1.706587 log_softmax: 0.604620\n",
            "Train Epoch: 158 [200/1000 (20%)]\tLosses F.softmax: 0.771792 log_softmax: 0.838973\n",
            "Train Epoch: 158 [400/1000 (40%)]\tLosses F.softmax: 0.347097 log_softmax: 0.240559\n",
            "Train Epoch: 158 [600/1000 (60%)]\tLosses F.softmax: 0.940146 log_softmax: 0.448443\n",
            "Train Epoch: 158 [800/1000 (80%)]\tLosses F.softmax: 0.680201 log_softmax: 0.470077\n",
            "Train Epoch: 158 [1000/1000 (100%)]\tLosses F.softmax: 0.767425 log_softmax: 0.588013\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0430\tAccuracy: 6623.0/10000 (66%)\n",
            "log_softmax: Loss: 0.8663\tAccuracy: 7232.0/10000 (72%)\n",
            "\n",
            "Train Epoch: 159 [0/1000 (0%)]\tLosses F.softmax: 1.923608 log_softmax: 1.305791\n",
            "Train Epoch: 159 [200/1000 (20%)]\tLosses F.softmax: 0.217662 log_softmax: 0.130572\n",
            "Train Epoch: 159 [400/1000 (40%)]\tLosses F.softmax: 1.898601 log_softmax: 1.534775\n",
            "Train Epoch: 159 [600/1000 (60%)]\tLosses F.softmax: 0.251591 log_softmax: 0.098729\n",
            "Train Epoch: 159 [800/1000 (80%)]\tLosses F.softmax: 0.816961 log_softmax: 0.326221\n",
            "Train Epoch: 159 [1000/1000 (100%)]\tLosses F.softmax: 0.729130 log_softmax: 0.641412\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0340\tAccuracy: 6614.0/10000 (66%)\n",
            "log_softmax: Loss: 0.8567\tAccuracy: 7272.0/10000 (73%)\n",
            "\n",
            "Train Epoch: 160 [0/1000 (0%)]\tLosses F.softmax: 0.155571 log_softmax: 0.108919\n",
            "Train Epoch: 160 [200/1000 (20%)]\tLosses F.softmax: 2.362037 log_softmax: 1.904031\n",
            "Train Epoch: 160 [400/1000 (40%)]\tLosses F.softmax: 1.177458 log_softmax: 0.621294\n",
            "Train Epoch: 160 [600/1000 (60%)]\tLosses F.softmax: 0.639245 log_softmax: 0.593217\n",
            "Train Epoch: 160 [800/1000 (80%)]\tLosses F.softmax: 0.354697 log_softmax: 0.326428\n",
            "Train Epoch: 160 [1000/1000 (100%)]\tLosses F.softmax: 0.397178 log_softmax: 0.172300\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0262\tAccuracy: 6623.0/10000 (66%)\n",
            "log_softmax: Loss: 0.8497\tAccuracy: 7276.0/10000 (73%)\n",
            "\n",
            "Train Epoch: 161 [0/1000 (0%)]\tLosses F.softmax: 1.069208 log_softmax: 1.412436\n",
            "Train Epoch: 161 [200/1000 (20%)]\tLosses F.softmax: 0.467849 log_softmax: 0.305292\n",
            "Train Epoch: 161 [400/1000 (40%)]\tLosses F.softmax: 3.128207 log_softmax: 2.634891\n",
            "Train Epoch: 161 [600/1000 (60%)]\tLosses F.softmax: 0.406553 log_softmax: 0.315835\n",
            "Train Epoch: 161 [800/1000 (80%)]\tLosses F.softmax: 1.067351 log_softmax: 0.610585\n",
            "Train Epoch: 161 [1000/1000 (100%)]\tLosses F.softmax: 0.572048 log_softmax: 0.402779\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0193\tAccuracy: 6656.0/10000 (67%)\n",
            "log_softmax: Loss: 0.8421\tAccuracy: 7302.0/10000 (73%)\n",
            "\n",
            "Train Epoch: 162 [0/1000 (0%)]\tLosses F.softmax: 0.171971 log_softmax: 0.139348\n",
            "Train Epoch: 162 [200/1000 (20%)]\tLosses F.softmax: 1.624350 log_softmax: 1.153553\n",
            "Train Epoch: 162 [400/1000 (40%)]\tLosses F.softmax: 0.541006 log_softmax: 0.220046\n",
            "Train Epoch: 162 [600/1000 (60%)]\tLosses F.softmax: 0.286250 log_softmax: 0.104491\n",
            "Train Epoch: 162 [800/1000 (80%)]\tLosses F.softmax: 1.417361 log_softmax: 1.745665\n",
            "Train Epoch: 162 [1000/1000 (100%)]\tLosses F.softmax: 0.372641 log_softmax: 0.308135\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0136\tAccuracy: 6760.0/10000 (68%)\n",
            "log_softmax: Loss: 0.8370\tAccuracy: 7335.0/10000 (73%)\n",
            "\n",
            "Train Epoch: 163 [0/1000 (0%)]\tLosses F.softmax: 1.076055 log_softmax: 0.634500\n",
            "Train Epoch: 163 [200/1000 (20%)]\tLosses F.softmax: 1.463926 log_softmax: 1.304738\n",
            "Train Epoch: 163 [400/1000 (40%)]\tLosses F.softmax: 1.027794 log_softmax: 1.039752\n",
            "Train Epoch: 163 [600/1000 (60%)]\tLosses F.softmax: 3.587698 log_softmax: 3.347141\n",
            "Train Epoch: 163 [800/1000 (80%)]\tLosses F.softmax: 1.394941 log_softmax: 0.869263\n",
            "Train Epoch: 163 [1000/1000 (100%)]\tLosses F.softmax: 0.638991 log_softmax: 0.324565\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0145\tAccuracy: 6737.0/10000 (67%)\n",
            "log_softmax: Loss: 0.8369\tAccuracy: 7292.0/10000 (73%)\n",
            "\n",
            "Train Epoch: 164 [0/1000 (0%)]\tLosses F.softmax: 1.175064 log_softmax: 0.717243\n",
            "Train Epoch: 164 [200/1000 (20%)]\tLosses F.softmax: 0.301623 log_softmax: 0.244416\n",
            "Train Epoch: 164 [400/1000 (40%)]\tLosses F.softmax: 1.799605 log_softmax: 1.725017\n",
            "Train Epoch: 164 [600/1000 (60%)]\tLosses F.softmax: 1.410005 log_softmax: 1.483017\n",
            "Train Epoch: 164 [800/1000 (80%)]\tLosses F.softmax: 1.957529 log_softmax: 1.602004\n",
            "Train Epoch: 164 [1000/1000 (100%)]\tLosses F.softmax: 0.394043 log_softmax: 0.262520\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0011\tAccuracy: 6773.0/10000 (68%)\n",
            "log_softmax: Loss: 0.8235\tAccuracy: 7360.0/10000 (74%)\n",
            "\n",
            "Train Epoch: 165 [0/1000 (0%)]\tLosses F.softmax: 0.901623 log_softmax: 0.621589\n",
            "Train Epoch: 165 [200/1000 (20%)]\tLosses F.softmax: 0.390875 log_softmax: 0.210042\n",
            "Train Epoch: 165 [400/1000 (40%)]\tLosses F.softmax: 0.062514 log_softmax: 0.124989\n",
            "Train Epoch: 165 [600/1000 (60%)]\tLosses F.softmax: 0.799541 log_softmax: 0.668289\n",
            "Train Epoch: 165 [800/1000 (80%)]\tLosses F.softmax: 0.783339 log_softmax: 0.659931\n",
            "Train Epoch: 165 [1000/1000 (100%)]\tLosses F.softmax: 0.701994 log_softmax: 0.460169\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9956\tAccuracy: 6765.0/10000 (68%)\n",
            "log_softmax: Loss: 0.8201\tAccuracy: 7353.0/10000 (74%)\n",
            "\n",
            "Train Epoch: 166 [0/1000 (0%)]\tLosses F.softmax: 0.404014 log_softmax: 0.154201\n",
            "Train Epoch: 166 [200/1000 (20%)]\tLosses F.softmax: 0.332796 log_softmax: 0.177380\n",
            "Train Epoch: 166 [400/1000 (40%)]\tLosses F.softmax: 0.283499 log_softmax: 0.533539\n",
            "Train Epoch: 166 [600/1000 (60%)]\tLosses F.softmax: 0.756127 log_softmax: 0.761705\n",
            "Train Epoch: 166 [800/1000 (80%)]\tLosses F.softmax: 0.333115 log_softmax: 0.096581\n",
            "Train Epoch: 166 [1000/1000 (100%)]\tLosses F.softmax: 2.778298 log_softmax: 2.751729\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9903\tAccuracy: 6778.0/10000 (68%)\n",
            "log_softmax: Loss: 0.8158\tAccuracy: 7353.0/10000 (74%)\n",
            "\n",
            "Train Epoch: 167 [0/1000 (0%)]\tLosses F.softmax: 0.626668 log_softmax: 0.574773\n",
            "Train Epoch: 167 [200/1000 (20%)]\tLosses F.softmax: 0.787763 log_softmax: 1.305209\n",
            "Train Epoch: 167 [400/1000 (40%)]\tLosses F.softmax: 1.222485 log_softmax: 1.092249\n",
            "Train Epoch: 167 [600/1000 (60%)]\tLosses F.softmax: 0.505088 log_softmax: 0.417646\n",
            "Train Epoch: 167 [800/1000 (80%)]\tLosses F.softmax: 2.389699 log_softmax: 2.566027\n",
            "Train Epoch: 167 [1000/1000 (100%)]\tLosses F.softmax: 0.117887 log_softmax: 0.055827\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9863\tAccuracy: 6803.0/10000 (68%)\n",
            "log_softmax: Loss: 0.8102\tAccuracy: 7388.0/10000 (74%)\n",
            "\n",
            "Train Epoch: 168 [0/1000 (0%)]\tLosses F.softmax: 0.305904 log_softmax: 0.239232\n",
            "Train Epoch: 168 [200/1000 (20%)]\tLosses F.softmax: 0.683435 log_softmax: 0.556333\n",
            "Train Epoch: 168 [400/1000 (40%)]\tLosses F.softmax: 0.168153 log_softmax: 0.056565\n",
            "Train Epoch: 168 [600/1000 (60%)]\tLosses F.softmax: 0.548412 log_softmax: 0.083561\n",
            "Train Epoch: 168 [800/1000 (80%)]\tLosses F.softmax: 1.890609 log_softmax: 1.534117\n",
            "Train Epoch: 168 [1000/1000 (100%)]\tLosses F.softmax: 0.098741 log_softmax: 0.078783\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9762\tAccuracy: 6859.0/10000 (69%)\n",
            "log_softmax: Loss: 0.8041\tAccuracy: 7431.0/10000 (74%)\n",
            "\n",
            "Train Epoch: 169 [0/1000 (0%)]\tLosses F.softmax: 0.064273 log_softmax: 0.058451\n",
            "Train Epoch: 169 [200/1000 (20%)]\tLosses F.softmax: 0.993620 log_softmax: 0.531395\n",
            "Train Epoch: 169 [400/1000 (40%)]\tLosses F.softmax: 0.435513 log_softmax: 0.231472\n",
            "Train Epoch: 169 [600/1000 (60%)]\tLosses F.softmax: 0.170679 log_softmax: 0.064728\n",
            "Train Epoch: 169 [800/1000 (80%)]\tLosses F.softmax: 0.340809 log_softmax: 0.305085\n",
            "Train Epoch: 169 [1000/1000 (100%)]\tLosses F.softmax: 0.965919 log_softmax: 0.881848\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9694\tAccuracy: 6882.0/10000 (69%)\n",
            "log_softmax: Loss: 0.7973\tAccuracy: 7452.0/10000 (75%)\n",
            "\n",
            "Train Epoch: 170 [0/1000 (0%)]\tLosses F.softmax: 2.879982 log_softmax: 1.782203\n",
            "Train Epoch: 170 [200/1000 (20%)]\tLosses F.softmax: 0.142786 log_softmax: 0.054789\n",
            "Train Epoch: 170 [400/1000 (40%)]\tLosses F.softmax: 0.753027 log_softmax: 0.752372\n",
            "Train Epoch: 170 [600/1000 (60%)]\tLosses F.softmax: 0.482316 log_softmax: 0.077008\n",
            "Train Epoch: 170 [800/1000 (80%)]\tLosses F.softmax: 1.337287 log_softmax: 1.173296\n",
            "Train Epoch: 170 [1000/1000 (100%)]\tLosses F.softmax: 0.900371 log_softmax: 0.750434\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9654\tAccuracy: 6865.0/10000 (69%)\n",
            "log_softmax: Loss: 0.7964\tAccuracy: 7441.0/10000 (74%)\n",
            "\n",
            "Train Epoch: 171 [0/1000 (0%)]\tLosses F.softmax: 1.012835 log_softmax: 0.751240\n",
            "Train Epoch: 171 [200/1000 (20%)]\tLosses F.softmax: 1.088341 log_softmax: 0.455059\n",
            "Train Epoch: 171 [400/1000 (40%)]\tLosses F.softmax: 0.301854 log_softmax: 0.153157\n",
            "Train Epoch: 171 [600/1000 (60%)]\tLosses F.softmax: 0.373810 log_softmax: 0.216665\n",
            "Train Epoch: 171 [800/1000 (80%)]\tLosses F.softmax: 0.062843 log_softmax: 0.023493\n",
            "Train Epoch: 171 [1000/1000 (100%)]\tLosses F.softmax: 0.616280 log_softmax: 0.584992\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9584\tAccuracy: 6897.0/10000 (69%)\n",
            "log_softmax: Loss: 0.7897\tAccuracy: 7453.0/10000 (75%)\n",
            "\n",
            "Train Epoch: 172 [0/1000 (0%)]\tLosses F.softmax: 0.255008 log_softmax: 0.195380\n",
            "Train Epoch: 172 [200/1000 (20%)]\tLosses F.softmax: 0.567761 log_softmax: 0.476124\n",
            "Train Epoch: 172 [400/1000 (40%)]\tLosses F.softmax: 0.086117 log_softmax: 0.116409\n",
            "Train Epoch: 172 [600/1000 (60%)]\tLosses F.softmax: 0.680015 log_softmax: 0.522744\n",
            "Train Epoch: 172 [800/1000 (80%)]\tLosses F.softmax: 3.135798 log_softmax: 2.657059\n",
            "Train Epoch: 172 [1000/1000 (100%)]\tLosses F.softmax: 0.996542 log_softmax: 0.828272\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9544\tAccuracy: 6905.0/10000 (69%)\n",
            "log_softmax: Loss: 0.7869\tAccuracy: 7475.0/10000 (75%)\n",
            "\n",
            "Train Epoch: 173 [0/1000 (0%)]\tLosses F.softmax: 0.121493 log_softmax: 0.075279\n",
            "Train Epoch: 173 [200/1000 (20%)]\tLosses F.softmax: 0.175307 log_softmax: 0.050911\n",
            "Train Epoch: 173 [400/1000 (40%)]\tLosses F.softmax: 0.112559 log_softmax: 0.052660\n",
            "Train Epoch: 173 [600/1000 (60%)]\tLosses F.softmax: 0.185737 log_softmax: 0.091820\n",
            "Train Epoch: 173 [800/1000 (80%)]\tLosses F.softmax: 0.276425 log_softmax: 0.076750\n",
            "Train Epoch: 173 [1000/1000 (100%)]\tLosses F.softmax: 0.073535 log_softmax: 0.069923\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9455\tAccuracy: 6957.0/10000 (70%)\n",
            "log_softmax: Loss: 0.7812\tAccuracy: 7476.0/10000 (75%)\n",
            "\n",
            "Train Epoch: 174 [0/1000 (0%)]\tLosses F.softmax: 0.095678 log_softmax: 0.079687\n",
            "Train Epoch: 174 [200/1000 (20%)]\tLosses F.softmax: 0.720414 log_softmax: 0.164566\n",
            "Train Epoch: 174 [400/1000 (40%)]\tLosses F.softmax: 1.490939 log_softmax: 1.014985\n",
            "Train Epoch: 174 [600/1000 (60%)]\tLosses F.softmax: 2.015743 log_softmax: 1.347472\n",
            "Train Epoch: 174 [800/1000 (80%)]\tLosses F.softmax: 0.522670 log_softmax: 0.100050\n",
            "Train Epoch: 174 [1000/1000 (100%)]\tLosses F.softmax: 0.584703 log_softmax: 0.435988\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9419\tAccuracy: 6992.0/10000 (70%)\n",
            "log_softmax: Loss: 0.7792\tAccuracy: 7490.0/10000 (75%)\n",
            "\n",
            "Train Epoch: 175 [0/1000 (0%)]\tLosses F.softmax: 0.394164 log_softmax: 0.216801\n",
            "Train Epoch: 175 [200/1000 (20%)]\tLosses F.softmax: 1.902293 log_softmax: 2.474684\n",
            "Train Epoch: 175 [400/1000 (40%)]\tLosses F.softmax: 0.878886 log_softmax: 0.632533\n",
            "Train Epoch: 175 [600/1000 (60%)]\tLosses F.softmax: 1.811573 log_softmax: 1.232246\n",
            "Train Epoch: 175 [800/1000 (80%)]\tLosses F.softmax: 1.345812 log_softmax: 0.904081\n",
            "Train Epoch: 175 [1000/1000 (100%)]\tLosses F.softmax: 1.010648 log_softmax: 0.850119\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9362\tAccuracy: 6944.0/10000 (69%)\n",
            "log_softmax: Loss: 0.7751\tAccuracy: 7506.0/10000 (75%)\n",
            "\n",
            "Train Epoch: 176 [0/1000 (0%)]\tLosses F.softmax: 0.080909 log_softmax: 0.055345\n",
            "Train Epoch: 176 [200/1000 (20%)]\tLosses F.softmax: 0.474846 log_softmax: 0.091720\n",
            "Train Epoch: 176 [400/1000 (40%)]\tLosses F.softmax: 0.616067 log_softmax: 0.405913\n",
            "Train Epoch: 176 [600/1000 (60%)]\tLosses F.softmax: 1.043867 log_softmax: 0.540439\n",
            "Train Epoch: 176 [800/1000 (80%)]\tLosses F.softmax: 8.053145 log_softmax: 7.817353\n",
            "Train Epoch: 176 [1000/1000 (100%)]\tLosses F.softmax: 0.175403 log_softmax: 0.323480\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9338\tAccuracy: 6992.0/10000 (70%)\n",
            "log_softmax: Loss: 0.7784\tAccuracy: 7493.0/10000 (75%)\n",
            "\n",
            "Train Epoch: 177 [0/1000 (0%)]\tLosses F.softmax: 1.847471 log_softmax: 1.017055\n",
            "Train Epoch: 177 [200/1000 (20%)]\tLosses F.softmax: 0.115441 log_softmax: 0.049071\n",
            "Train Epoch: 177 [400/1000 (40%)]\tLosses F.softmax: 0.265385 log_softmax: 0.080666\n",
            "Train Epoch: 177 [600/1000 (60%)]\tLosses F.softmax: 0.729515 log_softmax: 0.729691\n",
            "Train Epoch: 177 [800/1000 (80%)]\tLosses F.softmax: 0.518059 log_softmax: 0.524428\n",
            "Train Epoch: 177 [1000/1000 (100%)]\tLosses F.softmax: 1.196198 log_softmax: 1.717899\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9201\tAccuracy: 7065.0/10000 (71%)\n",
            "log_softmax: Loss: 0.7650\tAccuracy: 7554.0/10000 (76%)\n",
            "\n",
            "Train Epoch: 178 [0/1000 (0%)]\tLosses F.softmax: 0.324522 log_softmax: 0.274539\n",
            "Train Epoch: 178 [200/1000 (20%)]\tLosses F.softmax: 0.544133 log_softmax: 0.815018\n",
            "Train Epoch: 178 [400/1000 (40%)]\tLosses F.softmax: 1.833035 log_softmax: 2.491194\n",
            "Train Epoch: 178 [600/1000 (60%)]\tLosses F.softmax: 1.558206 log_softmax: 1.438888\n",
            "Train Epoch: 178 [800/1000 (80%)]\tLosses F.softmax: 0.222661 log_softmax: 0.135357\n",
            "Train Epoch: 178 [1000/1000 (100%)]\tLosses F.softmax: 0.049852 log_softmax: 0.022492\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9168\tAccuracy: 7045.0/10000 (70%)\n",
            "log_softmax: Loss: 0.7623\tAccuracy: 7527.0/10000 (75%)\n",
            "\n",
            "Train Epoch: 179 [0/1000 (0%)]\tLosses F.softmax: 0.278884 log_softmax: 0.083740\n",
            "Train Epoch: 179 [200/1000 (20%)]\tLosses F.softmax: 0.461389 log_softmax: 0.268947\n",
            "Train Epoch: 179 [400/1000 (40%)]\tLosses F.softmax: 0.343313 log_softmax: 0.383844\n",
            "Train Epoch: 179 [600/1000 (60%)]\tLosses F.softmax: 0.980560 log_softmax: 1.116569\n",
            "Train Epoch: 179 [800/1000 (80%)]\tLosses F.softmax: 0.058326 log_softmax: 0.029986\n",
            "Train Epoch: 179 [1000/1000 (100%)]\tLosses F.softmax: 0.465887 log_softmax: 0.351927\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9138\tAccuracy: 7072.0/10000 (71%)\n",
            "log_softmax: Loss: 0.7622\tAccuracy: 7554.0/10000 (76%)\n",
            "\n",
            "Train Epoch: 180 [0/1000 (0%)]\tLosses F.softmax: 0.263849 log_softmax: 0.129141\n",
            "Train Epoch: 180 [200/1000 (20%)]\tLosses F.softmax: 0.126863 log_softmax: 0.154000\n",
            "Train Epoch: 180 [400/1000 (40%)]\tLosses F.softmax: 0.245779 log_softmax: 0.172737\n",
            "Train Epoch: 180 [600/1000 (60%)]\tLosses F.softmax: 0.122343 log_softmax: 0.120551\n",
            "Train Epoch: 180 [800/1000 (80%)]\tLosses F.softmax: 2.016796 log_softmax: 2.010074\n",
            "Train Epoch: 180 [1000/1000 (100%)]\tLosses F.softmax: 0.502311 log_softmax: 0.361239\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9067\tAccuracy: 7068.0/10000 (71%)\n",
            "log_softmax: Loss: 0.7593\tAccuracy: 7536.0/10000 (75%)\n",
            "\n",
            "Train Epoch: 181 [0/1000 (0%)]\tLosses F.softmax: 0.582019 log_softmax: 0.616302\n",
            "Train Epoch: 181 [200/1000 (20%)]\tLosses F.softmax: 1.080062 log_softmax: 0.709960\n",
            "Train Epoch: 181 [400/1000 (40%)]\tLosses F.softmax: 0.042033 log_softmax: 0.018685\n",
            "Train Epoch: 181 [600/1000 (60%)]\tLosses F.softmax: 0.136325 log_softmax: 0.060733\n",
            "Train Epoch: 181 [800/1000 (80%)]\tLosses F.softmax: 2.300944 log_softmax: 1.108667\n",
            "Train Epoch: 181 [1000/1000 (100%)]\tLosses F.softmax: 0.067461 log_softmax: 0.032287\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8974\tAccuracy: 7124.0/10000 (71%)\n",
            "log_softmax: Loss: 0.7535\tAccuracy: 7581.0/10000 (76%)\n",
            "\n",
            "Train Epoch: 182 [0/1000 (0%)]\tLosses F.softmax: 0.108665 log_softmax: 0.065329\n",
            "Train Epoch: 182 [200/1000 (20%)]\tLosses F.softmax: 1.704188 log_softmax: 1.433011\n",
            "Train Epoch: 182 [400/1000 (40%)]\tLosses F.softmax: 0.658936 log_softmax: 0.544207\n",
            "Train Epoch: 182 [600/1000 (60%)]\tLosses F.softmax: 0.279947 log_softmax: 0.042342\n",
            "Train Epoch: 182 [800/1000 (80%)]\tLosses F.softmax: 0.548099 log_softmax: 0.381328\n",
            "Train Epoch: 182 [1000/1000 (100%)]\tLosses F.softmax: 0.468226 log_softmax: 0.127904\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8929\tAccuracy: 7121.0/10000 (71%)\n",
            "log_softmax: Loss: 0.7505\tAccuracy: 7562.0/10000 (76%)\n",
            "\n",
            "Train Epoch: 183 [0/1000 (0%)]\tLosses F.softmax: 0.325368 log_softmax: 0.312249\n",
            "Train Epoch: 183 [200/1000 (20%)]\tLosses F.softmax: 1.965258 log_softmax: 1.982754\n",
            "Train Epoch: 183 [400/1000 (40%)]\tLosses F.softmax: 0.533982 log_softmax: 0.172300\n",
            "Train Epoch: 183 [600/1000 (60%)]\tLosses F.softmax: 0.106790 log_softmax: 0.142326\n",
            "Train Epoch: 183 [800/1000 (80%)]\tLosses F.softmax: 0.131835 log_softmax: 0.041521\n",
            "Train Epoch: 183 [1000/1000 (100%)]\tLosses F.softmax: 0.364015 log_softmax: 0.219307\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8961\tAccuracy: 7040.0/10000 (70%)\n",
            "log_softmax: Loss: 0.7549\tAccuracy: 7508.0/10000 (75%)\n",
            "\n",
            "Train Epoch: 184 [0/1000 (0%)]\tLosses F.softmax: 1.774045 log_softmax: 1.211777\n",
            "Train Epoch: 184 [200/1000 (20%)]\tLosses F.softmax: 1.108001 log_softmax: 0.602090\n",
            "Train Epoch: 184 [400/1000 (40%)]\tLosses F.softmax: 0.177475 log_softmax: 0.092142\n",
            "Train Epoch: 184 [600/1000 (60%)]\tLosses F.softmax: 0.083754 log_softmax: 0.074036\n",
            "Train Epoch: 184 [800/1000 (80%)]\tLosses F.softmax: 2.095760 log_softmax: 1.036820\n",
            "Train Epoch: 184 [1000/1000 (100%)]\tLosses F.softmax: 0.389893 log_softmax: 0.363805\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8830\tAccuracy: 7168.0/10000 (72%)\n",
            "log_softmax: Loss: 0.7473\tAccuracy: 7579.0/10000 (76%)\n",
            "\n",
            "Train Epoch: 185 [0/1000 (0%)]\tLosses F.softmax: 0.270533 log_softmax: 0.156371\n",
            "Train Epoch: 185 [200/1000 (20%)]\tLosses F.softmax: 0.309992 log_softmax: 0.253513\n",
            "Train Epoch: 185 [400/1000 (40%)]\tLosses F.softmax: 0.133979 log_softmax: 0.118785\n",
            "Train Epoch: 185 [600/1000 (60%)]\tLosses F.softmax: 0.709853 log_softmax: 0.449113\n",
            "Train Epoch: 185 [800/1000 (80%)]\tLosses F.softmax: 0.201194 log_softmax: 0.078611\n",
            "Train Epoch: 185 [1000/1000 (100%)]\tLosses F.softmax: 0.761915 log_softmax: 0.572269\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8750\tAccuracy: 7179.0/10000 (72%)\n",
            "log_softmax: Loss: 0.7406\tAccuracy: 7593.0/10000 (76%)\n",
            "\n",
            "Train Epoch: 186 [0/1000 (0%)]\tLosses F.softmax: 0.421336 log_softmax: 0.315255\n",
            "Train Epoch: 186 [200/1000 (20%)]\tLosses F.softmax: 3.412695 log_softmax: 4.684056\n",
            "Train Epoch: 186 [400/1000 (40%)]\tLosses F.softmax: 0.483018 log_softmax: 0.423342\n",
            "Train Epoch: 186 [600/1000 (60%)]\tLosses F.softmax: 0.711320 log_softmax: 0.201357\n",
            "Train Epoch: 186 [800/1000 (80%)]\tLosses F.softmax: 0.366198 log_softmax: 0.291323\n",
            "Train Epoch: 186 [1000/1000 (100%)]\tLosses F.softmax: 2.415193 log_softmax: 2.497711\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8724\tAccuracy: 7195.0/10000 (72%)\n",
            "log_softmax: Loss: 0.7421\tAccuracy: 7610.0/10000 (76%)\n",
            "\n",
            "Train Epoch: 187 [0/1000 (0%)]\tLosses F.softmax: 0.224770 log_softmax: 0.098108\n",
            "Train Epoch: 187 [200/1000 (20%)]\tLosses F.softmax: 0.835085 log_softmax: 0.861960\n",
            "Train Epoch: 187 [400/1000 (40%)]\tLosses F.softmax: 0.416360 log_softmax: 0.147974\n",
            "Train Epoch: 187 [600/1000 (60%)]\tLosses F.softmax: 0.528097 log_softmax: 0.532981\n",
            "Train Epoch: 187 [800/1000 (80%)]\tLosses F.softmax: 0.120993 log_softmax: 0.037845\n",
            "Train Epoch: 187 [1000/1000 (100%)]\tLosses F.softmax: 0.476901 log_softmax: 0.376126\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8670\tAccuracy: 7252.0/10000 (73%)\n",
            "log_softmax: Loss: 0.7385\tAccuracy: 7662.0/10000 (77%)\n",
            "\n",
            "Train Epoch: 188 [0/1000 (0%)]\tLosses F.softmax: 0.721882 log_softmax: 0.576181\n",
            "Train Epoch: 188 [200/1000 (20%)]\tLosses F.softmax: 0.849959 log_softmax: 0.272488\n",
            "Train Epoch: 188 [400/1000 (40%)]\tLosses F.softmax: 0.557030 log_softmax: 0.400824\n",
            "Train Epoch: 188 [600/1000 (60%)]\tLosses F.softmax: 0.523062 log_softmax: 0.405213\n",
            "Train Epoch: 188 [800/1000 (80%)]\tLosses F.softmax: 0.030050 log_softmax: 0.012345\n",
            "Train Epoch: 188 [1000/1000 (100%)]\tLosses F.softmax: 1.747144 log_softmax: 1.624043\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8619\tAccuracy: 7207.0/10000 (72%)\n",
            "log_softmax: Loss: 0.7368\tAccuracy: 7618.0/10000 (76%)\n",
            "\n",
            "Train Epoch: 189 [0/1000 (0%)]\tLosses F.softmax: 0.131449 log_softmax: 0.037082\n",
            "Train Epoch: 189 [200/1000 (20%)]\tLosses F.softmax: 1.190163 log_softmax: 1.287061\n",
            "Train Epoch: 189 [400/1000 (40%)]\tLosses F.softmax: 0.068713 log_softmax: 0.019331\n",
            "Train Epoch: 189 [600/1000 (60%)]\tLosses F.softmax: 0.249605 log_softmax: 0.265681\n",
            "Train Epoch: 189 [800/1000 (80%)]\tLosses F.softmax: 0.074261 log_softmax: 0.071242\n",
            "Train Epoch: 189 [1000/1000 (100%)]\tLosses F.softmax: 0.993549 log_softmax: 0.916066\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8566\tAccuracy: 7232.0/10000 (72%)\n",
            "log_softmax: Loss: 0.7336\tAccuracy: 7632.0/10000 (76%)\n",
            "\n",
            "Train Epoch: 190 [0/1000 (0%)]\tLosses F.softmax: 0.105854 log_softmax: 0.047280\n",
            "Train Epoch: 190 [200/1000 (20%)]\tLosses F.softmax: 5.675044 log_softmax: 3.384889\n",
            "Train Epoch: 190 [400/1000 (40%)]\tLosses F.softmax: 1.103872 log_softmax: 2.219632\n",
            "Train Epoch: 190 [600/1000 (60%)]\tLosses F.softmax: 0.258865 log_softmax: 0.057640\n",
            "Train Epoch: 190 [800/1000 (80%)]\tLosses F.softmax: 0.178547 log_softmax: 0.115965\n",
            "Train Epoch: 190 [1000/1000 (100%)]\tLosses F.softmax: 0.887907 log_softmax: 0.861863\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8472\tAccuracy: 7273.0/10000 (73%)\n",
            "log_softmax: Loss: 0.7267\tAccuracy: 7679.0/10000 (77%)\n",
            "\n",
            "Train Epoch: 191 [0/1000 (0%)]\tLosses F.softmax: 0.408626 log_softmax: 0.283833\n",
            "Train Epoch: 191 [200/1000 (20%)]\tLosses F.softmax: 0.858672 log_softmax: 0.786184\n",
            "Train Epoch: 191 [400/1000 (40%)]\tLosses F.softmax: 0.087912 log_softmax: 0.059017\n",
            "Train Epoch: 191 [600/1000 (60%)]\tLosses F.softmax: 0.719846 log_softmax: 0.516126\n",
            "Train Epoch: 191 [800/1000 (80%)]\tLosses F.softmax: 0.733179 log_softmax: 0.429373\n",
            "Train Epoch: 191 [1000/1000 (100%)]\tLosses F.softmax: 0.437747 log_softmax: 0.240672\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8444\tAccuracy: 7274.0/10000 (73%)\n",
            "log_softmax: Loss: 0.7257\tAccuracy: 7666.0/10000 (77%)\n",
            "\n",
            "Train Epoch: 192 [0/1000 (0%)]\tLosses F.softmax: 0.060307 log_softmax: 0.022431\n",
            "Train Epoch: 192 [200/1000 (20%)]\tLosses F.softmax: 0.105719 log_softmax: 0.043838\n",
            "Train Epoch: 192 [400/1000 (40%)]\tLosses F.softmax: 0.936223 log_softmax: 0.634727\n",
            "Train Epoch: 192 [600/1000 (60%)]\tLosses F.softmax: 0.096420 log_softmax: 0.094686\n",
            "Train Epoch: 192 [800/1000 (80%)]\tLosses F.softmax: 0.169339 log_softmax: 0.076545\n",
            "Train Epoch: 192 [1000/1000 (100%)]\tLosses F.softmax: 0.736988 log_softmax: 0.454380\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8370\tAccuracy: 7358.0/10000 (74%)\n",
            "log_softmax: Loss: 0.7230\tAccuracy: 7725.0/10000 (77%)\n",
            "\n",
            "Train Epoch: 193 [0/1000 (0%)]\tLosses F.softmax: 1.192874 log_softmax: 0.678071\n",
            "Train Epoch: 193 [200/1000 (20%)]\tLosses F.softmax: 0.656941 log_softmax: 0.796225\n",
            "Train Epoch: 193 [400/1000 (40%)]\tLosses F.softmax: 0.760756 log_softmax: 0.700535\n",
            "Train Epoch: 193 [600/1000 (60%)]\tLosses F.softmax: 0.050637 log_softmax: 0.033848\n",
            "Train Epoch: 193 [800/1000 (80%)]\tLosses F.softmax: 1.075437 log_softmax: 1.028678\n",
            "Train Epoch: 193 [1000/1000 (100%)]\tLosses F.softmax: 0.021813 log_softmax: 0.011273\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8361\tAccuracy: 7286.0/10000 (73%)\n",
            "log_softmax: Loss: 0.7237\tAccuracy: 7660.0/10000 (77%)\n",
            "\n",
            "Train Epoch: 194 [0/1000 (0%)]\tLosses F.softmax: 0.826971 log_softmax: 0.736090\n",
            "Train Epoch: 194 [200/1000 (20%)]\tLosses F.softmax: 0.075337 log_softmax: 0.050383\n",
            "Train Epoch: 194 [400/1000 (40%)]\tLosses F.softmax: 0.462345 log_softmax: 0.356699\n",
            "Train Epoch: 194 [600/1000 (60%)]\tLosses F.softmax: 1.022832 log_softmax: 0.898852\n",
            "Train Epoch: 194 [800/1000 (80%)]\tLosses F.softmax: 0.744601 log_softmax: 0.359522\n",
            "Train Epoch: 194 [1000/1000 (100%)]\tLosses F.softmax: 0.456822 log_softmax: 0.419616\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8288\tAccuracy: 7355.0/10000 (74%)\n",
            "log_softmax: Loss: 0.7186\tAccuracy: 7720.0/10000 (77%)\n",
            "\n",
            "Train Epoch: 195 [0/1000 (0%)]\tLosses F.softmax: 1.018739 log_softmax: 1.206306\n",
            "Train Epoch: 195 [200/1000 (20%)]\tLosses F.softmax: 0.717219 log_softmax: 0.435008\n",
            "Train Epoch: 195 [400/1000 (40%)]\tLosses F.softmax: 0.138568 log_softmax: 0.122418\n",
            "Train Epoch: 195 [600/1000 (60%)]\tLosses F.softmax: 1.001512 log_softmax: 0.432455\n",
            "Train Epoch: 195 [800/1000 (80%)]\tLosses F.softmax: 0.398625 log_softmax: 0.399171\n",
            "Train Epoch: 195 [1000/1000 (100%)]\tLosses F.softmax: 0.079949 log_softmax: 0.043820\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8268\tAccuracy: 7333.0/10000 (73%)\n",
            "log_softmax: Loss: 0.7183\tAccuracy: 7713.0/10000 (77%)\n",
            "\n",
            "Train Epoch: 196 [0/1000 (0%)]\tLosses F.softmax: 0.285649 log_softmax: 0.090862\n",
            "Train Epoch: 196 [200/1000 (20%)]\tLosses F.softmax: 2.752724 log_softmax: 3.056101\n",
            "Train Epoch: 196 [400/1000 (40%)]\tLosses F.softmax: 1.006753 log_softmax: 0.394015\n",
            "Train Epoch: 196 [600/1000 (60%)]\tLosses F.softmax: 1.101912 log_softmax: 0.913925\n",
            "Train Epoch: 196 [800/1000 (80%)]\tLosses F.softmax: 0.133966 log_softmax: 0.093187\n",
            "Train Epoch: 196 [1000/1000 (100%)]\tLosses F.softmax: 1.069451 log_softmax: 0.595654\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8240\tAccuracy: 7348.0/10000 (73%)\n",
            "log_softmax: Loss: 0.7186\tAccuracy: 7712.0/10000 (77%)\n",
            "\n",
            "Train Epoch: 197 [0/1000 (0%)]\tLosses F.softmax: 0.520224 log_softmax: 0.248652\n",
            "Train Epoch: 197 [200/1000 (20%)]\tLosses F.softmax: 0.320850 log_softmax: 0.249032\n",
            "Train Epoch: 197 [400/1000 (40%)]\tLosses F.softmax: 0.530208 log_softmax: 0.533171\n",
            "Train Epoch: 197 [600/1000 (60%)]\tLosses F.softmax: 1.715466 log_softmax: 1.226926\n",
            "Train Epoch: 197 [800/1000 (80%)]\tLosses F.softmax: 0.026164 log_softmax: 0.013016\n",
            "Train Epoch: 197 [1000/1000 (100%)]\tLosses F.softmax: 0.632718 log_softmax: 0.883650\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8192\tAccuracy: 7398.0/10000 (74%)\n",
            "log_softmax: Loss: 0.7162\tAccuracy: 7731.0/10000 (77%)\n",
            "\n",
            "Train Epoch: 198 [0/1000 (0%)]\tLosses F.softmax: 1.424959 log_softmax: 1.982029\n",
            "Train Epoch: 198 [200/1000 (20%)]\tLosses F.softmax: 0.421352 log_softmax: 0.308102\n",
            "Train Epoch: 198 [400/1000 (40%)]\tLosses F.softmax: 0.448355 log_softmax: 0.449166\n",
            "Train Epoch: 198 [600/1000 (60%)]\tLosses F.softmax: 1.482023 log_softmax: 1.541378\n",
            "Train Epoch: 198 [800/1000 (80%)]\tLosses F.softmax: 0.402905 log_softmax: 0.333936\n",
            "Train Epoch: 198 [1000/1000 (100%)]\tLosses F.softmax: 0.091006 log_softmax: 0.037812\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8109\tAccuracy: 7394.0/10000 (74%)\n",
            "log_softmax: Loss: 0.7098\tAccuracy: 7750.0/10000 (78%)\n",
            "\n",
            "Train Epoch: 199 [0/1000 (0%)]\tLosses F.softmax: 0.422897 log_softmax: 0.472721\n",
            "Train Epoch: 199 [200/1000 (20%)]\tLosses F.softmax: 0.020366 log_softmax: 0.036689\n",
            "Train Epoch: 199 [400/1000 (40%)]\tLosses F.softmax: 2.963847 log_softmax: 4.708687\n",
            "Train Epoch: 199 [600/1000 (60%)]\tLosses F.softmax: 2.103362 log_softmax: 1.110146\n",
            "Train Epoch: 199 [800/1000 (80%)]\tLosses F.softmax: 0.018009 log_softmax: 0.008076\n",
            "Train Epoch: 199 [1000/1000 (100%)]\tLosses F.softmax: 0.131672 log_softmax: 0.060518\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8104\tAccuracy: 7405.0/10000 (74%)\n",
            "log_softmax: Loss: 0.7128\tAccuracy: 7728.0/10000 (77%)\n",
            "\n",
            "Train Epoch: 200 [0/1000 (0%)]\tLosses F.softmax: 0.388942 log_softmax: 0.269068\n",
            "Train Epoch: 200 [200/1000 (20%)]\tLosses F.softmax: 3.754122 log_softmax: 4.337635\n",
            "Train Epoch: 200 [400/1000 (40%)]\tLosses F.softmax: 0.077812 log_softmax: 0.021513\n",
            "Train Epoch: 200 [600/1000 (60%)]\tLosses F.softmax: 0.121926 log_softmax: 0.049505\n",
            "Train Epoch: 200 [800/1000 (80%)]\tLosses F.softmax: 1.983930 log_softmax: 2.240592\n",
            "Train Epoch: 200 [1000/1000 (100%)]\tLosses F.softmax: 0.015682 log_softmax: 0.005415\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8050\tAccuracy: 7411.0/10000 (74%)\n",
            "log_softmax: Loss: 0.7062\tAccuracy: 7767.0/10000 (78%)\n",
            "\n",
            "Train Epoch: 201 [0/1000 (0%)]\tLosses F.softmax: 0.319128 log_softmax: 0.155338\n",
            "Train Epoch: 201 [200/1000 (20%)]\tLosses F.softmax: 0.697641 log_softmax: 0.789520\n",
            "Train Epoch: 201 [400/1000 (40%)]\tLosses F.softmax: 0.404288 log_softmax: 0.381413\n",
            "Train Epoch: 201 [600/1000 (60%)]\tLosses F.softmax: 0.715723 log_softmax: 0.665535\n",
            "Train Epoch: 201 [800/1000 (80%)]\tLosses F.softmax: 0.965742 log_softmax: 0.914216\n",
            "Train Epoch: 201 [1000/1000 (100%)]\tLosses F.softmax: 0.901268 log_softmax: 0.847953\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8049\tAccuracy: 7448.0/10000 (74%)\n",
            "log_softmax: Loss: 0.7073\tAccuracy: 7783.0/10000 (78%)\n",
            "\n",
            "Train Epoch: 202 [0/1000 (0%)]\tLosses F.softmax: 0.164377 log_softmax: 0.163751\n",
            "Train Epoch: 202 [200/1000 (20%)]\tLosses F.softmax: 0.815160 log_softmax: 0.508639\n",
            "Train Epoch: 202 [400/1000 (40%)]\tLosses F.softmax: 0.145351 log_softmax: 0.075461\n",
            "Train Epoch: 202 [600/1000 (60%)]\tLosses F.softmax: 0.102665 log_softmax: 0.039706\n",
            "Train Epoch: 202 [800/1000 (80%)]\tLosses F.softmax: 1.249318 log_softmax: 1.654781\n",
            "Train Epoch: 202 [1000/1000 (100%)]\tLosses F.softmax: 0.086282 log_softmax: 0.031395\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7951\tAccuracy: 7422.0/10000 (74%)\n",
            "log_softmax: Loss: 0.7021\tAccuracy: 7759.0/10000 (78%)\n",
            "\n",
            "Train Epoch: 203 [0/1000 (0%)]\tLosses F.softmax: 0.314080 log_softmax: 0.216374\n",
            "Train Epoch: 203 [200/1000 (20%)]\tLosses F.softmax: 0.094601 log_softmax: 0.065933\n",
            "Train Epoch: 203 [400/1000 (40%)]\tLosses F.softmax: 0.890518 log_softmax: 0.856094\n",
            "Train Epoch: 203 [600/1000 (60%)]\tLosses F.softmax: 0.025809 log_softmax: 0.013147\n",
            "Train Epoch: 203 [800/1000 (80%)]\tLosses F.softmax: 0.354123 log_softmax: 0.288928\n",
            "Train Epoch: 203 [1000/1000 (100%)]\tLosses F.softmax: 0.410540 log_softmax: 0.650956\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7961\tAccuracy: 7410.0/10000 (74%)\n",
            "log_softmax: Loss: 0.7049\tAccuracy: 7738.0/10000 (77%)\n",
            "\n",
            "Train Epoch: 204 [0/1000 (0%)]\tLosses F.softmax: 0.307726 log_softmax: 0.260958\n",
            "Train Epoch: 204 [200/1000 (20%)]\tLosses F.softmax: 0.054600 log_softmax: 0.073031\n",
            "Train Epoch: 204 [400/1000 (40%)]\tLosses F.softmax: 0.018293 log_softmax: 0.010305\n",
            "Train Epoch: 204 [600/1000 (60%)]\tLosses F.softmax: 0.029294 log_softmax: 0.013487\n",
            "Train Epoch: 204 [800/1000 (80%)]\tLosses F.softmax: 0.406588 log_softmax: 0.309002\n",
            "Train Epoch: 204 [1000/1000 (100%)]\tLosses F.softmax: 0.857388 log_softmax: 1.180868\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7892\tAccuracy: 7466.0/10000 (75%)\n",
            "log_softmax: Loss: 0.7001\tAccuracy: 7787.0/10000 (78%)\n",
            "\n",
            "Train Epoch: 205 [0/1000 (0%)]\tLosses F.softmax: 0.347314 log_softmax: 0.185879\n",
            "Train Epoch: 205 [200/1000 (20%)]\tLosses F.softmax: 0.344799 log_softmax: 0.232087\n",
            "Train Epoch: 205 [400/1000 (40%)]\tLosses F.softmax: 7.542478 log_softmax: 6.290402\n",
            "Train Epoch: 205 [600/1000 (60%)]\tLosses F.softmax: 0.121959 log_softmax: 0.035984\n",
            "Train Epoch: 205 [800/1000 (80%)]\tLosses F.softmax: 0.106553 log_softmax: 0.073615\n",
            "Train Epoch: 205 [1000/1000 (100%)]\tLosses F.softmax: 2.220711 log_softmax: 1.513264\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7877\tAccuracy: 7444.0/10000 (74%)\n",
            "log_softmax: Loss: 0.6999\tAccuracy: 7759.0/10000 (78%)\n",
            "\n",
            "Train Epoch: 206 [0/1000 (0%)]\tLosses F.softmax: 0.532562 log_softmax: 0.267223\n",
            "Train Epoch: 206 [200/1000 (20%)]\tLosses F.softmax: 0.687543 log_softmax: 0.341427\n",
            "Train Epoch: 206 [400/1000 (40%)]\tLosses F.softmax: 1.102148 log_softmax: 0.539848\n",
            "Train Epoch: 206 [600/1000 (60%)]\tLosses F.softmax: 0.770089 log_softmax: 0.539249\n",
            "Train Epoch: 206 [800/1000 (80%)]\tLosses F.softmax: 0.136649 log_softmax: 0.016969\n",
            "Train Epoch: 206 [1000/1000 (100%)]\tLosses F.softmax: 0.065999 log_softmax: 0.027254\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7795\tAccuracy: 7491.0/10000 (75%)\n",
            "log_softmax: Loss: 0.6944\tAccuracy: 7792.0/10000 (78%)\n",
            "\n",
            "Train Epoch: 207 [0/1000 (0%)]\tLosses F.softmax: 0.894982 log_softmax: 0.613368\n",
            "Train Epoch: 207 [200/1000 (20%)]\tLosses F.softmax: 0.275724 log_softmax: 0.133501\n",
            "Train Epoch: 207 [400/1000 (40%)]\tLosses F.softmax: 0.541152 log_softmax: 0.338080\n",
            "Train Epoch: 207 [600/1000 (60%)]\tLosses F.softmax: 0.118803 log_softmax: 0.046638\n",
            "Train Epoch: 207 [800/1000 (80%)]\tLosses F.softmax: 0.070902 log_softmax: 0.033058\n",
            "Train Epoch: 207 [1000/1000 (100%)]\tLosses F.softmax: 0.051587 log_softmax: 0.026626\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7825\tAccuracy: 7463.0/10000 (75%)\n",
            "log_softmax: Loss: 0.6996\tAccuracy: 7746.0/10000 (77%)\n",
            "\n",
            "Train Epoch: 208 [0/1000 (0%)]\tLosses F.softmax: 0.053648 log_softmax: 0.034640\n",
            "Train Epoch: 208 [200/1000 (20%)]\tLosses F.softmax: 0.640028 log_softmax: 0.372438\n",
            "Train Epoch: 208 [400/1000 (40%)]\tLosses F.softmax: 1.774095 log_softmax: 1.517776\n",
            "Train Epoch: 208 [600/1000 (60%)]\tLosses F.softmax: 0.824172 log_softmax: 0.703102\n",
            "Train Epoch: 208 [800/1000 (80%)]\tLosses F.softmax: 0.043491 log_softmax: 0.019590\n",
            "Train Epoch: 208 [1000/1000 (100%)]\tLosses F.softmax: 0.287303 log_softmax: 0.207639\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7768\tAccuracy: 7512.0/10000 (75%)\n",
            "log_softmax: Loss: 0.6938\tAccuracy: 7810.0/10000 (78%)\n",
            "\n",
            "Train Epoch: 209 [0/1000 (0%)]\tLosses F.softmax: 3.620047 log_softmax: 4.817004\n",
            "Train Epoch: 209 [200/1000 (20%)]\tLosses F.softmax: 0.529548 log_softmax: 0.270205\n",
            "Train Epoch: 209 [400/1000 (40%)]\tLosses F.softmax: 0.529472 log_softmax: 0.456058\n",
            "Train Epoch: 209 [600/1000 (60%)]\tLosses F.softmax: 0.078756 log_softmax: 0.061357\n",
            "Train Epoch: 209 [800/1000 (80%)]\tLosses F.softmax: 0.481273 log_softmax: 0.343910\n",
            "Train Epoch: 209 [1000/1000 (100%)]\tLosses F.softmax: 0.091327 log_softmax: 0.035634\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7754\tAccuracy: 7525.0/10000 (75%)\n",
            "log_softmax: Loss: 0.6934\tAccuracy: 7801.0/10000 (78%)\n",
            "\n",
            "Train Epoch: 210 [0/1000 (0%)]\tLosses F.softmax: 0.891189 log_softmax: 0.822766\n",
            "Train Epoch: 210 [200/1000 (20%)]\tLosses F.softmax: 0.160760 log_softmax: 0.067242\n",
            "Train Epoch: 210 [400/1000 (40%)]\tLosses F.softmax: 0.472366 log_softmax: 0.510069\n",
            "Train Epoch: 210 [600/1000 (60%)]\tLosses F.softmax: 0.428793 log_softmax: 0.123149\n",
            "Train Epoch: 210 [800/1000 (80%)]\tLosses F.softmax: 0.430739 log_softmax: 0.246185\n",
            "Train Epoch: 210 [1000/1000 (100%)]\tLosses F.softmax: 0.062849 log_softmax: 0.054835\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7696\tAccuracy: 7523.0/10000 (75%)\n",
            "log_softmax: Loss: 0.6892\tAccuracy: 7833.0/10000 (78%)\n",
            "\n",
            "Train Epoch: 211 [0/1000 (0%)]\tLosses F.softmax: 0.980932 log_softmax: 0.826956\n",
            "Train Epoch: 211 [200/1000 (20%)]\tLosses F.softmax: 0.491668 log_softmax: 0.400966\n",
            "Train Epoch: 211 [400/1000 (40%)]\tLosses F.softmax: 0.665803 log_softmax: 0.414829\n",
            "Train Epoch: 211 [600/1000 (60%)]\tLosses F.softmax: 0.303884 log_softmax: 0.182103\n",
            "Train Epoch: 211 [800/1000 (80%)]\tLosses F.softmax: 0.124033 log_softmax: 0.070718\n",
            "Train Epoch: 211 [1000/1000 (100%)]\tLosses F.softmax: 0.174873 log_softmax: 0.118123\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7761\tAccuracy: 7471.0/10000 (75%)\n",
            "log_softmax: Loss: 0.6977\tAccuracy: 7752.0/10000 (78%)\n",
            "\n",
            "Train Epoch: 212 [0/1000 (0%)]\tLosses F.softmax: 0.543750 log_softmax: 0.490533\n",
            "Train Epoch: 212 [200/1000 (20%)]\tLosses F.softmax: 0.224915 log_softmax: 0.087669\n",
            "Train Epoch: 212 [400/1000 (40%)]\tLosses F.softmax: 0.210529 log_softmax: 0.069201\n",
            "Train Epoch: 212 [600/1000 (60%)]\tLosses F.softmax: 1.473827 log_softmax: 1.528936\n",
            "Train Epoch: 212 [800/1000 (80%)]\tLosses F.softmax: 0.037712 log_softmax: 0.020633\n",
            "Train Epoch: 212 [1000/1000 (100%)]\tLosses F.softmax: 0.851446 log_softmax: 0.678864\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7611\tAccuracy: 7560.0/10000 (76%)\n",
            "log_softmax: Loss: 0.6853\tAccuracy: 7824.0/10000 (78%)\n",
            "\n",
            "Train Epoch: 213 [0/1000 (0%)]\tLosses F.softmax: 0.052689 log_softmax: 0.032918\n",
            "Train Epoch: 213 [200/1000 (20%)]\tLosses F.softmax: 0.040684 log_softmax: 0.025155\n",
            "Train Epoch: 213 [400/1000 (40%)]\tLosses F.softmax: 2.658397 log_softmax: 3.021276\n",
            "Train Epoch: 213 [600/1000 (60%)]\tLosses F.softmax: 1.182231 log_softmax: 0.490152\n",
            "Train Epoch: 213 [800/1000 (80%)]\tLosses F.softmax: 0.157918 log_softmax: 0.064243\n",
            "Train Epoch: 213 [1000/1000 (100%)]\tLosses F.softmax: 2.038204 log_softmax: 1.432213\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7619\tAccuracy: 7549.0/10000 (75%)\n",
            "log_softmax: Loss: 0.6874\tAccuracy: 7821.0/10000 (78%)\n",
            "\n",
            "Train Epoch: 214 [0/1000 (0%)]\tLosses F.softmax: 0.263941 log_softmax: 0.206546\n",
            "Train Epoch: 214 [200/1000 (20%)]\tLosses F.softmax: 0.021262 log_softmax: 0.027633\n",
            "Train Epoch: 214 [400/1000 (40%)]\tLosses F.softmax: 0.116366 log_softmax: 0.061003\n",
            "Train Epoch: 214 [600/1000 (60%)]\tLosses F.softmax: 0.776750 log_softmax: 0.697175\n",
            "Train Epoch: 214 [800/1000 (80%)]\tLosses F.softmax: 0.047255 log_softmax: 0.044194\n",
            "Train Epoch: 214 [1000/1000 (100%)]\tLosses F.softmax: 0.078466 log_softmax: 0.101646\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7574\tAccuracy: 7572.0/10000 (76%)\n",
            "log_softmax: Loss: 0.6857\tAccuracy: 7843.0/10000 (78%)\n",
            "\n",
            "Train Epoch: 215 [0/1000 (0%)]\tLosses F.softmax: 0.115416 log_softmax: 0.098155\n",
            "Train Epoch: 215 [200/1000 (20%)]\tLosses F.softmax: 0.043750 log_softmax: 0.049265\n",
            "Train Epoch: 215 [400/1000 (40%)]\tLosses F.softmax: 0.049262 log_softmax: 0.025829\n",
            "Train Epoch: 215 [600/1000 (60%)]\tLosses F.softmax: 0.275899 log_softmax: 0.215270\n",
            "Train Epoch: 215 [800/1000 (80%)]\tLosses F.softmax: 0.024195 log_softmax: 0.010104\n",
            "Train Epoch: 215 [1000/1000 (100%)]\tLosses F.softmax: 0.980448 log_softmax: 0.797379\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7534\tAccuracy: 7597.0/10000 (76%)\n",
            "log_softmax: Loss: 0.6812\tAccuracy: 7872.0/10000 (79%)\n",
            "\n",
            "Train Epoch: 216 [0/1000 (0%)]\tLosses F.softmax: 0.075406 log_softmax: 0.238370\n",
            "Train Epoch: 216 [200/1000 (20%)]\tLosses F.softmax: 0.173396 log_softmax: 0.067048\n",
            "Train Epoch: 216 [400/1000 (40%)]\tLosses F.softmax: 0.075146 log_softmax: 0.026013\n",
            "Train Epoch: 216 [600/1000 (60%)]\tLosses F.softmax: 0.158533 log_softmax: 0.161865\n",
            "Train Epoch: 216 [800/1000 (80%)]\tLosses F.softmax: 0.256642 log_softmax: 0.226268\n",
            "Train Epoch: 216 [1000/1000 (100%)]\tLosses F.softmax: 0.586631 log_softmax: 0.327618\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7562\tAccuracy: 7535.0/10000 (75%)\n",
            "log_softmax: Loss: 0.6846\tAccuracy: 7815.0/10000 (78%)\n",
            "\n",
            "Train Epoch: 217 [0/1000 (0%)]\tLosses F.softmax: 0.042815 log_softmax: 0.037570\n",
            "Train Epoch: 217 [200/1000 (20%)]\tLosses F.softmax: 0.017036 log_softmax: 0.009040\n",
            "Train Epoch: 217 [400/1000 (40%)]\tLosses F.softmax: 0.656825 log_softmax: 0.642468\n",
            "Train Epoch: 217 [600/1000 (60%)]\tLosses F.softmax: 1.134187 log_softmax: 0.464575\n",
            "Train Epoch: 217 [800/1000 (80%)]\tLosses F.softmax: 0.629170 log_softmax: 0.783909\n",
            "Train Epoch: 217 [1000/1000 (100%)]\tLosses F.softmax: 0.311846 log_softmax: 0.332345\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7484\tAccuracy: 7628.0/10000 (76%)\n",
            "log_softmax: Loss: 0.6799\tAccuracy: 7868.0/10000 (79%)\n",
            "\n",
            "Train Epoch: 218 [0/1000 (0%)]\tLosses F.softmax: 0.048715 log_softmax: 0.032281\n",
            "Train Epoch: 218 [200/1000 (20%)]\tLosses F.softmax: 0.140418 log_softmax: 0.101038\n",
            "Train Epoch: 218 [400/1000 (40%)]\tLosses F.softmax: 0.515103 log_softmax: 0.782154\n",
            "Train Epoch: 218 [600/1000 (60%)]\tLosses F.softmax: 0.055396 log_softmax: 0.028708\n",
            "Train Epoch: 218 [800/1000 (80%)]\tLosses F.softmax: 0.117566 log_softmax: 0.102362\n",
            "Train Epoch: 218 [1000/1000 (100%)]\tLosses F.softmax: 0.164597 log_softmax: 0.049605\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7473\tAccuracy: 7602.0/10000 (76%)\n",
            "log_softmax: Loss: 0.6796\tAccuracy: 7868.0/10000 (79%)\n",
            "\n",
            "Train Epoch: 219 [0/1000 (0%)]\tLosses F.softmax: 0.254765 log_softmax: 0.220383\n",
            "Train Epoch: 219 [200/1000 (20%)]\tLosses F.softmax: 0.384396 log_softmax: 0.200683\n",
            "Train Epoch: 219 [400/1000 (40%)]\tLosses F.softmax: 0.240468 log_softmax: 0.581932\n",
            "Train Epoch: 219 [600/1000 (60%)]\tLosses F.softmax: 0.078582 log_softmax: 0.050025\n",
            "Train Epoch: 219 [800/1000 (80%)]\tLosses F.softmax: 1.787645 log_softmax: 0.691539\n",
            "Train Epoch: 219 [1000/1000 (100%)]\tLosses F.softmax: 0.017627 log_softmax: 0.019088\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7433\tAccuracy: 7662.0/10000 (77%)\n",
            "log_softmax: Loss: 0.6778\tAccuracy: 7891.0/10000 (79%)\n",
            "\n",
            "Train Epoch: 220 [0/1000 (0%)]\tLosses F.softmax: 0.091919 log_softmax: 0.069346\n",
            "Train Epoch: 220 [200/1000 (20%)]\tLosses F.softmax: 0.040060 log_softmax: 0.093880\n",
            "Train Epoch: 220 [400/1000 (40%)]\tLosses F.softmax: 0.146420 log_softmax: 0.109923\n",
            "Train Epoch: 220 [600/1000 (60%)]\tLosses F.softmax: 0.672447 log_softmax: 0.361984\n",
            "Train Epoch: 220 [800/1000 (80%)]\tLosses F.softmax: 0.159136 log_softmax: 0.122946\n",
            "Train Epoch: 220 [1000/1000 (100%)]\tLosses F.softmax: 1.312494 log_softmax: 0.701416\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7411\tAccuracy: 7657.0/10000 (77%)\n",
            "log_softmax: Loss: 0.6756\tAccuracy: 7909.0/10000 (79%)\n",
            "\n",
            "Train Epoch: 221 [0/1000 (0%)]\tLosses F.softmax: 0.065508 log_softmax: 0.041640\n",
            "Train Epoch: 221 [200/1000 (20%)]\tLosses F.softmax: 0.187304 log_softmax: 0.121519\n",
            "Train Epoch: 221 [400/1000 (40%)]\tLosses F.softmax: 0.293224 log_softmax: 0.323702\n",
            "Train Epoch: 221 [600/1000 (60%)]\tLosses F.softmax: 0.470326 log_softmax: 0.316082\n",
            "Train Epoch: 221 [800/1000 (80%)]\tLosses F.softmax: 1.636856 log_softmax: 1.059273\n",
            "Train Epoch: 221 [1000/1000 (100%)]\tLosses F.softmax: 0.380446 log_softmax: 0.263867\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7401\tAccuracy: 7639.0/10000 (76%)\n",
            "log_softmax: Loss: 0.6768\tAccuracy: 7886.0/10000 (79%)\n",
            "\n",
            "Train Epoch: 222 [0/1000 (0%)]\tLosses F.softmax: 0.480034 log_softmax: 0.120434\n",
            "Train Epoch: 222 [200/1000 (20%)]\tLosses F.softmax: 0.060735 log_softmax: 0.014204\n",
            "Train Epoch: 222 [400/1000 (40%)]\tLosses F.softmax: 0.262668 log_softmax: 0.244452\n",
            "Train Epoch: 222 [600/1000 (60%)]\tLosses F.softmax: 0.028646 log_softmax: 0.020732\n",
            "Train Epoch: 222 [800/1000 (80%)]\tLosses F.softmax: 0.351612 log_softmax: 0.231119\n",
            "Train Epoch: 222 [1000/1000 (100%)]\tLosses F.softmax: 0.086335 log_softmax: 0.073880\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7358\tAccuracy: 7674.0/10000 (77%)\n",
            "log_softmax: Loss: 0.6765\tAccuracy: 7876.0/10000 (79%)\n",
            "\n",
            "Train Epoch: 223 [0/1000 (0%)]\tLosses F.softmax: 1.258690 log_softmax: 0.579731\n",
            "Train Epoch: 223 [200/1000 (20%)]\tLosses F.softmax: 0.238099 log_softmax: 0.139400\n",
            "Train Epoch: 223 [400/1000 (40%)]\tLosses F.softmax: 2.039896 log_softmax: 1.387125\n",
            "Train Epoch: 223 [600/1000 (60%)]\tLosses F.softmax: 0.298379 log_softmax: 0.241660\n",
            "Train Epoch: 223 [800/1000 (80%)]\tLosses F.softmax: 1.208927 log_softmax: 0.739393\n",
            "Train Epoch: 223 [1000/1000 (100%)]\tLosses F.softmax: 0.688355 log_softmax: 0.598203\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7327\tAccuracy: 7691.0/10000 (77%)\n",
            "log_softmax: Loss: 0.6713\tAccuracy: 7925.0/10000 (79%)\n",
            "\n",
            "Train Epoch: 224 [0/1000 (0%)]\tLosses F.softmax: 0.018431 log_softmax: 0.023135\n",
            "Train Epoch: 224 [200/1000 (20%)]\tLosses F.softmax: 0.306439 log_softmax: 0.244266\n",
            "Train Epoch: 224 [400/1000 (40%)]\tLosses F.softmax: 0.306320 log_softmax: 0.156729\n",
            "Train Epoch: 224 [600/1000 (60%)]\tLosses F.softmax: 1.105718 log_softmax: 0.530559\n",
            "Train Epoch: 224 [800/1000 (80%)]\tLosses F.softmax: 0.797142 log_softmax: 0.467724\n",
            "Train Epoch: 224 [1000/1000 (100%)]\tLosses F.softmax: 0.687198 log_softmax: 0.602650\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7347\tAccuracy: 7663.0/10000 (77%)\n",
            "log_softmax: Loss: 0.6728\tAccuracy: 7933.0/10000 (79%)\n",
            "\n",
            "Train Epoch: 225 [0/1000 (0%)]\tLosses F.softmax: 0.132560 log_softmax: 0.066515\n",
            "Train Epoch: 225 [200/1000 (20%)]\tLosses F.softmax: 0.012442 log_softmax: 0.008064\n",
            "Train Epoch: 225 [400/1000 (40%)]\tLosses F.softmax: 0.755947 log_softmax: 0.678620\n",
            "Train Epoch: 225 [600/1000 (60%)]\tLosses F.softmax: 0.032758 log_softmax: 0.012078\n",
            "Train Epoch: 225 [800/1000 (80%)]\tLosses F.softmax: 0.327077 log_softmax: 0.277917\n",
            "Train Epoch: 225 [1000/1000 (100%)]\tLosses F.softmax: 1.055844 log_softmax: 0.869529\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7311\tAccuracy: 7707.0/10000 (77%)\n",
            "log_softmax: Loss: 0.6698\tAccuracy: 7931.0/10000 (79%)\n",
            "\n",
            "Train Epoch: 226 [0/1000 (0%)]\tLosses F.softmax: 0.064135 log_softmax: 0.039633\n",
            "Train Epoch: 226 [200/1000 (20%)]\tLosses F.softmax: 0.789062 log_softmax: 0.213277\n",
            "Train Epoch: 226 [400/1000 (40%)]\tLosses F.softmax: 0.138230 log_softmax: 0.125345\n",
            "Train Epoch: 226 [600/1000 (60%)]\tLosses F.softmax: 1.394011 log_softmax: 0.943476\n",
            "Train Epoch: 226 [800/1000 (80%)]\tLosses F.softmax: 0.232007 log_softmax: 0.054348\n",
            "Train Epoch: 226 [1000/1000 (100%)]\tLosses F.softmax: 0.044881 log_softmax: 0.028933\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7241\tAccuracy: 7746.0/10000 (77%)\n",
            "log_softmax: Loss: 0.6645\tAccuracy: 7948.0/10000 (79%)\n",
            "\n",
            "Train Epoch: 227 [0/1000 (0%)]\tLosses F.softmax: 0.029806 log_softmax: 0.010943\n",
            "Train Epoch: 227 [200/1000 (20%)]\tLosses F.softmax: 0.386310 log_softmax: 0.305795\n",
            "Train Epoch: 227 [400/1000 (40%)]\tLosses F.softmax: 0.356480 log_softmax: 0.133459\n",
            "Train Epoch: 227 [600/1000 (60%)]\tLosses F.softmax: 0.013821 log_softmax: 0.009878\n",
            "Train Epoch: 227 [800/1000 (80%)]\tLosses F.softmax: 0.064827 log_softmax: 0.077828\n",
            "Train Epoch: 227 [1000/1000 (100%)]\tLosses F.softmax: 0.041739 log_softmax: 0.028810\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7238\tAccuracy: 7710.0/10000 (77%)\n",
            "log_softmax: Loss: 0.6656\tAccuracy: 7951.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 228 [0/1000 (0%)]\tLosses F.softmax: 0.014626 log_softmax: 0.008603\n",
            "Train Epoch: 228 [200/1000 (20%)]\tLosses F.softmax: 0.429977 log_softmax: 0.416674\n",
            "Train Epoch: 228 [400/1000 (40%)]\tLosses F.softmax: 0.379068 log_softmax: 0.340541\n",
            "Train Epoch: 228 [600/1000 (60%)]\tLosses F.softmax: 0.053431 log_softmax: 0.049045\n",
            "Train Epoch: 228 [800/1000 (80%)]\tLosses F.softmax: 0.249846 log_softmax: 0.119451\n",
            "Train Epoch: 228 [1000/1000 (100%)]\tLosses F.softmax: 0.004013 log_softmax: 0.001930\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7257\tAccuracy: 7707.0/10000 (77%)\n",
            "log_softmax: Loss: 0.6666\tAccuracy: 7941.0/10000 (79%)\n",
            "\n",
            "Train Epoch: 229 [0/1000 (0%)]\tLosses F.softmax: 0.835747 log_softmax: 0.540443\n",
            "Train Epoch: 229 [200/1000 (20%)]\tLosses F.softmax: 0.155124 log_softmax: 0.129208\n",
            "Train Epoch: 229 [400/1000 (40%)]\tLosses F.softmax: 0.427226 log_softmax: 0.286084\n",
            "Train Epoch: 229 [600/1000 (60%)]\tLosses F.softmax: 0.321567 log_softmax: 0.199760\n",
            "Train Epoch: 229 [800/1000 (80%)]\tLosses F.softmax: 0.453426 log_softmax: 0.508714\n",
            "Train Epoch: 229 [1000/1000 (100%)]\tLosses F.softmax: 0.175792 log_softmax: 0.140842\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7243\tAccuracy: 7709.0/10000 (77%)\n",
            "log_softmax: Loss: 0.6661\tAccuracy: 7941.0/10000 (79%)\n",
            "\n",
            "Train Epoch: 230 [0/1000 (0%)]\tLosses F.softmax: 0.016863 log_softmax: 0.009792\n",
            "Train Epoch: 230 [200/1000 (20%)]\tLosses F.softmax: 0.043663 log_softmax: 0.011527\n",
            "Train Epoch: 230 [400/1000 (40%)]\tLosses F.softmax: 0.177284 log_softmax: 0.107440\n",
            "Train Epoch: 230 [600/1000 (60%)]\tLosses F.softmax: 0.612517 log_softmax: 0.514503\n",
            "Train Epoch: 230 [800/1000 (80%)]\tLosses F.softmax: 4.681362 log_softmax: 2.828414\n",
            "Train Epoch: 230 [1000/1000 (100%)]\tLosses F.softmax: 0.042772 log_softmax: 0.060438\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7190\tAccuracy: 7757.0/10000 (78%)\n",
            "log_softmax: Loss: 0.6637\tAccuracy: 7965.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 231 [0/1000 (0%)]\tLosses F.softmax: 0.098400 log_softmax: 0.081655\n",
            "Train Epoch: 231 [200/1000 (20%)]\tLosses F.softmax: 0.599378 log_softmax: 0.338454\n",
            "Train Epoch: 231 [400/1000 (40%)]\tLosses F.softmax: 1.263336 log_softmax: 0.932239\n",
            "Train Epoch: 231 [600/1000 (60%)]\tLosses F.softmax: 0.087834 log_softmax: 0.096721\n",
            "Train Epoch: 231 [800/1000 (80%)]\tLosses F.softmax: 0.248054 log_softmax: 0.179906\n",
            "Train Epoch: 231 [1000/1000 (100%)]\tLosses F.softmax: 0.052297 log_softmax: 0.037462\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7172\tAccuracy: 7799.0/10000 (78%)\n",
            "log_softmax: Loss: 0.6618\tAccuracy: 7976.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 232 [0/1000 (0%)]\tLosses F.softmax: 3.322597 log_softmax: 3.794417\n",
            "Train Epoch: 232 [200/1000 (20%)]\tLosses F.softmax: 0.310844 log_softmax: 0.240359\n",
            "Train Epoch: 232 [400/1000 (40%)]\tLosses F.softmax: 0.039213 log_softmax: 0.034732\n",
            "Train Epoch: 232 [600/1000 (60%)]\tLosses F.softmax: 0.037516 log_softmax: 0.021183\n",
            "Train Epoch: 232 [800/1000 (80%)]\tLosses F.softmax: 0.269475 log_softmax: 0.216195\n",
            "Train Epoch: 232 [1000/1000 (100%)]\tLosses F.softmax: 0.195722 log_softmax: 0.161659\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7154\tAccuracy: 7758.0/10000 (78%)\n",
            "log_softmax: Loss: 0.6601\tAccuracy: 7956.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 233 [0/1000 (0%)]\tLosses F.softmax: 1.014916 log_softmax: 0.904372\n",
            "Train Epoch: 233 [200/1000 (20%)]\tLosses F.softmax: 0.035344 log_softmax: 0.022821\n",
            "Train Epoch: 233 [400/1000 (40%)]\tLosses F.softmax: 0.348832 log_softmax: 0.067692\n",
            "Train Epoch: 233 [600/1000 (60%)]\tLosses F.softmax: 1.323465 log_softmax: 0.927734\n",
            "Train Epoch: 233 [800/1000 (80%)]\tLosses F.softmax: 0.602528 log_softmax: 0.508942\n",
            "Train Epoch: 233 [1000/1000 (100%)]\tLosses F.softmax: 0.325881 log_softmax: 0.128648\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7152\tAccuracy: 7753.0/10000 (78%)\n",
            "log_softmax: Loss: 0.6603\tAccuracy: 7966.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 234 [0/1000 (0%)]\tLosses F.softmax: 0.018829 log_softmax: 0.005856\n",
            "Train Epoch: 234 [200/1000 (20%)]\tLosses F.softmax: 0.652723 log_softmax: 0.563052\n",
            "Train Epoch: 234 [400/1000 (40%)]\tLosses F.softmax: 0.269348 log_softmax: 0.200315\n",
            "Train Epoch: 234 [600/1000 (60%)]\tLosses F.softmax: 0.008251 log_softmax: 0.005756\n",
            "Train Epoch: 234 [800/1000 (80%)]\tLosses F.softmax: 0.092120 log_softmax: 0.035298\n",
            "Train Epoch: 234 [1000/1000 (100%)]\tLosses F.softmax: 0.340892 log_softmax: 0.125144\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7149\tAccuracy: 7756.0/10000 (78%)\n",
            "log_softmax: Loss: 0.6608\tAccuracy: 7960.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 235 [0/1000 (0%)]\tLosses F.softmax: 0.110083 log_softmax: 0.063433\n",
            "Train Epoch: 235 [200/1000 (20%)]\tLosses F.softmax: 0.558479 log_softmax: 0.444120\n",
            "Train Epoch: 235 [400/1000 (40%)]\tLosses F.softmax: 0.260174 log_softmax: 0.215114\n",
            "Train Epoch: 235 [600/1000 (60%)]\tLosses F.softmax: 0.012750 log_softmax: 0.007864\n",
            "Train Epoch: 235 [800/1000 (80%)]\tLosses F.softmax: 0.033957 log_softmax: 0.025654\n",
            "Train Epoch: 235 [1000/1000 (100%)]\tLosses F.softmax: 0.189304 log_softmax: 0.133651\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7147\tAccuracy: 7789.0/10000 (78%)\n",
            "log_softmax: Loss: 0.6601\tAccuracy: 7991.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 236 [0/1000 (0%)]\tLosses F.softmax: 2.957641 log_softmax: 2.299945\n",
            "Train Epoch: 236 [200/1000 (20%)]\tLosses F.softmax: 0.039579 log_softmax: 0.021907\n",
            "Train Epoch: 236 [400/1000 (40%)]\tLosses F.softmax: 2.294851 log_softmax: 1.434895\n",
            "Train Epoch: 236 [600/1000 (60%)]\tLosses F.softmax: 0.534716 log_softmax: 0.333918\n",
            "Train Epoch: 236 [800/1000 (80%)]\tLosses F.softmax: 0.125533 log_softmax: 0.396671\n",
            "Train Epoch: 236 [1000/1000 (100%)]\tLosses F.softmax: 0.061005 log_softmax: 0.273886\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7128\tAccuracy: 7766.0/10000 (78%)\n",
            "log_softmax: Loss: 0.6601\tAccuracy: 7956.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 237 [0/1000 (0%)]\tLosses F.softmax: 0.073158 log_softmax: 0.044230\n",
            "Train Epoch: 237 [200/1000 (20%)]\tLosses F.softmax: 0.211080 log_softmax: 0.172551\n",
            "Train Epoch: 237 [400/1000 (40%)]\tLosses F.softmax: 0.462055 log_softmax: 0.337228\n",
            "Train Epoch: 237 [600/1000 (60%)]\tLosses F.softmax: 0.068226 log_softmax: 0.032923\n",
            "Train Epoch: 237 [800/1000 (80%)]\tLosses F.softmax: 0.026721 log_softmax: 0.016344\n",
            "Train Epoch: 237 [1000/1000 (100%)]\tLosses F.softmax: 2.084343 log_softmax: 0.964322\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7116\tAccuracy: 7799.0/10000 (78%)\n",
            "log_softmax: Loss: 0.6608\tAccuracy: 7967.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 238 [0/1000 (0%)]\tLosses F.softmax: 0.056781 log_softmax: 0.074491\n",
            "Train Epoch: 238 [200/1000 (20%)]\tLosses F.softmax: 0.038934 log_softmax: 0.061838\n",
            "Train Epoch: 238 [400/1000 (40%)]\tLosses F.softmax: 0.218788 log_softmax: 0.172165\n",
            "Train Epoch: 238 [600/1000 (60%)]\tLosses F.softmax: 0.081675 log_softmax: 0.059143\n",
            "Train Epoch: 238 [800/1000 (80%)]\tLosses F.softmax: 0.032877 log_softmax: 0.020269\n",
            "Train Epoch: 238 [1000/1000 (100%)]\tLosses F.softmax: 0.021066 log_softmax: 0.009389\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7058\tAccuracy: 7793.0/10000 (78%)\n",
            "log_softmax: Loss: 0.6535\tAccuracy: 7987.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 239 [0/1000 (0%)]\tLosses F.softmax: 0.225184 log_softmax: 0.118662\n",
            "Train Epoch: 239 [200/1000 (20%)]\tLosses F.softmax: 0.016569 log_softmax: 0.015763\n",
            "Train Epoch: 239 [400/1000 (40%)]\tLosses F.softmax: 0.038080 log_softmax: 0.033696\n",
            "Train Epoch: 239 [600/1000 (60%)]\tLosses F.softmax: 0.099758 log_softmax: 0.191399\n",
            "Train Epoch: 239 [800/1000 (80%)]\tLosses F.softmax: 0.217365 log_softmax: 0.172626\n",
            "Train Epoch: 239 [1000/1000 (100%)]\tLosses F.softmax: 0.036360 log_softmax: 0.040255\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7049\tAccuracy: 7815.0/10000 (78%)\n",
            "log_softmax: Loss: 0.6533\tAccuracy: 7994.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 240 [0/1000 (0%)]\tLosses F.softmax: 0.375768 log_softmax: 0.158388\n",
            "Train Epoch: 240 [200/1000 (20%)]\tLosses F.softmax: 0.064681 log_softmax: 0.022702\n",
            "Train Epoch: 240 [400/1000 (40%)]\tLosses F.softmax: 0.268429 log_softmax: 0.230026\n",
            "Train Epoch: 240 [600/1000 (60%)]\tLosses F.softmax: 0.291751 log_softmax: 0.250468\n",
            "Train Epoch: 240 [800/1000 (80%)]\tLosses F.softmax: 2.041959 log_softmax: 1.343069\n",
            "Train Epoch: 240 [1000/1000 (100%)]\tLosses F.softmax: 0.033192 log_softmax: 0.020858\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7050\tAccuracy: 7804.0/10000 (78%)\n",
            "log_softmax: Loss: 0.6542\tAccuracy: 7979.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 241 [0/1000 (0%)]\tLosses F.softmax: 0.013938 log_softmax: 0.009854\n",
            "Train Epoch: 241 [200/1000 (20%)]\tLosses F.softmax: 4.623547 log_softmax: 2.358649\n",
            "Train Epoch: 241 [400/1000 (40%)]\tLosses F.softmax: 0.056153 log_softmax: 0.071739\n",
            "Train Epoch: 241 [600/1000 (60%)]\tLosses F.softmax: 0.063942 log_softmax: 0.036787\n",
            "Train Epoch: 241 [800/1000 (80%)]\tLosses F.softmax: 0.278648 log_softmax: 0.225901\n",
            "Train Epoch: 241 [1000/1000 (100%)]\tLosses F.softmax: 0.366834 log_softmax: 0.356034\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7052\tAccuracy: 7808.0/10000 (78%)\n",
            "log_softmax: Loss: 0.6567\tAccuracy: 7972.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 242 [0/1000 (0%)]\tLosses F.softmax: 0.230859 log_softmax: 0.108774\n",
            "Train Epoch: 242 [200/1000 (20%)]\tLosses F.softmax: 0.360554 log_softmax: 0.299432\n",
            "Train Epoch: 242 [400/1000 (40%)]\tLosses F.softmax: 0.011893 log_softmax: 0.010996\n",
            "Train Epoch: 242 [600/1000 (60%)]\tLosses F.softmax: 0.022532 log_softmax: 0.012467\n",
            "Train Epoch: 242 [800/1000 (80%)]\tLosses F.softmax: 0.847794 log_softmax: 0.628480\n",
            "Train Epoch: 242 [1000/1000 (100%)]\tLosses F.softmax: 0.028708 log_softmax: 0.010581\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7066\tAccuracy: 7823.0/10000 (78%)\n",
            "log_softmax: Loss: 0.6549\tAccuracy: 7969.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 243 [0/1000 (0%)]\tLosses F.softmax: 0.263024 log_softmax: 0.376903\n",
            "Train Epoch: 243 [200/1000 (20%)]\tLosses F.softmax: 0.262932 log_softmax: 0.192122\n",
            "Train Epoch: 243 [400/1000 (40%)]\tLosses F.softmax: 0.517977 log_softmax: 0.278753\n",
            "Train Epoch: 243 [600/1000 (60%)]\tLosses F.softmax: 0.035185 log_softmax: 0.021995\n",
            "Train Epoch: 243 [800/1000 (80%)]\tLosses F.softmax: 0.273616 log_softmax: 0.112225\n",
            "Train Epoch: 243 [1000/1000 (100%)]\tLosses F.softmax: 0.026866 log_softmax: 0.017152\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7021\tAccuracy: 7843.0/10000 (78%)\n",
            "log_softmax: Loss: 0.6532\tAccuracy: 8007.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 244 [0/1000 (0%)]\tLosses F.softmax: 0.252004 log_softmax: 0.188399\n",
            "Train Epoch: 244 [200/1000 (20%)]\tLosses F.softmax: 0.036349 log_softmax: 0.040528\n",
            "Train Epoch: 244 [400/1000 (40%)]\tLosses F.softmax: 0.137353 log_softmax: 0.144118\n",
            "Train Epoch: 244 [600/1000 (60%)]\tLosses F.softmax: 0.017100 log_softmax: 0.031023\n",
            "Train Epoch: 244 [800/1000 (80%)]\tLosses F.softmax: 0.074934 log_softmax: 0.230844\n",
            "Train Epoch: 244 [1000/1000 (100%)]\tLosses F.softmax: 0.035556 log_softmax: 0.021754\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7013\tAccuracy: 7836.0/10000 (78%)\n",
            "log_softmax: Loss: 0.6514\tAccuracy: 8006.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 245 [0/1000 (0%)]\tLosses F.softmax: 1.079108 log_softmax: 0.814001\n",
            "Train Epoch: 245 [200/1000 (20%)]\tLosses F.softmax: 0.015495 log_softmax: 0.023243\n",
            "Train Epoch: 245 [400/1000 (40%)]\tLosses F.softmax: 0.694733 log_softmax: 0.679290\n",
            "Train Epoch: 245 [600/1000 (60%)]\tLosses F.softmax: 0.252007 log_softmax: 0.315924\n",
            "Train Epoch: 245 [800/1000 (80%)]\tLosses F.softmax: 0.054697 log_softmax: 0.006459\n",
            "Train Epoch: 245 [1000/1000 (100%)]\tLosses F.softmax: 0.228004 log_softmax: 0.196460\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7024\tAccuracy: 7833.0/10000 (78%)\n",
            "log_softmax: Loss: 0.6534\tAccuracy: 7993.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 246 [0/1000 (0%)]\tLosses F.softmax: 0.389334 log_softmax: 0.316684\n",
            "Train Epoch: 246 [200/1000 (20%)]\tLosses F.softmax: 0.239794 log_softmax: 0.157190\n",
            "Train Epoch: 246 [400/1000 (40%)]\tLosses F.softmax: 0.173205 log_softmax: 0.078609\n",
            "Train Epoch: 246 [600/1000 (60%)]\tLosses F.softmax: 3.493092 log_softmax: 3.402227\n",
            "Train Epoch: 246 [800/1000 (80%)]\tLosses F.softmax: 0.124252 log_softmax: 0.136479\n",
            "Train Epoch: 246 [1000/1000 (100%)]\tLosses F.softmax: 1.494309 log_softmax: 1.072299\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7050\tAccuracy: 7796.0/10000 (78%)\n",
            "log_softmax: Loss: 0.6565\tAccuracy: 7959.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 247 [0/1000 (0%)]\tLosses F.softmax: 0.283696 log_softmax: 0.208897\n",
            "Train Epoch: 247 [200/1000 (20%)]\tLosses F.softmax: 0.009300 log_softmax: 0.009228\n",
            "Train Epoch: 247 [400/1000 (40%)]\tLosses F.softmax: 0.220114 log_softmax: 0.168338\n",
            "Train Epoch: 247 [600/1000 (60%)]\tLosses F.softmax: 1.160110 log_softmax: 0.661997\n",
            "Train Epoch: 247 [800/1000 (80%)]\tLosses F.softmax: 0.183101 log_softmax: 0.134801\n",
            "Train Epoch: 247 [1000/1000 (100%)]\tLosses F.softmax: 5.121810 log_softmax: 3.290862\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6988\tAccuracy: 7856.0/10000 (79%)\n",
            "log_softmax: Loss: 0.6508\tAccuracy: 8016.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 248 [0/1000 (0%)]\tLosses F.softmax: 0.280161 log_softmax: 0.233528\n",
            "Train Epoch: 248 [200/1000 (20%)]\tLosses F.softmax: 1.462036 log_softmax: 1.252230\n",
            "Train Epoch: 248 [400/1000 (40%)]\tLosses F.softmax: 1.301768 log_softmax: 0.634863\n",
            "Train Epoch: 248 [600/1000 (60%)]\tLosses F.softmax: 0.994726 log_softmax: 0.499220\n",
            "Train Epoch: 248 [800/1000 (80%)]\tLosses F.softmax: 0.009152 log_softmax: 0.003453\n",
            "Train Epoch: 248 [1000/1000 (100%)]\tLosses F.softmax: 0.239998 log_softmax: 0.160967\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6947\tAccuracy: 7871.0/10000 (79%)\n",
            "log_softmax: Loss: 0.6487\tAccuracy: 8024.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 249 [0/1000 (0%)]\tLosses F.softmax: 1.047739 log_softmax: 0.955247\n",
            "Train Epoch: 249 [200/1000 (20%)]\tLosses F.softmax: 0.437246 log_softmax: 0.282623\n",
            "Train Epoch: 249 [400/1000 (40%)]\tLosses F.softmax: 0.044313 log_softmax: 0.037615\n",
            "Train Epoch: 249 [600/1000 (60%)]\tLosses F.softmax: 0.178903 log_softmax: 0.168697\n",
            "Train Epoch: 249 [800/1000 (80%)]\tLosses F.softmax: 0.025050 log_softmax: 0.008173\n",
            "Train Epoch: 249 [1000/1000 (100%)]\tLosses F.softmax: 1.593435 log_softmax: 1.012065\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6940\tAccuracy: 7861.0/10000 (79%)\n",
            "log_softmax: Loss: 0.6479\tAccuracy: 8019.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 250 [0/1000 (0%)]\tLosses F.softmax: 2.621417 log_softmax: 2.970465\n",
            "Train Epoch: 250 [200/1000 (20%)]\tLosses F.softmax: 0.102395 log_softmax: 0.083426\n",
            "Train Epoch: 250 [400/1000 (40%)]\tLosses F.softmax: 0.085585 log_softmax: 0.051977\n",
            "Train Epoch: 250 [600/1000 (60%)]\tLosses F.softmax: 0.144396 log_softmax: 0.072128\n",
            "Train Epoch: 250 [800/1000 (80%)]\tLosses F.softmax: 0.748626 log_softmax: 0.847477\n",
            "Train Epoch: 250 [1000/1000 (100%)]\tLosses F.softmax: 0.285410 log_softmax: 0.207237\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6915\tAccuracy: 7899.0/10000 (79%)\n",
            "log_softmax: Loss: 0.6451\tAccuracy: 8042.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 251 [0/1000 (0%)]\tLosses F.softmax: 0.036337 log_softmax: 0.041823\n",
            "Train Epoch: 251 [200/1000 (20%)]\tLosses F.softmax: 0.003099 log_softmax: 0.002010\n",
            "Train Epoch: 251 [400/1000 (40%)]\tLosses F.softmax: 0.150197 log_softmax: 0.094321\n",
            "Train Epoch: 251 [600/1000 (60%)]\tLosses F.softmax: 0.011232 log_softmax: 0.004350\n",
            "Train Epoch: 251 [800/1000 (80%)]\tLosses F.softmax: 0.019880 log_softmax: 0.008164\n",
            "Train Epoch: 251 [1000/1000 (100%)]\tLosses F.softmax: 1.064649 log_softmax: 0.817729\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6951\tAccuracy: 7867.0/10000 (79%)\n",
            "log_softmax: Loss: 0.6487\tAccuracy: 8015.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 252 [0/1000 (0%)]\tLosses F.softmax: 0.057575 log_softmax: 0.025343\n",
            "Train Epoch: 252 [200/1000 (20%)]\tLosses F.softmax: 2.901218 log_softmax: 4.064756\n",
            "Train Epoch: 252 [400/1000 (40%)]\tLosses F.softmax: 0.096244 log_softmax: 0.038183\n",
            "Train Epoch: 252 [600/1000 (60%)]\tLosses F.softmax: 0.070608 log_softmax: 0.027107\n",
            "Train Epoch: 252 [800/1000 (80%)]\tLosses F.softmax: 0.030078 log_softmax: 0.013100\n",
            "Train Epoch: 252 [1000/1000 (100%)]\tLosses F.softmax: 0.011033 log_softmax: 0.014684\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6967\tAccuracy: 7830.0/10000 (78%)\n",
            "log_softmax: Loss: 0.6497\tAccuracy: 7984.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 253 [0/1000 (0%)]\tLosses F.softmax: 0.764912 log_softmax: 0.775817\n",
            "Train Epoch: 253 [200/1000 (20%)]\tLosses F.softmax: 0.983982 log_softmax: 0.805702\n",
            "Train Epoch: 253 [400/1000 (40%)]\tLosses F.softmax: 0.778855 log_softmax: 0.309900\n",
            "Train Epoch: 253 [600/1000 (60%)]\tLosses F.softmax: 4.126754 log_softmax: 3.299310\n",
            "Train Epoch: 253 [800/1000 (80%)]\tLosses F.softmax: 0.334651 log_softmax: 0.307918\n",
            "Train Epoch: 253 [1000/1000 (100%)]\tLosses F.softmax: 0.288832 log_softmax: 0.141981\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6925\tAccuracy: 7891.0/10000 (79%)\n",
            "log_softmax: Loss: 0.6468\tAccuracy: 8018.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 254 [0/1000 (0%)]\tLosses F.softmax: 0.011739 log_softmax: 0.005979\n",
            "Train Epoch: 254 [200/1000 (20%)]\tLosses F.softmax: 0.176434 log_softmax: 0.128925\n",
            "Train Epoch: 254 [400/1000 (40%)]\tLosses F.softmax: 0.007504 log_softmax: 0.002337\n",
            "Train Epoch: 254 [600/1000 (60%)]\tLosses F.softmax: 0.003033 log_softmax: 0.002338\n",
            "Train Epoch: 254 [800/1000 (80%)]\tLosses F.softmax: 0.118612 log_softmax: 0.037694\n",
            "Train Epoch: 254 [1000/1000 (100%)]\tLosses F.softmax: 0.361786 log_softmax: 0.176312\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6900\tAccuracy: 7886.0/10000 (79%)\n",
            "log_softmax: Loss: 0.6435\tAccuracy: 8036.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 255 [0/1000 (0%)]\tLosses F.softmax: 0.028350 log_softmax: 0.006771\n",
            "Train Epoch: 255 [200/1000 (20%)]\tLosses F.softmax: 1.054272 log_softmax: 1.247214\n",
            "Train Epoch: 255 [400/1000 (40%)]\tLosses F.softmax: 0.034450 log_softmax: 0.013464\n",
            "Train Epoch: 255 [600/1000 (60%)]\tLosses F.softmax: 0.017208 log_softmax: 0.003849\n",
            "Train Epoch: 255 [800/1000 (80%)]\tLosses F.softmax: 0.022961 log_softmax: 0.016907\n",
            "Train Epoch: 255 [1000/1000 (100%)]\tLosses F.softmax: 0.376110 log_softmax: 0.219527\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6883\tAccuracy: 7898.0/10000 (79%)\n",
            "log_softmax: Loss: 0.6434\tAccuracy: 8041.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 256 [0/1000 (0%)]\tLosses F.softmax: 0.105964 log_softmax: 0.092101\n",
            "Train Epoch: 256 [200/1000 (20%)]\tLosses F.softmax: 0.127492 log_softmax: 0.101564\n",
            "Train Epoch: 256 [400/1000 (40%)]\tLosses F.softmax: 0.272783 log_softmax: 0.180924\n",
            "Train Epoch: 256 [600/1000 (60%)]\tLosses F.softmax: 0.027484 log_softmax: 0.016965\n",
            "Train Epoch: 256 [800/1000 (80%)]\tLosses F.softmax: 0.059333 log_softmax: 0.030414\n",
            "Train Epoch: 256 [1000/1000 (100%)]\tLosses F.softmax: 0.046822 log_softmax: 0.019842\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6928\tAccuracy: 7862.0/10000 (79%)\n",
            "log_softmax: Loss: 0.6465\tAccuracy: 8021.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 257 [0/1000 (0%)]\tLosses F.softmax: 0.396385 log_softmax: 0.314064\n",
            "Train Epoch: 257 [200/1000 (20%)]\tLosses F.softmax: 0.157209 log_softmax: 0.063267\n",
            "Train Epoch: 257 [400/1000 (40%)]\tLosses F.softmax: 0.524472 log_softmax: 0.466596\n",
            "Train Epoch: 257 [600/1000 (60%)]\tLosses F.softmax: 0.064388 log_softmax: 0.050833\n",
            "Train Epoch: 257 [800/1000 (80%)]\tLosses F.softmax: 0.687062 log_softmax: 0.471204\n",
            "Train Epoch: 257 [1000/1000 (100%)]\tLosses F.softmax: 0.993923 log_softmax: 0.747028\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6869\tAccuracy: 7921.0/10000 (79%)\n",
            "log_softmax: Loss: 0.6430\tAccuracy: 8048.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 258 [0/1000 (0%)]\tLosses F.softmax: 0.075271 log_softmax: 0.073271\n",
            "Train Epoch: 258 [200/1000 (20%)]\tLosses F.softmax: 0.344554 log_softmax: 0.330114\n",
            "Train Epoch: 258 [400/1000 (40%)]\tLosses F.softmax: 1.521735 log_softmax: 1.606413\n",
            "Train Epoch: 258 [600/1000 (60%)]\tLosses F.softmax: 0.036091 log_softmax: 0.014730\n",
            "Train Epoch: 258 [800/1000 (80%)]\tLosses F.softmax: 0.062420 log_softmax: 0.042433\n",
            "Train Epoch: 258 [1000/1000 (100%)]\tLosses F.softmax: 0.318029 log_softmax: 0.273921\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6859\tAccuracy: 7899.0/10000 (79%)\n",
            "log_softmax: Loss: 0.6426\tAccuracy: 8021.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 259 [0/1000 (0%)]\tLosses F.softmax: 0.062907 log_softmax: 0.004174\n",
            "Train Epoch: 259 [200/1000 (20%)]\tLosses F.softmax: 3.926109 log_softmax: 3.141544\n",
            "Train Epoch: 259 [400/1000 (40%)]\tLosses F.softmax: 1.049359 log_softmax: 1.234245\n",
            "Train Epoch: 259 [600/1000 (60%)]\tLosses F.softmax: 0.021735 log_softmax: 0.013479\n",
            "Train Epoch: 259 [800/1000 (80%)]\tLosses F.softmax: 0.946618 log_softmax: 0.748012\n",
            "Train Epoch: 259 [1000/1000 (100%)]\tLosses F.softmax: 0.092904 log_softmax: 0.069338\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6864\tAccuracy: 7954.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6412\tAccuracy: 8087.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 260 [0/1000 (0%)]\tLosses F.softmax: 0.005814 log_softmax: 0.002326\n",
            "Train Epoch: 260 [200/1000 (20%)]\tLosses F.softmax: 0.031219 log_softmax: 0.030393\n",
            "Train Epoch: 260 [400/1000 (40%)]\tLosses F.softmax: 0.224643 log_softmax: 0.133510\n",
            "Train Epoch: 260 [600/1000 (60%)]\tLosses F.softmax: 0.910749 log_softmax: 0.414719\n",
            "Train Epoch: 260 [800/1000 (80%)]\tLosses F.softmax: 0.001425 log_softmax: 0.000826\n",
            "Train Epoch: 260 [1000/1000 (100%)]\tLosses F.softmax: 0.062119 log_softmax: 0.036713\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6842\tAccuracy: 7915.0/10000 (79%)\n",
            "log_softmax: Loss: 0.6410\tAccuracy: 8044.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 261 [0/1000 (0%)]\tLosses F.softmax: 0.649833 log_softmax: 1.152463\n",
            "Train Epoch: 261 [200/1000 (20%)]\tLosses F.softmax: 0.781107 log_softmax: 0.300164\n",
            "Train Epoch: 261 [400/1000 (40%)]\tLosses F.softmax: 1.756311 log_softmax: 1.497398\n",
            "Train Epoch: 261 [600/1000 (60%)]\tLosses F.softmax: 0.099091 log_softmax: 0.032214\n",
            "Train Epoch: 261 [800/1000 (80%)]\tLosses F.softmax: 1.286008 log_softmax: 0.665716\n",
            "Train Epoch: 261 [1000/1000 (100%)]\tLosses F.softmax: 0.455931 log_softmax: 0.215359\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6852\tAccuracy: 7918.0/10000 (79%)\n",
            "log_softmax: Loss: 0.6399\tAccuracy: 8050.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 262 [0/1000 (0%)]\tLosses F.softmax: 0.655320 log_softmax: 1.119299\n",
            "Train Epoch: 262 [200/1000 (20%)]\tLosses F.softmax: 0.244288 log_softmax: 0.121738\n",
            "Train Epoch: 262 [400/1000 (40%)]\tLosses F.softmax: 0.198724 log_softmax: 0.186210\n",
            "Train Epoch: 262 [600/1000 (60%)]\tLosses F.softmax: 0.051344 log_softmax: 0.050739\n",
            "Train Epoch: 262 [800/1000 (80%)]\tLosses F.softmax: 0.045936 log_softmax: 0.050790\n",
            "Train Epoch: 262 [1000/1000 (100%)]\tLosses F.softmax: 1.447695 log_softmax: 1.298544\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6813\tAccuracy: 7932.0/10000 (79%)\n",
            "log_softmax: Loss: 0.6381\tAccuracy: 8058.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 263 [0/1000 (0%)]\tLosses F.softmax: 0.157476 log_softmax: 0.111371\n",
            "Train Epoch: 263 [200/1000 (20%)]\tLosses F.softmax: 0.048380 log_softmax: 0.023270\n",
            "Train Epoch: 263 [400/1000 (40%)]\tLosses F.softmax: 0.524356 log_softmax: 0.350264\n",
            "Train Epoch: 263 [600/1000 (60%)]\tLosses F.softmax: 1.362051 log_softmax: 1.017124\n",
            "Train Epoch: 263 [800/1000 (80%)]\tLosses F.softmax: 0.043721 log_softmax: 0.031583\n",
            "Train Epoch: 263 [1000/1000 (100%)]\tLosses F.softmax: 0.050521 log_softmax: 0.026669\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6815\tAccuracy: 7940.0/10000 (79%)\n",
            "log_softmax: Loss: 0.6388\tAccuracy: 8067.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 264 [0/1000 (0%)]\tLosses F.softmax: 0.011115 log_softmax: 0.003527\n",
            "Train Epoch: 264 [200/1000 (20%)]\tLosses F.softmax: 0.508777 log_softmax: 0.431241\n",
            "Train Epoch: 264 [400/1000 (40%)]\tLosses F.softmax: 0.010199 log_softmax: 0.012408\n",
            "Train Epoch: 264 [600/1000 (60%)]\tLosses F.softmax: 0.241587 log_softmax: 0.223251\n",
            "Train Epoch: 264 [800/1000 (80%)]\tLosses F.softmax: 0.156640 log_softmax: 0.204390\n",
            "Train Epoch: 264 [1000/1000 (100%)]\tLosses F.softmax: 0.242767 log_softmax: 0.220073\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6823\tAccuracy: 7955.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6411\tAccuracy: 8066.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 265 [0/1000 (0%)]\tLosses F.softmax: 0.022030 log_softmax: 0.014599\n",
            "Train Epoch: 265 [200/1000 (20%)]\tLosses F.softmax: 0.356098 log_softmax: 0.354507\n",
            "Train Epoch: 265 [400/1000 (40%)]\tLosses F.softmax: 0.023141 log_softmax: 0.017447\n",
            "Train Epoch: 265 [600/1000 (60%)]\tLosses F.softmax: 0.245683 log_softmax: 0.109272\n",
            "Train Epoch: 265 [800/1000 (80%)]\tLosses F.softmax: 0.141424 log_softmax: 0.098016\n",
            "Train Epoch: 265 [1000/1000 (100%)]\tLosses F.softmax: 0.061591 log_softmax: 0.065652\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6841\tAccuracy: 7932.0/10000 (79%)\n",
            "log_softmax: Loss: 0.6417\tAccuracy: 8064.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 266 [0/1000 (0%)]\tLosses F.softmax: 0.618588 log_softmax: 0.295579\n",
            "Train Epoch: 266 [200/1000 (20%)]\tLosses F.softmax: 0.017924 log_softmax: 0.018098\n",
            "Train Epoch: 266 [400/1000 (40%)]\tLosses F.softmax: 0.010850 log_softmax: 0.004607\n",
            "Train Epoch: 266 [600/1000 (60%)]\tLosses F.softmax: 2.491693 log_softmax: 1.927097\n",
            "Train Epoch: 266 [800/1000 (80%)]\tLosses F.softmax: 0.031749 log_softmax: 0.022922\n",
            "Train Epoch: 266 [1000/1000 (100%)]\tLosses F.softmax: 0.004834 log_softmax: 0.003037\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6816\tAccuracy: 7966.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6395\tAccuracy: 8074.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 267 [0/1000 (0%)]\tLosses F.softmax: 0.190059 log_softmax: 0.174895\n",
            "Train Epoch: 267 [200/1000 (20%)]\tLosses F.softmax: 0.023378 log_softmax: 0.014447\n",
            "Train Epoch: 267 [400/1000 (40%)]\tLosses F.softmax: 2.695287 log_softmax: 0.979428\n",
            "Train Epoch: 267 [600/1000 (60%)]\tLosses F.softmax: 0.103634 log_softmax: 0.074385\n",
            "Train Epoch: 267 [800/1000 (80%)]\tLosses F.softmax: 0.104890 log_softmax: 0.097606\n",
            "Train Epoch: 267 [1000/1000 (100%)]\tLosses F.softmax: 0.194788 log_softmax: 0.216148\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6802\tAccuracy: 7940.0/10000 (79%)\n",
            "log_softmax: Loss: 0.6390\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 268 [0/1000 (0%)]\tLosses F.softmax: 0.225896 log_softmax: 0.187321\n",
            "Train Epoch: 268 [200/1000 (20%)]\tLosses F.softmax: 0.193682 log_softmax: 0.102499\n",
            "Train Epoch: 268 [400/1000 (40%)]\tLosses F.softmax: 0.011261 log_softmax: 0.007954\n",
            "Train Epoch: 268 [600/1000 (60%)]\tLosses F.softmax: 0.290233 log_softmax: 0.238904\n",
            "Train Epoch: 268 [800/1000 (80%)]\tLosses F.softmax: 2.295900 log_softmax: 1.679291\n",
            "Train Epoch: 268 [1000/1000 (100%)]\tLosses F.softmax: 0.001410 log_softmax: 0.001638\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6785\tAccuracy: 8003.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6368\tAccuracy: 8090.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 269 [0/1000 (0%)]\tLosses F.softmax: 0.007386 log_softmax: 0.003493\n",
            "Train Epoch: 269 [200/1000 (20%)]\tLosses F.softmax: 0.267906 log_softmax: 0.379898\n",
            "Train Epoch: 269 [400/1000 (40%)]\tLosses F.softmax: 0.364189 log_softmax: 0.243232\n",
            "Train Epoch: 269 [600/1000 (60%)]\tLosses F.softmax: 1.452637 log_softmax: 0.720244\n",
            "Train Epoch: 269 [800/1000 (80%)]\tLosses F.softmax: 0.079124 log_softmax: 0.037290\n",
            "Train Epoch: 269 [1000/1000 (100%)]\tLosses F.softmax: 0.085026 log_softmax: 0.066133\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6833\tAccuracy: 7942.0/10000 (79%)\n",
            "log_softmax: Loss: 0.6402\tAccuracy: 8068.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 270 [0/1000 (0%)]\tLosses F.softmax: 0.232231 log_softmax: 0.157736\n",
            "Train Epoch: 270 [200/1000 (20%)]\tLosses F.softmax: 0.060802 log_softmax: 0.043872\n",
            "Train Epoch: 270 [400/1000 (40%)]\tLosses F.softmax: 0.134802 log_softmax: 0.070060\n",
            "Train Epoch: 270 [600/1000 (60%)]\tLosses F.softmax: 0.661499 log_softmax: 0.539912\n",
            "Train Epoch: 270 [800/1000 (80%)]\tLosses F.softmax: 0.022603 log_softmax: 0.008908\n",
            "Train Epoch: 270 [1000/1000 (100%)]\tLosses F.softmax: 0.051508 log_softmax: 0.014804\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6853\tAccuracy: 7949.0/10000 (79%)\n",
            "log_softmax: Loss: 0.6417\tAccuracy: 8063.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 271 [0/1000 (0%)]\tLosses F.softmax: 0.502979 log_softmax: 0.682729\n",
            "Train Epoch: 271 [200/1000 (20%)]\tLosses F.softmax: 0.054277 log_softmax: 0.049152\n",
            "Train Epoch: 271 [400/1000 (40%)]\tLosses F.softmax: 0.051615 log_softmax: 0.047890\n",
            "Train Epoch: 271 [600/1000 (60%)]\tLosses F.softmax: 0.107484 log_softmax: 0.032153\n",
            "Train Epoch: 271 [800/1000 (80%)]\tLosses F.softmax: 0.994435 log_softmax: 0.862288\n",
            "Train Epoch: 271 [1000/1000 (100%)]\tLosses F.softmax: 0.311848 log_softmax: 0.144370\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6843\tAccuracy: 7929.0/10000 (79%)\n",
            "log_softmax: Loss: 0.6435\tAccuracy: 8048.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 272 [0/1000 (0%)]\tLosses F.softmax: 0.003264 log_softmax: 0.001262\n",
            "Train Epoch: 272 [200/1000 (20%)]\tLosses F.softmax: 0.083293 log_softmax: 0.021789\n",
            "Train Epoch: 272 [400/1000 (40%)]\tLosses F.softmax: 0.097869 log_softmax: 0.076352\n",
            "Train Epoch: 272 [600/1000 (60%)]\tLosses F.softmax: 0.383172 log_softmax: 0.110128\n",
            "Train Epoch: 272 [800/1000 (80%)]\tLosses F.softmax: 0.012480 log_softmax: 0.004335\n",
            "Train Epoch: 272 [1000/1000 (100%)]\tLosses F.softmax: 0.107548 log_softmax: 0.041609\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6897\tAccuracy: 7896.0/10000 (79%)\n",
            "log_softmax: Loss: 0.6513\tAccuracy: 7985.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 273 [0/1000 (0%)]\tLosses F.softmax: 0.015055 log_softmax: 0.007641\n",
            "Train Epoch: 273 [200/1000 (20%)]\tLosses F.softmax: 0.003536 log_softmax: 0.000768\n",
            "Train Epoch: 273 [400/1000 (40%)]\tLosses F.softmax: 0.341637 log_softmax: 0.284666\n",
            "Train Epoch: 273 [600/1000 (60%)]\tLosses F.softmax: 0.150908 log_softmax: 0.148688\n",
            "Train Epoch: 273 [800/1000 (80%)]\tLosses F.softmax: 0.003893 log_softmax: 0.003892\n",
            "Train Epoch: 273 [1000/1000 (100%)]\tLosses F.softmax: 0.532531 log_softmax: 0.788225\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6825\tAccuracy: 7932.0/10000 (79%)\n",
            "log_softmax: Loss: 0.6442\tAccuracy: 8045.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 274 [0/1000 (0%)]\tLosses F.softmax: 0.167219 log_softmax: 0.102305\n",
            "Train Epoch: 274 [200/1000 (20%)]\tLosses F.softmax: 0.060472 log_softmax: 0.052471\n",
            "Train Epoch: 274 [400/1000 (40%)]\tLosses F.softmax: 0.134792 log_softmax: 0.104755\n",
            "Train Epoch: 274 [600/1000 (60%)]\tLosses F.softmax: 0.206970 log_softmax: 0.110194\n",
            "Train Epoch: 274 [800/1000 (80%)]\tLosses F.softmax: 0.003558 log_softmax: 0.000993\n",
            "Train Epoch: 274 [1000/1000 (100%)]\tLosses F.softmax: 0.055630 log_softmax: 0.036385\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6809\tAccuracy: 7966.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6393\tAccuracy: 8090.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 275 [0/1000 (0%)]\tLosses F.softmax: 0.014312 log_softmax: 0.004004\n",
            "Train Epoch: 275 [200/1000 (20%)]\tLosses F.softmax: 0.019113 log_softmax: 0.008541\n",
            "Train Epoch: 275 [400/1000 (40%)]\tLosses F.softmax: 4.840926 log_softmax: 2.317487\n",
            "Train Epoch: 275 [600/1000 (60%)]\tLosses F.softmax: 0.041504 log_softmax: 0.017657\n",
            "Train Epoch: 275 [800/1000 (80%)]\tLosses F.softmax: 0.030988 log_softmax: 0.011625\n",
            "Train Epoch: 275 [1000/1000 (100%)]\tLosses F.softmax: 0.337269 log_softmax: 0.337616\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6778\tAccuracy: 7962.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6410\tAccuracy: 8080.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 276 [0/1000 (0%)]\tLosses F.softmax: 0.015307 log_softmax: 0.018414\n",
            "Train Epoch: 276 [200/1000 (20%)]\tLosses F.softmax: 1.187164 log_softmax: 0.888070\n",
            "Train Epoch: 276 [400/1000 (40%)]\tLosses F.softmax: 0.167485 log_softmax: 0.103582\n",
            "Train Epoch: 276 [600/1000 (60%)]\tLosses F.softmax: 0.004604 log_softmax: 0.003215\n",
            "Train Epoch: 276 [800/1000 (80%)]\tLosses F.softmax: 0.114951 log_softmax: 0.070517\n",
            "Train Epoch: 276 [1000/1000 (100%)]\tLosses F.softmax: 0.269295 log_softmax: 0.226631\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6823\tAccuracy: 7937.0/10000 (79%)\n",
            "log_softmax: Loss: 0.6437\tAccuracy: 8047.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 277 [0/1000 (0%)]\tLosses F.softmax: 0.014201 log_softmax: 0.016653\n",
            "Train Epoch: 277 [200/1000 (20%)]\tLosses F.softmax: 0.019259 log_softmax: 0.007723\n",
            "Train Epoch: 277 [400/1000 (40%)]\tLosses F.softmax: 0.054798 log_softmax: 0.105724\n",
            "Train Epoch: 277 [600/1000 (60%)]\tLosses F.softmax: 0.140110 log_softmax: 0.195542\n",
            "Train Epoch: 277 [800/1000 (80%)]\tLosses F.softmax: 0.028278 log_softmax: 0.019594\n",
            "Train Epoch: 277 [1000/1000 (100%)]\tLosses F.softmax: 0.063591 log_softmax: 0.011230\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6775\tAccuracy: 7962.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6409\tAccuracy: 8060.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 278 [0/1000 (0%)]\tLosses F.softmax: 0.013548 log_softmax: 0.033303\n",
            "Train Epoch: 278 [200/1000 (20%)]\tLosses F.softmax: 0.855043 log_softmax: 0.964968\n",
            "Train Epoch: 278 [400/1000 (40%)]\tLosses F.softmax: 0.203179 log_softmax: 0.163168\n",
            "Train Epoch: 278 [600/1000 (60%)]\tLosses F.softmax: 0.247354 log_softmax: 0.150949\n",
            "Train Epoch: 278 [800/1000 (80%)]\tLosses F.softmax: 0.030929 log_softmax: 0.021517\n",
            "Train Epoch: 278 [1000/1000 (100%)]\tLosses F.softmax: 0.119840 log_softmax: 0.116054\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6791\tAccuracy: 7981.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6421\tAccuracy: 8060.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 279 [0/1000 (0%)]\tLosses F.softmax: 0.004445 log_softmax: 0.002907\n",
            "Train Epoch: 279 [200/1000 (20%)]\tLosses F.softmax: 0.018736 log_softmax: 0.008778\n",
            "Train Epoch: 279 [400/1000 (40%)]\tLosses F.softmax: 0.019061 log_softmax: 0.019335\n",
            "Train Epoch: 279 [600/1000 (60%)]\tLosses F.softmax: 0.867873 log_softmax: 0.392619\n",
            "Train Epoch: 279 [800/1000 (80%)]\tLosses F.softmax: 0.046383 log_softmax: 0.050784\n",
            "Train Epoch: 279 [1000/1000 (100%)]\tLosses F.softmax: 0.033270 log_softmax: 0.031917\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6756\tAccuracy: 7976.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6415\tAccuracy: 8071.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 280 [0/1000 (0%)]\tLosses F.softmax: 0.167896 log_softmax: 0.086696\n",
            "Train Epoch: 280 [200/1000 (20%)]\tLosses F.softmax: 0.017198 log_softmax: 0.021973\n",
            "Train Epoch: 280 [400/1000 (40%)]\tLosses F.softmax: 0.808803 log_softmax: 0.694455\n",
            "Train Epoch: 280 [600/1000 (60%)]\tLosses F.softmax: 0.045612 log_softmax: 0.029540\n",
            "Train Epoch: 280 [800/1000 (80%)]\tLosses F.softmax: 0.010624 log_softmax: 0.008189\n",
            "Train Epoch: 280 [1000/1000 (100%)]\tLosses F.softmax: 0.479759 log_softmax: 0.375567\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6734\tAccuracy: 8003.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6382\tAccuracy: 8098.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 281 [0/1000 (0%)]\tLosses F.softmax: 0.156295 log_softmax: 0.104093\n",
            "Train Epoch: 281 [200/1000 (20%)]\tLosses F.softmax: 0.287505 log_softmax: 0.246447\n",
            "Train Epoch: 281 [400/1000 (40%)]\tLosses F.softmax: 0.002148 log_softmax: 0.001479\n",
            "Train Epoch: 281 [600/1000 (60%)]\tLosses F.softmax: 0.058726 log_softmax: 0.046129\n",
            "Train Epoch: 281 [800/1000 (80%)]\tLosses F.softmax: 0.090497 log_softmax: 0.087624\n",
            "Train Epoch: 281 [1000/1000 (100%)]\tLosses F.softmax: 0.653076 log_softmax: 0.410005\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6780\tAccuracy: 7968.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6417\tAccuracy: 8069.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 282 [0/1000 (0%)]\tLosses F.softmax: 0.205722 log_softmax: 0.078539\n",
            "Train Epoch: 282 [200/1000 (20%)]\tLosses F.softmax: 0.006700 log_softmax: 0.001876\n",
            "Train Epoch: 282 [400/1000 (40%)]\tLosses F.softmax: 1.807866 log_softmax: 0.847034\n",
            "Train Epoch: 282 [600/1000 (60%)]\tLosses F.softmax: 0.042204 log_softmax: 0.033831\n",
            "Train Epoch: 282 [800/1000 (80%)]\tLosses F.softmax: 0.095325 log_softmax: 0.132882\n",
            "Train Epoch: 282 [1000/1000 (100%)]\tLosses F.softmax: 0.101905 log_softmax: 0.073629\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6762\tAccuracy: 7989.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6409\tAccuracy: 8084.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 283 [0/1000 (0%)]\tLosses F.softmax: 1.186260 log_softmax: 0.621275\n",
            "Train Epoch: 283 [200/1000 (20%)]\tLosses F.softmax: 1.260880 log_softmax: 2.042334\n",
            "Train Epoch: 283 [400/1000 (40%)]\tLosses F.softmax: 0.019731 log_softmax: 0.006861\n",
            "Train Epoch: 283 [600/1000 (60%)]\tLosses F.softmax: 0.006065 log_softmax: 0.004486\n",
            "Train Epoch: 283 [800/1000 (80%)]\tLosses F.softmax: 0.124225 log_softmax: 0.147413\n",
            "Train Epoch: 283 [1000/1000 (100%)]\tLosses F.softmax: 0.010520 log_softmax: 0.007343\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6736\tAccuracy: 7997.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6405\tAccuracy: 8086.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 284 [0/1000 (0%)]\tLosses F.softmax: 0.103322 log_softmax: 0.072134\n",
            "Train Epoch: 284 [200/1000 (20%)]\tLosses F.softmax: 0.039674 log_softmax: 0.011582\n",
            "Train Epoch: 284 [400/1000 (40%)]\tLosses F.softmax: 0.242394 log_softmax: 0.155187\n",
            "Train Epoch: 284 [600/1000 (60%)]\tLosses F.softmax: 1.941656 log_softmax: 1.036238\n",
            "Train Epoch: 284 [800/1000 (80%)]\tLosses F.softmax: 0.087460 log_softmax: 0.045943\n",
            "Train Epoch: 284 [1000/1000 (100%)]\tLosses F.softmax: 0.480135 log_softmax: 0.352852\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6820\tAccuracy: 7956.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6442\tAccuracy: 8067.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 285 [0/1000 (0%)]\tLosses F.softmax: 0.089777 log_softmax: 0.022003\n",
            "Train Epoch: 285 [200/1000 (20%)]\tLosses F.softmax: 0.404961 log_softmax: 0.245936\n",
            "Train Epoch: 285 [400/1000 (40%)]\tLosses F.softmax: 0.059805 log_softmax: 0.041021\n",
            "Train Epoch: 285 [600/1000 (60%)]\tLosses F.softmax: 0.158590 log_softmax: 0.054194\n",
            "Train Epoch: 285 [800/1000 (80%)]\tLosses F.softmax: 2.486912 log_softmax: 1.863469\n",
            "Train Epoch: 285 [1000/1000 (100%)]\tLosses F.softmax: 0.019876 log_softmax: 0.014238\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6774\tAccuracy: 7988.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6416\tAccuracy: 8082.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 286 [0/1000 (0%)]\tLosses F.softmax: 0.154693 log_softmax: 0.146842\n",
            "Train Epoch: 286 [200/1000 (20%)]\tLosses F.softmax: 0.878810 log_softmax: 0.661103\n",
            "Train Epoch: 286 [400/1000 (40%)]\tLosses F.softmax: 0.063887 log_softmax: 0.037233\n",
            "Train Epoch: 286 [600/1000 (60%)]\tLosses F.softmax: 0.014728 log_softmax: 0.009210\n",
            "Train Epoch: 286 [800/1000 (80%)]\tLosses F.softmax: 0.059653 log_softmax: 0.049552\n",
            "Train Epoch: 286 [1000/1000 (100%)]\tLosses F.softmax: 0.016267 log_softmax: 0.028935\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6773\tAccuracy: 7983.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6421\tAccuracy: 8084.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 287 [0/1000 (0%)]\tLosses F.softmax: 0.062984 log_softmax: 0.293167\n",
            "Train Epoch: 287 [200/1000 (20%)]\tLosses F.softmax: 0.006000 log_softmax: 0.003947\n",
            "Train Epoch: 287 [400/1000 (40%)]\tLosses F.softmax: 0.089748 log_softmax: 0.044057\n",
            "Train Epoch: 287 [600/1000 (60%)]\tLosses F.softmax: 0.006083 log_softmax: 0.001853\n",
            "Train Epoch: 287 [800/1000 (80%)]\tLosses F.softmax: 0.090251 log_softmax: 0.012636\n",
            "Train Epoch: 287 [1000/1000 (100%)]\tLosses F.softmax: 0.033185 log_softmax: 0.017922\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6759\tAccuracy: 7991.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6440\tAccuracy: 8076.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 288 [0/1000 (0%)]\tLosses F.softmax: 0.127394 log_softmax: 0.116318\n",
            "Train Epoch: 288 [200/1000 (20%)]\tLosses F.softmax: 0.944342 log_softmax: 0.215051\n",
            "Train Epoch: 288 [400/1000 (40%)]\tLosses F.softmax: 0.053729 log_softmax: 0.039246\n",
            "Train Epoch: 288 [600/1000 (60%)]\tLosses F.softmax: 0.004706 log_softmax: 0.003965\n",
            "Train Epoch: 288 [800/1000 (80%)]\tLosses F.softmax: 0.066458 log_softmax: 0.065343\n",
            "Train Epoch: 288 [1000/1000 (100%)]\tLosses F.softmax: 0.002847 log_softmax: 0.001449\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6766\tAccuracy: 7991.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6418\tAccuracy: 8103.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 289 [0/1000 (0%)]\tLosses F.softmax: 0.095012 log_softmax: 0.063013\n",
            "Train Epoch: 289 [200/1000 (20%)]\tLosses F.softmax: 0.002650 log_softmax: 0.001594\n",
            "Train Epoch: 289 [400/1000 (40%)]\tLosses F.softmax: 0.009118 log_softmax: 0.004749\n",
            "Train Epoch: 289 [600/1000 (60%)]\tLosses F.softmax: 3.881320 log_softmax: 4.617302\n",
            "Train Epoch: 289 [800/1000 (80%)]\tLosses F.softmax: 0.116836 log_softmax: 0.098042\n",
            "Train Epoch: 289 [1000/1000 (100%)]\tLosses F.softmax: 0.082649 log_softmax: 0.060137\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6759\tAccuracy: 7998.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6434\tAccuracy: 8105.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 290 [0/1000 (0%)]\tLosses F.softmax: 0.004962 log_softmax: 0.004729\n",
            "Train Epoch: 290 [200/1000 (20%)]\tLosses F.softmax: 0.013814 log_softmax: 0.007459\n",
            "Train Epoch: 290 [400/1000 (40%)]\tLosses F.softmax: 0.001297 log_softmax: 0.003352\n",
            "Train Epoch: 290 [600/1000 (60%)]\tLosses F.softmax: 0.038759 log_softmax: 0.007926\n",
            "Train Epoch: 290 [800/1000 (80%)]\tLosses F.softmax: 1.014904 log_softmax: 0.737480\n",
            "Train Epoch: 290 [1000/1000 (100%)]\tLosses F.softmax: 0.044565 log_softmax: 0.069066\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6788\tAccuracy: 7985.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6466\tAccuracy: 8077.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 291 [0/1000 (0%)]\tLosses F.softmax: 0.012381 log_softmax: 0.009225\n",
            "Train Epoch: 291 [200/1000 (20%)]\tLosses F.softmax: 0.317337 log_softmax: 0.226952\n",
            "Train Epoch: 291 [400/1000 (40%)]\tLosses F.softmax: 0.002125 log_softmax: 0.000854\n",
            "Train Epoch: 291 [600/1000 (60%)]\tLosses F.softmax: 0.049562 log_softmax: 0.059791\n",
            "Train Epoch: 291 [800/1000 (80%)]\tLosses F.softmax: 0.001005 log_softmax: 0.000548\n",
            "Train Epoch: 291 [1000/1000 (100%)]\tLosses F.softmax: 0.561911 log_softmax: 0.396049\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6780\tAccuracy: 8005.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6471\tAccuracy: 8084.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 292 [0/1000 (0%)]\tLosses F.softmax: 0.024051 log_softmax: 0.025342\n",
            "Train Epoch: 292 [200/1000 (20%)]\tLosses F.softmax: 0.013964 log_softmax: 0.010074\n",
            "Train Epoch: 292 [400/1000 (40%)]\tLosses F.softmax: 0.950242 log_softmax: 0.752705\n",
            "Train Epoch: 292 [600/1000 (60%)]\tLosses F.softmax: 0.210954 log_softmax: 0.189255\n",
            "Train Epoch: 292 [800/1000 (80%)]\tLosses F.softmax: 0.601889 log_softmax: 1.090247\n",
            "Train Epoch: 292 [1000/1000 (100%)]\tLosses F.softmax: 0.263936 log_softmax: 0.175142\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6770\tAccuracy: 8007.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6455\tAccuracy: 8096.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 293 [0/1000 (0%)]\tLosses F.softmax: 0.091446 log_softmax: 0.066073\n",
            "Train Epoch: 293 [200/1000 (20%)]\tLosses F.softmax: 0.000963 log_softmax: 0.000524\n",
            "Train Epoch: 293 [400/1000 (40%)]\tLosses F.softmax: 0.062565 log_softmax: 0.049924\n",
            "Train Epoch: 293 [600/1000 (60%)]\tLosses F.softmax: 0.151168 log_softmax: 0.171867\n",
            "Train Epoch: 293 [800/1000 (80%)]\tLosses F.softmax: 0.270952 log_softmax: 0.191159\n",
            "Train Epoch: 293 [1000/1000 (100%)]\tLosses F.softmax: 0.096850 log_softmax: 0.100155\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6851\tAccuracy: 7979.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6532\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 294 [0/1000 (0%)]\tLosses F.softmax: 0.148612 log_softmax: 0.123288\n",
            "Train Epoch: 294 [200/1000 (20%)]\tLosses F.softmax: 0.120844 log_softmax: 0.064037\n",
            "Train Epoch: 294 [400/1000 (40%)]\tLosses F.softmax: 0.029033 log_softmax: 0.008945\n",
            "Train Epoch: 294 [600/1000 (60%)]\tLosses F.softmax: 0.287581 log_softmax: 0.217945\n",
            "Train Epoch: 294 [800/1000 (80%)]\tLosses F.softmax: 0.141238 log_softmax: 0.091426\n",
            "Train Epoch: 294 [1000/1000 (100%)]\tLosses F.softmax: 0.141926 log_softmax: 0.189077\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6757\tAccuracy: 8019.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6455\tAccuracy: 8099.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 295 [0/1000 (0%)]\tLosses F.softmax: 0.009606 log_softmax: 0.004957\n",
            "Train Epoch: 295 [200/1000 (20%)]\tLosses F.softmax: 0.161853 log_softmax: 0.207182\n",
            "Train Epoch: 295 [400/1000 (40%)]\tLosses F.softmax: 0.008661 log_softmax: 0.002915\n",
            "Train Epoch: 295 [600/1000 (60%)]\tLosses F.softmax: 0.703627 log_softmax: 0.240286\n",
            "Train Epoch: 295 [800/1000 (80%)]\tLosses F.softmax: 0.183453 log_softmax: 0.138529\n",
            "Train Epoch: 295 [1000/1000 (100%)]\tLosses F.softmax: 0.149955 log_softmax: 0.101728\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6796\tAccuracy: 8018.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6489\tAccuracy: 8094.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 296 [0/1000 (0%)]\tLosses F.softmax: 0.004757 log_softmax: 0.017512\n",
            "Train Epoch: 296 [200/1000 (20%)]\tLosses F.softmax: 0.027470 log_softmax: 0.024357\n",
            "Train Epoch: 296 [400/1000 (40%)]\tLosses F.softmax: 0.101531 log_softmax: 0.109470\n",
            "Train Epoch: 296 [600/1000 (60%)]\tLosses F.softmax: 0.372421 log_softmax: 0.214058\n",
            "Train Epoch: 296 [800/1000 (80%)]\tLosses F.softmax: 0.003989 log_softmax: 0.003632\n",
            "Train Epoch: 296 [1000/1000 (100%)]\tLosses F.softmax: 0.057982 log_softmax: 0.075897\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6746\tAccuracy: 8024.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6464\tAccuracy: 8102.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 297 [0/1000 (0%)]\tLosses F.softmax: 0.015007 log_softmax: 0.012664\n",
            "Train Epoch: 297 [200/1000 (20%)]\tLosses F.softmax: 0.127994 log_softmax: 0.068449\n",
            "Train Epoch: 297 [400/1000 (40%)]\tLosses F.softmax: 0.311679 log_softmax: 0.210622\n",
            "Train Epoch: 297 [600/1000 (60%)]\tLosses F.softmax: 0.219803 log_softmax: 0.186296\n",
            "Train Epoch: 297 [800/1000 (80%)]\tLosses F.softmax: 0.006494 log_softmax: 0.001220\n",
            "Train Epoch: 297 [1000/1000 (100%)]\tLosses F.softmax: 0.003258 log_softmax: 0.003930\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6757\tAccuracy: 8008.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6485\tAccuracy: 8090.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 298 [0/1000 (0%)]\tLosses F.softmax: 0.022726 log_softmax: 0.009700\n",
            "Train Epoch: 298 [200/1000 (20%)]\tLosses F.softmax: 0.003270 log_softmax: 0.004791\n",
            "Train Epoch: 298 [400/1000 (40%)]\tLosses F.softmax: 0.274342 log_softmax: 0.153343\n",
            "Train Epoch: 298 [600/1000 (60%)]\tLosses F.softmax: 0.021674 log_softmax: 0.018114\n",
            "Train Epoch: 298 [800/1000 (80%)]\tLosses F.softmax: 0.930034 log_softmax: 0.800796\n",
            "Train Epoch: 298 [1000/1000 (100%)]\tLosses F.softmax: 0.449587 log_softmax: 0.128432\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6733\tAccuracy: 8031.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6464\tAccuracy: 8099.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 299 [0/1000 (0%)]\tLosses F.softmax: 0.035843 log_softmax: 0.007623\n",
            "Train Epoch: 299 [200/1000 (20%)]\tLosses F.softmax: 0.034544 log_softmax: 0.020322\n",
            "Train Epoch: 299 [400/1000 (40%)]\tLosses F.softmax: 0.016443 log_softmax: 0.004227\n",
            "Train Epoch: 299 [600/1000 (60%)]\tLosses F.softmax: 0.333403 log_softmax: 0.502512\n",
            "Train Epoch: 299 [800/1000 (80%)]\tLosses F.softmax: 0.047107 log_softmax: 0.026625\n",
            "Train Epoch: 299 [1000/1000 (100%)]\tLosses F.softmax: 0.365119 log_softmax: 0.433718\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6708\tAccuracy: 8051.0/10000 (81%)\n",
            "log_softmax: Loss: 0.6428\tAccuracy: 8117.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 300 [0/1000 (0%)]\tLosses F.softmax: 0.172973 log_softmax: 0.015255\n",
            "Train Epoch: 300 [200/1000 (20%)]\tLosses F.softmax: 0.111558 log_softmax: 0.045331\n",
            "Train Epoch: 300 [400/1000 (40%)]\tLosses F.softmax: 1.470941 log_softmax: 1.459996\n",
            "Train Epoch: 300 [600/1000 (60%)]\tLosses F.softmax: 0.184266 log_softmax: 0.086629\n",
            "Train Epoch: 300 [800/1000 (80%)]\tLosses F.softmax: 0.080626 log_softmax: 0.053184\n",
            "Train Epoch: 300 [1000/1000 (100%)]\tLosses F.softmax: 3.668256 log_softmax: 4.475678\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6732\tAccuracy: 8019.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6468\tAccuracy: 8102.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 301 [0/1000 (0%)]\tLosses F.softmax: 0.078831 log_softmax: 0.067225\n",
            "Train Epoch: 301 [200/1000 (20%)]\tLosses F.softmax: 0.190254 log_softmax: 0.100839\n",
            "Train Epoch: 301 [400/1000 (40%)]\tLosses F.softmax: 1.942765 log_softmax: 3.783537\n",
            "Train Epoch: 301 [600/1000 (60%)]\tLosses F.softmax: 0.255740 log_softmax: 0.176038\n",
            "Train Epoch: 301 [800/1000 (80%)]\tLosses F.softmax: 1.384018 log_softmax: 0.547930\n",
            "Train Epoch: 301 [1000/1000 (100%)]\tLosses F.softmax: 0.027288 log_softmax: 0.011093\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6802\tAccuracy: 8018.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6510\tAccuracy: 8099.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 302 [0/1000 (0%)]\tLosses F.softmax: 0.016295 log_softmax: 0.004740\n",
            "Train Epoch: 302 [200/1000 (20%)]\tLosses F.softmax: 0.346280 log_softmax: 0.190417\n",
            "Train Epoch: 302 [400/1000 (40%)]\tLosses F.softmax: 0.002079 log_softmax: 0.000658\n",
            "Train Epoch: 302 [600/1000 (60%)]\tLosses F.softmax: 0.293366 log_softmax: 0.317133\n",
            "Train Epoch: 302 [800/1000 (80%)]\tLosses F.softmax: 2.042307 log_softmax: 3.741046\n",
            "Train Epoch: 302 [1000/1000 (100%)]\tLosses F.softmax: 0.003430 log_softmax: 0.001586\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6751\tAccuracy: 8029.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6482\tAccuracy: 8113.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 303 [0/1000 (0%)]\tLosses F.softmax: 0.007638 log_softmax: 0.003824\n",
            "Train Epoch: 303 [200/1000 (20%)]\tLosses F.softmax: 0.601903 log_softmax: 0.972216\n",
            "Train Epoch: 303 [400/1000 (40%)]\tLosses F.softmax: 0.003922 log_softmax: 0.002737\n",
            "Train Epoch: 303 [600/1000 (60%)]\tLosses F.softmax: 0.005043 log_softmax: 0.002053\n",
            "Train Epoch: 303 [800/1000 (80%)]\tLosses F.softmax: 0.190225 log_softmax: 0.083900\n",
            "Train Epoch: 303 [1000/1000 (100%)]\tLosses F.softmax: 0.007373 log_softmax: 0.004288\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6768\tAccuracy: 8034.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6508\tAccuracy: 8093.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 304 [0/1000 (0%)]\tLosses F.softmax: 0.002908 log_softmax: 0.001214\n",
            "Train Epoch: 304 [200/1000 (20%)]\tLosses F.softmax: 0.004429 log_softmax: 0.007411\n",
            "Train Epoch: 304 [400/1000 (40%)]\tLosses F.softmax: 0.005909 log_softmax: 0.020649\n",
            "Train Epoch: 304 [600/1000 (60%)]\tLosses F.softmax: 0.015056 log_softmax: 0.001147\n",
            "Train Epoch: 304 [800/1000 (80%)]\tLosses F.softmax: 0.070468 log_softmax: 0.053823\n",
            "Train Epoch: 304 [1000/1000 (100%)]\tLosses F.softmax: 0.160442 log_softmax: 0.108954\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6808\tAccuracy: 7999.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6566\tAccuracy: 8092.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 305 [0/1000 (0%)]\tLosses F.softmax: 0.001360 log_softmax: 0.000713\n",
            "Train Epoch: 305 [200/1000 (20%)]\tLosses F.softmax: 0.107566 log_softmax: 0.224822\n",
            "Train Epoch: 305 [400/1000 (40%)]\tLosses F.softmax: 0.011707 log_softmax: 0.004446\n",
            "Train Epoch: 305 [600/1000 (60%)]\tLosses F.softmax: 0.008583 log_softmax: 0.003446\n",
            "Train Epoch: 305 [800/1000 (80%)]\tLosses F.softmax: 0.057129 log_softmax: 0.049714\n",
            "Train Epoch: 305 [1000/1000 (100%)]\tLosses F.softmax: 0.017488 log_softmax: 0.004936\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6817\tAccuracy: 8029.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6533\tAccuracy: 8096.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 306 [0/1000 (0%)]\tLosses F.softmax: 0.015943 log_softmax: 0.008788\n",
            "Train Epoch: 306 [200/1000 (20%)]\tLosses F.softmax: 0.004434 log_softmax: 0.002166\n",
            "Train Epoch: 306 [400/1000 (40%)]\tLosses F.softmax: 0.088002 log_softmax: 0.095208\n",
            "Train Epoch: 306 [600/1000 (60%)]\tLosses F.softmax: 0.005974 log_softmax: 0.000915\n",
            "Train Epoch: 306 [800/1000 (80%)]\tLosses F.softmax: 0.044642 log_softmax: 0.055277\n",
            "Train Epoch: 306 [1000/1000 (100%)]\tLosses F.softmax: 0.127212 log_softmax: 0.266296\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6786\tAccuracy: 8029.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6538\tAccuracy: 8103.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 307 [0/1000 (0%)]\tLosses F.softmax: 0.124680 log_softmax: 0.546807\n",
            "Train Epoch: 307 [200/1000 (20%)]\tLosses F.softmax: 0.007399 log_softmax: 0.006278\n",
            "Train Epoch: 307 [400/1000 (40%)]\tLosses F.softmax: 0.015132 log_softmax: 0.005563\n",
            "Train Epoch: 307 [600/1000 (60%)]\tLosses F.softmax: 0.003517 log_softmax: 0.001147\n",
            "Train Epoch: 307 [800/1000 (80%)]\tLosses F.softmax: 0.003978 log_softmax: 0.001060\n",
            "Train Epoch: 307 [1000/1000 (100%)]\tLosses F.softmax: 0.041531 log_softmax: 0.053597\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6737\tAccuracy: 8039.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6518\tAccuracy: 8107.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 308 [0/1000 (0%)]\tLosses F.softmax: 0.092293 log_softmax: 0.096276\n",
            "Train Epoch: 308 [200/1000 (20%)]\tLosses F.softmax: 0.003503 log_softmax: 0.001783\n",
            "Train Epoch: 308 [400/1000 (40%)]\tLosses F.softmax: 0.005065 log_softmax: 0.001544\n",
            "Train Epoch: 308 [600/1000 (60%)]\tLosses F.softmax: 0.742564 log_softmax: 0.790043\n",
            "Train Epoch: 308 [800/1000 (80%)]\tLosses F.softmax: 0.065557 log_softmax: 0.017066\n",
            "Train Epoch: 308 [1000/1000 (100%)]\tLosses F.softmax: 0.029198 log_softmax: 0.006436\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6768\tAccuracy: 8031.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6535\tAccuracy: 8106.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 309 [0/1000 (0%)]\tLosses F.softmax: 0.666408 log_softmax: 0.239446\n",
            "Train Epoch: 309 [200/1000 (20%)]\tLosses F.softmax: 0.193480 log_softmax: 0.129219\n",
            "Train Epoch: 309 [400/1000 (40%)]\tLosses F.softmax: 0.198056 log_softmax: 0.213485\n",
            "Train Epoch: 309 [600/1000 (60%)]\tLosses F.softmax: 0.075488 log_softmax: 0.056940\n",
            "Train Epoch: 309 [800/1000 (80%)]\tLosses F.softmax: 0.193633 log_softmax: 0.108877\n",
            "Train Epoch: 309 [1000/1000 (100%)]\tLosses F.softmax: 0.326715 log_softmax: 0.293739\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6774\tAccuracy: 8025.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6555\tAccuracy: 8096.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 310 [0/1000 (0%)]\tLosses F.softmax: 0.174398 log_softmax: 0.094431\n",
            "Train Epoch: 310 [200/1000 (20%)]\tLosses F.softmax: 0.379336 log_softmax: 0.437790\n",
            "Train Epoch: 310 [400/1000 (40%)]\tLosses F.softmax: 0.077602 log_softmax: 0.172944\n",
            "Train Epoch: 310 [600/1000 (60%)]\tLosses F.softmax: 0.047817 log_softmax: 0.027077\n",
            "Train Epoch: 310 [800/1000 (80%)]\tLosses F.softmax: 0.065016 log_softmax: 0.066413\n",
            "Train Epoch: 310 [1000/1000 (100%)]\tLosses F.softmax: 0.032833 log_softmax: 0.408026\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6803\tAccuracy: 8022.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6579\tAccuracy: 8089.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 311 [0/1000 (0%)]\tLosses F.softmax: 0.013209 log_softmax: 0.009827\n",
            "Train Epoch: 311 [200/1000 (20%)]\tLosses F.softmax: 0.032681 log_softmax: 0.006333\n",
            "Train Epoch: 311 [400/1000 (40%)]\tLosses F.softmax: 0.018794 log_softmax: 0.013432\n",
            "Train Epoch: 311 [600/1000 (60%)]\tLosses F.softmax: 0.155233 log_softmax: 0.074068\n",
            "Train Epoch: 311 [800/1000 (80%)]\tLosses F.softmax: 0.038348 log_softmax: 0.036513\n",
            "Train Epoch: 311 [1000/1000 (100%)]\tLosses F.softmax: 0.154950 log_softmax: 0.117368\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6725\tAccuracy: 8072.0/10000 (81%)\n",
            "log_softmax: Loss: 0.6520\tAccuracy: 8114.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 312 [0/1000 (0%)]\tLosses F.softmax: 0.092723 log_softmax: 0.170185\n",
            "Train Epoch: 312 [200/1000 (20%)]\tLosses F.softmax: 0.020214 log_softmax: 0.003929\n",
            "Train Epoch: 312 [400/1000 (40%)]\tLosses F.softmax: 0.015255 log_softmax: 0.004031\n",
            "Train Epoch: 312 [600/1000 (60%)]\tLosses F.softmax: 0.060817 log_softmax: 0.065058\n",
            "Train Epoch: 312 [800/1000 (80%)]\tLosses F.softmax: 0.113180 log_softmax: 0.126841\n",
            "Train Epoch: 312 [1000/1000 (100%)]\tLosses F.softmax: 0.141537 log_softmax: 0.128316\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6785\tAccuracy: 8048.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6566\tAccuracy: 8103.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 313 [0/1000 (0%)]\tLosses F.softmax: 0.014298 log_softmax: 0.008353\n",
            "Train Epoch: 313 [200/1000 (20%)]\tLosses F.softmax: 0.000407 log_softmax: 0.001402\n",
            "Train Epoch: 313 [400/1000 (40%)]\tLosses F.softmax: 0.081722 log_softmax: 0.074398\n",
            "Train Epoch: 313 [600/1000 (60%)]\tLosses F.softmax: 0.015468 log_softmax: 0.008046\n",
            "Train Epoch: 313 [800/1000 (80%)]\tLosses F.softmax: 0.163351 log_softmax: 0.121191\n",
            "Train Epoch: 313 [1000/1000 (100%)]\tLosses F.softmax: 0.018312 log_softmax: 0.007203\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6799\tAccuracy: 8022.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6593\tAccuracy: 8098.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 314 [0/1000 (0%)]\tLosses F.softmax: 0.010167 log_softmax: 0.005205\n",
            "Train Epoch: 314 [200/1000 (20%)]\tLosses F.softmax: 0.181528 log_softmax: 0.196865\n",
            "Train Epoch: 314 [400/1000 (40%)]\tLosses F.softmax: 0.040485 log_softmax: 0.001928\n",
            "Train Epoch: 314 [600/1000 (60%)]\tLosses F.softmax: 0.015025 log_softmax: 0.005260\n",
            "Train Epoch: 314 [800/1000 (80%)]\tLosses F.softmax: 0.024818 log_softmax: 0.045689\n",
            "Train Epoch: 314 [1000/1000 (100%)]\tLosses F.softmax: 0.138042 log_softmax: 0.028772\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6779\tAccuracy: 8037.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6575\tAccuracy: 8102.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 315 [0/1000 (0%)]\tLosses F.softmax: 0.021990 log_softmax: 0.017542\n",
            "Train Epoch: 315 [200/1000 (20%)]\tLosses F.softmax: 0.005280 log_softmax: 0.002317\n",
            "Train Epoch: 315 [400/1000 (40%)]\tLosses F.softmax: 0.076163 log_softmax: 0.078789\n",
            "Train Epoch: 315 [600/1000 (60%)]\tLosses F.softmax: 0.048796 log_softmax: 0.015838\n",
            "Train Epoch: 315 [800/1000 (80%)]\tLosses F.softmax: 0.077720 log_softmax: 0.048757\n",
            "Train Epoch: 315 [1000/1000 (100%)]\tLosses F.softmax: 0.065673 log_softmax: 0.130141\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6819\tAccuracy: 8012.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6638\tAccuracy: 8088.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 316 [0/1000 (0%)]\tLosses F.softmax: 0.003623 log_softmax: 0.002368\n",
            "Train Epoch: 316 [200/1000 (20%)]\tLosses F.softmax: 0.094890 log_softmax: 0.086774\n",
            "Train Epoch: 316 [400/1000 (40%)]\tLosses F.softmax: 0.198987 log_softmax: 0.011794\n",
            "Train Epoch: 316 [600/1000 (60%)]\tLosses F.softmax: 0.093099 log_softmax: 0.070133\n",
            "Train Epoch: 316 [800/1000 (80%)]\tLosses F.softmax: 0.231397 log_softmax: 0.171100\n",
            "Train Epoch: 316 [1000/1000 (100%)]\tLosses F.softmax: 0.241758 log_softmax: 0.060945\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6844\tAccuracy: 8014.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6654\tAccuracy: 8076.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 317 [0/1000 (0%)]\tLosses F.softmax: 0.073780 log_softmax: 0.078515\n",
            "Train Epoch: 317 [200/1000 (20%)]\tLosses F.softmax: 0.808880 log_softmax: 0.318874\n",
            "Train Epoch: 317 [400/1000 (40%)]\tLosses F.softmax: 0.004612 log_softmax: 0.004681\n",
            "Train Epoch: 317 [600/1000 (60%)]\tLosses F.softmax: 0.065863 log_softmax: 0.061375\n",
            "Train Epoch: 317 [800/1000 (80%)]\tLosses F.softmax: 0.198790 log_softmax: 0.098071\n",
            "Train Epoch: 317 [1000/1000 (100%)]\tLosses F.softmax: 0.067030 log_softmax: 0.057181\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6818\tAccuracy: 8030.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6632\tAccuracy: 8096.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 318 [0/1000 (0%)]\tLosses F.softmax: 0.000327 log_softmax: 0.001102\n",
            "Train Epoch: 318 [200/1000 (20%)]\tLosses F.softmax: 0.055191 log_softmax: 0.020266\n",
            "Train Epoch: 318 [400/1000 (40%)]\tLosses F.softmax: 0.554440 log_softmax: 0.663361\n",
            "Train Epoch: 318 [600/1000 (60%)]\tLosses F.softmax: 0.014149 log_softmax: 0.007579\n",
            "Train Epoch: 318 [800/1000 (80%)]\tLosses F.softmax: 0.033158 log_softmax: 0.072327\n",
            "Train Epoch: 318 [1000/1000 (100%)]\tLosses F.softmax: 0.024754 log_softmax: 0.013128\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6826\tAccuracy: 8039.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6634\tAccuracy: 8107.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 319 [0/1000 (0%)]\tLosses F.softmax: 0.108000 log_softmax: 0.101520\n",
            "Train Epoch: 319 [200/1000 (20%)]\tLosses F.softmax: 0.234248 log_softmax: 0.150809\n",
            "Train Epoch: 319 [400/1000 (40%)]\tLosses F.softmax: 0.067445 log_softmax: 0.052458\n",
            "Train Epoch: 319 [600/1000 (60%)]\tLosses F.softmax: 0.038414 log_softmax: 0.007692\n",
            "Train Epoch: 319 [800/1000 (80%)]\tLosses F.softmax: 0.096629 log_softmax: 0.070338\n",
            "Train Epoch: 319 [1000/1000 (100%)]\tLosses F.softmax: 0.226908 log_softmax: 0.468244\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6804\tAccuracy: 8037.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6627\tAccuracy: 8084.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 320 [0/1000 (0%)]\tLosses F.softmax: 0.430286 log_softmax: 0.323395\n",
            "Train Epoch: 320 [200/1000 (20%)]\tLosses F.softmax: 0.052695 log_softmax: 0.054004\n",
            "Train Epoch: 320 [400/1000 (40%)]\tLosses F.softmax: 0.073515 log_softmax: 0.046511\n",
            "Train Epoch: 320 [600/1000 (60%)]\tLosses F.softmax: 0.068544 log_softmax: 0.041510\n",
            "Train Epoch: 320 [800/1000 (80%)]\tLosses F.softmax: 0.120525 log_softmax: 0.075131\n",
            "Train Epoch: 320 [1000/1000 (100%)]\tLosses F.softmax: 0.150558 log_softmax: 0.167287\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6807\tAccuracy: 8052.0/10000 (81%)\n",
            "log_softmax: Loss: 0.6631\tAccuracy: 8092.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 321 [0/1000 (0%)]\tLosses F.softmax: 0.193156 log_softmax: 0.075840\n",
            "Train Epoch: 321 [200/1000 (20%)]\tLosses F.softmax: 0.090617 log_softmax: 0.065531\n",
            "Train Epoch: 321 [400/1000 (40%)]\tLosses F.softmax: 0.113004 log_softmax: 0.104558\n",
            "Train Epoch: 321 [600/1000 (60%)]\tLosses F.softmax: 0.105587 log_softmax: 0.031488\n",
            "Train Epoch: 321 [800/1000 (80%)]\tLosses F.softmax: 0.022962 log_softmax: 0.050786\n",
            "Train Epoch: 321 [1000/1000 (100%)]\tLosses F.softmax: 0.009498 log_softmax: 0.003679\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6857\tAccuracy: 8020.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6669\tAccuracy: 8087.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 322 [0/1000 (0%)]\tLosses F.softmax: 0.219066 log_softmax: 0.021639\n",
            "Train Epoch: 322 [200/1000 (20%)]\tLosses F.softmax: 0.042554 log_softmax: 0.018273\n",
            "Train Epoch: 322 [400/1000 (40%)]\tLosses F.softmax: 0.121667 log_softmax: 0.165169\n",
            "Train Epoch: 322 [600/1000 (60%)]\tLosses F.softmax: 0.009077 log_softmax: 0.010560\n",
            "Train Epoch: 322 [800/1000 (80%)]\tLosses F.softmax: 0.060718 log_softmax: 0.059279\n",
            "Train Epoch: 322 [1000/1000 (100%)]\tLosses F.softmax: 0.012248 log_softmax: 0.003292\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6909\tAccuracy: 8006.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6709\tAccuracy: 8051.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 323 [0/1000 (0%)]\tLosses F.softmax: 0.101569 log_softmax: 0.081861\n",
            "Train Epoch: 323 [200/1000 (20%)]\tLosses F.softmax: 0.001044 log_softmax: 0.001005\n",
            "Train Epoch: 323 [400/1000 (40%)]\tLosses F.softmax: 0.032757 log_softmax: 0.012131\n",
            "Train Epoch: 323 [600/1000 (60%)]\tLosses F.softmax: 0.001881 log_softmax: 0.000884\n",
            "Train Epoch: 323 [800/1000 (80%)]\tLosses F.softmax: 0.015645 log_softmax: 0.001326\n",
            "Train Epoch: 323 [1000/1000 (100%)]\tLosses F.softmax: 0.097549 log_softmax: 0.044614\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6847\tAccuracy: 8038.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6684\tAccuracy: 8099.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 324 [0/1000 (0%)]\tLosses F.softmax: 0.012139 log_softmax: 0.008579\n",
            "Train Epoch: 324 [200/1000 (20%)]\tLosses F.softmax: 1.299371 log_softmax: 0.694856\n",
            "Train Epoch: 324 [400/1000 (40%)]\tLosses F.softmax: 0.021578 log_softmax: 0.007959\n",
            "Train Epoch: 324 [600/1000 (60%)]\tLosses F.softmax: 0.024837 log_softmax: 0.030443\n",
            "Train Epoch: 324 [800/1000 (80%)]\tLosses F.softmax: 0.211472 log_softmax: 0.176564\n",
            "Train Epoch: 324 [1000/1000 (100%)]\tLosses F.softmax: 0.084023 log_softmax: 0.043695\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6880\tAccuracy: 8022.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6734\tAccuracy: 8067.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 325 [0/1000 (0%)]\tLosses F.softmax: 0.150544 log_softmax: 0.170058\n",
            "Train Epoch: 325 [200/1000 (20%)]\tLosses F.softmax: 0.005895 log_softmax: 0.003084\n",
            "Train Epoch: 325 [400/1000 (40%)]\tLosses F.softmax: 0.961427 log_softmax: 0.796459\n",
            "Train Epoch: 325 [600/1000 (60%)]\tLosses F.softmax: 0.204609 log_softmax: 0.112813\n",
            "Train Epoch: 325 [800/1000 (80%)]\tLosses F.softmax: 0.011112 log_softmax: 0.011905\n",
            "Train Epoch: 325 [1000/1000 (100%)]\tLosses F.softmax: 0.043446 log_softmax: 0.040201\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6867\tAccuracy: 8029.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6718\tAccuracy: 8081.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 326 [0/1000 (0%)]\tLosses F.softmax: 0.004877 log_softmax: 0.001000\n",
            "Train Epoch: 326 [200/1000 (20%)]\tLosses F.softmax: 0.005277 log_softmax: 0.020568\n",
            "Train Epoch: 326 [400/1000 (40%)]\tLosses F.softmax: 0.029088 log_softmax: 0.006730\n",
            "Train Epoch: 326 [600/1000 (60%)]\tLosses F.softmax: 1.901498 log_softmax: 1.685700\n",
            "Train Epoch: 326 [800/1000 (80%)]\tLosses F.softmax: 0.089626 log_softmax: 0.032926\n",
            "Train Epoch: 326 [1000/1000 (100%)]\tLosses F.softmax: 0.166497 log_softmax: 0.240949\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6871\tAccuracy: 8025.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6728\tAccuracy: 8087.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 327 [0/1000 (0%)]\tLosses F.softmax: 0.011409 log_softmax: 0.004803\n",
            "Train Epoch: 327 [200/1000 (20%)]\tLosses F.softmax: 0.098236 log_softmax: 0.411630\n",
            "Train Epoch: 327 [400/1000 (40%)]\tLosses F.softmax: 0.083562 log_softmax: 0.023657\n",
            "Train Epoch: 327 [600/1000 (60%)]\tLosses F.softmax: 0.065922 log_softmax: 0.044919\n",
            "Train Epoch: 327 [800/1000 (80%)]\tLosses F.softmax: 0.554501 log_softmax: 0.499517\n",
            "Train Epoch: 327 [1000/1000 (100%)]\tLosses F.softmax: 0.021352 log_softmax: 0.002391\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6858\tAccuracy: 8035.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6723\tAccuracy: 8093.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 328 [0/1000 (0%)]\tLosses F.softmax: 0.359755 log_softmax: 0.155860\n",
            "Train Epoch: 328 [200/1000 (20%)]\tLosses F.softmax: 0.065061 log_softmax: 0.060845\n",
            "Train Epoch: 328 [400/1000 (40%)]\tLosses F.softmax: 1.986408 log_softmax: 0.420081\n",
            "Train Epoch: 328 [600/1000 (60%)]\tLosses F.softmax: 0.304548 log_softmax: 0.800272\n",
            "Train Epoch: 328 [800/1000 (80%)]\tLosses F.softmax: 0.241613 log_softmax: 0.201997\n",
            "Train Epoch: 328 [1000/1000 (100%)]\tLosses F.softmax: 0.029963 log_softmax: 0.019842\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6851\tAccuracy: 8051.0/10000 (81%)\n",
            "log_softmax: Loss: 0.6719\tAccuracy: 8085.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 329 [0/1000 (0%)]\tLosses F.softmax: 0.000370 log_softmax: 0.001731\n",
            "Train Epoch: 329 [200/1000 (20%)]\tLosses F.softmax: 0.095986 log_softmax: 0.037224\n",
            "Train Epoch: 329 [400/1000 (40%)]\tLosses F.softmax: 0.007198 log_softmax: 0.001125\n",
            "Train Epoch: 329 [600/1000 (60%)]\tLosses F.softmax: 0.004147 log_softmax: 0.002171\n",
            "Train Epoch: 329 [800/1000 (80%)]\tLosses F.softmax: 0.047976 log_softmax: 0.106682\n",
            "Train Epoch: 329 [1000/1000 (100%)]\tLosses F.softmax: 0.011698 log_softmax: 0.020427\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6952\tAccuracy: 8028.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6808\tAccuracy: 8073.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 330 [0/1000 (0%)]\tLosses F.softmax: 0.058185 log_softmax: 0.085948\n",
            "Train Epoch: 330 [200/1000 (20%)]\tLosses F.softmax: 0.443950 log_softmax: 0.422393\n",
            "Train Epoch: 330 [400/1000 (40%)]\tLosses F.softmax: 0.028372 log_softmax: 0.021770\n",
            "Train Epoch: 330 [600/1000 (60%)]\tLosses F.softmax: 0.002901 log_softmax: 0.014486\n",
            "Train Epoch: 330 [800/1000 (80%)]\tLosses F.softmax: 0.140617 log_softmax: 0.094835\n",
            "Train Epoch: 330 [1000/1000 (100%)]\tLosses F.softmax: 0.548853 log_softmax: 0.600216\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6896\tAccuracy: 8021.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6764\tAccuracy: 8088.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 331 [0/1000 (0%)]\tLosses F.softmax: 0.452800 log_softmax: 0.202834\n",
            "Train Epoch: 331 [200/1000 (20%)]\tLosses F.softmax: 0.129123 log_softmax: 0.107992\n",
            "Train Epoch: 331 [400/1000 (40%)]\tLosses F.softmax: 0.006582 log_softmax: 0.001753\n",
            "Train Epoch: 331 [600/1000 (60%)]\tLosses F.softmax: 0.217007 log_softmax: 0.049342\n",
            "Train Epoch: 331 [800/1000 (80%)]\tLosses F.softmax: 0.120898 log_softmax: 0.069060\n",
            "Train Epoch: 331 [1000/1000 (100%)]\tLosses F.softmax: 0.000545 log_softmax: 0.000953\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6923\tAccuracy: 8001.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6832\tAccuracy: 8050.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 332 [0/1000 (0%)]\tLosses F.softmax: 0.051768 log_softmax: 0.044347\n",
            "Train Epoch: 332 [200/1000 (20%)]\tLosses F.softmax: 0.422625 log_softmax: 0.257715\n",
            "Train Epoch: 332 [400/1000 (40%)]\tLosses F.softmax: 0.002092 log_softmax: 0.002323\n",
            "Train Epoch: 332 [600/1000 (60%)]\tLosses F.softmax: 0.033771 log_softmax: 0.029186\n",
            "Train Epoch: 332 [800/1000 (80%)]\tLosses F.softmax: 0.412693 log_softmax: 0.371784\n",
            "Train Epoch: 332 [1000/1000 (100%)]\tLosses F.softmax: 0.027276 log_softmax: 0.025592\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6919\tAccuracy: 8011.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6780\tAccuracy: 8087.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 333 [0/1000 (0%)]\tLosses F.softmax: 0.036888 log_softmax: 0.010483\n",
            "Train Epoch: 333 [200/1000 (20%)]\tLosses F.softmax: 0.071850 log_softmax: 0.046518\n",
            "Train Epoch: 333 [400/1000 (40%)]\tLosses F.softmax: 0.153061 log_softmax: 0.184832\n",
            "Train Epoch: 333 [600/1000 (60%)]\tLosses F.softmax: 0.017819 log_softmax: 0.019883\n",
            "Train Epoch: 333 [800/1000 (80%)]\tLosses F.softmax: 0.012215 log_softmax: 0.002717\n",
            "Train Epoch: 333 [1000/1000 (100%)]\tLosses F.softmax: 0.031649 log_softmax: 0.021393\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6915\tAccuracy: 8041.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6799\tAccuracy: 8083.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 334 [0/1000 (0%)]\tLosses F.softmax: 0.006946 log_softmax: 0.001713\n",
            "Train Epoch: 334 [200/1000 (20%)]\tLosses F.softmax: 0.000804 log_softmax: 0.000320\n",
            "Train Epoch: 334 [400/1000 (40%)]\tLosses F.softmax: 0.025281 log_softmax: 0.011288\n",
            "Train Epoch: 334 [600/1000 (60%)]\tLosses F.softmax: 2.304294 log_softmax: 0.618606\n",
            "Train Epoch: 334 [800/1000 (80%)]\tLosses F.softmax: 0.001423 log_softmax: 0.000321\n",
            "Train Epoch: 334 [1000/1000 (100%)]\tLosses F.softmax: 0.080361 log_softmax: 0.133923\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6905\tAccuracy: 8034.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6808\tAccuracy: 8079.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 335 [0/1000 (0%)]\tLosses F.softmax: 0.022624 log_softmax: 0.043865\n",
            "Train Epoch: 335 [200/1000 (20%)]\tLosses F.softmax: 0.322954 log_softmax: 0.211576\n",
            "Train Epoch: 335 [400/1000 (40%)]\tLosses F.softmax: 0.058212 log_softmax: 0.008179\n",
            "Train Epoch: 335 [600/1000 (60%)]\tLosses F.softmax: 0.042428 log_softmax: 0.027573\n",
            "Train Epoch: 335 [800/1000 (80%)]\tLosses F.softmax: 0.035720 log_softmax: 0.011377\n",
            "Train Epoch: 335 [1000/1000 (100%)]\tLosses F.softmax: 0.009365 log_softmax: 0.003055\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6941\tAccuracy: 8034.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6809\tAccuracy: 8087.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 336 [0/1000 (0%)]\tLosses F.softmax: 0.002219 log_softmax: 0.000431\n",
            "Train Epoch: 336 [200/1000 (20%)]\tLosses F.softmax: 0.017674 log_softmax: 0.003316\n",
            "Train Epoch: 336 [400/1000 (40%)]\tLosses F.softmax: 0.000239 log_softmax: 0.000151\n",
            "Train Epoch: 336 [600/1000 (60%)]\tLosses F.softmax: 0.088695 log_softmax: 0.037487\n",
            "Train Epoch: 336 [800/1000 (80%)]\tLosses F.softmax: 0.004511 log_softmax: 0.006581\n",
            "Train Epoch: 336 [1000/1000 (100%)]\tLosses F.softmax: 0.004550 log_softmax: 0.001649\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6961\tAccuracy: 8017.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6832\tAccuracy: 8078.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 337 [0/1000 (0%)]\tLosses F.softmax: 0.161323 log_softmax: 0.153801\n",
            "Train Epoch: 337 [200/1000 (20%)]\tLosses F.softmax: 0.024362 log_softmax: 0.014477\n",
            "Train Epoch: 337 [400/1000 (40%)]\tLosses F.softmax: 0.087593 log_softmax: 0.031644\n",
            "Train Epoch: 337 [600/1000 (60%)]\tLosses F.softmax: 0.165739 log_softmax: 0.007252\n",
            "Train Epoch: 337 [800/1000 (80%)]\tLosses F.softmax: 0.077297 log_softmax: 0.016748\n",
            "Train Epoch: 337 [1000/1000 (100%)]\tLosses F.softmax: 0.110925 log_softmax: 0.063609\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6923\tAccuracy: 8035.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6838\tAccuracy: 8066.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 338 [0/1000 (0%)]\tLosses F.softmax: 0.069499 log_softmax: 0.059383\n",
            "Train Epoch: 338 [200/1000 (20%)]\tLosses F.softmax: 0.042449 log_softmax: 0.056136\n",
            "Train Epoch: 338 [400/1000 (40%)]\tLosses F.softmax: 0.168104 log_softmax: 0.357868\n",
            "Train Epoch: 338 [600/1000 (60%)]\tLosses F.softmax: 0.084802 log_softmax: 0.165126\n",
            "Train Epoch: 338 [800/1000 (80%)]\tLosses F.softmax: 0.176811 log_softmax: 0.125467\n",
            "Train Epoch: 338 [1000/1000 (100%)]\tLosses F.softmax: 0.141242 log_softmax: 0.104391\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6971\tAccuracy: 8029.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6865\tAccuracy: 8077.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 339 [0/1000 (0%)]\tLosses F.softmax: 0.002937 log_softmax: 0.000095\n",
            "Train Epoch: 339 [200/1000 (20%)]\tLosses F.softmax: 0.024574 log_softmax: 0.017643\n",
            "Train Epoch: 339 [400/1000 (40%)]\tLosses F.softmax: 0.455753 log_softmax: 0.452934\n",
            "Train Epoch: 339 [600/1000 (60%)]\tLosses F.softmax: 0.173145 log_softmax: 0.117457\n",
            "Train Epoch: 339 [800/1000 (80%)]\tLosses F.softmax: 0.109044 log_softmax: 0.114604\n",
            "Train Epoch: 339 [1000/1000 (100%)]\tLosses F.softmax: 0.036764 log_softmax: 0.014495\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7017\tAccuracy: 8015.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6904\tAccuracy: 8060.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 340 [0/1000 (0%)]\tLosses F.softmax: 0.683477 log_softmax: 0.617624\n",
            "Train Epoch: 340 [200/1000 (20%)]\tLosses F.softmax: 0.103462 log_softmax: 0.057633\n",
            "Train Epoch: 340 [400/1000 (40%)]\tLosses F.softmax: 0.577412 log_softmax: 0.280674\n",
            "Train Epoch: 340 [600/1000 (60%)]\tLosses F.softmax: 0.000359 log_softmax: 0.000575\n",
            "Train Epoch: 340 [800/1000 (80%)]\tLosses F.softmax: 0.017457 log_softmax: 0.035542\n",
            "Train Epoch: 340 [1000/1000 (100%)]\tLosses F.softmax: 0.040002 log_softmax: 0.019873\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7043\tAccuracy: 7994.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6932\tAccuracy: 8035.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 341 [0/1000 (0%)]\tLosses F.softmax: 0.050643 log_softmax: 0.025515\n",
            "Train Epoch: 341 [200/1000 (20%)]\tLosses F.softmax: 0.023776 log_softmax: 0.004749\n",
            "Train Epoch: 341 [400/1000 (40%)]\tLosses F.softmax: 0.317432 log_softmax: 0.528901\n",
            "Train Epoch: 341 [600/1000 (60%)]\tLosses F.softmax: 0.002912 log_softmax: 0.001773\n",
            "Train Epoch: 341 [800/1000 (80%)]\tLosses F.softmax: 0.117895 log_softmax: 0.070965\n",
            "Train Epoch: 341 [1000/1000 (100%)]\tLosses F.softmax: 0.073921 log_softmax: 0.046382\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6992\tAccuracy: 8029.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6903\tAccuracy: 8074.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 342 [0/1000 (0%)]\tLosses F.softmax: 0.309347 log_softmax: 0.101506\n",
            "Train Epoch: 342 [200/1000 (20%)]\tLosses F.softmax: 0.027112 log_softmax: 0.022467\n",
            "Train Epoch: 342 [400/1000 (40%)]\tLosses F.softmax: 0.009886 log_softmax: 0.006464\n",
            "Train Epoch: 342 [600/1000 (60%)]\tLosses F.softmax: 0.053975 log_softmax: 0.024694\n",
            "Train Epoch: 342 [800/1000 (80%)]\tLosses F.softmax: 0.019230 log_softmax: 0.004649\n",
            "Train Epoch: 342 [1000/1000 (100%)]\tLosses F.softmax: 0.002862 log_softmax: 0.000536\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6980\tAccuracy: 8025.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6882\tAccuracy: 8076.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 343 [0/1000 (0%)]\tLosses F.softmax: 0.176602 log_softmax: 0.229177\n",
            "Train Epoch: 343 [200/1000 (20%)]\tLosses F.softmax: 0.062330 log_softmax: 0.085032\n",
            "Train Epoch: 343 [400/1000 (40%)]\tLosses F.softmax: 0.001613 log_softmax: 0.000629\n",
            "Train Epoch: 343 [600/1000 (60%)]\tLosses F.softmax: 0.006474 log_softmax: 0.007038\n",
            "Train Epoch: 343 [800/1000 (80%)]\tLosses F.softmax: 0.000392 log_softmax: 0.000228\n",
            "Train Epoch: 343 [1000/1000 (100%)]\tLosses F.softmax: 0.004128 log_softmax: 0.002376\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6988\tAccuracy: 8042.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6908\tAccuracy: 8047.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 344 [0/1000 (0%)]\tLosses F.softmax: 0.491520 log_softmax: 0.244328\n",
            "Train Epoch: 344 [200/1000 (20%)]\tLosses F.softmax: 0.024625 log_softmax: 0.010731\n",
            "Train Epoch: 344 [400/1000 (40%)]\tLosses F.softmax: 0.002479 log_softmax: 0.001723\n",
            "Train Epoch: 344 [600/1000 (60%)]\tLosses F.softmax: 0.486650 log_softmax: 0.141447\n",
            "Train Epoch: 344 [800/1000 (80%)]\tLosses F.softmax: 0.563287 log_softmax: 0.160166\n",
            "Train Epoch: 344 [1000/1000 (100%)]\tLosses F.softmax: 0.027776 log_softmax: 0.005918\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6991\tAccuracy: 8033.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6903\tAccuracy: 8073.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 345 [0/1000 (0%)]\tLosses F.softmax: 0.000244 log_softmax: 0.000461\n",
            "Train Epoch: 345 [200/1000 (20%)]\tLosses F.softmax: 0.323262 log_softmax: 0.207583\n",
            "Train Epoch: 345 [400/1000 (40%)]\tLosses F.softmax: 0.012513 log_softmax: 0.013787\n",
            "Train Epoch: 345 [600/1000 (60%)]\tLosses F.softmax: 0.003771 log_softmax: 0.000308\n",
            "Train Epoch: 345 [800/1000 (80%)]\tLosses F.softmax: 0.068994 log_softmax: 0.087532\n",
            "Train Epoch: 345 [1000/1000 (100%)]\tLosses F.softmax: 0.177462 log_softmax: 0.266385\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7012\tAccuracy: 8031.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6974\tAccuracy: 8043.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 346 [0/1000 (0%)]\tLosses F.softmax: 0.007975 log_softmax: 0.013447\n",
            "Train Epoch: 346 [200/1000 (20%)]\tLosses F.softmax: 0.011315 log_softmax: 0.017978\n",
            "Train Epoch: 346 [400/1000 (40%)]\tLosses F.softmax: 0.063229 log_softmax: 0.027595\n",
            "Train Epoch: 346 [600/1000 (60%)]\tLosses F.softmax: 0.012209 log_softmax: 0.023178\n",
            "Train Epoch: 346 [800/1000 (80%)]\tLosses F.softmax: 0.101362 log_softmax: 0.105168\n",
            "Train Epoch: 346 [1000/1000 (100%)]\tLosses F.softmax: 0.048550 log_softmax: 0.032973\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7057\tAccuracy: 8013.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6964\tAccuracy: 8040.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 347 [0/1000 (0%)]\tLosses F.softmax: 0.006695 log_softmax: 0.003286\n",
            "Train Epoch: 347 [200/1000 (20%)]\tLosses F.softmax: 0.004051 log_softmax: 0.000737\n",
            "Train Epoch: 347 [400/1000 (40%)]\tLosses F.softmax: 0.014307 log_softmax: 0.007305\n",
            "Train Epoch: 347 [600/1000 (60%)]\tLosses F.softmax: 0.092686 log_softmax: 0.037928\n",
            "Train Epoch: 347 [800/1000 (80%)]\tLosses F.softmax: 0.058921 log_softmax: 0.016613\n",
            "Train Epoch: 347 [1000/1000 (100%)]\tLosses F.softmax: 0.015995 log_softmax: 0.032011\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7015\tAccuracy: 8032.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6952\tAccuracy: 8070.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 348 [0/1000 (0%)]\tLosses F.softmax: 0.011910 log_softmax: 0.006618\n",
            "Train Epoch: 348 [200/1000 (20%)]\tLosses F.softmax: 0.001374 log_softmax: 0.000924\n",
            "Train Epoch: 348 [400/1000 (40%)]\tLosses F.softmax: 0.011833 log_softmax: 0.012760\n",
            "Train Epoch: 348 [600/1000 (60%)]\tLosses F.softmax: 0.450730 log_softmax: 0.100321\n",
            "Train Epoch: 348 [800/1000 (80%)]\tLosses F.softmax: 0.044119 log_softmax: 0.011498\n",
            "Train Epoch: 348 [1000/1000 (100%)]\tLosses F.softmax: 0.152434 log_softmax: 0.341724\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7027\tAccuracy: 8035.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6944\tAccuracy: 8093.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 349 [0/1000 (0%)]\tLosses F.softmax: 0.057655 log_softmax: 0.018605\n",
            "Train Epoch: 349 [200/1000 (20%)]\tLosses F.softmax: 0.006752 log_softmax: 0.002718\n",
            "Train Epoch: 349 [400/1000 (40%)]\tLosses F.softmax: 0.110558 log_softmax: 0.070352\n",
            "Train Epoch: 349 [600/1000 (60%)]\tLosses F.softmax: 0.003966 log_softmax: 0.002206\n",
            "Train Epoch: 349 [800/1000 (80%)]\tLosses F.softmax: 0.006166 log_softmax: 0.007972\n",
            "Train Epoch: 349 [1000/1000 (100%)]\tLosses F.softmax: 0.218027 log_softmax: 0.254014\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7026\tAccuracy: 8047.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6956\tAccuracy: 8077.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 350 [0/1000 (0%)]\tLosses F.softmax: 0.142435 log_softmax: 0.065144\n",
            "Train Epoch: 350 [200/1000 (20%)]\tLosses F.softmax: 0.002139 log_softmax: 0.001094\n",
            "Train Epoch: 350 [400/1000 (40%)]\tLosses F.softmax: 0.521738 log_softmax: 0.149430\n",
            "Train Epoch: 350 [600/1000 (60%)]\tLosses F.softmax: 0.226805 log_softmax: 0.085990\n",
            "Train Epoch: 350 [800/1000 (80%)]\tLosses F.softmax: 0.012603 log_softmax: 0.000267\n",
            "Train Epoch: 350 [1000/1000 (100%)]\tLosses F.softmax: 0.046911 log_softmax: 0.034853\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7022\tAccuracy: 8046.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6972\tAccuracy: 8058.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 351 [0/1000 (0%)]\tLosses F.softmax: 0.125254 log_softmax: 0.067737\n",
            "Train Epoch: 351 [200/1000 (20%)]\tLosses F.softmax: 0.063660 log_softmax: 0.051764\n",
            "Train Epoch: 351 [400/1000 (40%)]\tLosses F.softmax: 0.027434 log_softmax: 0.013235\n",
            "Train Epoch: 351 [600/1000 (60%)]\tLosses F.softmax: 0.002531 log_softmax: 0.000415\n",
            "Train Epoch: 351 [800/1000 (80%)]\tLosses F.softmax: 0.005747 log_softmax: 0.002974\n",
            "Train Epoch: 351 [1000/1000 (100%)]\tLosses F.softmax: 0.048681 log_softmax: 0.046495\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7097\tAccuracy: 8014.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7016\tAccuracy: 8059.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 352 [0/1000 (0%)]\tLosses F.softmax: 0.043951 log_softmax: 0.044209\n",
            "Train Epoch: 352 [200/1000 (20%)]\tLosses F.softmax: 0.003319 log_softmax: 0.000912\n",
            "Train Epoch: 352 [400/1000 (40%)]\tLosses F.softmax: 1.501131 log_softmax: 1.602429\n",
            "Train Epoch: 352 [600/1000 (60%)]\tLosses F.softmax: 0.024552 log_softmax: 0.004343\n",
            "Train Epoch: 352 [800/1000 (80%)]\tLosses F.softmax: 0.157437 log_softmax: 0.194278\n",
            "Train Epoch: 352 [1000/1000 (100%)]\tLosses F.softmax: 0.339787 log_softmax: 0.066819\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7088\tAccuracy: 8034.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7010\tAccuracy: 8068.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 353 [0/1000 (0%)]\tLosses F.softmax: 0.101727 log_softmax: 0.076776\n",
            "Train Epoch: 353 [200/1000 (20%)]\tLosses F.softmax: 0.242787 log_softmax: 0.277380\n",
            "Train Epoch: 353 [400/1000 (40%)]\tLosses F.softmax: 0.003850 log_softmax: 0.002245\n",
            "Train Epoch: 353 [600/1000 (60%)]\tLosses F.softmax: 0.093988 log_softmax: 0.142072\n",
            "Train Epoch: 353 [800/1000 (80%)]\tLosses F.softmax: 0.015845 log_softmax: 0.004864\n",
            "Train Epoch: 353 [1000/1000 (100%)]\tLosses F.softmax: 0.319295 log_softmax: 0.150980\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7052\tAccuracy: 8063.0/10000 (81%)\n",
            "log_softmax: Loss: 0.6996\tAccuracy: 8081.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 354 [0/1000 (0%)]\tLosses F.softmax: 0.420797 log_softmax: 0.268712\n",
            "Train Epoch: 354 [200/1000 (20%)]\tLosses F.softmax: 0.211976 log_softmax: 0.154427\n",
            "Train Epoch: 354 [400/1000 (40%)]\tLosses F.softmax: 0.129275 log_softmax: 0.050148\n",
            "Train Epoch: 354 [600/1000 (60%)]\tLosses F.softmax: 0.004945 log_softmax: 0.014203\n",
            "Train Epoch: 354 [800/1000 (80%)]\tLosses F.softmax: 0.565515 log_softmax: 0.149669\n",
            "Train Epoch: 354 [1000/1000 (100%)]\tLosses F.softmax: 0.006901 log_softmax: 0.007269\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7112\tAccuracy: 8028.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7058\tAccuracy: 8048.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 355 [0/1000 (0%)]\tLosses F.softmax: 0.109157 log_softmax: 0.033244\n",
            "Train Epoch: 355 [200/1000 (20%)]\tLosses F.softmax: 0.029123 log_softmax: 0.037154\n",
            "Train Epoch: 355 [400/1000 (40%)]\tLosses F.softmax: 0.081145 log_softmax: 0.029534\n",
            "Train Epoch: 355 [600/1000 (60%)]\tLosses F.softmax: 0.054048 log_softmax: 0.048337\n",
            "Train Epoch: 355 [800/1000 (80%)]\tLosses F.softmax: 0.104374 log_softmax: 0.078118\n",
            "Train Epoch: 355 [1000/1000 (100%)]\tLosses F.softmax: 0.202029 log_softmax: 0.224219\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7143\tAccuracy: 8009.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7077\tAccuracy: 8047.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 356 [0/1000 (0%)]\tLosses F.softmax: 0.257653 log_softmax: 0.281506\n",
            "Train Epoch: 356 [200/1000 (20%)]\tLosses F.softmax: 0.068485 log_softmax: 0.034953\n",
            "Train Epoch: 356 [400/1000 (40%)]\tLosses F.softmax: 0.051089 log_softmax: 0.129233\n",
            "Train Epoch: 356 [600/1000 (60%)]\tLosses F.softmax: 0.143702 log_softmax: 0.203112\n",
            "Train Epoch: 356 [800/1000 (80%)]\tLosses F.softmax: 0.114794 log_softmax: 0.044660\n",
            "Train Epoch: 356 [1000/1000 (100%)]\tLosses F.softmax: 0.176440 log_softmax: 0.233894\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7103\tAccuracy: 8033.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7055\tAccuracy: 8066.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 357 [0/1000 (0%)]\tLosses F.softmax: 0.000814 log_softmax: 0.000673\n",
            "Train Epoch: 357 [200/1000 (20%)]\tLosses F.softmax: 0.000905 log_softmax: 0.000549\n",
            "Train Epoch: 357 [400/1000 (40%)]\tLosses F.softmax: 0.007948 log_softmax: 0.039537\n",
            "Train Epoch: 357 [600/1000 (60%)]\tLosses F.softmax: 0.011365 log_softmax: 0.005840\n",
            "Train Epoch: 357 [800/1000 (80%)]\tLosses F.softmax: 0.284516 log_softmax: 0.460847\n",
            "Train Epoch: 357 [1000/1000 (100%)]\tLosses F.softmax: 0.001371 log_softmax: 0.000225\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7152\tAccuracy: 8018.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7097\tAccuracy: 8073.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 358 [0/1000 (0%)]\tLosses F.softmax: 0.013742 log_softmax: 0.015367\n",
            "Train Epoch: 358 [200/1000 (20%)]\tLosses F.softmax: 0.007853 log_softmax: 0.003778\n",
            "Train Epoch: 358 [400/1000 (40%)]\tLosses F.softmax: 1.658287 log_softmax: 1.106786\n",
            "Train Epoch: 358 [600/1000 (60%)]\tLosses F.softmax: 0.118862 log_softmax: 0.054391\n",
            "Train Epoch: 358 [800/1000 (80%)]\tLosses F.softmax: 0.009715 log_softmax: 0.001927\n",
            "Train Epoch: 358 [1000/1000 (100%)]\tLosses F.softmax: 0.442032 log_softmax: 0.501364\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7137\tAccuracy: 8018.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7084\tAccuracy: 8072.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 359 [0/1000 (0%)]\tLosses F.softmax: 0.077278 log_softmax: 0.133970\n",
            "Train Epoch: 359 [200/1000 (20%)]\tLosses F.softmax: 0.077857 log_softmax: 0.050953\n",
            "Train Epoch: 359 [400/1000 (40%)]\tLosses F.softmax: 0.172698 log_softmax: 0.011298\n",
            "Train Epoch: 359 [600/1000 (60%)]\tLosses F.softmax: 0.047404 log_softmax: 0.043589\n",
            "Train Epoch: 359 [800/1000 (80%)]\tLosses F.softmax: 0.000073 log_softmax: 0.000597\n",
            "Train Epoch: 359 [1000/1000 (100%)]\tLosses F.softmax: 0.010788 log_softmax: 0.002359\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7180\tAccuracy: 8022.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7107\tAccuracy: 8062.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 360 [0/1000 (0%)]\tLosses F.softmax: 0.037333 log_softmax: 0.042815\n",
            "Train Epoch: 360 [200/1000 (20%)]\tLosses F.softmax: 0.011559 log_softmax: 0.011867\n",
            "Train Epoch: 360 [400/1000 (40%)]\tLosses F.softmax: 0.014982 log_softmax: 0.007644\n",
            "Train Epoch: 360 [600/1000 (60%)]\tLosses F.softmax: 0.002949 log_softmax: 0.000706\n",
            "Train Epoch: 360 [800/1000 (80%)]\tLosses F.softmax: 0.076521 log_softmax: 0.059621\n",
            "Train Epoch: 360 [1000/1000 (100%)]\tLosses F.softmax: 0.236997 log_softmax: 0.128879\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7193\tAccuracy: 8043.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7146\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 361 [0/1000 (0%)]\tLosses F.softmax: 0.011160 log_softmax: 0.000560\n",
            "Train Epoch: 361 [200/1000 (20%)]\tLosses F.softmax: 0.014032 log_softmax: 0.011960\n",
            "Train Epoch: 361 [400/1000 (40%)]\tLosses F.softmax: 0.078344 log_softmax: 0.010111\n",
            "Train Epoch: 361 [600/1000 (60%)]\tLosses F.softmax: 0.002240 log_softmax: 0.002149\n",
            "Train Epoch: 361 [800/1000 (80%)]\tLosses F.softmax: 0.002143 log_softmax: 0.001991\n",
            "Train Epoch: 361 [1000/1000 (100%)]\tLosses F.softmax: 0.374431 log_softmax: 0.533649\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7209\tAccuracy: 8009.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7136\tAccuracy: 8045.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 362 [0/1000 (0%)]\tLosses F.softmax: 0.040620 log_softmax: 0.039717\n",
            "Train Epoch: 362 [200/1000 (20%)]\tLosses F.softmax: 0.001486 log_softmax: 0.004083\n",
            "Train Epoch: 362 [400/1000 (40%)]\tLosses F.softmax: 0.297689 log_softmax: 0.209977\n",
            "Train Epoch: 362 [600/1000 (60%)]\tLosses F.softmax: 0.086123 log_softmax: 0.106431\n",
            "Train Epoch: 362 [800/1000 (80%)]\tLosses F.softmax: 0.599251 log_softmax: 0.920799\n",
            "Train Epoch: 362 [1000/1000 (100%)]\tLosses F.softmax: 0.003388 log_softmax: 0.002203\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7257\tAccuracy: 7990.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7192\tAccuracy: 8026.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 363 [0/1000 (0%)]\tLosses F.softmax: 0.290502 log_softmax: 0.133362\n",
            "Train Epoch: 363 [200/1000 (20%)]\tLosses F.softmax: 0.000708 log_softmax: 0.000212\n",
            "Train Epoch: 363 [400/1000 (40%)]\tLosses F.softmax: 0.003830 log_softmax: 0.002366\n",
            "Train Epoch: 363 [600/1000 (60%)]\tLosses F.softmax: 0.000395 log_softmax: 0.000236\n",
            "Train Epoch: 363 [800/1000 (80%)]\tLosses F.softmax: 0.044716 log_softmax: 0.091865\n",
            "Train Epoch: 363 [1000/1000 (100%)]\tLosses F.softmax: 0.008598 log_softmax: 0.002629\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7206\tAccuracy: 8045.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7166\tAccuracy: 8048.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 364 [0/1000 (0%)]\tLosses F.softmax: 0.002852 log_softmax: 0.001276\n",
            "Train Epoch: 364 [200/1000 (20%)]\tLosses F.softmax: 0.002467 log_softmax: 0.000329\n",
            "Train Epoch: 364 [400/1000 (40%)]\tLosses F.softmax: 0.088552 log_softmax: 0.070099\n",
            "Train Epoch: 364 [600/1000 (60%)]\tLosses F.softmax: 0.020183 log_softmax: 0.007936\n",
            "Train Epoch: 364 [800/1000 (80%)]\tLosses F.softmax: 0.041400 log_softmax: 0.037949\n",
            "Train Epoch: 364 [1000/1000 (100%)]\tLosses F.softmax: 0.008730 log_softmax: 0.010941\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7192\tAccuracy: 8047.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7149\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 365 [0/1000 (0%)]\tLosses F.softmax: 0.306589 log_softmax: 0.413114\n",
            "Train Epoch: 365 [200/1000 (20%)]\tLosses F.softmax: 0.093596 log_softmax: 0.050721\n",
            "Train Epoch: 365 [400/1000 (40%)]\tLosses F.softmax: 0.114018 log_softmax: 0.036349\n",
            "Train Epoch: 365 [600/1000 (60%)]\tLosses F.softmax: 0.006823 log_softmax: 0.001751\n",
            "Train Epoch: 365 [800/1000 (80%)]\tLosses F.softmax: 0.118693 log_softmax: 0.270097\n",
            "Train Epoch: 365 [1000/1000 (100%)]\tLosses F.softmax: 0.069302 log_softmax: 0.072179\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7191\tAccuracy: 8036.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7161\tAccuracy: 8068.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 366 [0/1000 (0%)]\tLosses F.softmax: 0.009280 log_softmax: 0.000453\n",
            "Train Epoch: 366 [200/1000 (20%)]\tLosses F.softmax: 0.024724 log_softmax: 0.009828\n",
            "Train Epoch: 366 [400/1000 (40%)]\tLosses F.softmax: 0.000294 log_softmax: 0.000079\n",
            "Train Epoch: 366 [600/1000 (60%)]\tLosses F.softmax: 0.021576 log_softmax: 0.012645\n",
            "Train Epoch: 366 [800/1000 (80%)]\tLosses F.softmax: 0.093504 log_softmax: 0.053487\n",
            "Train Epoch: 366 [1000/1000 (100%)]\tLosses F.softmax: 0.004879 log_softmax: 0.002786\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7216\tAccuracy: 8030.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7192\tAccuracy: 8074.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 367 [0/1000 (0%)]\tLosses F.softmax: 0.002029 log_softmax: 0.000686\n",
            "Train Epoch: 367 [200/1000 (20%)]\tLosses F.softmax: 0.350416 log_softmax: 0.496867\n",
            "Train Epoch: 367 [400/1000 (40%)]\tLosses F.softmax: 0.029719 log_softmax: 0.036696\n",
            "Train Epoch: 367 [600/1000 (60%)]\tLosses F.softmax: 0.025384 log_softmax: 0.004414\n",
            "Train Epoch: 367 [800/1000 (80%)]\tLosses F.softmax: 0.298157 log_softmax: 0.288259\n",
            "Train Epoch: 367 [1000/1000 (100%)]\tLosses F.softmax: 0.005595 log_softmax: 0.000937\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7250\tAccuracy: 8022.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7208\tAccuracy: 8069.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 368 [0/1000 (0%)]\tLosses F.softmax: 0.018035 log_softmax: 0.018371\n",
            "Train Epoch: 368 [200/1000 (20%)]\tLosses F.softmax: 0.000264 log_softmax: 0.001055\n",
            "Train Epoch: 368 [400/1000 (40%)]\tLosses F.softmax: 0.001512 log_softmax: 0.002191\n",
            "Train Epoch: 368 [600/1000 (60%)]\tLosses F.softmax: 0.034176 log_softmax: 0.037883\n",
            "Train Epoch: 368 [800/1000 (80%)]\tLosses F.softmax: 0.007796 log_softmax: 0.004318\n",
            "Train Epoch: 368 [1000/1000 (100%)]\tLosses F.softmax: 0.001090 log_softmax: 0.002782\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7249\tAccuracy: 8031.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7204\tAccuracy: 8052.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 369 [0/1000 (0%)]\tLosses F.softmax: 0.020232 log_softmax: 0.062731\n",
            "Train Epoch: 369 [200/1000 (20%)]\tLosses F.softmax: 0.086709 log_softmax: 0.059990\n",
            "Train Epoch: 369 [400/1000 (40%)]\tLosses F.softmax: 0.003910 log_softmax: 0.003833\n",
            "Train Epoch: 369 [600/1000 (60%)]\tLosses F.softmax: 0.662840 log_softmax: 0.292053\n",
            "Train Epoch: 369 [800/1000 (80%)]\tLosses F.softmax: 0.040492 log_softmax: 0.093510\n",
            "Train Epoch: 369 [1000/1000 (100%)]\tLosses F.softmax: 0.084719 log_softmax: 0.087527\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7284\tAccuracy: 8022.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7254\tAccuracy: 8034.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 370 [0/1000 (0%)]\tLosses F.softmax: 0.079255 log_softmax: 0.001836\n",
            "Train Epoch: 370 [200/1000 (20%)]\tLosses F.softmax: 0.063440 log_softmax: 0.090378\n",
            "Train Epoch: 370 [400/1000 (40%)]\tLosses F.softmax: 0.108577 log_softmax: 0.126737\n",
            "Train Epoch: 370 [600/1000 (60%)]\tLosses F.softmax: 0.005300 log_softmax: 0.005598\n",
            "Train Epoch: 370 [800/1000 (80%)]\tLosses F.softmax: 0.005856 log_softmax: 0.001177\n",
            "Train Epoch: 370 [1000/1000 (100%)]\tLosses F.softmax: 0.027488 log_softmax: 0.025524\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7403\tAccuracy: 7971.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7345\tAccuracy: 8022.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 371 [0/1000 (0%)]\tLosses F.softmax: 0.041409 log_softmax: 0.039051\n",
            "Train Epoch: 371 [200/1000 (20%)]\tLosses F.softmax: 0.002817 log_softmax: 0.000313\n",
            "Train Epoch: 371 [400/1000 (40%)]\tLosses F.softmax: 0.013765 log_softmax: 0.019761\n",
            "Train Epoch: 371 [600/1000 (60%)]\tLosses F.softmax: 0.276943 log_softmax: 0.096868\n",
            "Train Epoch: 371 [800/1000 (80%)]\tLosses F.softmax: 0.006476 log_softmax: 0.005155\n",
            "Train Epoch: 371 [1000/1000 (100%)]\tLosses F.softmax: 0.003769 log_softmax: 0.003129\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7282\tAccuracy: 8047.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7251\tAccuracy: 8071.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 372 [0/1000 (0%)]\tLosses F.softmax: 0.118281 log_softmax: 0.007935\n",
            "Train Epoch: 372 [200/1000 (20%)]\tLosses F.softmax: 0.492093 log_softmax: 0.220645\n",
            "Train Epoch: 372 [400/1000 (40%)]\tLosses F.softmax: 0.001438 log_softmax: 0.003206\n",
            "Train Epoch: 372 [600/1000 (60%)]\tLosses F.softmax: 0.241147 log_softmax: 0.291205\n",
            "Train Epoch: 372 [800/1000 (80%)]\tLosses F.softmax: 0.027827 log_softmax: 0.035833\n",
            "Train Epoch: 372 [1000/1000 (100%)]\tLosses F.softmax: 0.230499 log_softmax: 0.258344\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7274\tAccuracy: 8054.0/10000 (81%)\n",
            "log_softmax: Loss: 0.7256\tAccuracy: 8064.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 373 [0/1000 (0%)]\tLosses F.softmax: 1.112558 log_softmax: 0.763005\n",
            "Train Epoch: 373 [200/1000 (20%)]\tLosses F.softmax: 0.024823 log_softmax: 0.029764\n",
            "Train Epoch: 373 [400/1000 (40%)]\tLosses F.softmax: 0.001135 log_softmax: 0.000397\n",
            "Train Epoch: 373 [600/1000 (60%)]\tLosses F.softmax: 0.005538 log_softmax: 0.002470\n",
            "Train Epoch: 373 [800/1000 (80%)]\tLosses F.softmax: 0.003158 log_softmax: 0.000474\n",
            "Train Epoch: 373 [1000/1000 (100%)]\tLosses F.softmax: 0.129082 log_softmax: 0.056082\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7321\tAccuracy: 8019.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7300\tAccuracy: 8040.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 374 [0/1000 (0%)]\tLosses F.softmax: 1.245237 log_softmax: 1.645578\n",
            "Train Epoch: 374 [200/1000 (20%)]\tLosses F.softmax: 0.042810 log_softmax: 0.074055\n",
            "Train Epoch: 374 [400/1000 (40%)]\tLosses F.softmax: 0.009617 log_softmax: 0.012866\n",
            "Train Epoch: 374 [600/1000 (60%)]\tLosses F.softmax: 0.340069 log_softmax: 0.083753\n",
            "Train Epoch: 374 [800/1000 (80%)]\tLosses F.softmax: 0.295268 log_softmax: 0.205305\n",
            "Train Epoch: 374 [1000/1000 (100%)]\tLosses F.softmax: 0.000745 log_softmax: 0.003446\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7310\tAccuracy: 8059.0/10000 (81%)\n",
            "log_softmax: Loss: 0.7291\tAccuracy: 8077.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 375 [0/1000 (0%)]\tLosses F.softmax: 0.014812 log_softmax: 0.007249\n",
            "Train Epoch: 375 [200/1000 (20%)]\tLosses F.softmax: 0.020206 log_softmax: 0.020454\n",
            "Train Epoch: 375 [400/1000 (40%)]\tLosses F.softmax: 0.039978 log_softmax: 0.057290\n",
            "Train Epoch: 375 [600/1000 (60%)]\tLosses F.softmax: 0.011340 log_softmax: 0.003812\n",
            "Train Epoch: 375 [800/1000 (80%)]\tLosses F.softmax: 0.015077 log_softmax: 0.019327\n",
            "Train Epoch: 375 [1000/1000 (100%)]\tLosses F.softmax: 0.008736 log_softmax: 0.012963\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7334\tAccuracy: 8031.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7307\tAccuracy: 8044.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 376 [0/1000 (0%)]\tLosses F.softmax: 0.015201 log_softmax: 0.008625\n",
            "Train Epoch: 376 [200/1000 (20%)]\tLosses F.softmax: 0.016122 log_softmax: 0.005024\n",
            "Train Epoch: 376 [400/1000 (40%)]\tLosses F.softmax: 0.102433 log_softmax: 0.099780\n",
            "Train Epoch: 376 [600/1000 (60%)]\tLosses F.softmax: 0.010213 log_softmax: 0.007152\n",
            "Train Epoch: 376 [800/1000 (80%)]\tLosses F.softmax: 0.285467 log_softmax: 0.100588\n",
            "Train Epoch: 376 [1000/1000 (100%)]\tLosses F.softmax: 0.035047 log_softmax: 0.077605\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7356\tAccuracy: 8021.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7313\tAccuracy: 8043.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 377 [0/1000 (0%)]\tLosses F.softmax: 1.283804 log_softmax: 0.797840\n",
            "Train Epoch: 377 [200/1000 (20%)]\tLosses F.softmax: 0.000533 log_softmax: 0.001172\n",
            "Train Epoch: 377 [400/1000 (40%)]\tLosses F.softmax: 2.697278 log_softmax: 0.723797\n",
            "Train Epoch: 377 [600/1000 (60%)]\tLosses F.softmax: 0.002487 log_softmax: 0.000795\n",
            "Train Epoch: 377 [800/1000 (80%)]\tLosses F.softmax: 0.000030 log_softmax: 0.000259\n",
            "Train Epoch: 377 [1000/1000 (100%)]\tLosses F.softmax: 0.001068 log_softmax: 0.000300\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7337\tAccuracy: 8035.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7316\tAccuracy: 8067.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 378 [0/1000 (0%)]\tLosses F.softmax: 0.006140 log_softmax: 0.003144\n",
            "Train Epoch: 378 [200/1000 (20%)]\tLosses F.softmax: 0.192163 log_softmax: 0.257532\n",
            "Train Epoch: 378 [400/1000 (40%)]\tLosses F.softmax: 0.001509 log_softmax: 0.003387\n",
            "Train Epoch: 378 [600/1000 (60%)]\tLosses F.softmax: 0.052558 log_softmax: 0.033114\n",
            "Train Epoch: 378 [800/1000 (80%)]\tLosses F.softmax: 0.004621 log_softmax: 0.010204\n",
            "Train Epoch: 378 [1000/1000 (100%)]\tLosses F.softmax: 0.000323 log_softmax: 0.000967\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7405\tAccuracy: 8015.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7361\tAccuracy: 8040.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 379 [0/1000 (0%)]\tLosses F.softmax: 0.061426 log_softmax: 0.126351\n",
            "Train Epoch: 379 [200/1000 (20%)]\tLosses F.softmax: 0.030825 log_softmax: 0.030225\n",
            "Train Epoch: 379 [400/1000 (40%)]\tLosses F.softmax: 0.121475 log_softmax: 0.077751\n",
            "Train Epoch: 379 [600/1000 (60%)]\tLosses F.softmax: 0.002401 log_softmax: 0.002638\n",
            "Train Epoch: 379 [800/1000 (80%)]\tLosses F.softmax: 0.006300 log_softmax: 0.003753\n",
            "Train Epoch: 379 [1000/1000 (100%)]\tLosses F.softmax: 0.424201 log_softmax: 0.561055\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7370\tAccuracy: 8044.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7362\tAccuracy: 8057.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 380 [0/1000 (0%)]\tLosses F.softmax: 0.041692 log_softmax: 0.021122\n",
            "Train Epoch: 380 [200/1000 (20%)]\tLosses F.softmax: 0.004971 log_softmax: 0.002843\n",
            "Train Epoch: 380 [400/1000 (40%)]\tLosses F.softmax: 0.006234 log_softmax: 0.006022\n",
            "Train Epoch: 380 [600/1000 (60%)]\tLosses F.softmax: 0.000169 log_softmax: 0.000105\n",
            "Train Epoch: 380 [800/1000 (80%)]\tLosses F.softmax: 0.001534 log_softmax: 0.001529\n",
            "Train Epoch: 380 [1000/1000 (100%)]\tLosses F.softmax: 0.098515 log_softmax: 0.118640\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7407\tAccuracy: 8030.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7370\tAccuracy: 8048.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 381 [0/1000 (0%)]\tLosses F.softmax: 0.100939 log_softmax: 0.258464\n",
            "Train Epoch: 381 [200/1000 (20%)]\tLosses F.softmax: 0.343135 log_softmax: 0.759458\n",
            "Train Epoch: 381 [400/1000 (40%)]\tLosses F.softmax: 0.001211 log_softmax: 0.000129\n",
            "Train Epoch: 381 [600/1000 (60%)]\tLosses F.softmax: 0.224704 log_softmax: 0.199516\n",
            "Train Epoch: 381 [800/1000 (80%)]\tLosses F.softmax: 0.006616 log_softmax: 0.001401\n",
            "Train Epoch: 381 [1000/1000 (100%)]\tLosses F.softmax: 0.007602 log_softmax: 0.004515\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7403\tAccuracy: 8032.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7417\tAccuracy: 8045.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 382 [0/1000 (0%)]\tLosses F.softmax: 0.034547 log_softmax: 0.029303\n",
            "Train Epoch: 382 [200/1000 (20%)]\tLosses F.softmax: 0.001590 log_softmax: 0.000482\n",
            "Train Epoch: 382 [400/1000 (40%)]\tLosses F.softmax: 0.018084 log_softmax: 0.003647\n",
            "Train Epoch: 382 [600/1000 (60%)]\tLosses F.softmax: 0.011170 log_softmax: 0.001840\n",
            "Train Epoch: 382 [800/1000 (80%)]\tLosses F.softmax: 0.054314 log_softmax: 0.014839\n",
            "Train Epoch: 382 [1000/1000 (100%)]\tLosses F.softmax: 0.075929 log_softmax: 0.046499\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7474\tAccuracy: 8003.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7432\tAccuracy: 8025.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 383 [0/1000 (0%)]\tLosses F.softmax: 0.025284 log_softmax: 0.002726\n",
            "Train Epoch: 383 [200/1000 (20%)]\tLosses F.softmax: 0.003369 log_softmax: 0.009143\n",
            "Train Epoch: 383 [400/1000 (40%)]\tLosses F.softmax: 0.643871 log_softmax: 0.806666\n",
            "Train Epoch: 383 [600/1000 (60%)]\tLosses F.softmax: 0.015312 log_softmax: 0.018359\n",
            "Train Epoch: 383 [800/1000 (80%)]\tLosses F.softmax: 0.003872 log_softmax: 0.001678\n",
            "Train Epoch: 383 [1000/1000 (100%)]\tLosses F.softmax: 0.044550 log_softmax: 0.019955\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7459\tAccuracy: 8022.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7481\tAccuracy: 8027.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 384 [0/1000 (0%)]\tLosses F.softmax: 0.016597 log_softmax: 0.016956\n",
            "Train Epoch: 384 [200/1000 (20%)]\tLosses F.softmax: 0.163335 log_softmax: 0.234957\n",
            "Train Epoch: 384 [400/1000 (40%)]\tLosses F.softmax: 0.117649 log_softmax: 0.051434\n",
            "Train Epoch: 384 [600/1000 (60%)]\tLosses F.softmax: 0.001611 log_softmax: 0.000939\n",
            "Train Epoch: 384 [800/1000 (80%)]\tLosses F.softmax: 0.024835 log_softmax: 0.014294\n",
            "Train Epoch: 384 [1000/1000 (100%)]\tLosses F.softmax: 0.014542 log_softmax: 0.026118\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7491\tAccuracy: 8006.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7455\tAccuracy: 8032.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 385 [0/1000 (0%)]\tLosses F.softmax: 0.184263 log_softmax: 0.120741\n",
            "Train Epoch: 385 [200/1000 (20%)]\tLosses F.softmax: 0.001239 log_softmax: 0.000125\n",
            "Train Epoch: 385 [400/1000 (40%)]\tLosses F.softmax: 0.025884 log_softmax: 0.006876\n",
            "Train Epoch: 385 [600/1000 (60%)]\tLosses F.softmax: 0.004819 log_softmax: 0.005869\n",
            "Train Epoch: 385 [800/1000 (80%)]\tLosses F.softmax: 0.137460 log_softmax: 0.100861\n",
            "Train Epoch: 385 [1000/1000 (100%)]\tLosses F.softmax: 0.023118 log_softmax: 0.010099\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7480\tAccuracy: 8020.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7471\tAccuracy: 8036.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 386 [0/1000 (0%)]\tLosses F.softmax: 0.009182 log_softmax: 0.029213\n",
            "Train Epoch: 386 [200/1000 (20%)]\tLosses F.softmax: 0.001381 log_softmax: 0.001246\n",
            "Train Epoch: 386 [400/1000 (40%)]\tLosses F.softmax: 0.001061 log_softmax: 0.000534\n",
            "Train Epoch: 386 [600/1000 (60%)]\tLosses F.softmax: 0.456581 log_softmax: 1.592121\n",
            "Train Epoch: 386 [800/1000 (80%)]\tLosses F.softmax: 0.003822 log_softmax: 0.010443\n",
            "Train Epoch: 386 [1000/1000 (100%)]\tLosses F.softmax: 0.000664 log_softmax: 0.003681\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7471\tAccuracy: 8032.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7463\tAccuracy: 8052.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 387 [0/1000 (0%)]\tLosses F.softmax: 0.102465 log_softmax: 0.076404\n",
            "Train Epoch: 387 [200/1000 (20%)]\tLosses F.softmax: 0.003896 log_softmax: 0.000691\n",
            "Train Epoch: 387 [400/1000 (40%)]\tLosses F.softmax: 0.019433 log_softmax: 0.006078\n",
            "Train Epoch: 387 [600/1000 (60%)]\tLosses F.softmax: 0.000585 log_softmax: 0.006222\n",
            "Train Epoch: 387 [800/1000 (80%)]\tLosses F.softmax: 0.398525 log_softmax: 0.272139\n",
            "Train Epoch: 387 [1000/1000 (100%)]\tLosses F.softmax: 0.231093 log_softmax: 0.210007\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7491\tAccuracy: 8032.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7470\tAccuracy: 8058.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 388 [0/1000 (0%)]\tLosses F.softmax: 0.006607 log_softmax: 0.001675\n",
            "Train Epoch: 388 [200/1000 (20%)]\tLosses F.softmax: 0.008531 log_softmax: 0.074357\n",
            "Train Epoch: 388 [400/1000 (40%)]\tLosses F.softmax: 0.000742 log_softmax: 0.004842\n",
            "Train Epoch: 388 [600/1000 (60%)]\tLosses F.softmax: 0.406231 log_softmax: 0.157921\n",
            "Train Epoch: 388 [800/1000 (80%)]\tLosses F.softmax: 0.041596 log_softmax: 0.006918\n",
            "Train Epoch: 388 [1000/1000 (100%)]\tLosses F.softmax: 0.078387 log_softmax: 0.077533\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7499\tAccuracy: 8030.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7500\tAccuracy: 8050.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 389 [0/1000 (0%)]\tLosses F.softmax: 0.038453 log_softmax: 0.017438\n",
            "Train Epoch: 389 [200/1000 (20%)]\tLosses F.softmax: 0.049059 log_softmax: 0.074835\n",
            "Train Epoch: 389 [400/1000 (40%)]\tLosses F.softmax: 0.066484 log_softmax: 0.021541\n",
            "Train Epoch: 389 [600/1000 (60%)]\tLosses F.softmax: 0.126558 log_softmax: 0.069312\n",
            "Train Epoch: 389 [800/1000 (80%)]\tLosses F.softmax: 0.004855 log_softmax: 0.004299\n",
            "Train Epoch: 389 [1000/1000 (100%)]\tLosses F.softmax: 0.389645 log_softmax: 0.684326\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7616\tAccuracy: 7989.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7569\tAccuracy: 8028.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 390 [0/1000 (0%)]\tLosses F.softmax: 0.001751 log_softmax: 0.003963\n",
            "Train Epoch: 390 [200/1000 (20%)]\tLosses F.softmax: 0.000793 log_softmax: 0.000139\n",
            "Train Epoch: 390 [400/1000 (40%)]\tLosses F.softmax: 0.125088 log_softmax: 0.098647\n",
            "Train Epoch: 390 [600/1000 (60%)]\tLosses F.softmax: 0.010369 log_softmax: 0.003709\n",
            "Train Epoch: 390 [800/1000 (80%)]\tLosses F.softmax: 0.119502 log_softmax: 0.117139\n",
            "Train Epoch: 390 [1000/1000 (100%)]\tLosses F.softmax: 0.334607 log_softmax: 0.254578\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7551\tAccuracy: 8024.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7533\tAccuracy: 8043.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 391 [0/1000 (0%)]\tLosses F.softmax: 0.012039 log_softmax: 0.003892\n",
            "Train Epoch: 391 [200/1000 (20%)]\tLosses F.softmax: 0.001358 log_softmax: 0.001448\n",
            "Train Epoch: 391 [400/1000 (40%)]\tLosses F.softmax: 0.028019 log_softmax: 0.026473\n",
            "Train Epoch: 391 [600/1000 (60%)]\tLosses F.softmax: 0.150641 log_softmax: 0.160780\n",
            "Train Epoch: 391 [800/1000 (80%)]\tLosses F.softmax: 0.070486 log_softmax: 0.046507\n",
            "Train Epoch: 391 [1000/1000 (100%)]\tLosses F.softmax: 0.014712 log_softmax: 0.024679\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7519\tAccuracy: 8038.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7515\tAccuracy: 8067.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 392 [0/1000 (0%)]\tLosses F.softmax: 0.004914 log_softmax: 0.006150\n",
            "Train Epoch: 392 [200/1000 (20%)]\tLosses F.softmax: 0.025849 log_softmax: 0.019786\n",
            "Train Epoch: 392 [400/1000 (40%)]\tLosses F.softmax: 0.530135 log_softmax: 0.131932\n",
            "Train Epoch: 392 [600/1000 (60%)]\tLosses F.softmax: 0.006684 log_softmax: 0.025821\n",
            "Train Epoch: 392 [800/1000 (80%)]\tLosses F.softmax: 0.301400 log_softmax: 0.077995\n",
            "Train Epoch: 392 [1000/1000 (100%)]\tLosses F.softmax: 0.083458 log_softmax: 0.027909\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7553\tAccuracy: 8025.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7539\tAccuracy: 8052.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 393 [0/1000 (0%)]\tLosses F.softmax: 0.339550 log_softmax: 0.216635\n",
            "Train Epoch: 393 [200/1000 (20%)]\tLosses F.softmax: 0.007438 log_softmax: 0.001711\n",
            "Train Epoch: 393 [400/1000 (40%)]\tLosses F.softmax: 0.302974 log_softmax: 0.229792\n",
            "Train Epoch: 393 [600/1000 (60%)]\tLosses F.softmax: 0.002022 log_softmax: 0.002031\n",
            "Train Epoch: 393 [800/1000 (80%)]\tLosses F.softmax: 0.091272 log_softmax: 0.066374\n",
            "Train Epoch: 393 [1000/1000 (100%)]\tLosses F.softmax: 0.156010 log_softmax: 0.108195\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7530\tAccuracy: 8059.0/10000 (81%)\n",
            "log_softmax: Loss: 0.7537\tAccuracy: 8075.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 394 [0/1000 (0%)]\tLosses F.softmax: 0.011535 log_softmax: 0.002426\n",
            "Train Epoch: 394 [200/1000 (20%)]\tLosses F.softmax: 0.029167 log_softmax: 0.009259\n",
            "Train Epoch: 394 [400/1000 (40%)]\tLosses F.softmax: 0.002864 log_softmax: 0.003130\n",
            "Train Epoch: 394 [600/1000 (60%)]\tLosses F.softmax: 0.035691 log_softmax: 0.016486\n",
            "Train Epoch: 394 [800/1000 (80%)]\tLosses F.softmax: 0.010141 log_softmax: 0.003345\n",
            "Train Epoch: 394 [1000/1000 (100%)]\tLosses F.softmax: 0.009559 log_softmax: 0.017933\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7586\tAccuracy: 8022.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7569\tAccuracy: 8038.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 395 [0/1000 (0%)]\tLosses F.softmax: 0.024980 log_softmax: 0.012083\n",
            "Train Epoch: 395 [200/1000 (20%)]\tLosses F.softmax: 0.166829 log_softmax: 0.094468\n",
            "Train Epoch: 395 [400/1000 (40%)]\tLosses F.softmax: 0.061522 log_softmax: 0.006200\n",
            "Train Epoch: 395 [600/1000 (60%)]\tLosses F.softmax: 0.202653 log_softmax: 0.315895\n",
            "Train Epoch: 395 [800/1000 (80%)]\tLosses F.softmax: 0.000366 log_softmax: 0.000345\n",
            "Train Epoch: 395 [1000/1000 (100%)]\tLosses F.softmax: 0.000123 log_softmax: 0.000080\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7570\tAccuracy: 8041.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7577\tAccuracy: 8052.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 396 [0/1000 (0%)]\tLosses F.softmax: 0.001164 log_softmax: 0.000447\n",
            "Train Epoch: 396 [200/1000 (20%)]\tLosses F.softmax: 0.199622 log_softmax: 0.135356\n",
            "Train Epoch: 396 [400/1000 (40%)]\tLosses F.softmax: 0.009400 log_softmax: 0.005741\n",
            "Train Epoch: 396 [600/1000 (60%)]\tLosses F.softmax: 0.024295 log_softmax: 0.011764\n",
            "Train Epoch: 396 [800/1000 (80%)]\tLosses F.softmax: 0.000865 log_softmax: 0.001393\n",
            "Train Epoch: 396 [1000/1000 (100%)]\tLosses F.softmax: 0.072370 log_softmax: 0.027079\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7591\tAccuracy: 8026.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7597\tAccuracy: 8048.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 397 [0/1000 (0%)]\tLosses F.softmax: 0.000757 log_softmax: 0.001276\n",
            "Train Epoch: 397 [200/1000 (20%)]\tLosses F.softmax: 0.036640 log_softmax: 0.025915\n",
            "Train Epoch: 397 [400/1000 (40%)]\tLosses F.softmax: 0.027946 log_softmax: 0.024333\n",
            "Train Epoch: 397 [600/1000 (60%)]\tLosses F.softmax: 0.000197 log_softmax: 0.000265\n",
            "Train Epoch: 397 [800/1000 (80%)]\tLosses F.softmax: 0.007509 log_softmax: 0.016260\n",
            "Train Epoch: 397 [1000/1000 (100%)]\tLosses F.softmax: 0.313021 log_softmax: 0.135354\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7637\tAccuracy: 8029.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7629\tAccuracy: 8044.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 398 [0/1000 (0%)]\tLosses F.softmax: 0.394724 log_softmax: 0.894589\n",
            "Train Epoch: 398 [200/1000 (20%)]\tLosses F.softmax: 0.018397 log_softmax: 0.031873\n",
            "Train Epoch: 398 [400/1000 (40%)]\tLosses F.softmax: 0.000680 log_softmax: 0.000223\n",
            "Train Epoch: 398 [600/1000 (60%)]\tLosses F.softmax: 0.000489 log_softmax: 0.000241\n",
            "Train Epoch: 398 [800/1000 (80%)]\tLosses F.softmax: 0.037852 log_softmax: 0.026019\n",
            "Train Epoch: 398 [1000/1000 (100%)]\tLosses F.softmax: 0.055490 log_softmax: 0.030035\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7601\tAccuracy: 8042.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7615\tAccuracy: 8044.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 399 [0/1000 (0%)]\tLosses F.softmax: 0.098909 log_softmax: 0.077563\n",
            "Train Epoch: 399 [200/1000 (20%)]\tLosses F.softmax: 0.113008 log_softmax: 0.099858\n",
            "Train Epoch: 399 [400/1000 (40%)]\tLosses F.softmax: 0.021045 log_softmax: 0.044286\n",
            "Train Epoch: 399 [600/1000 (60%)]\tLosses F.softmax: 0.004012 log_softmax: 0.004514\n",
            "Train Epoch: 399 [800/1000 (80%)]\tLosses F.softmax: 0.002086 log_softmax: 0.000190\n",
            "Train Epoch: 399 [1000/1000 (100%)]\tLosses F.softmax: 0.004635 log_softmax: 0.002848\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7614\tAccuracy: 8041.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7623\tAccuracy: 8060.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 400 [0/1000 (0%)]\tLosses F.softmax: 0.000697 log_softmax: 0.000072\n",
            "Train Epoch: 400 [200/1000 (20%)]\tLosses F.softmax: 0.964400 log_softmax: 1.168215\n",
            "Train Epoch: 400 [400/1000 (40%)]\tLosses F.softmax: 0.003234 log_softmax: 0.001076\n",
            "Train Epoch: 400 [600/1000 (60%)]\tLosses F.softmax: 0.173296 log_softmax: 0.126218\n",
            "Train Epoch: 400 [800/1000 (80%)]\tLosses F.softmax: 0.013390 log_softmax: 0.020058\n",
            "Train Epoch: 400 [1000/1000 (100%)]\tLosses F.softmax: 0.001738 log_softmax: 0.000248\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7631\tAccuracy: 8055.0/10000 (81%)\n",
            "log_softmax: Loss: 0.7648\tAccuracy: 8066.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 401 [0/1000 (0%)]\tLosses F.softmax: 0.002484 log_softmax: 0.001651\n",
            "Train Epoch: 401 [200/1000 (20%)]\tLosses F.softmax: 0.005902 log_softmax: 0.007398\n",
            "Train Epoch: 401 [400/1000 (40%)]\tLosses F.softmax: 0.009845 log_softmax: 0.060141\n",
            "Train Epoch: 401 [600/1000 (60%)]\tLosses F.softmax: 0.013898 log_softmax: 0.067785\n",
            "Train Epoch: 401 [800/1000 (80%)]\tLosses F.softmax: 0.058385 log_softmax: 0.030523\n",
            "Train Epoch: 401 [1000/1000 (100%)]\tLosses F.softmax: 0.434641 log_softmax: 0.336607\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7726\tAccuracy: 8012.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7720\tAccuracy: 8023.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 402 [0/1000 (0%)]\tLosses F.softmax: 0.147603 log_softmax: 0.073941\n",
            "Train Epoch: 402 [200/1000 (20%)]\tLosses F.softmax: 0.010440 log_softmax: 0.003114\n",
            "Train Epoch: 402 [400/1000 (40%)]\tLosses F.softmax: 0.097310 log_softmax: 0.109505\n",
            "Train Epoch: 402 [600/1000 (60%)]\tLosses F.softmax: 0.005280 log_softmax: 0.015700\n",
            "Train Epoch: 402 [800/1000 (80%)]\tLosses F.softmax: 0.010052 log_softmax: 0.003498\n",
            "Train Epoch: 402 [1000/1000 (100%)]\tLosses F.softmax: 0.037098 log_softmax: 0.017609\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7692\tAccuracy: 8025.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7707\tAccuracy: 8038.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 403 [0/1000 (0%)]\tLosses F.softmax: 0.000477 log_softmax: 0.000097\n",
            "Train Epoch: 403 [200/1000 (20%)]\tLosses F.softmax: 0.104139 log_softmax: 0.092480\n",
            "Train Epoch: 403 [400/1000 (40%)]\tLosses F.softmax: 0.046115 log_softmax: 0.030173\n",
            "Train Epoch: 403 [600/1000 (60%)]\tLosses F.softmax: 0.014939 log_softmax: 0.004910\n",
            "Train Epoch: 403 [800/1000 (80%)]\tLosses F.softmax: 0.033768 log_softmax: 0.023937\n",
            "Train Epoch: 403 [1000/1000 (100%)]\tLosses F.softmax: 0.131404 log_softmax: 0.194735\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7734\tAccuracy: 8021.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7708\tAccuracy: 8041.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 404 [0/1000 (0%)]\tLosses F.softmax: 0.013139 log_softmax: 0.002020\n",
            "Train Epoch: 404 [200/1000 (20%)]\tLosses F.softmax: 0.005131 log_softmax: 0.001340\n",
            "Train Epoch: 404 [400/1000 (40%)]\tLosses F.softmax: 0.170633 log_softmax: 0.139105\n",
            "Train Epoch: 404 [600/1000 (60%)]\tLosses F.softmax: 0.012738 log_softmax: 0.004105\n",
            "Train Epoch: 404 [800/1000 (80%)]\tLosses F.softmax: 0.017637 log_softmax: 0.005217\n",
            "Train Epoch: 404 [1000/1000 (100%)]\tLosses F.softmax: 0.002667 log_softmax: 0.003100\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7729\tAccuracy: 8022.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7728\tAccuracy: 8044.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 405 [0/1000 (0%)]\tLosses F.softmax: 0.056147 log_softmax: 0.050584\n",
            "Train Epoch: 405 [200/1000 (20%)]\tLosses F.softmax: 0.978867 log_softmax: 0.642096\n",
            "Train Epoch: 405 [400/1000 (40%)]\tLosses F.softmax: 0.000226 log_softmax: 0.001284\n",
            "Train Epoch: 405 [600/1000 (60%)]\tLosses F.softmax: 0.003511 log_softmax: 0.004122\n",
            "Train Epoch: 405 [800/1000 (80%)]\tLosses F.softmax: 0.001273 log_softmax: 0.001277\n",
            "Train Epoch: 405 [1000/1000 (100%)]\tLosses F.softmax: 1.370983 log_softmax: 0.815156\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7823\tAccuracy: 7991.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7776\tAccuracy: 8022.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 406 [0/1000 (0%)]\tLosses F.softmax: 0.173153 log_softmax: 0.179937\n",
            "Train Epoch: 406 [200/1000 (20%)]\tLosses F.softmax: 0.068567 log_softmax: 0.036442\n",
            "Train Epoch: 406 [400/1000 (40%)]\tLosses F.softmax: 0.000053 log_softmax: 0.000037\n",
            "Train Epoch: 406 [600/1000 (60%)]\tLosses F.softmax: 0.002411 log_softmax: 0.001224\n",
            "Train Epoch: 406 [800/1000 (80%)]\tLosses F.softmax: 0.001576 log_softmax: 0.000999\n",
            "Train Epoch: 406 [1000/1000 (100%)]\tLosses F.softmax: 0.032323 log_softmax: 0.013672\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7812\tAccuracy: 8001.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7801\tAccuracy: 8035.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 407 [0/1000 (0%)]\tLosses F.softmax: 0.017187 log_softmax: 0.027928\n",
            "Train Epoch: 407 [200/1000 (20%)]\tLosses F.softmax: 0.023092 log_softmax: 0.031990\n",
            "Train Epoch: 407 [400/1000 (40%)]\tLosses F.softmax: 0.001280 log_softmax: 0.000391\n",
            "Train Epoch: 407 [600/1000 (60%)]\tLosses F.softmax: 0.003471 log_softmax: 0.004233\n",
            "Train Epoch: 407 [800/1000 (80%)]\tLosses F.softmax: 0.161674 log_softmax: 0.208135\n",
            "Train Epoch: 407 [1000/1000 (100%)]\tLosses F.softmax: 0.368663 log_softmax: 0.467277\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7768\tAccuracy: 8034.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7783\tAccuracy: 8041.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 408 [0/1000 (0%)]\tLosses F.softmax: 0.013954 log_softmax: 0.006539\n",
            "Train Epoch: 408 [200/1000 (20%)]\tLosses F.softmax: 0.002832 log_softmax: 0.003581\n",
            "Train Epoch: 408 [400/1000 (40%)]\tLosses F.softmax: 0.026769 log_softmax: 0.043265\n",
            "Train Epoch: 408 [600/1000 (60%)]\tLosses F.softmax: 0.002685 log_softmax: 0.009841\n",
            "Train Epoch: 408 [800/1000 (80%)]\tLosses F.softmax: 0.078621 log_softmax: 0.026227\n",
            "Train Epoch: 408 [1000/1000 (100%)]\tLosses F.softmax: 0.000203 log_softmax: 0.000031\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7797\tAccuracy: 8020.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7801\tAccuracy: 8029.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 409 [0/1000 (0%)]\tLosses F.softmax: 0.004572 log_softmax: 0.005377\n",
            "Train Epoch: 409 [200/1000 (20%)]\tLosses F.softmax: 0.075527 log_softmax: 0.089379\n",
            "Train Epoch: 409 [400/1000 (40%)]\tLosses F.softmax: 0.000010 log_softmax: 0.000004\n",
            "Train Epoch: 409 [600/1000 (60%)]\tLosses F.softmax: 0.000056 log_softmax: 0.000024\n",
            "Train Epoch: 409 [800/1000 (80%)]\tLosses F.softmax: 0.042824 log_softmax: 0.052683\n",
            "Train Epoch: 409 [1000/1000 (100%)]\tLosses F.softmax: 0.052170 log_softmax: 0.036949\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7748\tAccuracy: 8043.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7778\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 410 [0/1000 (0%)]\tLosses F.softmax: 0.004576 log_softmax: 0.001497\n",
            "Train Epoch: 410 [200/1000 (20%)]\tLosses F.softmax: 0.004765 log_softmax: 0.000182\n",
            "Train Epoch: 410 [400/1000 (40%)]\tLosses F.softmax: 0.027812 log_softmax: 0.054844\n",
            "Train Epoch: 410 [600/1000 (60%)]\tLosses F.softmax: 0.003119 log_softmax: 0.001531\n",
            "Train Epoch: 410 [800/1000 (80%)]\tLosses F.softmax: 0.002677 log_softmax: 0.002613\n",
            "Train Epoch: 410 [1000/1000 (100%)]\tLosses F.softmax: 0.122812 log_softmax: 0.143620\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7815\tAccuracy: 8026.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7829\tAccuracy: 8038.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 411 [0/1000 (0%)]\tLosses F.softmax: 0.003232 log_softmax: 0.000509\n",
            "Train Epoch: 411 [200/1000 (20%)]\tLosses F.softmax: 0.066972 log_softmax: 0.041869\n",
            "Train Epoch: 411 [400/1000 (40%)]\tLosses F.softmax: 0.170566 log_softmax: 0.123471\n",
            "Train Epoch: 411 [600/1000 (60%)]\tLosses F.softmax: 0.197650 log_softmax: 0.118754\n",
            "Train Epoch: 411 [800/1000 (80%)]\tLosses F.softmax: 0.004846 log_softmax: 0.005930\n",
            "Train Epoch: 411 [1000/1000 (100%)]\tLosses F.softmax: 0.200797 log_softmax: 0.265977\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7804\tAccuracy: 8045.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7824\tAccuracy: 8043.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 412 [0/1000 (0%)]\tLosses F.softmax: 0.018655 log_softmax: 0.016068\n",
            "Train Epoch: 412 [200/1000 (20%)]\tLosses F.softmax: 0.040200 log_softmax: 0.018240\n",
            "Train Epoch: 412 [400/1000 (40%)]\tLosses F.softmax: 0.034645 log_softmax: 0.022517\n",
            "Train Epoch: 412 [600/1000 (60%)]\tLosses F.softmax: 0.000207 log_softmax: 0.001935\n",
            "Train Epoch: 412 [800/1000 (80%)]\tLosses F.softmax: 0.397295 log_softmax: 0.851369\n",
            "Train Epoch: 412 [1000/1000 (100%)]\tLosses F.softmax: 0.287899 log_softmax: 0.297156\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7812\tAccuracy: 8046.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7838\tAccuracy: 8052.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 413 [0/1000 (0%)]\tLosses F.softmax: 0.001806 log_softmax: 0.006521\n",
            "Train Epoch: 413 [200/1000 (20%)]\tLosses F.softmax: 0.138055 log_softmax: 0.160688\n",
            "Train Epoch: 413 [400/1000 (40%)]\tLosses F.softmax: 0.052157 log_softmax: 0.035863\n",
            "Train Epoch: 413 [600/1000 (60%)]\tLosses F.softmax: 0.276910 log_softmax: 0.143574\n",
            "Train Epoch: 413 [800/1000 (80%)]\tLosses F.softmax: 0.124926 log_softmax: 0.047390\n",
            "Train Epoch: 413 [1000/1000 (100%)]\tLosses F.softmax: 0.015105 log_softmax: 0.001746\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7804\tAccuracy: 8043.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7828\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 414 [0/1000 (0%)]\tLosses F.softmax: 0.029438 log_softmax: 0.028339\n",
            "Train Epoch: 414 [200/1000 (20%)]\tLosses F.softmax: 0.009857 log_softmax: 0.009938\n",
            "Train Epoch: 414 [400/1000 (40%)]\tLosses F.softmax: 0.000212 log_softmax: 0.000423\n",
            "Train Epoch: 414 [600/1000 (60%)]\tLosses F.softmax: 0.000966 log_softmax: 0.000311\n",
            "Train Epoch: 414 [800/1000 (80%)]\tLosses F.softmax: 0.002342 log_softmax: 0.001402\n",
            "Train Epoch: 414 [1000/1000 (100%)]\tLosses F.softmax: 0.023278 log_softmax: 0.063011\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7832\tAccuracy: 8049.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7864\tAccuracy: 8050.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 415 [0/1000 (0%)]\tLosses F.softmax: 0.000233 log_softmax: 0.000513\n",
            "Train Epoch: 415 [200/1000 (20%)]\tLosses F.softmax: 0.000655 log_softmax: 0.001532\n",
            "Train Epoch: 415 [400/1000 (40%)]\tLosses F.softmax: 0.203792 log_softmax: 0.091819\n",
            "Train Epoch: 415 [600/1000 (60%)]\tLosses F.softmax: 0.004902 log_softmax: 0.001101\n",
            "Train Epoch: 415 [800/1000 (80%)]\tLosses F.softmax: 0.010609 log_softmax: 0.004155\n",
            "Train Epoch: 415 [1000/1000 (100%)]\tLosses F.softmax: 0.172301 log_softmax: 0.503095\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7867\tAccuracy: 8028.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7886\tAccuracy: 8044.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 416 [0/1000 (0%)]\tLosses F.softmax: 0.145577 log_softmax: 0.121149\n",
            "Train Epoch: 416 [200/1000 (20%)]\tLosses F.softmax: 0.043995 log_softmax: 0.013866\n",
            "Train Epoch: 416 [400/1000 (40%)]\tLosses F.softmax: 0.427134 log_softmax: 0.830076\n",
            "Train Epoch: 416 [600/1000 (60%)]\tLosses F.softmax: 0.156536 log_softmax: 0.161076\n",
            "Train Epoch: 416 [800/1000 (80%)]\tLosses F.softmax: 0.028626 log_softmax: 0.042167\n",
            "Train Epoch: 416 [1000/1000 (100%)]\tLosses F.softmax: 0.023130 log_softmax: 0.023702\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7863\tAccuracy: 8055.0/10000 (81%)\n",
            "log_softmax: Loss: 0.7899\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 417 [0/1000 (0%)]\tLosses F.softmax: 0.015360 log_softmax: 0.025565\n",
            "Train Epoch: 417 [200/1000 (20%)]\tLosses F.softmax: 0.015855 log_softmax: 0.007645\n",
            "Train Epoch: 417 [400/1000 (40%)]\tLosses F.softmax: 0.138160 log_softmax: 0.174184\n",
            "Train Epoch: 417 [600/1000 (60%)]\tLosses F.softmax: 0.041708 log_softmax: 0.021684\n",
            "Train Epoch: 417 [800/1000 (80%)]\tLosses F.softmax: 0.025837 log_softmax: 0.016248\n",
            "Train Epoch: 417 [1000/1000 (100%)]\tLosses F.softmax: 0.088589 log_softmax: 0.360501\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7866\tAccuracy: 8036.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7901\tAccuracy: 8047.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 418 [0/1000 (0%)]\tLosses F.softmax: 0.000608 log_softmax: 0.000051\n",
            "Train Epoch: 418 [200/1000 (20%)]\tLosses F.softmax: 0.245732 log_softmax: 0.182524\n",
            "Train Epoch: 418 [400/1000 (40%)]\tLosses F.softmax: 0.266181 log_softmax: 0.136278\n",
            "Train Epoch: 418 [600/1000 (60%)]\tLosses F.softmax: 0.001723 log_softmax: 0.000143\n",
            "Train Epoch: 418 [800/1000 (80%)]\tLosses F.softmax: 0.236373 log_softmax: 0.171583\n",
            "Train Epoch: 418 [1000/1000 (100%)]\tLosses F.softmax: 0.017120 log_softmax: 0.000634\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7921\tAccuracy: 8031.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7948\tAccuracy: 8031.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 419 [0/1000 (0%)]\tLosses F.softmax: 0.435962 log_softmax: 0.801945\n",
            "Train Epoch: 419 [200/1000 (20%)]\tLosses F.softmax: 0.002066 log_softmax: 0.001176\n",
            "Train Epoch: 419 [400/1000 (40%)]\tLosses F.softmax: 0.005913 log_softmax: 0.000411\n",
            "Train Epoch: 419 [600/1000 (60%)]\tLosses F.softmax: 0.000192 log_softmax: 0.000620\n",
            "Train Epoch: 419 [800/1000 (80%)]\tLosses F.softmax: 0.369473 log_softmax: 0.825338\n",
            "Train Epoch: 419 [1000/1000 (100%)]\tLosses F.softmax: 0.001231 log_softmax: 0.000826\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7975\tAccuracy: 8021.0/10000 (80%)\n",
            "log_softmax: Loss: 0.8019\tAccuracy: 8024.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 420 [0/1000 (0%)]\tLosses F.softmax: 0.016006 log_softmax: 0.038730\n",
            "Train Epoch: 420 [200/1000 (20%)]\tLosses F.softmax: 0.017774 log_softmax: 0.003844\n",
            "Train Epoch: 420 [400/1000 (40%)]\tLosses F.softmax: 0.003532 log_softmax: 0.011688\n",
            "Train Epoch: 420 [600/1000 (60%)]\tLosses F.softmax: 0.004436 log_softmax: 0.005157\n",
            "Train Epoch: 420 [800/1000 (80%)]\tLosses F.softmax: 0.026155 log_softmax: 0.006920\n",
            "Train Epoch: 420 [1000/1000 (100%)]\tLosses F.softmax: 0.026550 log_softmax: 0.024846\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7928\tAccuracy: 8038.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7955\tAccuracy: 8050.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 421 [0/1000 (0%)]\tLosses F.softmax: 0.035022 log_softmax: 0.011382\n",
            "Train Epoch: 421 [200/1000 (20%)]\tLosses F.softmax: 0.199080 log_softmax: 0.255801\n",
            "Train Epoch: 421 [400/1000 (40%)]\tLosses F.softmax: 0.000055 log_softmax: 0.000021\n",
            "Train Epoch: 421 [600/1000 (60%)]\tLosses F.softmax: 0.020567 log_softmax: 0.012134\n",
            "Train Epoch: 421 [800/1000 (80%)]\tLosses F.softmax: 0.076604 log_softmax: 0.167124\n",
            "Train Epoch: 421 [1000/1000 (100%)]\tLosses F.softmax: 0.048959 log_softmax: 0.043348\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7948\tAccuracy: 8035.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7980\tAccuracy: 8043.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 422 [0/1000 (0%)]\tLosses F.softmax: 0.110827 log_softmax: 0.123283\n",
            "Train Epoch: 422 [200/1000 (20%)]\tLosses F.softmax: 0.004996 log_softmax: 0.002929\n",
            "Train Epoch: 422 [400/1000 (40%)]\tLosses F.softmax: 0.012443 log_softmax: 0.007269\n",
            "Train Epoch: 422 [600/1000 (60%)]\tLosses F.softmax: 0.007405 log_softmax: 0.002044\n",
            "Train Epoch: 422 [800/1000 (80%)]\tLosses F.softmax: 0.159541 log_softmax: 0.075969\n",
            "Train Epoch: 422 [1000/1000 (100%)]\tLosses F.softmax: 0.000157 log_softmax: 0.000359\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7958\tAccuracy: 8047.0/10000 (80%)\n",
            "log_softmax: Loss: 0.7984\tAccuracy: 8057.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 423 [0/1000 (0%)]\tLosses F.softmax: 0.000262 log_softmax: 0.000298\n",
            "Train Epoch: 423 [200/1000 (20%)]\tLosses F.softmax: 0.041781 log_softmax: 0.003319\n",
            "Train Epoch: 423 [400/1000 (40%)]\tLosses F.softmax: 0.004716 log_softmax: 0.004500\n",
            "Train Epoch: 423 [600/1000 (60%)]\tLosses F.softmax: 0.010023 log_softmax: 0.001456\n",
            "Train Epoch: 423 [800/1000 (80%)]\tLosses F.softmax: 0.008948 log_softmax: 0.064859\n",
            "Train Epoch: 423 [1000/1000 (100%)]\tLosses F.softmax: 0.089658 log_softmax: 0.092839\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8031\tAccuracy: 8019.0/10000 (80%)\n",
            "log_softmax: Loss: 0.8026\tAccuracy: 8032.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 424 [0/1000 (0%)]\tLosses F.softmax: 0.008808 log_softmax: 0.035694\n",
            "Train Epoch: 424 [200/1000 (20%)]\tLosses F.softmax: 0.018602 log_softmax: 0.035660\n",
            "Train Epoch: 424 [400/1000 (40%)]\tLosses F.softmax: 0.001987 log_softmax: 0.000241\n",
            "Train Epoch: 424 [600/1000 (60%)]\tLosses F.softmax: 0.000608 log_softmax: 0.000709\n",
            "Train Epoch: 424 [800/1000 (80%)]\tLosses F.softmax: 0.080782 log_softmax: 0.071932\n",
            "Train Epoch: 424 [1000/1000 (100%)]\tLosses F.softmax: 0.010194 log_softmax: 0.013554\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7991\tAccuracy: 8031.0/10000 (80%)\n",
            "log_softmax: Loss: 0.8021\tAccuracy: 8037.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 425 [0/1000 (0%)]\tLosses F.softmax: 0.210915 log_softmax: 0.181888\n",
            "Train Epoch: 425 [200/1000 (20%)]\tLosses F.softmax: 0.083217 log_softmax: 0.043964\n",
            "Train Epoch: 425 [400/1000 (40%)]\tLosses F.softmax: 0.006483 log_softmax: 0.008264\n",
            "Train Epoch: 425 [600/1000 (60%)]\tLosses F.softmax: 0.012458 log_softmax: 0.009815\n",
            "Train Epoch: 425 [800/1000 (80%)]\tLosses F.softmax: 0.023165 log_softmax: 0.002834\n",
            "Train Epoch: 425 [1000/1000 (100%)]\tLosses F.softmax: 0.027545 log_softmax: 0.017482\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8008\tAccuracy: 8029.0/10000 (80%)\n",
            "log_softmax: Loss: 0.8013\tAccuracy: 8044.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 426 [0/1000 (0%)]\tLosses F.softmax: 0.011645 log_softmax: 0.013722\n",
            "Train Epoch: 426 [200/1000 (20%)]\tLosses F.softmax: 0.002322 log_softmax: 0.000604\n",
            "Train Epoch: 426 [400/1000 (40%)]\tLosses F.softmax: 0.165205 log_softmax: 0.157527\n",
            "Train Epoch: 426 [600/1000 (60%)]\tLosses F.softmax: 0.081056 log_softmax: 0.123898\n",
            "Train Epoch: 426 [800/1000 (80%)]\tLosses F.softmax: 0.020887 log_softmax: 0.055020\n",
            "Train Epoch: 426 [1000/1000 (100%)]\tLosses F.softmax: 0.000323 log_softmax: 0.001748\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7993\tAccuracy: 8049.0/10000 (80%)\n",
            "log_softmax: Loss: 0.8041\tAccuracy: 8049.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 427 [0/1000 (0%)]\tLosses F.softmax: 0.000257 log_softmax: 0.000130\n",
            "Train Epoch: 427 [200/1000 (20%)]\tLosses F.softmax: 0.115364 log_softmax: 0.050649\n",
            "Train Epoch: 427 [400/1000 (40%)]\tLosses F.softmax: 0.000120 log_softmax: 0.000507\n",
            "Train Epoch: 427 [600/1000 (60%)]\tLosses F.softmax: 0.000292 log_softmax: 0.005143\n",
            "Train Epoch: 427 [800/1000 (80%)]\tLosses F.softmax: 0.000983 log_softmax: 0.004500\n",
            "Train Epoch: 427 [1000/1000 (100%)]\tLosses F.softmax: 0.078313 log_softmax: 0.124244\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8035\tAccuracy: 8029.0/10000 (80%)\n",
            "log_softmax: Loss: 0.8051\tAccuracy: 8043.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 428 [0/1000 (0%)]\tLosses F.softmax: 0.007944 log_softmax: 0.000723\n",
            "Train Epoch: 428 [200/1000 (20%)]\tLosses F.softmax: 0.004061 log_softmax: 0.003731\n",
            "Train Epoch: 428 [400/1000 (40%)]\tLosses F.softmax: 0.163503 log_softmax: 0.145563\n",
            "Train Epoch: 428 [600/1000 (60%)]\tLosses F.softmax: 0.053594 log_softmax: 0.044426\n",
            "Train Epoch: 428 [800/1000 (80%)]\tLosses F.softmax: 0.085151 log_softmax: 0.065464\n",
            "Train Epoch: 428 [1000/1000 (100%)]\tLosses F.softmax: 0.051874 log_softmax: 0.090424\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8061\tAccuracy: 8036.0/10000 (80%)\n",
            "log_softmax: Loss: 0.8088\tAccuracy: 8037.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 429 [0/1000 (0%)]\tLosses F.softmax: 0.023842 log_softmax: 0.021478\n",
            "Train Epoch: 429 [200/1000 (20%)]\tLosses F.softmax: 0.000294 log_softmax: 0.004572\n",
            "Train Epoch: 429 [400/1000 (40%)]\tLosses F.softmax: 0.207259 log_softmax: 0.298523\n",
            "Train Epoch: 429 [600/1000 (60%)]\tLosses F.softmax: 0.005264 log_softmax: 0.002615\n",
            "Train Epoch: 429 [800/1000 (80%)]\tLosses F.softmax: 0.045266 log_softmax: 0.010744\n",
            "Train Epoch: 429 [1000/1000 (100%)]\tLosses F.softmax: 0.000756 log_softmax: 0.000626\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8063\tAccuracy: 8041.0/10000 (80%)\n",
            "log_softmax: Loss: 0.8102\tAccuracy: 8046.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 430 [0/1000 (0%)]\tLosses F.softmax: 0.084028 log_softmax: 0.133789\n",
            "Train Epoch: 430 [200/1000 (20%)]\tLosses F.softmax: 0.007122 log_softmax: 0.006297\n",
            "Train Epoch: 430 [400/1000 (40%)]\tLosses F.softmax: 0.129460 log_softmax: 0.096456\n",
            "Train Epoch: 430 [600/1000 (60%)]\tLosses F.softmax: 0.001863 log_softmax: 0.000843\n",
            "Train Epoch: 430 [800/1000 (80%)]\tLosses F.softmax: 0.065753 log_softmax: 0.019879\n",
            "Train Epoch: 430 [1000/1000 (100%)]\tLosses F.softmax: 0.008134 log_softmax: 0.007781\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8074\tAccuracy: 8044.0/10000 (80%)\n",
            "log_softmax: Loss: 0.8117\tAccuracy: 8039.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 431 [0/1000 (0%)]\tLosses F.softmax: 0.001826 log_softmax: 0.000997\n",
            "Train Epoch: 431 [200/1000 (20%)]\tLosses F.softmax: 0.030984 log_softmax: 0.019476\n",
            "Train Epoch: 431 [400/1000 (40%)]\tLosses F.softmax: 0.009368 log_softmax: 0.004552\n",
            "Train Epoch: 431 [600/1000 (60%)]\tLosses F.softmax: 0.000286 log_softmax: 0.000049\n",
            "Train Epoch: 431 [800/1000 (80%)]\tLosses F.softmax: 0.005935 log_softmax: 0.000784\n",
            "Train Epoch: 431 [1000/1000 (100%)]\tLosses F.softmax: 0.079490 log_softmax: 0.082369\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8096\tAccuracy: 8037.0/10000 (80%)\n",
            "log_softmax: Loss: 0.8132\tAccuracy: 8037.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 432 [0/1000 (0%)]\tLosses F.softmax: 0.002841 log_softmax: 0.006141\n",
            "Train Epoch: 432 [200/1000 (20%)]\tLosses F.softmax: 0.064853 log_softmax: 0.035670\n",
            "Train Epoch: 432 [400/1000 (40%)]\tLosses F.softmax: 0.011877 log_softmax: 0.010107\n",
            "Train Epoch: 432 [600/1000 (60%)]\tLosses F.softmax: 0.000327 log_softmax: 0.000153\n",
            "Train Epoch: 432 [800/1000 (80%)]\tLosses F.softmax: 0.018254 log_softmax: 0.048074\n",
            "Train Epoch: 432 [1000/1000 (100%)]\tLosses F.softmax: 0.001307 log_softmax: 0.000542\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8087\tAccuracy: 8046.0/10000 (80%)\n",
            "log_softmax: Loss: 0.8144\tAccuracy: 8036.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 433 [0/1000 (0%)]\tLosses F.softmax: 0.010088 log_softmax: 0.004905\n",
            "Train Epoch: 433 [200/1000 (20%)]\tLosses F.softmax: 0.004132 log_softmax: 0.007109\n",
            "Train Epoch: 433 [400/1000 (40%)]\tLosses F.softmax: 0.021176 log_softmax: 0.003906\n",
            "Train Epoch: 433 [600/1000 (60%)]\tLosses F.softmax: 0.000030 log_softmax: 0.000013\n",
            "Train Epoch: 433 [800/1000 (80%)]\tLosses F.softmax: 0.004337 log_softmax: 0.004501\n",
            "Train Epoch: 433 [1000/1000 (100%)]\tLosses F.softmax: 0.083960 log_softmax: 0.043739\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8078\tAccuracy: 8048.0/10000 (80%)\n",
            "log_softmax: Loss: 0.8127\tAccuracy: 8060.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 434 [0/1000 (0%)]\tLosses F.softmax: 0.019329 log_softmax: 0.027202\n",
            "Train Epoch: 434 [200/1000 (20%)]\tLosses F.softmax: 0.003838 log_softmax: 0.000597\n",
            "Train Epoch: 434 [400/1000 (40%)]\tLosses F.softmax: 0.000192 log_softmax: 0.000003\n",
            "Train Epoch: 434 [600/1000 (60%)]\tLosses F.softmax: 0.010803 log_softmax: 0.003097\n",
            "Train Epoch: 434 [800/1000 (80%)]\tLosses F.softmax: 0.001432 log_softmax: 0.001658\n",
            "Train Epoch: 434 [1000/1000 (100%)]\tLosses F.softmax: 0.025442 log_softmax: 0.005824\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8130\tAccuracy: 8039.0/10000 (80%)\n",
            "log_softmax: Loss: 0.8167\tAccuracy: 8045.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 435 [0/1000 (0%)]\tLosses F.softmax: 0.000052 log_softmax: 0.000240\n",
            "Train Epoch: 435 [200/1000 (20%)]\tLosses F.softmax: 0.001368 log_softmax: 0.000604\n",
            "Train Epoch: 435 [400/1000 (40%)]\tLosses F.softmax: 0.028946 log_softmax: 0.016188\n",
            "Train Epoch: 435 [600/1000 (60%)]\tLosses F.softmax: 0.105097 log_softmax: 0.073955\n",
            "Train Epoch: 435 [800/1000 (80%)]\tLosses F.softmax: 0.164425 log_softmax: 0.097972\n",
            "Train Epoch: 435 [1000/1000 (100%)]\tLosses F.softmax: 0.005442 log_softmax: 0.000172\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8123\tAccuracy: 8042.0/10000 (80%)\n",
            "log_softmax: Loss: 0.8175\tAccuracy: 8045.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 436 [0/1000 (0%)]\tLosses F.softmax: 0.082153 log_softmax: 0.128288\n",
            "Train Epoch: 436 [200/1000 (20%)]\tLosses F.softmax: 0.224879 log_softmax: 0.133725\n",
            "Train Epoch: 436 [400/1000 (40%)]\tLosses F.softmax: 0.001033 log_softmax: 0.000395\n",
            "Train Epoch: 436 [600/1000 (60%)]\tLosses F.softmax: 0.001345 log_softmax: 0.000191\n",
            "Train Epoch: 436 [800/1000 (80%)]\tLosses F.softmax: 0.209494 log_softmax: 0.495086\n",
            "Train Epoch: 436 [1000/1000 (100%)]\tLosses F.softmax: 0.000413 log_softmax: 0.000091\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8160\tAccuracy: 8025.0/10000 (80%)\n",
            "log_softmax: Loss: 0.8203\tAccuracy: 8039.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 437 [0/1000 (0%)]\tLosses F.softmax: 0.066408 log_softmax: 0.106602\n",
            "Train Epoch: 437 [200/1000 (20%)]\tLosses F.softmax: 0.032388 log_softmax: 0.025103\n",
            "Train Epoch: 437 [400/1000 (40%)]\tLosses F.softmax: 0.000055 log_softmax: 0.000036\n",
            "Train Epoch: 437 [600/1000 (60%)]\tLosses F.softmax: 0.005529 log_softmax: 0.006211\n",
            "Train Epoch: 437 [800/1000 (80%)]\tLosses F.softmax: 0.002944 log_softmax: 0.000538\n",
            "Train Epoch: 437 [1000/1000 (100%)]\tLosses F.softmax: 0.030722 log_softmax: 0.066594\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8179\tAccuracy: 8048.0/10000 (80%)\n",
            "log_softmax: Loss: 0.8221\tAccuracy: 8036.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 438 [0/1000 (0%)]\tLosses F.softmax: 0.189242 log_softmax: 0.091779\n",
            "Train Epoch: 438 [200/1000 (20%)]\tLosses F.softmax: 0.112416 log_softmax: 0.175425\n",
            "Train Epoch: 438 [400/1000 (40%)]\tLosses F.softmax: 0.033350 log_softmax: 0.029141\n",
            "Train Epoch: 438 [600/1000 (60%)]\tLosses F.softmax: 0.148407 log_softmax: 0.255038\n",
            "Train Epoch: 438 [800/1000 (80%)]\tLosses F.softmax: 0.012171 log_softmax: 0.205297\n",
            "Train Epoch: 438 [1000/1000 (100%)]\tLosses F.softmax: 0.085217 log_softmax: 0.096187\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8165\tAccuracy: 8049.0/10000 (80%)\n",
            "log_softmax: Loss: 0.8219\tAccuracy: 8043.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 439 [0/1000 (0%)]\tLosses F.softmax: 0.001785 log_softmax: 0.003566\n",
            "Train Epoch: 439 [200/1000 (20%)]\tLosses F.softmax: 0.006714 log_softmax: 0.006974\n",
            "Train Epoch: 439 [400/1000 (40%)]\tLosses F.softmax: 0.000207 log_softmax: 0.000425\n",
            "Train Epoch: 439 [600/1000 (60%)]\tLosses F.softmax: 0.034906 log_softmax: 0.040922\n",
            "Train Epoch: 439 [800/1000 (80%)]\tLosses F.softmax: 0.019198 log_softmax: 0.000528\n",
            "Train Epoch: 439 [1000/1000 (100%)]\tLosses F.softmax: 0.001289 log_softmax: 0.000522\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8190\tAccuracy: 8047.0/10000 (80%)\n",
            "log_softmax: Loss: 0.8241\tAccuracy: 8040.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 440 [0/1000 (0%)]\tLosses F.softmax: 0.023545 log_softmax: 0.009111\n",
            "Train Epoch: 440 [200/1000 (20%)]\tLosses F.softmax: 0.000844 log_softmax: 0.000480\n",
            "Train Epoch: 440 [400/1000 (40%)]\tLosses F.softmax: 0.001169 log_softmax: 0.000193\n",
            "Train Epoch: 440 [600/1000 (60%)]\tLosses F.softmax: 0.000430 log_softmax: 0.000032\n",
            "Train Epoch: 440 [800/1000 (80%)]\tLosses F.softmax: 0.004166 log_softmax: 0.003280\n",
            "Train Epoch: 440 [1000/1000 (100%)]\tLosses F.softmax: 0.000613 log_softmax: 0.000213\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8197\tAccuracy: 8052.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8263\tAccuracy: 8033.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 441 [0/1000 (0%)]\tLosses F.softmax: 0.041032 log_softmax: 0.038348\n",
            "Train Epoch: 441 [200/1000 (20%)]\tLosses F.softmax: 0.006961 log_softmax: 0.003740\n",
            "Train Epoch: 441 [400/1000 (40%)]\tLosses F.softmax: 0.018301 log_softmax: 0.041084\n",
            "Train Epoch: 441 [600/1000 (60%)]\tLosses F.softmax: 0.102078 log_softmax: 0.061886\n",
            "Train Epoch: 441 [800/1000 (80%)]\tLosses F.softmax: 0.022813 log_softmax: 0.004515\n",
            "Train Epoch: 441 [1000/1000 (100%)]\tLosses F.softmax: 0.000340 log_softmax: 0.000107\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8181\tAccuracy: 8056.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8236\tAccuracy: 8040.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 442 [0/1000 (0%)]\tLosses F.softmax: 0.000544 log_softmax: 0.000527\n",
            "Train Epoch: 442 [200/1000 (20%)]\tLosses F.softmax: 0.014271 log_softmax: 0.016270\n",
            "Train Epoch: 442 [400/1000 (40%)]\tLosses F.softmax: 0.032268 log_softmax: 0.023378\n",
            "Train Epoch: 442 [600/1000 (60%)]\tLosses F.softmax: 0.005842 log_softmax: 0.006418\n",
            "Train Epoch: 442 [800/1000 (80%)]\tLosses F.softmax: 0.016956 log_softmax: 0.003088\n",
            "Train Epoch: 442 [1000/1000 (100%)]\tLosses F.softmax: 0.004162 log_softmax: 0.003849\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8232\tAccuracy: 8041.0/10000 (80%)\n",
            "log_softmax: Loss: 0.8276\tAccuracy: 8039.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 443 [0/1000 (0%)]\tLosses F.softmax: 0.000664 log_softmax: 0.000813\n",
            "Train Epoch: 443 [200/1000 (20%)]\tLosses F.softmax: 0.101946 log_softmax: 0.178041\n",
            "Train Epoch: 443 [400/1000 (40%)]\tLosses F.softmax: 0.010055 log_softmax: 0.034369\n",
            "Train Epoch: 443 [600/1000 (60%)]\tLosses F.softmax: 0.006997 log_softmax: 0.027825\n",
            "Train Epoch: 443 [800/1000 (80%)]\tLosses F.softmax: 0.006528 log_softmax: 0.001393\n",
            "Train Epoch: 443 [1000/1000 (100%)]\tLosses F.softmax: 0.000060 log_softmax: 0.000154\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8216\tAccuracy: 8055.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8276\tAccuracy: 8037.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 444 [0/1000 (0%)]\tLosses F.softmax: 0.179512 log_softmax: 0.443398\n",
            "Train Epoch: 444 [200/1000 (20%)]\tLosses F.softmax: 0.005895 log_softmax: 0.004140\n",
            "Train Epoch: 444 [400/1000 (40%)]\tLosses F.softmax: 0.001097 log_softmax: 0.000182\n",
            "Train Epoch: 444 [600/1000 (60%)]\tLosses F.softmax: 0.001233 log_softmax: 0.000310\n",
            "Train Epoch: 444 [800/1000 (80%)]\tLosses F.softmax: 0.019017 log_softmax: 0.025991\n",
            "Train Epoch: 444 [1000/1000 (100%)]\tLosses F.softmax: 0.003087 log_softmax: 0.000110\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8215\tAccuracy: 8060.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8280\tAccuracy: 8057.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 445 [0/1000 (0%)]\tLosses F.softmax: 0.021528 log_softmax: 0.008062\n",
            "Train Epoch: 445 [200/1000 (20%)]\tLosses F.softmax: 0.000440 log_softmax: 0.000171\n",
            "Train Epoch: 445 [400/1000 (40%)]\tLosses F.softmax: 0.031485 log_softmax: 0.019116\n",
            "Train Epoch: 445 [600/1000 (60%)]\tLosses F.softmax: 0.196768 log_softmax: 0.139671\n",
            "Train Epoch: 445 [800/1000 (80%)]\tLosses F.softmax: 0.000097 log_softmax: 0.000074\n",
            "Train Epoch: 445 [1000/1000 (100%)]\tLosses F.softmax: 0.021393 log_softmax: 0.012321\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8264\tAccuracy: 8042.0/10000 (80%)\n",
            "log_softmax: Loss: 0.8306\tAccuracy: 8040.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 446 [0/1000 (0%)]\tLosses F.softmax: 0.007880 log_softmax: 0.005229\n",
            "Train Epoch: 446 [200/1000 (20%)]\tLosses F.softmax: 0.028399 log_softmax: 0.009291\n",
            "Train Epoch: 446 [400/1000 (40%)]\tLosses F.softmax: 0.037226 log_softmax: 0.070856\n",
            "Train Epoch: 446 [600/1000 (60%)]\tLosses F.softmax: 0.172418 log_softmax: 0.084420\n",
            "Train Epoch: 446 [800/1000 (80%)]\tLosses F.softmax: 0.024516 log_softmax: 0.030086\n",
            "Train Epoch: 446 [1000/1000 (100%)]\tLosses F.softmax: 0.257598 log_softmax: 0.069547\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8269\tAccuracy: 8043.0/10000 (80%)\n",
            "log_softmax: Loss: 0.8331\tAccuracy: 8041.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 447 [0/1000 (0%)]\tLosses F.softmax: 0.000386 log_softmax: 0.000090\n",
            "Train Epoch: 447 [200/1000 (20%)]\tLosses F.softmax: 0.002807 log_softmax: 0.000485\n",
            "Train Epoch: 447 [400/1000 (40%)]\tLosses F.softmax: 0.011351 log_softmax: 0.007564\n",
            "Train Epoch: 447 [600/1000 (60%)]\tLosses F.softmax: 0.009373 log_softmax: 0.001092\n",
            "Train Epoch: 447 [800/1000 (80%)]\tLosses F.softmax: 0.107882 log_softmax: 0.078816\n",
            "Train Epoch: 447 [1000/1000 (100%)]\tLosses F.softmax: 0.000077 log_softmax: 0.000042\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8274\tAccuracy: 8050.0/10000 (80%)\n",
            "log_softmax: Loss: 0.8339\tAccuracy: 8049.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 448 [0/1000 (0%)]\tLosses F.softmax: 0.008089 log_softmax: 0.003252\n",
            "Train Epoch: 448 [200/1000 (20%)]\tLosses F.softmax: 0.000107 log_softmax: 0.000001\n",
            "Train Epoch: 448 [400/1000 (40%)]\tLosses F.softmax: 0.000372 log_softmax: 0.000113\n",
            "Train Epoch: 448 [600/1000 (60%)]\tLosses F.softmax: 0.000336 log_softmax: 0.000156\n",
            "Train Epoch: 448 [800/1000 (80%)]\tLosses F.softmax: 0.005360 log_softmax: 0.000920\n",
            "Train Epoch: 448 [1000/1000 (100%)]\tLosses F.softmax: 0.001820 log_softmax: 0.000228\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8278\tAccuracy: 8064.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8334\tAccuracy: 8044.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 449 [0/1000 (0%)]\tLosses F.softmax: 0.023127 log_softmax: 0.010907\n",
            "Train Epoch: 449 [200/1000 (20%)]\tLosses F.softmax: 0.004317 log_softmax: 0.003275\n",
            "Train Epoch: 449 [400/1000 (40%)]\tLosses F.softmax: 0.000116 log_softmax: 0.001517\n",
            "Train Epoch: 449 [600/1000 (60%)]\tLosses F.softmax: 0.001665 log_softmax: 0.000329\n",
            "Train Epoch: 449 [800/1000 (80%)]\tLosses F.softmax: 0.637252 log_softmax: 0.606528\n",
            "Train Epoch: 449 [1000/1000 (100%)]\tLosses F.softmax: 0.027142 log_softmax: 0.003305\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8291\tAccuracy: 8065.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8350\tAccuracy: 8050.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 450 [0/1000 (0%)]\tLosses F.softmax: 0.173478 log_softmax: 0.149855\n",
            "Train Epoch: 450 [200/1000 (20%)]\tLosses F.softmax: 0.082332 log_softmax: 0.132946\n",
            "Train Epoch: 450 [400/1000 (40%)]\tLosses F.softmax: 0.176002 log_softmax: 0.380464\n",
            "Train Epoch: 450 [600/1000 (60%)]\tLosses F.softmax: 0.007048 log_softmax: 0.004013\n",
            "Train Epoch: 450 [800/1000 (80%)]\tLosses F.softmax: 0.034888 log_softmax: 0.017363\n",
            "Train Epoch: 450 [1000/1000 (100%)]\tLosses F.softmax: 0.056397 log_softmax: 0.057175\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8290\tAccuracy: 8070.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8354\tAccuracy: 8052.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 451 [0/1000 (0%)]\tLosses F.softmax: 0.004139 log_softmax: 0.002867\n",
            "Train Epoch: 451 [200/1000 (20%)]\tLosses F.softmax: 0.033235 log_softmax: 0.008130\n",
            "Train Epoch: 451 [400/1000 (40%)]\tLosses F.softmax: 0.005000 log_softmax: 0.000884\n",
            "Train Epoch: 451 [600/1000 (60%)]\tLosses F.softmax: 0.040714 log_softmax: 0.043010\n",
            "Train Epoch: 451 [800/1000 (80%)]\tLosses F.softmax: 0.000002 log_softmax: 0.000001\n",
            "Train Epoch: 451 [1000/1000 (100%)]\tLosses F.softmax: 0.001017 log_softmax: 0.000149\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8305\tAccuracy: 8064.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8377\tAccuracy: 8047.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 452 [0/1000 (0%)]\tLosses F.softmax: 0.000387 log_softmax: 0.000183\n",
            "Train Epoch: 452 [200/1000 (20%)]\tLosses F.softmax: 0.016856 log_softmax: 0.016700\n",
            "Train Epoch: 452 [400/1000 (40%)]\tLosses F.softmax: 0.107956 log_softmax: 0.090203\n",
            "Train Epoch: 452 [600/1000 (60%)]\tLosses F.softmax: 0.117731 log_softmax: 0.123432\n",
            "Train Epoch: 452 [800/1000 (80%)]\tLosses F.softmax: 0.029468 log_softmax: 0.013946\n",
            "Train Epoch: 452 [1000/1000 (100%)]\tLosses F.softmax: 0.002140 log_softmax: 0.002304\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8343\tAccuracy: 8052.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8400\tAccuracy: 8051.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 453 [0/1000 (0%)]\tLosses F.softmax: 0.004389 log_softmax: 0.000845\n",
            "Train Epoch: 453 [200/1000 (20%)]\tLosses F.softmax: 0.000526 log_softmax: 0.000347\n",
            "Train Epoch: 453 [400/1000 (40%)]\tLosses F.softmax: 0.001818 log_softmax: 0.021044\n",
            "Train Epoch: 453 [600/1000 (60%)]\tLosses F.softmax: 0.000569 log_softmax: 0.000187\n",
            "Train Epoch: 453 [800/1000 (80%)]\tLosses F.softmax: 0.002982 log_softmax: 0.006351\n",
            "Train Epoch: 453 [1000/1000 (100%)]\tLosses F.softmax: 0.004364 log_softmax: 0.001445\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8336\tAccuracy: 8061.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8408\tAccuracy: 8049.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 454 [0/1000 (0%)]\tLosses F.softmax: 0.457222 log_softmax: 0.375133\n",
            "Train Epoch: 454 [200/1000 (20%)]\tLosses F.softmax: 0.000073 log_softmax: 0.000733\n",
            "Train Epoch: 454 [400/1000 (40%)]\tLosses F.softmax: 0.006351 log_softmax: 0.005155\n",
            "Train Epoch: 454 [600/1000 (60%)]\tLosses F.softmax: 0.003314 log_softmax: 0.000100\n",
            "Train Epoch: 454 [800/1000 (80%)]\tLosses F.softmax: 0.001199 log_softmax: 0.006916\n",
            "Train Epoch: 454 [1000/1000 (100%)]\tLosses F.softmax: 0.004002 log_softmax: 0.009346\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8376\tAccuracy: 8052.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8432\tAccuracy: 8037.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 455 [0/1000 (0%)]\tLosses F.softmax: 0.008475 log_softmax: 0.003128\n",
            "Train Epoch: 455 [200/1000 (20%)]\tLosses F.softmax: 0.077071 log_softmax: 0.045401\n",
            "Train Epoch: 455 [400/1000 (40%)]\tLosses F.softmax: 0.000397 log_softmax: 0.000067\n",
            "Train Epoch: 455 [600/1000 (60%)]\tLosses F.softmax: 0.000105 log_softmax: 0.000002\n",
            "Train Epoch: 455 [800/1000 (80%)]\tLosses F.softmax: 0.014858 log_softmax: 0.017554\n",
            "Train Epoch: 455 [1000/1000 (100%)]\tLosses F.softmax: 0.003548 log_softmax: 0.004572\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8426\tAccuracy: 8034.0/10000 (80%)\n",
            "log_softmax: Loss: 0.8486\tAccuracy: 8030.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 456 [0/1000 (0%)]\tLosses F.softmax: 0.000002 log_softmax: 0.000001\n",
            "Train Epoch: 456 [200/1000 (20%)]\tLosses F.softmax: 0.179249 log_softmax: 0.117504\n",
            "Train Epoch: 456 [400/1000 (40%)]\tLosses F.softmax: 0.007692 log_softmax: 0.009824\n",
            "Train Epoch: 456 [600/1000 (60%)]\tLosses F.softmax: 0.001650 log_softmax: 0.003654\n",
            "Train Epoch: 456 [800/1000 (80%)]\tLosses F.softmax: 0.175367 log_softmax: 0.105110\n",
            "Train Epoch: 456 [1000/1000 (100%)]\tLosses F.softmax: 0.031778 log_softmax: 0.063848\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8386\tAccuracy: 8071.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8464\tAccuracy: 8058.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 457 [0/1000 (0%)]\tLosses F.softmax: 0.019139 log_softmax: 0.019445\n",
            "Train Epoch: 457 [200/1000 (20%)]\tLosses F.softmax: 0.005092 log_softmax: 0.001156\n",
            "Train Epoch: 457 [400/1000 (40%)]\tLosses F.softmax: 0.148954 log_softmax: 0.069278\n",
            "Train Epoch: 457 [600/1000 (60%)]\tLosses F.softmax: 0.001654 log_softmax: 0.000747\n",
            "Train Epoch: 457 [800/1000 (80%)]\tLosses F.softmax: 0.007031 log_softmax: 0.004344\n",
            "Train Epoch: 457 [1000/1000 (100%)]\tLosses F.softmax: 0.145114 log_softmax: 0.050742\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8397\tAccuracy: 8047.0/10000 (80%)\n",
            "log_softmax: Loss: 0.8460\tAccuracy: 8051.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 458 [0/1000 (0%)]\tLosses F.softmax: 0.054028 log_softmax: 0.087430\n",
            "Train Epoch: 458 [200/1000 (20%)]\tLosses F.softmax: 0.047046 log_softmax: 0.045330\n",
            "Train Epoch: 458 [400/1000 (40%)]\tLosses F.softmax: 0.016121 log_softmax: 0.007204\n",
            "Train Epoch: 458 [600/1000 (60%)]\tLosses F.softmax: 0.003399 log_softmax: 0.000980\n",
            "Train Epoch: 458 [800/1000 (80%)]\tLosses F.softmax: 0.011134 log_softmax: 0.019538\n",
            "Train Epoch: 458 [1000/1000 (100%)]\tLosses F.softmax: 0.000384 log_softmax: 0.000164\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8400\tAccuracy: 8071.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8480\tAccuracy: 8049.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 459 [0/1000 (0%)]\tLosses F.softmax: 0.009316 log_softmax: 0.006325\n",
            "Train Epoch: 459 [200/1000 (20%)]\tLosses F.softmax: 0.001343 log_softmax: 0.002387\n",
            "Train Epoch: 459 [400/1000 (40%)]\tLosses F.softmax: 0.085395 log_softmax: 0.115596\n",
            "Train Epoch: 459 [600/1000 (60%)]\tLosses F.softmax: 0.001416 log_softmax: 0.000161\n",
            "Train Epoch: 459 [800/1000 (80%)]\tLosses F.softmax: 0.006684 log_softmax: 0.007308\n",
            "Train Epoch: 459 [1000/1000 (100%)]\tLosses F.softmax: 0.111554 log_softmax: 0.188612\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8422\tAccuracy: 8062.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8490\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 460 [0/1000 (0%)]\tLosses F.softmax: 0.001878 log_softmax: 0.005499\n",
            "Train Epoch: 460 [200/1000 (20%)]\tLosses F.softmax: 0.057604 log_softmax: 0.050009\n",
            "Train Epoch: 460 [400/1000 (40%)]\tLosses F.softmax: 0.044723 log_softmax: 0.083104\n",
            "Train Epoch: 460 [600/1000 (60%)]\tLosses F.softmax: 0.000677 log_softmax: 0.000259\n",
            "Train Epoch: 460 [800/1000 (80%)]\tLosses F.softmax: 0.001800 log_softmax: 0.002071\n",
            "Train Epoch: 460 [1000/1000 (100%)]\tLosses F.softmax: 0.006981 log_softmax: 0.002698\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8461\tAccuracy: 8047.0/10000 (80%)\n",
            "log_softmax: Loss: 0.8531\tAccuracy: 8039.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 461 [0/1000 (0%)]\tLosses F.softmax: 0.189339 log_softmax: 0.053868\n",
            "Train Epoch: 461 [200/1000 (20%)]\tLosses F.softmax: 0.000495 log_softmax: 0.000150\n",
            "Train Epoch: 461 [400/1000 (40%)]\tLosses F.softmax: 0.010558 log_softmax: 0.011190\n",
            "Train Epoch: 461 [600/1000 (60%)]\tLosses F.softmax: 0.073591 log_softmax: 0.129123\n",
            "Train Epoch: 461 [800/1000 (80%)]\tLosses F.softmax: 0.020686 log_softmax: 0.011135\n",
            "Train Epoch: 461 [1000/1000 (100%)]\tLosses F.softmax: 0.000002 log_softmax: 0.000055\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8440\tAccuracy: 8077.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8511\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 462 [0/1000 (0%)]\tLosses F.softmax: 0.081341 log_softmax: 0.012279\n",
            "Train Epoch: 462 [200/1000 (20%)]\tLosses F.softmax: 0.040731 log_softmax: 0.006072\n",
            "Train Epoch: 462 [400/1000 (40%)]\tLosses F.softmax: 0.002624 log_softmax: 0.003652\n",
            "Train Epoch: 462 [600/1000 (60%)]\tLosses F.softmax: 0.011124 log_softmax: 0.037501\n",
            "Train Epoch: 462 [800/1000 (80%)]\tLosses F.softmax: 0.081107 log_softmax: 0.080194\n",
            "Train Epoch: 462 [1000/1000 (100%)]\tLosses F.softmax: 0.011288 log_softmax: 0.173157\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8457\tAccuracy: 8072.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8529\tAccuracy: 8049.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 463 [0/1000 (0%)]\tLosses F.softmax: 0.013089 log_softmax: 0.016781\n",
            "Train Epoch: 463 [200/1000 (20%)]\tLosses F.softmax: 0.097429 log_softmax: 0.054048\n",
            "Train Epoch: 463 [400/1000 (40%)]\tLosses F.softmax: 0.026064 log_softmax: 0.017312\n",
            "Train Epoch: 463 [600/1000 (60%)]\tLosses F.softmax: 0.008714 log_softmax: 0.000162\n",
            "Train Epoch: 463 [800/1000 (80%)]\tLosses F.softmax: 0.003001 log_softmax: 0.003225\n",
            "Train Epoch: 463 [1000/1000 (100%)]\tLosses F.softmax: 0.727742 log_softmax: 1.065806\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8472\tAccuracy: 8061.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8545\tAccuracy: 8052.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 464 [0/1000 (0%)]\tLosses F.softmax: 0.000256 log_softmax: 0.000258\n",
            "Train Epoch: 464 [200/1000 (20%)]\tLosses F.softmax: 0.039783 log_softmax: 0.065742\n",
            "Train Epoch: 464 [400/1000 (40%)]\tLosses F.softmax: 0.001038 log_softmax: 0.000866\n",
            "Train Epoch: 464 [600/1000 (60%)]\tLosses F.softmax: 0.176173 log_softmax: 0.116285\n",
            "Train Epoch: 464 [800/1000 (80%)]\tLosses F.softmax: 0.070836 log_softmax: 0.043277\n",
            "Train Epoch: 464 [1000/1000 (100%)]\tLosses F.softmax: 0.032737 log_softmax: 0.013556\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8488\tAccuracy: 8067.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8562\tAccuracy: 8041.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 465 [0/1000 (0%)]\tLosses F.softmax: 0.048068 log_softmax: 0.016802\n",
            "Train Epoch: 465 [200/1000 (20%)]\tLosses F.softmax: 0.000873 log_softmax: 0.000271\n",
            "Train Epoch: 465 [400/1000 (40%)]\tLosses F.softmax: 0.001243 log_softmax: 0.011420\n",
            "Train Epoch: 465 [600/1000 (60%)]\tLosses F.softmax: 0.000421 log_softmax: 0.000479\n",
            "Train Epoch: 465 [800/1000 (80%)]\tLosses F.softmax: 0.073352 log_softmax: 0.104647\n",
            "Train Epoch: 465 [1000/1000 (100%)]\tLosses F.softmax: 0.000060 log_softmax: 0.000518\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8474\tAccuracy: 8078.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8558\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 466 [0/1000 (0%)]\tLosses F.softmax: 0.000455 log_softmax: 0.000201\n",
            "Train Epoch: 466 [200/1000 (20%)]\tLosses F.softmax: 0.064166 log_softmax: 0.020986\n",
            "Train Epoch: 466 [400/1000 (40%)]\tLosses F.softmax: 0.002084 log_softmax: 0.002173\n",
            "Train Epoch: 466 [600/1000 (60%)]\tLosses F.softmax: 0.000039 log_softmax: 0.000285\n",
            "Train Epoch: 466 [800/1000 (80%)]\tLosses F.softmax: 0.000002 log_softmax: 0.000005\n",
            "Train Epoch: 466 [1000/1000 (100%)]\tLosses F.softmax: 0.001219 log_softmax: 0.000682\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8516\tAccuracy: 8045.0/10000 (80%)\n",
            "log_softmax: Loss: 0.8615\tAccuracy: 8034.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 467 [0/1000 (0%)]\tLosses F.softmax: 0.084087 log_softmax: 0.085681\n",
            "Train Epoch: 467 [200/1000 (20%)]\tLosses F.softmax: 0.039619 log_softmax: 0.037380\n",
            "Train Epoch: 467 [400/1000 (40%)]\tLosses F.softmax: 0.002023 log_softmax: 0.000307\n",
            "Train Epoch: 467 [600/1000 (60%)]\tLosses F.softmax: 0.155827 log_softmax: 0.010222\n",
            "Train Epoch: 467 [800/1000 (80%)]\tLosses F.softmax: 0.002007 log_softmax: 0.000079\n",
            "Train Epoch: 467 [1000/1000 (100%)]\tLosses F.softmax: 0.009519 log_softmax: 0.010987\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8538\tAccuracy: 8049.0/10000 (80%)\n",
            "log_softmax: Loss: 0.8608\tAccuracy: 8044.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 468 [0/1000 (0%)]\tLosses F.softmax: 0.003392 log_softmax: 0.000945\n",
            "Train Epoch: 468 [200/1000 (20%)]\tLosses F.softmax: 0.012410 log_softmax: 0.014794\n",
            "Train Epoch: 468 [400/1000 (40%)]\tLosses F.softmax: 0.155334 log_softmax: 0.115010\n",
            "Train Epoch: 468 [600/1000 (60%)]\tLosses F.softmax: 0.003382 log_softmax: 0.000090\n",
            "Train Epoch: 468 [800/1000 (80%)]\tLosses F.softmax: 0.000102 log_softmax: 0.000153\n",
            "Train Epoch: 468 [1000/1000 (100%)]\tLosses F.softmax: 0.000055 log_softmax: 0.000029\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8543\tAccuracy: 8060.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8623\tAccuracy: 8044.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 469 [0/1000 (0%)]\tLosses F.softmax: 0.003943 log_softmax: 0.003635\n",
            "Train Epoch: 469 [200/1000 (20%)]\tLosses F.softmax: 0.004600 log_softmax: 0.006207\n",
            "Train Epoch: 469 [400/1000 (40%)]\tLosses F.softmax: 0.032370 log_softmax: 0.038838\n",
            "Train Epoch: 469 [600/1000 (60%)]\tLosses F.softmax: 0.000358 log_softmax: 0.000303\n",
            "Train Epoch: 469 [800/1000 (80%)]\tLosses F.softmax: 0.003504 log_softmax: 0.001952\n",
            "Train Epoch: 469 [1000/1000 (100%)]\tLosses F.softmax: 0.004975 log_softmax: 0.039534\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8586\tAccuracy: 8048.0/10000 (80%)\n",
            "log_softmax: Loss: 0.8673\tAccuracy: 8030.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 470 [0/1000 (0%)]\tLosses F.softmax: 0.005037 log_softmax: 0.001344\n",
            "Train Epoch: 470 [200/1000 (20%)]\tLosses F.softmax: 0.001150 log_softmax: 0.001379\n",
            "Train Epoch: 470 [400/1000 (40%)]\tLosses F.softmax: 0.007649 log_softmax: 0.005398\n",
            "Train Epoch: 470 [600/1000 (60%)]\tLosses F.softmax: 0.000439 log_softmax: 0.000587\n",
            "Train Epoch: 470 [800/1000 (80%)]\tLosses F.softmax: 0.015724 log_softmax: 0.020906\n",
            "Train Epoch: 470 [1000/1000 (100%)]\tLosses F.softmax: 0.001363 log_softmax: 0.004858\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8556\tAccuracy: 8068.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8653\tAccuracy: 8046.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 471 [0/1000 (0%)]\tLosses F.softmax: 0.007393 log_softmax: 0.009900\n",
            "Train Epoch: 471 [200/1000 (20%)]\tLosses F.softmax: 0.000205 log_softmax: 0.000055\n",
            "Train Epoch: 471 [400/1000 (40%)]\tLosses F.softmax: 0.011538 log_softmax: 0.007750\n",
            "Train Epoch: 471 [600/1000 (60%)]\tLosses F.softmax: 0.042642 log_softmax: 0.039248\n",
            "Train Epoch: 471 [800/1000 (80%)]\tLosses F.softmax: 0.001921 log_softmax: 0.000425\n",
            "Train Epoch: 471 [1000/1000 (100%)]\tLosses F.softmax: 0.000106 log_softmax: 0.002768\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8593\tAccuracy: 8051.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8677\tAccuracy: 8039.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 472 [0/1000 (0%)]\tLosses F.softmax: 0.120598 log_softmax: 0.152761\n",
            "Train Epoch: 472 [200/1000 (20%)]\tLosses F.softmax: 0.041098 log_softmax: 0.013544\n",
            "Train Epoch: 472 [400/1000 (40%)]\tLosses F.softmax: 0.002932 log_softmax: 0.019882\n",
            "Train Epoch: 472 [600/1000 (60%)]\tLosses F.softmax: 0.001529 log_softmax: 0.001872\n",
            "Train Epoch: 472 [800/1000 (80%)]\tLosses F.softmax: 0.000029 log_softmax: 0.000027\n",
            "Train Epoch: 472 [1000/1000 (100%)]\tLosses F.softmax: 0.000649 log_softmax: 0.000164\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8564\tAccuracy: 8084.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8662\tAccuracy: 8061.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 473 [0/1000 (0%)]\tLosses F.softmax: 0.001402 log_softmax: 0.000423\n",
            "Train Epoch: 473 [200/1000 (20%)]\tLosses F.softmax: 0.093524 log_softmax: 0.163493\n",
            "Train Epoch: 473 [400/1000 (40%)]\tLosses F.softmax: 0.000209 log_softmax: 0.024803\n",
            "Train Epoch: 473 [600/1000 (60%)]\tLosses F.softmax: 0.061677 log_softmax: 0.025810\n",
            "Train Epoch: 473 [800/1000 (80%)]\tLosses F.softmax: 0.115704 log_softmax: 0.100624\n",
            "Train Epoch: 473 [1000/1000 (100%)]\tLosses F.softmax: 0.003363 log_softmax: 0.002331\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8603\tAccuracy: 8057.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8690\tAccuracy: 8043.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 474 [0/1000 (0%)]\tLosses F.softmax: 0.025889 log_softmax: 0.022619\n",
            "Train Epoch: 474 [200/1000 (20%)]\tLosses F.softmax: 0.008455 log_softmax: 0.006010\n",
            "Train Epoch: 474 [400/1000 (40%)]\tLosses F.softmax: 0.017501 log_softmax: 0.030066\n",
            "Train Epoch: 474 [600/1000 (60%)]\tLosses F.softmax: 0.000490 log_softmax: 0.000080\n",
            "Train Epoch: 474 [800/1000 (80%)]\tLosses F.softmax: 0.001320 log_softmax: 0.002602\n",
            "Train Epoch: 474 [1000/1000 (100%)]\tLosses F.softmax: 0.001437 log_softmax: 0.002509\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8613\tAccuracy: 8066.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8691\tAccuracy: 8043.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 475 [0/1000 (0%)]\tLosses F.softmax: 0.002414 log_softmax: 0.001726\n",
            "Train Epoch: 475 [200/1000 (20%)]\tLosses F.softmax: 0.054502 log_softmax: 0.053001\n",
            "Train Epoch: 475 [400/1000 (40%)]\tLosses F.softmax: 0.000355 log_softmax: 0.000211\n",
            "Train Epoch: 475 [600/1000 (60%)]\tLosses F.softmax: 0.020785 log_softmax: 0.018738\n",
            "Train Epoch: 475 [800/1000 (80%)]\tLosses F.softmax: 0.009839 log_softmax: 0.002435\n",
            "Train Epoch: 475 [1000/1000 (100%)]\tLosses F.softmax: 0.051945 log_softmax: 0.093079\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8623\tAccuracy: 8067.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8702\tAccuracy: 8051.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 476 [0/1000 (0%)]\tLosses F.softmax: 0.132427 log_softmax: 0.103707\n",
            "Train Epoch: 476 [200/1000 (20%)]\tLosses F.softmax: 0.000229 log_softmax: 0.000095\n",
            "Train Epoch: 476 [400/1000 (40%)]\tLosses F.softmax: 0.000184 log_softmax: 0.026250\n",
            "Train Epoch: 476 [600/1000 (60%)]\tLosses F.softmax: 0.060372 log_softmax: 0.013475\n",
            "Train Epoch: 476 [800/1000 (80%)]\tLosses F.softmax: 0.002706 log_softmax: 0.003847\n",
            "Train Epoch: 476 [1000/1000 (100%)]\tLosses F.softmax: 0.003668 log_softmax: 0.011287\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8634\tAccuracy: 8065.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8716\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 477 [0/1000 (0%)]\tLosses F.softmax: 0.000207 log_softmax: 0.000162\n",
            "Train Epoch: 477 [200/1000 (20%)]\tLosses F.softmax: 0.000264 log_softmax: 0.000280\n",
            "Train Epoch: 477 [400/1000 (40%)]\tLosses F.softmax: 0.001424 log_softmax: 0.001492\n",
            "Train Epoch: 477 [600/1000 (60%)]\tLosses F.softmax: 0.005682 log_softmax: 0.013781\n",
            "Train Epoch: 477 [800/1000 (80%)]\tLosses F.softmax: 0.000179 log_softmax: 0.000177\n",
            "Train Epoch: 477 [1000/1000 (100%)]\tLosses F.softmax: 0.007405 log_softmax: 0.006360\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8653\tAccuracy: 8054.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8738\tAccuracy: 8045.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 478 [0/1000 (0%)]\tLosses F.softmax: 0.000757 log_softmax: 0.000121\n",
            "Train Epoch: 478 [200/1000 (20%)]\tLosses F.softmax: 0.021343 log_softmax: 0.003296\n",
            "Train Epoch: 478 [400/1000 (40%)]\tLosses F.softmax: 0.000244 log_softmax: 0.000176\n",
            "Train Epoch: 478 [600/1000 (60%)]\tLosses F.softmax: 0.000273 log_softmax: 0.000377\n",
            "Train Epoch: 478 [800/1000 (80%)]\tLosses F.softmax: 0.001166 log_softmax: 0.000695\n",
            "Train Epoch: 478 [1000/1000 (100%)]\tLosses F.softmax: 0.015124 log_softmax: 0.002974\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8653\tAccuracy: 8069.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8730\tAccuracy: 8048.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 479 [0/1000 (0%)]\tLosses F.softmax: 0.009531 log_softmax: 0.002603\n",
            "Train Epoch: 479 [200/1000 (20%)]\tLosses F.softmax: 0.001667 log_softmax: 0.000340\n",
            "Train Epoch: 479 [400/1000 (40%)]\tLosses F.softmax: 0.002431 log_softmax: 0.001927\n",
            "Train Epoch: 479 [600/1000 (60%)]\tLosses F.softmax: 0.000305 log_softmax: 0.000030\n",
            "Train Epoch: 479 [800/1000 (80%)]\tLosses F.softmax: 0.039433 log_softmax: 0.059720\n",
            "Train Epoch: 479 [1000/1000 (100%)]\tLosses F.softmax: 0.004483 log_softmax: 0.013481\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8670\tAccuracy: 8068.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8745\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 480 [0/1000 (0%)]\tLosses F.softmax: 0.027046 log_softmax: 0.050408\n",
            "Train Epoch: 480 [200/1000 (20%)]\tLosses F.softmax: 0.014791 log_softmax: 0.016361\n",
            "Train Epoch: 480 [400/1000 (40%)]\tLosses F.softmax: 0.061362 log_softmax: 0.070821\n",
            "Train Epoch: 480 [600/1000 (60%)]\tLosses F.softmax: 0.000197 log_softmax: 0.000172\n",
            "Train Epoch: 480 [800/1000 (80%)]\tLosses F.softmax: 0.000133 log_softmax: 0.000030\n",
            "Train Epoch: 480 [1000/1000 (100%)]\tLosses F.softmax: 0.046216 log_softmax: 0.079667\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8674\tAccuracy: 8060.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8771\tAccuracy: 8043.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 481 [0/1000 (0%)]\tLosses F.softmax: 0.000663 log_softmax: 0.000366\n",
            "Train Epoch: 481 [200/1000 (20%)]\tLosses F.softmax: 0.001669 log_softmax: 0.002252\n",
            "Train Epoch: 481 [400/1000 (40%)]\tLosses F.softmax: 0.019243 log_softmax: 0.009555\n",
            "Train Epoch: 481 [600/1000 (60%)]\tLosses F.softmax: 0.006901 log_softmax: 0.003160\n",
            "Train Epoch: 481 [800/1000 (80%)]\tLosses F.softmax: 0.006268 log_softmax: 0.001000\n",
            "Train Epoch: 481 [1000/1000 (100%)]\tLosses F.softmax: 0.002484 log_softmax: 0.003117\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8692\tAccuracy: 8065.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8779\tAccuracy: 8051.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 482 [0/1000 (0%)]\tLosses F.softmax: 0.149001 log_softmax: 0.244128\n",
            "Train Epoch: 482 [200/1000 (20%)]\tLosses F.softmax: 0.000878 log_softmax: 0.001721\n",
            "Train Epoch: 482 [400/1000 (40%)]\tLosses F.softmax: 0.019962 log_softmax: 0.006872\n",
            "Train Epoch: 482 [600/1000 (60%)]\tLosses F.softmax: 0.000033 log_softmax: 0.000232\n",
            "Train Epoch: 482 [800/1000 (80%)]\tLosses F.softmax: 0.000012 log_softmax: 0.000079\n",
            "Train Epoch: 482 [1000/1000 (100%)]\tLosses F.softmax: 0.000184 log_softmax: 0.000235\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8686\tAccuracy: 8072.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8787\tAccuracy: 8046.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 483 [0/1000 (0%)]\tLosses F.softmax: 0.014959 log_softmax: 0.006823\n",
            "Train Epoch: 483 [200/1000 (20%)]\tLosses F.softmax: 0.003262 log_softmax: 0.001197\n",
            "Train Epoch: 483 [400/1000 (40%)]\tLosses F.softmax: 0.045275 log_softmax: 0.013796\n",
            "Train Epoch: 483 [600/1000 (60%)]\tLosses F.softmax: 0.000234 log_softmax: 0.000194\n",
            "Train Epoch: 483 [800/1000 (80%)]\tLosses F.softmax: 0.002642 log_softmax: 0.000173\n",
            "Train Epoch: 483 [1000/1000 (100%)]\tLosses F.softmax: 0.014639 log_softmax: 0.002690\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8714\tAccuracy: 8061.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8808\tAccuracy: 8045.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 484 [0/1000 (0%)]\tLosses F.softmax: 0.091957 log_softmax: 0.082051\n",
            "Train Epoch: 484 [200/1000 (20%)]\tLosses F.softmax: 0.041881 log_softmax: 0.030535\n",
            "Train Epoch: 484 [400/1000 (40%)]\tLosses F.softmax: 0.024352 log_softmax: 0.009228\n",
            "Train Epoch: 484 [600/1000 (60%)]\tLosses F.softmax: 0.000577 log_softmax: 0.002696\n",
            "Train Epoch: 484 [800/1000 (80%)]\tLosses F.softmax: 0.000819 log_softmax: 0.000942\n",
            "Train Epoch: 484 [1000/1000 (100%)]\tLosses F.softmax: 0.005204 log_softmax: 0.005743\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8735\tAccuracy: 8069.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8827\tAccuracy: 8045.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 485 [0/1000 (0%)]\tLosses F.softmax: 0.128202 log_softmax: 0.099718\n",
            "Train Epoch: 485 [200/1000 (20%)]\tLosses F.softmax: 0.012215 log_softmax: 0.004890\n",
            "Train Epoch: 485 [400/1000 (40%)]\tLosses F.softmax: 0.010381 log_softmax: 0.008748\n",
            "Train Epoch: 485 [600/1000 (60%)]\tLosses F.softmax: 0.003157 log_softmax: 0.002101\n",
            "Train Epoch: 485 [800/1000 (80%)]\tLosses F.softmax: 0.000028 log_softmax: 0.000202\n",
            "Train Epoch: 485 [1000/1000 (100%)]\tLosses F.softmax: 0.000162 log_softmax: 0.000147\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8760\tAccuracy: 8057.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8841\tAccuracy: 8044.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 486 [0/1000 (0%)]\tLosses F.softmax: 0.314713 log_softmax: 0.286660\n",
            "Train Epoch: 486 [200/1000 (20%)]\tLosses F.softmax: 0.001783 log_softmax: 0.001599\n",
            "Train Epoch: 486 [400/1000 (40%)]\tLosses F.softmax: 0.000446 log_softmax: 0.000196\n",
            "Train Epoch: 486 [600/1000 (60%)]\tLosses F.softmax: 0.002875 log_softmax: 0.000643\n",
            "Train Epoch: 486 [800/1000 (80%)]\tLosses F.softmax: 0.028643 log_softmax: 0.004237\n",
            "Train Epoch: 486 [1000/1000 (100%)]\tLosses F.softmax: 0.003123 log_softmax: 0.012173\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8766\tAccuracy: 8068.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8853\tAccuracy: 8049.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 487 [0/1000 (0%)]\tLosses F.softmax: 0.010984 log_softmax: 0.010952\n",
            "Train Epoch: 487 [200/1000 (20%)]\tLosses F.softmax: 0.121994 log_softmax: 0.073980\n",
            "Train Epoch: 487 [400/1000 (40%)]\tLosses F.softmax: 0.000014 log_softmax: 0.000017\n",
            "Train Epoch: 487 [600/1000 (60%)]\tLosses F.softmax: 0.003000 log_softmax: 0.001071\n",
            "Train Epoch: 487 [800/1000 (80%)]\tLosses F.softmax: 0.000034 log_softmax: 0.000023\n",
            "Train Epoch: 487 [1000/1000 (100%)]\tLosses F.softmax: 0.000605 log_softmax: 0.000206\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8745\tAccuracy: 8063.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8855\tAccuracy: 8043.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 488 [0/1000 (0%)]\tLosses F.softmax: 0.000174 log_softmax: 0.027009\n",
            "Train Epoch: 488 [200/1000 (20%)]\tLosses F.softmax: 0.000633 log_softmax: 0.000264\n",
            "Train Epoch: 488 [400/1000 (40%)]\tLosses F.softmax: 0.000891 log_softmax: 0.002761\n",
            "Train Epoch: 488 [600/1000 (60%)]\tLosses F.softmax: 0.090541 log_softmax: 0.068152\n",
            "Train Epoch: 488 [800/1000 (80%)]\tLosses F.softmax: 0.009462 log_softmax: 0.007061\n",
            "Train Epoch: 488 [1000/1000 (100%)]\tLosses F.softmax: 0.019279 log_softmax: 0.027034\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8753\tAccuracy: 8074.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8857\tAccuracy: 8045.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 489 [0/1000 (0%)]\tLosses F.softmax: 0.000290 log_softmax: 0.000093\n",
            "Train Epoch: 489 [200/1000 (20%)]\tLosses F.softmax: 0.040454 log_softmax: 0.044377\n",
            "Train Epoch: 489 [400/1000 (40%)]\tLosses F.softmax: 0.007707 log_softmax: 0.008167\n",
            "Train Epoch: 489 [600/1000 (60%)]\tLosses F.softmax: 0.002682 log_softmax: 0.001711\n",
            "Train Epoch: 489 [800/1000 (80%)]\tLosses F.softmax: 0.007257 log_softmax: 0.001244\n",
            "Train Epoch: 489 [1000/1000 (100%)]\tLosses F.softmax: 0.584516 log_softmax: 0.387922\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8792\tAccuracy: 8064.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8879\tAccuracy: 8049.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 490 [0/1000 (0%)]\tLosses F.softmax: 0.013850 log_softmax: 0.010967\n",
            "Train Epoch: 490 [200/1000 (20%)]\tLosses F.softmax: 0.004894 log_softmax: 0.001969\n",
            "Train Epoch: 490 [400/1000 (40%)]\tLosses F.softmax: 0.010545 log_softmax: 0.008731\n",
            "Train Epoch: 490 [600/1000 (60%)]\tLosses F.softmax: 0.129018 log_softmax: 0.010667\n",
            "Train Epoch: 490 [800/1000 (80%)]\tLosses F.softmax: 0.000778 log_softmax: 0.001106\n",
            "Train Epoch: 490 [1000/1000 (100%)]\tLosses F.softmax: 0.008336 log_softmax: 0.120933\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8798\tAccuracy: 8067.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8894\tAccuracy: 8053.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 491 [0/1000 (0%)]\tLosses F.softmax: 0.003410 log_softmax: 0.001919\n",
            "Train Epoch: 491 [200/1000 (20%)]\tLosses F.softmax: 0.020283 log_softmax: 0.008609\n",
            "Train Epoch: 491 [400/1000 (40%)]\tLosses F.softmax: 0.000017 log_softmax: 0.000031\n",
            "Train Epoch: 491 [600/1000 (60%)]\tLosses F.softmax: 0.129738 log_softmax: 0.035929\n",
            "Train Epoch: 491 [800/1000 (80%)]\tLosses F.softmax: 0.000787 log_softmax: 0.000065\n",
            "Train Epoch: 491 [1000/1000 (100%)]\tLosses F.softmax: 0.013621 log_softmax: 0.019345\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8828\tAccuracy: 8059.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8915\tAccuracy: 8034.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 492 [0/1000 (0%)]\tLosses F.softmax: 0.008167 log_softmax: 0.019893\n",
            "Train Epoch: 492 [200/1000 (20%)]\tLosses F.softmax: 0.036124 log_softmax: 0.023525\n",
            "Train Epoch: 492 [400/1000 (40%)]\tLosses F.softmax: 0.007073 log_softmax: 0.008935\n",
            "Train Epoch: 492 [600/1000 (60%)]\tLosses F.softmax: 0.004922 log_softmax: 0.007025\n",
            "Train Epoch: 492 [800/1000 (80%)]\tLosses F.softmax: 0.000098 log_softmax: 0.000375\n",
            "Train Epoch: 492 [1000/1000 (100%)]\tLosses F.softmax: 0.174541 log_softmax: 0.101200\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8824\tAccuracy: 8064.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8919\tAccuracy: 8046.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 493 [0/1000 (0%)]\tLosses F.softmax: 0.000252 log_softmax: 0.000389\n",
            "Train Epoch: 493 [200/1000 (20%)]\tLosses F.softmax: 0.000009 log_softmax: 0.000003\n",
            "Train Epoch: 493 [400/1000 (40%)]\tLosses F.softmax: 0.007778 log_softmax: 0.000114\n",
            "Train Epoch: 493 [600/1000 (60%)]\tLosses F.softmax: 0.006658 log_softmax: 0.007641\n",
            "Train Epoch: 493 [800/1000 (80%)]\tLosses F.softmax: 0.036837 log_softmax: 0.072872\n",
            "Train Epoch: 493 [1000/1000 (100%)]\tLosses F.softmax: 0.008340 log_softmax: 0.004418\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8834\tAccuracy: 8069.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8921\tAccuracy: 8048.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 494 [0/1000 (0%)]\tLosses F.softmax: 0.114924 log_softmax: 0.086899\n",
            "Train Epoch: 494 [200/1000 (20%)]\tLosses F.softmax: 0.000863 log_softmax: 0.000084\n",
            "Train Epoch: 494 [400/1000 (40%)]\tLosses F.softmax: 0.001202 log_softmax: 0.003492\n",
            "Train Epoch: 494 [600/1000 (60%)]\tLosses F.softmax: 0.000616 log_softmax: 0.000080\n",
            "Train Epoch: 494 [800/1000 (80%)]\tLosses F.softmax: 0.000715 log_softmax: 0.001827\n",
            "Train Epoch: 494 [1000/1000 (100%)]\tLosses F.softmax: 0.000039 log_softmax: 0.000000\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8831\tAccuracy: 8071.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8934\tAccuracy: 8056.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 495 [0/1000 (0%)]\tLosses F.softmax: 0.000863 log_softmax: 0.001087\n",
            "Train Epoch: 495 [200/1000 (20%)]\tLosses F.softmax: 0.123194 log_softmax: 0.062677\n",
            "Train Epoch: 495 [400/1000 (40%)]\tLosses F.softmax: 0.044081 log_softmax: 0.010322\n",
            "Train Epoch: 495 [600/1000 (60%)]\tLosses F.softmax: 0.012500 log_softmax: 0.007561\n",
            "Train Epoch: 495 [800/1000 (80%)]\tLosses F.softmax: 0.033526 log_softmax: 0.010006\n",
            "Train Epoch: 495 [1000/1000 (100%)]\tLosses F.softmax: 0.114604 log_softmax: 0.090050\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8841\tAccuracy: 8078.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8941\tAccuracy: 8064.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 496 [0/1000 (0%)]\tLosses F.softmax: 0.009854 log_softmax: 0.001666\n",
            "Train Epoch: 496 [200/1000 (20%)]\tLosses F.softmax: 0.000142 log_softmax: 0.000068\n",
            "Train Epoch: 496 [400/1000 (40%)]\tLosses F.softmax: 0.001966 log_softmax: 0.016878\n",
            "Train Epoch: 496 [600/1000 (60%)]\tLosses F.softmax: 0.000026 log_softmax: 0.000022\n",
            "Train Epoch: 496 [800/1000 (80%)]\tLosses F.softmax: 0.000110 log_softmax: 0.000068\n",
            "Train Epoch: 496 [1000/1000 (100%)]\tLosses F.softmax: 0.010042 log_softmax: 0.005816\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8873\tAccuracy: 8054.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8973\tAccuracy: 8044.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 497 [0/1000 (0%)]\tLosses F.softmax: 0.120426 log_softmax: 0.141124\n",
            "Train Epoch: 497 [200/1000 (20%)]\tLosses F.softmax: 0.017723 log_softmax: 0.078174\n",
            "Train Epoch: 497 [400/1000 (40%)]\tLosses F.softmax: 0.000060 log_softmax: 0.000015\n",
            "Train Epoch: 497 [600/1000 (60%)]\tLosses F.softmax: 0.000814 log_softmax: 0.000007\n",
            "Train Epoch: 497 [800/1000 (80%)]\tLosses F.softmax: 0.004718 log_softmax: 0.001910\n",
            "Train Epoch: 497 [1000/1000 (100%)]\tLosses F.softmax: 0.006895 log_softmax: 0.012404\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8880\tAccuracy: 8067.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8982\tAccuracy: 8054.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 498 [0/1000 (0%)]\tLosses F.softmax: 0.000261 log_softmax: 0.006395\n",
            "Train Epoch: 498 [200/1000 (20%)]\tLosses F.softmax: 0.000003 log_softmax: 0.000002\n",
            "Train Epoch: 498 [400/1000 (40%)]\tLosses F.softmax: 0.001354 log_softmax: 0.002067\n",
            "Train Epoch: 498 [600/1000 (60%)]\tLosses F.softmax: 0.024004 log_softmax: 0.019985\n",
            "Train Epoch: 498 [800/1000 (80%)]\tLosses F.softmax: 0.002878 log_softmax: 0.012151\n",
            "Train Epoch: 498 [1000/1000 (100%)]\tLosses F.softmax: 0.000289 log_softmax: 0.000208\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8885\tAccuracy: 8082.0/10000 (81%)\n",
            "log_softmax: Loss: 0.8989\tAccuracy: 8063.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 499 [0/1000 (0%)]\tLosses F.softmax: 0.003521 log_softmax: 0.000497\n",
            "Train Epoch: 499 [200/1000 (20%)]\tLosses F.softmax: 0.002479 log_softmax: 0.000028\n",
            "Train Epoch: 499 [400/1000 (40%)]\tLosses F.softmax: 0.006219 log_softmax: 0.011987\n",
            "Train Epoch: 499 [600/1000 (60%)]\tLosses F.softmax: 0.005666 log_softmax: 0.012475\n",
            "Train Epoch: 499 [800/1000 (80%)]\tLosses F.softmax: 0.196168 log_softmax: 0.194751\n",
            "Train Epoch: 499 [1000/1000 (100%)]\tLosses F.softmax: 0.000060 log_softmax: 0.000015\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8910\tAccuracy: 8067.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9012\tAccuracy: 8047.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 500 [0/1000 (0%)]\tLosses F.softmax: 0.000013 log_softmax: 0.000024\n",
            "Train Epoch: 500 [200/1000 (20%)]\tLosses F.softmax: 0.002851 log_softmax: 0.001477\n",
            "Train Epoch: 500 [400/1000 (40%)]\tLosses F.softmax: 0.000136 log_softmax: 0.000151\n",
            "Train Epoch: 500 [600/1000 (60%)]\tLosses F.softmax: 0.114457 log_softmax: 0.095616\n",
            "Train Epoch: 500 [800/1000 (80%)]\tLosses F.softmax: 0.065367 log_softmax: 0.069091\n",
            "Train Epoch: 500 [1000/1000 (100%)]\tLosses F.softmax: 0.000386 log_softmax: 0.001051\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8909\tAccuracy: 8072.0/10000 (81%)\n",
            "log_softmax: Loss: 0.9014\tAccuracy: 8049.0/10000 (80%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jv5dQ_scbLX4",
        "colab_type": "code",
        "outputId": "c7d20e3b-6e85-4d01-87c3-c0f16b802c03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        }
      },
      "source": [
        "from pylab import rcParams\n",
        "rcParams['figure.figsize'] = 20, 10\n",
        "plot_graphs(test_log, 'loss')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZJYmqLhmuAf",
        "colab_type": "code",
        "outputId": "74893956-9bc0-448c-8cf8-83607a9601cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        }
      },
      "source": [
        "plot_graphs(test_log, 'accuracy')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYi8f4pRmw73",
        "colab_type": "code",
        "outputId": "7eee6176-12d8-4ffb-dd14-e9c053ed8b1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "test_log"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'F.softmax': [(2.304551475071907, tensor(8.9200)),\n",
              "  (2.303847157692909, tensor(8.9200)),\n",
              "  (2.303181861424446, tensor(8.9200)),\n",
              "  (2.3025501314640047, tensor(8.9200)),\n",
              "  (2.301946649646759, tensor(8.8900)),\n",
              "  (2.3013658689022063, tensor(8.8600)),\n",
              "  (2.3008060288906096, tensor(11.3800)),\n",
              "  (2.300259348535538, tensor(14.3000)),\n",
              "  (2.2997272054195403, tensor(15.1600)),\n",
              "  (2.299208303928375, tensor(16.5400)),\n",
              "  (2.298701072525978, tensor(17.8100)),\n",
              "  (2.2982047543764113, tensor(18.7200)),\n",
              "  (2.297715859556198, tensor(19.2100)),\n",
              "  (2.297231854367256, tensor(19.3600)),\n",
              "  (2.2967528824567793, tensor(19.4800)),\n",
              "  (2.2962812967538833, tensor(19.5900)),\n",
              "  (2.2958118710517885, tensor(19.4500)),\n",
              "  (2.295340613102913, tensor(19.4600)),\n",
              "  (2.2948691939353942, tensor(19.5100)),\n",
              "  (2.294390573835373, tensor(19.4800)),\n",
              "  (2.2939018749475477, tensor(19.4800)),\n",
              "  (2.293406449460983, tensor(19.4500)),\n",
              "  (2.2929005105495452, tensor(19.5000)),\n",
              "  (2.2923816130399706, tensor(19.4400)),\n",
              "  (2.291855415225029, tensor(19.5600)),\n",
              "  (2.291315243768692, tensor(19.5500)),\n",
              "  (2.2907617955207824, tensor(19.5700)),\n",
              "  (2.290190109705925, tensor(19.6300)),\n",
              "  (2.2896024260520935, tensor(19.6300)),\n",
              "  (2.2890008364677428, tensor(19.6800)),\n",
              "  (2.2883772092342376, tensor(19.7600)),\n",
              "  (2.287735017514229, tensor(19.8100)),\n",
              "  (2.287065384078026, tensor(19.8200)),\n",
              "  (2.2863743426561354, tensor(19.8700)),\n",
              "  (2.2856606293678285, tensor(19.9300)),\n",
              "  (2.2849152121782303, tensor(19.9900)),\n",
              "  (2.284142865204811, tensor(19.9700)),\n",
              "  (2.2833407975912094, tensor(19.9600)),\n",
              "  (2.282511142849922, tensor(20.0100)),\n",
              "  (2.281648454642296, tensor(20.0800)),\n",
              "  (2.280747130346298, tensor(20.1000)),\n",
              "  (2.2798085918188096, tensor(20.1500)),\n",
              "  (2.278831224155426, tensor(20.2000)),\n",
              "  (2.277806774234772, tensor(20.2100)),\n",
              "  (2.2767438103675843, tensor(20.2500)),\n",
              "  (2.2756306968450546, tensor(20.2700)),\n",
              "  (2.2744577265262604, tensor(20.2700)),\n",
              "  (2.2732416696548463, tensor(20.3000)),\n",
              "  (2.2719678364038467, tensor(20.3000)),\n",
              "  (2.2706438932180406, tensor(20.3200)),\n",
              "  (2.2692492731809617, tensor(20.3400)),\n",
              "  (2.267785398364067, tensor(20.3500)),\n",
              "  (2.266247190237045, tensor(20.3400)),\n",
              "  (2.26462723236084, tensor(20.3400)),\n",
              "  (2.2629243807792663, tensor(20.3400)),\n",
              "  (2.2611339611291887, tensor(20.3500)),\n",
              "  (2.259256744980812, tensor(20.3700)),\n",
              "  (2.2572698333501817, tensor(20.4600)),\n",
              "  (2.255191293859482, tensor(20.6200)),\n",
              "  (2.252987124836445, tensor(20.9000)),\n",
              "  (2.2506569699048997, tensor(21.1800)),\n",
              "  (2.248189132845402, tensor(21.5600)),\n",
              "  (2.2455806130170823, tensor(21.9900)),\n",
              "  (2.242809173321724, tensor(22.4500)),\n",
              "  (2.2398922568202018, tensor(22.7600)),\n",
              "  (2.2367921944737432, tensor(23.3100)),\n",
              "  (2.2335024356484414, tensor(23.7100)),\n",
              "  (2.230009541845322, tensor(24.2300)),\n",
              "  (2.2263026240348815, tensor(24.6700)),\n",
              "  (2.222326708614826, tensor(25.1600)),\n",
              "  (2.218104458510876, tensor(25.5700)),\n",
              "  (2.213590088570118, tensor(25.9300)),\n",
              "  (2.208810194694996, tensor(26.5000)),\n",
              "  (2.203687749505043, tensor(27.0500)),\n",
              "  (2.1982391971588133, tensor(27.5200)),\n",
              "  (2.1924233686447145, tensor(27.9600)),\n",
              "  (2.1861350621223448, tensor(28.3900)),\n",
              "  (2.1794649084329607, tensor(28.7300)),\n",
              "  (2.172303294992447, tensor(29.0700)),\n",
              "  (2.164649887907505, tensor(29.2900)),\n",
              "  (2.156460469841957, tensor(29.5900)),\n",
              "  (2.1475953814864157, tensor(30.0900)),\n",
              "  (2.1382015640854837, tensor(30.2400)),\n",
              "  (2.128120809316635, tensor(30.5500)),\n",
              "  (2.1173679050445555, tensor(30.8900)),\n",
              "  (2.105730950343609, tensor(31.1600)),\n",
              "  (2.093295254313946, tensor(31.4700)),\n",
              "  (2.0800073819637297, tensor(31.8600)),\n",
              "  (2.0657107633471488, tensor(32.2000)),\n",
              "  (2.0506116475105287, tensor(32.6000)),\n",
              "  (2.03472463953495, tensor(32.8500)),\n",
              "  (2.0173182436704638, tensor(33.0900)),\n",
              "  (1.9991043424606323, tensor(33.4200)),\n",
              "  (1.9799827070832252, tensor(33.4400)),\n",
              "  (1.9598618333876132, tensor(33.9800)),\n",
              "  (1.9385180675566196, tensor(34.3700)),\n",
              "  (1.9165001199901104, tensor(34.8300)),\n",
              "  (1.893794929176569, tensor(35.)),\n",
              "  (1.8700255234241485, tensor(35.7000)),\n",
              "  (1.8456313696861266, tensor(36.0500)),\n",
              "  (1.8212448925077915, tensor(36.3400)),\n",
              "  (1.7959960416674614, tensor(36.9300)),\n",
              "  (1.770829125869274, tensor(37.3500)),\n",
              "  (1.746529632538557, tensor(37.8900)),\n",
              "  (1.721499841940403, tensor(38.6500)),\n",
              "  (1.6970845614522696, tensor(39.4800)),\n",
              "  (1.6724743461221456, tensor(39.9300)),\n",
              "  (1.648720426094532, tensor(40.8500)),\n",
              "  (1.6250288347750903, tensor(41.8700)),\n",
              "  (1.6037268224090337, tensor(42.9600)),\n",
              "  (1.5815275649815799, tensor(43.9100)),\n",
              "  (1.560726956117153, tensor(44.8400)),\n",
              "  (1.5398805160045623, tensor(45.9200)),\n",
              "  (1.5210104154855013, tensor(46.8700)),\n",
              "  (1.5016662535518408, tensor(47.9100)),\n",
              "  (1.4826956912100315, tensor(48.8600)),\n",
              "  (1.465952423608303, tensor(49.1700)),\n",
              "  (1.4474505048885942, tensor(49.7500)),\n",
              "  (1.4309910327732562, tensor(50.4300)),\n",
              "  (1.4143349695026874, tensor(50.9100)),\n",
              "  (1.39842070209831, tensor(51.2800)),\n",
              "  (1.3824579004734754, tensor(51.9700)),\n",
              "  (1.3682076113805175, tensor(52.6600)),\n",
              "  (1.3560257462739944, tensor(52.8500)),\n",
              "  (1.3392193591833115, tensor(53.6100)),\n",
              "  (1.324914712318778, tensor(53.8600)),\n",
              "  (1.3130070241764187, tensor(54.4600)),\n",
              "  (1.2999740543544291, tensor(55.0700)),\n",
              "  (1.2893377731233835, tensor(55.1900)),\n",
              "  (1.277734935298562, tensor(55.4600)),\n",
              "  (1.2648402774557472, tensor(56.1800)),\n",
              "  (1.2524678105279803, tensor(56.6900)),\n",
              "  (1.2420934663891792, tensor(57.2700)),\n",
              "  (1.2309942489638925, tensor(58.2200)),\n",
              "  (1.2181053236737847, tensor(58.7200)),\n",
              "  (1.2099250106021762, tensor(58.6200)),\n",
              "  (1.2004589324519037, tensor(58.7500)),\n",
              "  (1.1904565375521778, tensor(59.5000)),\n",
              "  (1.1805469577252865, tensor(59.9500)),\n",
              "  (1.1726148856289684, tensor(60.4400)),\n",
              "  (1.1662555793367326, tensor(60.3400)),\n",
              "  (1.1540694239161908, tensor(60.8700)),\n",
              "  (1.1471812714606524, tensor(60.8700)),\n",
              "  (1.139867101957649, tensor(61.1900)),\n",
              "  (1.1304975237593056, tensor(63.2000)),\n",
              "  (1.1241849139049649, tensor(61.8900)),\n",
              "  (1.1170071047104895, tensor(63.4000)),\n",
              "  (1.1087928397253155, tensor(63.1400)),\n",
              "  (1.101013266339153, tensor(63.7200)),\n",
              "  (1.0938961716920137, tensor(64.0400)),\n",
              "  (1.085092305111885, tensor(63.8200)),\n",
              "  (1.0818766976639629, tensor(64.4600)),\n",
              "  (1.072647505439073, tensor(65.1400)),\n",
              "  (1.066662851525098, tensor(64.7800)),\n",
              "  (1.0582205416180193, tensor(65.0600)),\n",
              "  (1.0535191604927183, tensor(65.4900)),\n",
              "  (1.0448922615870833, tensor(66.0300)),\n",
              "  (1.0430120711714028, tensor(66.2300)),\n",
              "  (1.03401827224195, tensor(66.1400)),\n",
              "  (1.026183025931567, tensor(66.2300)),\n",
              "  (1.0193361859228462, tensor(66.5600)),\n",
              "  (1.0136479365617037, tensor(67.6000)),\n",
              "  (1.0145196699753403, tensor(67.3700)),\n",
              "  (1.0011443245202303, tensor(67.7300)),\n",
              "  (0.9956306411519646, tensor(67.6500)),\n",
              "  (0.9902662916615605, tensor(67.7800)),\n",
              "  (0.9862726002659649, tensor(68.0300)),\n",
              "  (0.9762466658439487, tensor(68.5900)),\n",
              "  (0.9693726123936475, tensor(68.8200)),\n",
              "  (0.9653564732603729, tensor(68.6500)),\n",
              "  (0.958416101232171, tensor(68.9700)),\n",
              "  (0.9543794637195766, tensor(69.0500)),\n",
              "  (0.9455321010552347, tensor(69.5700)),\n",
              "  (0.9419026329662651, tensor(69.9200)),\n",
              "  (0.9361678497515619, tensor(69.4400)),\n",
              "  (0.9338424078511074, tensor(69.9200)),\n",
              "  (0.920081972451508, tensor(70.6500)),\n",
              "  (0.9168374264769256, tensor(70.4500)),\n",
              "  (0.9138416306877509, tensor(70.7200)),\n",
              "  (0.9067089423030615, tensor(70.6800)),\n",
              "  (0.8974078223431483, tensor(71.2400)),\n",
              "  (0.8929406231394038, tensor(71.2100)),\n",
              "  (0.8961027463672683, tensor(70.4000)),\n",
              "  (0.8830399931732565, tensor(71.6800)),\n",
              "  (0.8750061880318448, tensor(71.7900)),\n",
              "  (0.8723606030927971, tensor(71.9500)),\n",
              "  (0.866997486906685, tensor(72.5200)),\n",
              "  (0.8619193984413519, tensor(72.0700)),\n",
              "  (0.8565714696314186, tensor(72.3200)),\n",
              "  (0.8471582936570048, tensor(72.7300)),\n",
              "  (0.8443780357422307, tensor(72.7400)),\n",
              "  (0.8370139234464615, tensor(73.5800)),\n",
              "  (0.8361239478253759, tensor(72.8600)),\n",
              "  (0.8288278396569192, tensor(73.5500)),\n",
              "  (0.8267591025408357, tensor(73.3300)),\n",
              "  (0.8239567509230227, tensor(73.4800)),\n",
              "  (0.8191569165523164, tensor(73.9800)),\n",
              "  (0.8108959409021772, tensor(73.9400)),\n",
              "  (0.8104390732329338, tensor(74.0500)),\n",
              "  (0.8049810338303447, tensor(74.1100)),\n",
              "  (0.8049130528986455, tensor(74.4800)),\n",
              "  (0.795056330345478, tensor(74.2200)),\n",
              "  (0.7960831387594343, tensor(74.1000)),\n",
              "  (0.7892057877491694, tensor(74.6600)),\n",
              "  (0.7877351687139366, tensor(74.4400)),\n",
              "  (0.7794541861434467, tensor(74.9100)),\n",
              "  (0.782511193374265, tensor(74.6300)),\n",
              "  (0.7767651378524024, tensor(75.1200)),\n",
              "  (0.775352007739013, tensor(75.2500)),\n",
              "  (0.7695524178855587, tensor(75.2300)),\n",
              "  (0.7760795959956944, tensor(74.7100)),\n",
              "  (0.7610578749340028, tensor(75.6000)),\n",
              "  (0.7618906391641125, tensor(75.4900)),\n",
              "  (0.7574388637026772, tensor(75.7200)),\n",
              "  (0.7534382515214384, tensor(75.9700)),\n",
              "  (0.7562253675259184, tensor(75.3500)),\n",
              "  (0.7483972010217607, tensor(76.2800)),\n",
              "  (0.7472750082308427, tensor(76.0200)),\n",
              "  (0.743298010803666, tensor(76.6200)),\n",
              "  (0.7410789869854226, tensor(76.5700)),\n",
              "  (0.7400589835789986, tensor(76.3900)),\n",
              "  (0.735801524351188, tensor(76.7400)),\n",
              "  (0.7327238550295122, tensor(76.9100)),\n",
              "  (0.7346610138209304, tensor(76.6300)),\n",
              "  (0.7311033191111405, tensor(77.0700)),\n",
              "  (0.7240602745486889, tensor(77.4600)),\n",
              "  (0.723816410242347, tensor(77.1000)),\n",
              "  (0.7257345512482105, tensor(77.0700)),\n",
              "  (0.7243413464002079, tensor(77.0900)),\n",
              "  (0.7190069484768435, tensor(77.5700)),\n",
              "  (0.7172198154369834, tensor(77.9900)),\n",
              "  (0.7153972386595793, tensor(77.5800)),\n",
              "  (0.7152178998229094, tensor(77.5300)),\n",
              "  (0.7149294407997281, tensor(77.5600)),\n",
              "  (0.7147248717410722, tensor(77.8900)),\n",
              "  (0.7128040168867447, tensor(77.6600)),\n",
              "  (0.7116199148666812, tensor(77.9900)),\n",
              "  (0.7057567808653926, tensor(77.9300)),\n",
              "  (0.7049000088743865, tensor(78.1500)),\n",
              "  (0.7050279498161748, tensor(78.0400)),\n",
              "  (0.7051743001237046, tensor(78.0800)),\n",
              "  (0.7066378731045173, tensor(78.2300)),\n",
              "  (0.7020878163164714, tensor(78.4300)),\n",
              "  (0.7012822060125764, tensor(78.3600)),\n",
              "  (0.7023742474962491, tensor(78.3300)),\n",
              "  (0.7049611033953843, tensor(77.9600)),\n",
              "  (0.6988495209389249, tensor(78.5600)),\n",
              "  (0.6947157086281455, tensor(78.7100)),\n",
              "  (0.6940074189346865, tensor(78.6100)),\n",
              "  (0.6914921870225226, tensor(78.9900)),\n",
              "  (0.6951346528335475, tensor(78.6700)),\n",
              "  (0.6966571059315465, tensor(78.3000)),\n",
              "  (0.692476738582889, tensor(78.9100)),\n",
              "  (0.6900342326005455, tensor(78.8600)),\n",
              "  (0.6882952068524435, tensor(78.9800)),\n",
              "  (0.6927559951479896, tensor(78.6200)),\n",
              "  (0.6868524590212037, tensor(79.2100)),\n",
              "  (0.6859348233293276, tensor(78.9900)),\n",
              "  (0.6864325884520542, tensor(79.5400)),\n",
              "  (0.6842004551543505, tensor(79.1500)),\n",
              "  (0.68523540207888, tensor(79.1800)),\n",
              "  (0.6812757313426817, tensor(79.3200)),\n",
              "  (0.6815348470443161, tensor(79.4000)),\n",
              "  (0.6823216004529852, tensor(79.5500)),\n",
              "  (0.6841429287868493, tensor(79.3200)),\n",
              "  (0.6816475748649274, tensor(79.6600)),\n",
              "  (0.6802287005238119, tensor(79.4000)),\n",
              "  (0.678502421070286, tensor(80.0300)),\n",
              "  (0.6832682689351961, tensor(79.4200)),\n",
              "  (0.6852654670378601, tensor(79.4900)),\n",
              "  (0.6843351399697305, tensor(79.2900)),\n",
              "  (0.6897321363611031, tensor(78.9600)),\n",
              "  (0.682521457217785, tensor(79.3200)),\n",
              "  (0.680912428785389, tensor(79.6600)),\n",
              "  (0.6777678224779841, tensor(79.6200)),\n",
              "  (0.6823197385751526, tensor(79.3700)),\n",
              "  (0.6775337588253489, tensor(79.6200)),\n",
              "  (0.6791072415423696, tensor(79.8100)),\n",
              "  (0.6755562058010605, tensor(79.7600)),\n",
              "  (0.673395721012092, tensor(80.0300)),\n",
              "  (0.6780404110052041, tensor(79.6800)),\n",
              "  (0.6761658394612255, tensor(79.8900)),\n",
              "  (0.6736223119774019, tensor(79.9700)),\n",
              "  (0.6819650223793986, tensor(79.5600)),\n",
              "  (0.677370664003538, tensor(79.8800)),\n",
              "  (0.6773019565322961, tensor(79.8300)),\n",
              "  (0.6759228432362259, tensor(79.9100)),\n",
              "  (0.6765547039027413, tensor(79.9100)),\n",
              "  (0.675928379673083, tensor(79.9800)),\n",
              "  (0.6788271277514636, tensor(79.8500)),\n",
              "  (0.6779622804335522, tensor(80.0500)),\n",
              "  (0.6770494283606531, tensor(80.0700)),\n",
              "  (0.6850617483707844, tensor(79.7900)),\n",
              "  (0.6757397166449053, tensor(80.1900)),\n",
              "  (0.6796324707481748, tensor(80.1800)),\n",
              "  (0.6746145492466458, tensor(80.2400)),\n",
              "  (0.6756539967987948, tensor(80.0800)),\n",
              "  (0.6733152077829465, tensor(80.3100)),\n",
              "  (0.6708414284741885, tensor(80.5100)),\n",
              "  (0.6732350839512016, tensor(80.1900)),\n",
              "  (0.6801796912526566, tensor(80.1800)),\n",
              "  (0.6751138733340893, tensor(80.2900)),\n",
              "  (0.6768313550592662, tensor(80.3400)),\n",
              "  (0.6807691129705403, tensor(79.9900)),\n",
              "  (0.6816841884434922, tensor(80.2900)),\n",
              "  (0.678620849487136, tensor(80.2900)),\n",
              "  (0.6737156031299237, tensor(80.3900)),\n",
              "  (0.6768400076872902, tensor(80.3100)),\n",
              "  (0.6774446821751772, tensor(80.2500)),\n",
              "  (0.680255964170958, tensor(80.2200)),\n",
              "  (0.6725244758830929, tensor(80.7200)),\n",
              "  (0.6785250319743107, tensor(80.4800)),\n",
              "  (0.6798988208201757, tensor(80.2200)),\n",
              "  (0.6779124596869311, tensor(80.3700)),\n",
              "  (0.6819383520340969, tensor(80.1200)),\n",
              "  (0.6843828343100162, tensor(80.1400)),\n",
              "  (0.6818089835564446, tensor(80.3000)),\n",
              "  (0.6825869775863844, tensor(80.3900)),\n",
              "  (0.6803516291896377, tensor(80.3700)),\n",
              "  (0.6806861323538076, tensor(80.5200)),\n",
              "  (0.6856619970517568, tensor(80.2000)),\n",
              "  (0.6908917412703318, tensor(80.0600)),\n",
              "  (0.6847375223933398, tensor(80.3800)),\n",
              "  (0.6879689762356953, tensor(80.2200)),\n",
              "  (0.6867474397660611, tensor(80.2900)),\n",
              "  (0.6870914074007757, tensor(80.2500)),\n",
              "  (0.6857843427187967, tensor(80.3500)),\n",
              "  (0.6850801237753309, tensor(80.5100)),\n",
              "  (0.69515991627201, tensor(80.2800)),\n",
              "  (0.6896116096622805, tensor(80.2100)),\n",
              "  (0.6923030683846955, tensor(80.0100)),\n",
              "  (0.6918538823199779, tensor(80.1100)),\n",
              "  (0.6915128207584974, tensor(80.4100)),\n",
              "  (0.6905021159292599, tensor(80.3400)),\n",
              "  (0.6941063149085676, tensor(80.3400)),\n",
              "  (0.6961153247104441, tensor(80.1700)),\n",
              "  (0.6923444351416147, tensor(80.3500)),\n",
              "  (0.6970932546135896, tensor(80.2900)),\n",
              "  (0.7016669094883072, tensor(80.1500)),\n",
              "  (0.7043439345054787, tensor(79.9400)),\n",
              "  (0.6992396820731818, tensor(80.2900)),\n",
              "  (0.6979660719624801, tensor(80.2500)),\n",
              "  (0.6988121521356297, tensor(80.4200)),\n",
              "  (0.6990871108675376, tensor(80.3300)),\n",
              "  (0.7012030293536525, tensor(80.3100)),\n",
              "  (0.7057235552657032, tensor(80.1300)),\n",
              "  (0.7014955430889306, tensor(80.3200)),\n",
              "  (0.7026905204724692, tensor(80.3500)),\n",
              "  (0.702563441358519, tensor(80.4700)),\n",
              "  (0.7021699508989834, tensor(80.4600)),\n",
              "  (0.7097058822987696, tensor(80.1400)),\n",
              "  (0.7087504296345094, tensor(80.3400)),\n",
              "  (0.705207664343221, tensor(80.6300)),\n",
              "  (0.7112186796172416, tensor(80.2800)),\n",
              "  (0.714275404597339, tensor(80.0900)),\n",
              "  (0.7102507284786952, tensor(80.3300)),\n",
              "  (0.7151775008789792, tensor(80.1800)),\n",
              "  (0.7136727843928571, tensor(80.1800)),\n",
              "  (0.7180474685044592, tensor(80.2200)),\n",
              "  (0.7193408558062063, tensor(80.4300)),\n",
              "  (0.7208634678441489, tensor(80.0900)),\n",
              "  (0.7256899413477491, tensor(79.9000)),\n",
              "  (0.7205959294946149, tensor(80.4500)),\n",
              "  (0.7191907568675104, tensor(80.4700)),\n",
              "  (0.7190500821817017, tensor(80.3600)),\n",
              "  (0.7215622323663898, tensor(80.3000)),\n",
              "  (0.7250125694421049, tensor(80.2200)),\n",
              "  (0.7248761834924909, tensor(80.3100)),\n",
              "  (0.7283978968543408, tensor(80.2200)),\n",
              "  (0.740256391883844, tensor(79.7100)),\n",
              "  (0.7281777227070173, tensor(80.4700)),\n",
              "  (0.7273897004189918, tensor(80.5400)),\n",
              "  (0.7320709607221355, tensor(80.1900)),\n",
              "  (0.7309982153607268, tensor(80.5900)),\n",
              "  (0.733353938380712, tensor(80.3100)),\n",
              "  (0.7355611002922018, tensor(80.2100)),\n",
              "  (0.7336693922929133, tensor(80.3500)),\n",
              "  (0.7405008412936802, tensor(80.1500)),\n",
              "  (0.7370089450723208, tensor(80.4400)),\n",
              "  (0.7406653272804564, tensor(80.3000)),\n",
              "  (0.7402691493666091, tensor(80.3200)),\n",
              "  (0.7473878296890591, tensor(80.0300)),\n",
              "  (0.7458912352777276, tensor(80.2200)),\n",
              "  (0.7490695261231644, tensor(80.0600)),\n",
              "  (0.7479986728988941, tensor(80.2000)),\n",
              "  (0.7471265452816342, tensor(80.3200)),\n",
              "  (0.7490697110773408, tensor(80.3200)),\n",
              "  (0.7499332841165053, tensor(80.3000)),\n",
              "  (0.7616358411977314, tensor(79.8900)),\n",
              "  (0.7550631518425904, tensor(80.2400)),\n",
              "  (0.7519346758161793, tensor(80.3800)),\n",
              "  (0.7552863139176257, tensor(80.2500)),\n",
              "  (0.7529698980283394, tensor(80.5900)),\n",
              "  (0.758593965286441, tensor(80.2200)),\n",
              "  (0.7570074973497334, tensor(80.4100)),\n",
              "  (0.7591276141659394, tensor(80.2600)),\n",
              "  (0.7636942207835138, tensor(80.2900)),\n",
              "  (0.7600762398834958, tensor(80.4200)),\n",
              "  (0.7613984736028425, tensor(80.4100)),\n",
              "  (0.7631266829359618, tensor(80.5500)),\n",
              "  (0.7725745982299383, tensor(80.1200)),\n",
              "  (0.7691870597482666, tensor(80.2500)),\n",
              "  (0.7734264737395345, tensor(80.2100)),\n",
              "  (0.7728975517866082, tensor(80.2200)),\n",
              "  (0.7823110775132651, tensor(79.9100)),\n",
              "  (0.7812308696669632, tensor(80.0100)),\n",
              "  (0.7768407720765331, tensor(80.3400)),\n",
              "  (0.7796816727449228, tensor(80.2000)),\n",
              "  (0.7748225093365644, tensor(80.4300)),\n",
              "  (0.7814951667992666, tensor(80.2600)),\n",
              "  (0.7803857636110926, tensor(80.4500)),\n",
              "  (0.7811759210205879, tensor(80.4600)),\n",
              "  (0.7803837902773254, tensor(80.4300)),\n",
              "  (0.7831662519068492, tensor(80.4900)),\n",
              "  (0.7867378542884026, tensor(80.2800)),\n",
              "  (0.7863143195876839, tensor(80.5500)),\n",
              "  (0.7865716217640476, tensor(80.3600)),\n",
              "  (0.7920970618219636, tensor(80.3100)),\n",
              "  (0.7975428264904917, tensor(80.2100)),\n",
              "  (0.7927905518692521, tensor(80.3800)),\n",
              "  (0.7948212250175114, tensor(80.3500)),\n",
              "  (0.7958104203936152, tensor(80.4700)),\n",
              "  (0.8031411474565513, tensor(80.1900)),\n",
              "  (0.7991310497393433, tensor(80.3100)),\n",
              "  (0.8008203062599281, tensor(80.2900)),\n",
              "  (0.7992602293041885, tensor(80.4900)),\n",
              "  (0.8035188067927302, tensor(80.2900)),\n",
              "  (0.8060889757219755, tensor(80.3600)),\n",
              "  (0.8063015559987521, tensor(80.4100)),\n",
              "  (0.8074365279458827, tensor(80.4400)),\n",
              "  (0.8095662209336459, tensor(80.3700)),\n",
              "  (0.8086818725081545, tensor(80.4600)),\n",
              "  (0.8077919777301094, tensor(80.4800)),\n",
              "  (0.8130207783020426, tensor(80.3900)),\n",
              "  (0.812264691958283, tensor(80.4200)),\n",
              "  (0.8160356621116699, tensor(80.2500)),\n",
              "  (0.817902868392917, tensor(80.4800)),\n",
              "  (0.8165008382816398, tensor(80.4900)),\n",
              "  (0.8189872572880893, tensor(80.4700)),\n",
              "  (0.8196777612749647, tensor(80.5200)),\n",
              "  (0.8180758103186805, tensor(80.5600)),\n",
              "  (0.8231940967774423, tensor(80.4100)),\n",
              "  (0.8215894335398931, tensor(80.5500)),\n",
              "  (0.8215360334597526, tensor(80.6000)),\n",
              "  (0.8263624802840446, tensor(80.4200)),\n",
              "  (0.8268656901828941, tensor(80.4300)),\n",
              "  (0.8273956559316921, tensor(80.5000)),\n",
              "  (0.8277591533413883, tensor(80.6400)),\n",
              "  (0.829113804348403, tensor(80.6500)),\n",
              "  (0.8290159280448208, tensor(80.7000)),\n",
              "  (0.8304872421547826, tensor(80.6400)),\n",
              "  (0.8342599662719876, tensor(80.5200)),\n",
              "  (0.8335997311339305, tensor(80.6100)),\n",
              "  (0.8375867581506407, tensor(80.5200)),\n",
              "  (0.8425621465411381, tensor(80.3400)),\n",
              "  (0.8385539883685562, tensor(80.7100)),\n",
              "  (0.8396949788035086, tensor(80.4700)),\n",
              "  (0.8399918382249132, tensor(80.7100)),\n",
              "  (0.8421549877026311, tensor(80.6200)),\n",
              "  (0.846056272461855, tensor(80.4700)),\n",
              "  (0.8439669477851337, tensor(80.7700)),\n",
              "  (0.845692866761588, tensor(80.7200)),\n",
              "  (0.8471864195999731, tensor(80.6100)),\n",
              "  (0.8487849861698719, tensor(80.6700)),\n",
              "  (0.847407444632523, tensor(80.7800)),\n",
              "  (0.8515964571466442, tensor(80.4500)),\n",
              "  (0.853844573730827, tensor(80.4900)),\n",
              "  (0.8543267543497132, tensor(80.6000)),\n",
              "  (0.8585870368164111, tensor(80.4800)),\n",
              "  (0.8555747958740183, tensor(80.6800)),\n",
              "  (0.8592677476253712, tensor(80.5100)),\n",
              "  (0.8564086374944553, tensor(80.8400)),\n",
              "  (0.8602554710518936, tensor(80.5700)),\n",
              "  (0.8613089410993369, tensor(80.6600)),\n",
              "  (0.8622567942340166, tensor(80.6700)),\n",
              "  (0.8633637777746453, tensor(80.6500)),\n",
              "  (0.8652861698996017, tensor(80.5400)),\n",
              "  (0.8652825215737022, tensor(80.6900)),\n",
              "  (0.8670121064458169, tensor(80.6800)),\n",
              "  (0.8674380806943147, tensor(80.6000)),\n",
              "  (0.8692034877354415, tensor(80.6500)),\n",
              "  (0.8686270779233187, tensor(80.7200)),\n",
              "  (0.8714187529757368, tensor(80.6100)),\n",
              "  (0.8735416013034403, tensor(80.6900)),\n",
              "  (0.8759638945565743, tensor(80.5700)),\n",
              "  (0.876584980987219, tensor(80.6800)),\n",
              "  (0.8745326820234928, tensor(80.6300)),\n",
              "  (0.8752767537904242, tensor(80.7400)),\n",
              "  (0.8791565465715375, tensor(80.6400)),\n",
              "  (0.8797898911770556, tensor(80.6700)),\n",
              "  (0.8828433205997529, tensor(80.5900)),\n",
              "  (0.8824388580806343, tensor(80.6400)),\n",
              "  (0.8834050169495069, tensor(80.6900)),\n",
              "  (0.8831093240829184, tensor(80.7100)),\n",
              "  (0.8840502570731922, tensor(80.7800)),\n",
              "  (0.8872786515527056, tensor(80.5400)),\n",
              "  (0.8879903027920045, tensor(80.6700)),\n",
              "  (0.8884785436707809, tensor(80.8200)),\n",
              "  (0.8909853601588195, tensor(80.6700)),\n",
              "  (0.8909463191819527, tensor(80.7200))],\n",
              " 'log_softmax': [(2.303101036286354, tensor(10.0900)),\n",
              "  (2.3024731009960173, tensor(10.0900)),\n",
              "  (2.301862487816811, tensor(10.1000)),\n",
              "  (2.301264090490341, tensor(9.9800)),\n",
              "  (2.3006789343833924, tensor(12.2700)),\n",
              "  (2.3001054943084718, tensor(13.0900)),\n",
              "  (2.2995424325466156, tensor(10.3900)),\n",
              "  (2.298986019849777, tensor(10.2800)),\n",
              "  (2.298438438630104, tensor(10.2800)),\n",
              "  (2.2978987448453903, tensor(10.2800)),\n",
              "  (2.2973667362213135, tensor(10.2800)),\n",
              "  (2.296843343114853, tensor(10.2800)),\n",
              "  (2.2963249523878098, tensor(10.2800)),\n",
              "  (2.2958114810943604, tensor(10.2800)),\n",
              "  (2.295297423386574, tensor(10.2800)),\n",
              "  (2.2947817198753357, tensor(10.2800)),\n",
              "  (2.294264592719078, tensor(10.2800)),\n",
              "  (2.2937445932626725, tensor(10.2800)),\n",
              "  (2.293219595837593, tensor(10.2800)),\n",
              "  (2.292685276222229, tensor(10.2900)),\n",
              "  (2.2921381835222245, tensor(10.3100)),\n",
              "  (2.291582089829445, tensor(10.3800)),\n",
              "  (2.291013379764557, tensor(10.4400)),\n",
              "  (2.290427771091461, tensor(10.5300)),\n",
              "  (2.2898301929712295, tensor(10.6800)),\n",
              "  (2.2892173222064973, tensor(10.8600)),\n",
              "  (2.2885857975006103, tensor(11.1700)),\n",
              "  (2.287932025051117, tensor(11.5000)),\n",
              "  (2.2872578048706056, tensor(11.8800)),\n",
              "  (2.286563755249977, tensor(12.2400)),\n",
              "  (2.2858407568216323, tensor(12.5400)),\n",
              "  (2.285094050073624, tensor(12.9700)),\n",
              "  (2.284312142896652, tensor(13.3900)),\n",
              "  (2.283504449224472, tensor(13.8800)),\n",
              "  (2.2826690838575363, tensor(14.3400)),\n",
              "  (2.2817917778730394, tensor(14.9000)),\n",
              "  (2.280878898191452, tensor(15.5900)),\n",
              "  (2.2799301167964936, tensor(16.2000)),\n",
              "  (2.278943868517876, tensor(16.7900)),\n",
              "  (2.277914681863785, tensor(17.0400)),\n",
              "  (2.276834213423729, tensor(17.5000)),\n",
              "  (2.2757052168130874, tensor(17.9300)),\n",
              "  (2.2745273856163024, tensor(18.3400)),\n",
              "  (2.27329123775959, tensor(18.7500)),\n",
              "  (2.2720062219142916, tensor(19.0900)),\n",
              "  (2.2706599888563157, tensor(19.4400)),\n",
              "  (2.2692406177282334, tensor(19.7000)),\n",
              "  (2.267759325695038, tensor(20.0200)),\n",
              "  (2.2662104074239733, tensor(20.2500)),\n",
              "  (2.264590957403183, tensor(20.3900)),\n",
              "  (2.262889904999733, tensor(20.5300)),\n",
              "  (2.2611003881931304, tensor(20.6300)),\n",
              "  (2.2592189123392106, tensor(20.8200)),\n",
              "  (2.2572354068517684, tensor(21.)),\n",
              "  (2.2551476386785505, tensor(21.3000)),\n",
              "  (2.252946000123024, tensor(21.7000)),\n",
              "  (2.250632824277878, tensor(22.0400)),\n",
              "  (2.248178599286079, tensor(22.6100)),\n",
              "  (2.245604249548912, tensor(23.1300)),\n",
              "  (2.2428773000240327, tensor(23.7100)),\n",
              "  (2.2399943423390387, tensor(24.1700)),\n",
              "  (2.236941632258892, tensor(24.8400)),\n",
              "  (2.23371836591959, tensor(25.3200)),\n",
              "  (2.2303002542972563, tensor(25.7800)),\n",
              "  (2.2266945323586462, tensor(26.1000)),\n",
              "  (2.2228639942646025, tensor(26.5100)),\n",
              "  (2.218792138683796, tensor(26.8200)),\n",
              "  (2.214464285647869, tensor(27.1000)),\n",
              "  (2.209888316512108, tensor(27.3700)),\n",
              "  (2.2049653083205225, tensor(27.7400)),\n",
              "  (2.1997350965619087, tensor(28.1100)),\n",
              "  (2.1941393447637556, tensor(28.5600)),\n",
              "  (2.1882197336554525, tensor(28.9400)),\n",
              "  (2.1818728526592253, tensor(29.2700)),\n",
              "  (2.1751237920165063, tensor(29.5200)),\n",
              "  (2.167935451388359, tensor(29.8200)),\n",
              "  (2.1601648359179495, tensor(30.2100)),\n",
              "  (2.1519337360024453, tensor(30.6000)),\n",
              "  (2.1431065133213996, tensor(30.9300)),\n",
              "  (2.133687694621086, tensor(31.1500)),\n",
              "  (2.123626660001278, tensor(31.4900)),\n",
              "  (2.1127537997603416, tensor(31.7600)),\n",
              "  (2.1012569850325584, tensor(31.8600)),\n",
              "  (2.0889331208467485, tensor(32.1100)),\n",
              "  (2.075822365796566, tensor(32.5700)),\n",
              "  (2.0617024853229524, tensor(32.9200)),\n",
              "  (2.0466410893440248, tensor(33.0400)),\n",
              "  (2.030626563692093, tensor(33.4800)),\n",
              "  (2.0135715499043463, tensor(33.8500)),\n",
              "  (1.9956686851620675, tensor(34.3600)),\n",
              "  (1.9769721494436263, tensor(34.5700)),\n",
              "  (1.9566123981833459, tensor(34.8000)),\n",
              "  (1.9355004559516906, tensor(35.3000)),\n",
              "  (1.9135804416298867, tensor(35.6900)),\n",
              "  (1.890798098707199, tensor(36.2200)),\n",
              "  (1.8666976365625858, tensor(36.8700)),\n",
              "  (1.8422232399702072, tensor(37.6300)),\n",
              "  (1.817113870859146, tensor(38.2800)),\n",
              "  (1.7913312433362008, tensor(39.1500)),\n",
              "  (1.7648028473734856, tensor(40.2100)),\n",
              "  (1.7383741011083127, tensor(40.9100)),\n",
              "  (1.7111021180033683, tensor(42.1900)),\n",
              "  (1.6841202868163585, tensor(42.9700)),\n",
              "  (1.657906843841076, tensor(43.8300)),\n",
              "  (1.631104106426239, tensor(44.8900)),\n",
              "  (1.6045335099458695, tensor(45.9300)),\n",
              "  (1.5775802139014006, tensor(46.8100)),\n",
              "  (1.5516045445382596, tensor(47.8900)),\n",
              "  (1.5258110055714846, tensor(48.9200)),\n",
              "  (1.502550866779685, tensor(50.3200)),\n",
              "  (1.4777829194426537, tensor(51.0300)),\n",
              "  (1.4549167248100043, tensor(52.1300)),\n",
              "  (1.4318216838777065, tensor(53.1000)),\n",
              "  (1.4112242056638002, tensor(53.8200)),\n",
              "  (1.3896456509232522, tensor(54.4800)),\n",
              "  (1.3686362110614776, tensor(55.1900)),\n",
              "  (1.3505273539185525, tensor(55.3900)),\n",
              "  (1.3306141659259796, tensor(55.9200)),\n",
              "  (1.31301074655056, tensor(56.6700)),\n",
              "  (1.2942802777826785, tensor(57.2000)),\n",
              "  (1.2771750945210456, tensor(57.4300)),\n",
              "  (1.2604380832403899, tensor(57.9300)),\n",
              "  (1.2451149720206858, tensor(58.8500)),\n",
              "  (1.2318210181459786, tensor(58.8300)),\n",
              "  (1.214006381240487, tensor(60.1700)),\n",
              "  (1.1992368917942047, tensor(59.8900)),\n",
              "  (1.1862646226361393, tensor(61.0100)),\n",
              "  (1.1717675639793277, tensor(61.6300)),\n",
              "  (1.1604936907216907, tensor(61.7700)),\n",
              "  (1.1476040471553803, tensor(61.9400)),\n",
              "  (1.1333678093716502, tensor(62.7100)),\n",
              "  (1.1192385027006269, tensor(63.4800)),\n",
              "  (1.1067173685133458, tensor(64.1600)),\n",
              "  (1.0940027615979313, tensor(64.6100)),\n",
              "  (1.0799374918609859, tensor(65.5500)),\n",
              "  (1.0707658853963018, tensor(65.0700)),\n",
              "  (1.0587288930170238, tensor(65.6500)),\n",
              "  (1.0466588810034096, tensor(66.2300)),\n",
              "  (1.0352331405594946, tensor(66.7700)),\n",
              "  (1.0251618491008878, tensor(66.9800)),\n",
              "  (1.0169158514395356, tensor(67.3700)),\n",
              "  (1.0026134077616036, tensor(67.2900)),\n",
              "  (0.9929245828084647, tensor(67.5100)),\n",
              "  (0.9843708949103952, tensor(68.0900)),\n",
              "  (0.9727126022234559, tensor(69.2200)),\n",
              "  (0.9628074741400778, tensor(68.5000)),\n",
              "  (0.9559756624516099, tensor(69.3200)),\n",
              "  (0.9443667995207011, tensor(69.6100)),\n",
              "  (0.9358112397141755, tensor(69.7900)),\n",
              "  (0.9257066362742334, tensor(70.4000)),\n",
              "  (0.9169410636473447, tensor(70.3100)),\n",
              "  (0.9106394560236484, tensor(71.1200)),\n",
              "  (0.9003798751872033, tensor(71.5300)),\n",
              "  (0.8931694899391383, tensor(71.2900)),\n",
              "  (0.8832689445741475, tensor(71.6900)),\n",
              "  (0.8781952542003244, tensor(72.4100)),\n",
              "  (0.8689027346406132, tensor(72.2400)),\n",
              "  (0.8662835763003677, tensor(72.3200)),\n",
              "  (0.8566749204682186, tensor(72.7200)),\n",
              "  (0.8497478245554492, tensor(72.7600)),\n",
              "  (0.8421442477937787, tensor(73.0200)),\n",
              "  (0.8370260281803087, tensor(73.3500)),\n",
              "  (0.8368898691635579, tensor(72.9200)),\n",
              "  (0.8234678600890561, tensor(73.6000)),\n",
              "  (0.8200939883641899, tensor(73.5300)),\n",
              "  (0.8157984122194349, tensor(73.5300)),\n",
              "  (0.8101828018248082, tensor(73.8800)),\n",
              "  (0.8040562605546787, tensor(74.3100)),\n",
              "  (0.7972510008173995, tensor(74.5200)),\n",
              "  (0.796355099689588, tensor(74.4100)),\n",
              "  (0.7896982541860081, tensor(74.5300)),\n",
              "  (0.7868942951506004, tensor(74.7500)),\n",
              "  (0.7812427902685478, tensor(74.7600)),\n",
              "  (0.7791613540682941, tensor(74.9000)),\n",
              "  (0.7751369366525672, tensor(75.0600)),\n",
              "  (0.7783750815682113, tensor(74.9300)),\n",
              "  (0.7650029069653712, tensor(75.5400)),\n",
              "  (0.7623392894037999, tensor(75.2700)),\n",
              "  (0.7621846343760379, tensor(75.5400)),\n",
              "  (0.7593242452085018, tensor(75.3600)),\n",
              "  (0.7534541405956261, tensor(75.8100)),\n",
              "  (0.7504657845273613, tensor(75.6200)),\n",
              "  (0.754886607259512, tensor(75.0800)),\n",
              "  (0.7473090559008997, tensor(75.7900)),\n",
              "  (0.7405529461751227, tensor(75.9300)),\n",
              "  (0.7421358199175913, tensor(76.1000)),\n",
              "  (0.7385415215140674, tensor(76.6200)),\n",
              "  (0.7367558803045656, tensor(76.1800)),\n",
              "  (0.7335513273413293, tensor(76.3200)),\n",
              "  (0.7267418449368793, tensor(76.7900)),\n",
              "  (0.7256811627669726, tensor(76.6600)),\n",
              "  (0.7230324811351951, tensor(77.2500)),\n",
              "  (0.7237347338321153, tensor(76.6000)),\n",
              "  (0.7185776686989702, tensor(77.2000)),\n",
              "  (0.7183135479559191, tensor(77.1300)),\n",
              "  (0.7186296854305547, tensor(77.1200)),\n",
              "  (0.7162199889113661, tensor(77.3100)),\n",
              "  (0.7097808320914861, tensor(77.5000)),\n",
              "  (0.7127609458069317, tensor(77.2800)),\n",
              "  (0.7061856853820849, tensor(77.6700)),\n",
              "  (0.7072996207136195, tensor(77.8300)),\n",
              "  (0.7020940297319088, tensor(77.5900)),\n",
              "  (0.704918027715804, tensor(77.3800)),\n",
              "  (0.7001229148338549, tensor(77.8700)),\n",
              "  (0.6998907544100658, tensor(77.5900)),\n",
              "  (0.6944494418998249, tensor(77.9200)),\n",
              "  (0.6995718881905777, tensor(77.4600)),\n",
              "  (0.6938126579840901, tensor(78.1000)),\n",
              "  (0.6933564977135044, tensor(78.0100)),\n",
              "  (0.6892419060538989, tensor(78.3300)),\n",
              "  (0.6977434010591591, tensor(77.5200)),\n",
              "  (0.6853465029103681, tensor(78.2400)),\n",
              "  (0.6874091465900186, tensor(78.2100)),\n",
              "  (0.6856504463787424, tensor(78.4300)),\n",
              "  (0.6811596245926339, tensor(78.7200)),\n",
              "  (0.6846304104606388, tensor(78.1500)),\n",
              "  (0.6799159727221821, tensor(78.6800)),\n",
              "  (0.6796433839621255, tensor(78.6800)),\n",
              "  (0.67779095502696, tensor(78.9100)),\n",
              "  (0.6756060217497172, tensor(79.0900)),\n",
              "  (0.6767787159637548, tensor(78.8600)),\n",
              "  (0.6764807514638873, tensor(78.7600)),\n",
              "  (0.6712539356436348, tensor(79.2500)),\n",
              "  (0.6728454757467378, tensor(79.3300)),\n",
              "  (0.6698487640873878, tensor(79.3100)),\n",
              "  (0.6644623723049066, tensor(79.4800)),\n",
              "  (0.6655985457491596, tensor(79.5100)),\n",
              "  (0.6665999872210203, tensor(79.4100)),\n",
              "  (0.666087614754634, tensor(79.4100)),\n",
              "  (0.663698067294457, tensor(79.6500)),\n",
              "  (0.661774502798228, tensor(79.7600)),\n",
              "  (0.6601404471027432, tensor(79.5600)),\n",
              "  (0.6602941581870662, tensor(79.6600)),\n",
              "  (0.6607887141485116, tensor(79.6000)),\n",
              "  (0.6600882959313341, tensor(79.9100)),\n",
              "  (0.6600854439711896, tensor(79.5600)),\n",
              "  (0.6608044210412889, tensor(79.6700)),\n",
              "  (0.6535054188674316, tensor(79.8700)),\n",
              "  (0.6533027276400942, tensor(79.9400)),\n",
              "  (0.6541791478510015, tensor(79.7900)),\n",
              "  (0.6567286847196985, tensor(79.7200)),\n",
              "  (0.654879934229271, tensor(79.6900)),\n",
              "  (0.6531959164433297, tensor(80.0700)),\n",
              "  (0.6513813080965075, tensor(80.0600)),\n",
              "  (0.653431668223883, tensor(79.9300)),\n",
              "  (0.6565040862625349, tensor(79.5900)),\n",
              "  (0.6507508197975694, tensor(80.1600)),\n",
              "  (0.6487151496401405, tensor(80.2400)),\n",
              "  (0.6479084607294179, tensor(80.1900)),\n",
              "  (0.6451458289732167, tensor(80.4200)),\n",
              "  (0.6487422892972944, tensor(80.1500)),\n",
              "  (0.6496964874379045, tensor(79.8400)),\n",
              "  (0.6468159571644791, tensor(80.1800)),\n",
              "  (0.6435127889985218, tensor(80.3600)),\n",
              "  (0.643350984224613, tensor(80.4100)),\n",
              "  (0.6465276258326368, tensor(80.2100)),\n",
              "  (0.6430365394907247, tensor(80.4800)),\n",
              "  (0.6426444229498971, tensor(80.2100)),\n",
              "  (0.6411650108069298, tensor(80.8700)),\n",
              "  (0.6410086771207454, tensor(80.4400)),\n",
              "  (0.6399418013521412, tensor(80.5000)),\n",
              "  (0.638091031097807, tensor(80.5800)),\n",
              "  (0.6387600758091081, tensor(80.6700)),\n",
              "  (0.6410620176604949, tensor(80.6600)),\n",
              "  (0.6416607715077466, tensor(80.6400)),\n",
              "  (0.639547577373602, tensor(80.7400)),\n",
              "  (0.6389667777833296, tensor(80.5300)),\n",
              "  (0.636817859038437, tensor(80.9000)),\n",
              "  (0.6401846217461687, tensor(80.6800)),\n",
              "  (0.6417080549600738, tensor(80.6300)),\n",
              "  (0.6434656474232295, tensor(80.4800)),\n",
              "  (0.6512916012031142, tensor(79.8500)),\n",
              "  (0.6442220517490059, tensor(80.4500)),\n",
              "  (0.6392741632309015, tensor(80.9000)),\n",
              "  (0.6409792571510042, tensor(80.8000)),\n",
              "  (0.6436626979533466, tensor(80.4700)),\n",
              "  (0.6408832660628483, tensor(80.6000)),\n",
              "  (0.6421392605214409, tensor(80.6000)),\n",
              "  (0.6415425985903508, tensor(80.7100)),\n",
              "  (0.6381780096515606, tensor(80.9800)),\n",
              "  (0.6417089489297796, tensor(80.6900)),\n",
              "  (0.640942794533752, tensor(80.8400)),\n",
              "  (0.640494841088244, tensor(80.8600)),\n",
              "  (0.6442358442417666, tensor(80.6700)),\n",
              "  (0.6415574056057202, tensor(80.8200)),\n",
              "  (0.642124085238688, tensor(80.8400)),\n",
              "  (0.6440214935637006, tensor(80.7600)),\n",
              "  (0.6417781827569837, tensor(81.0300)),\n",
              "  (0.6434422848741015, tensor(81.0500)),\n",
              "  (0.6465873177533941, tensor(80.7700)),\n",
              "  (0.6470547072764486, tensor(80.8400)),\n",
              "  (0.6455473930412234, tensor(80.9600)),\n",
              "  (0.6532079953519307, tensor(80.5300)),\n",
              "  (0.6454915494874905, tensor(80.9900)),\n",
              "  (0.6488838376470594, tensor(80.9400)),\n",
              "  (0.6464230044442578, tensor(81.0200)),\n",
              "  (0.6485015339650417, tensor(80.9000)),\n",
              "  (0.6464009802798333, tensor(80.9900)),\n",
              "  (0.6427945059762896, tensor(81.1700)),\n",
              "  (0.6467673023666132, tensor(81.0200)),\n",
              "  (0.6509782151319298, tensor(80.9900)),\n",
              "  (0.6481538203451244, tensor(81.1300)),\n",
              "  (0.6508029536433336, tensor(80.9300)),\n",
              "  (0.6565786810104117, tensor(80.9200)),\n",
              "  (0.6532599581146257, tensor(80.9600)),\n",
              "  (0.6537773293091647, tensor(81.0300)),\n",
              "  (0.65175393012446, tensor(81.0700)),\n",
              "  (0.6534710899401965, tensor(81.0600)),\n",
              "  (0.6554693551817574, tensor(80.9600)),\n",
              "  (0.6578952537654572, tensor(80.8900)),\n",
              "  (0.6519515500297879, tensor(81.1400)),\n",
              "  (0.6565946424597751, tensor(81.0300)),\n",
              "  (0.6593471254786367, tensor(80.9800)),\n",
              "  (0.6574695206794742, tensor(81.0200)),\n",
              "  (0.6638275551382867, tensor(80.8800)),\n",
              "  (0.6653634618492706, tensor(80.7600)),\n",
              "  (0.6632213390915745, tensor(80.9600)),\n",
              "  (0.6634252847045046, tensor(81.0700)),\n",
              "  (0.662650064672291, tensor(80.8400)),\n",
              "  (0.6630571437224677, tensor(80.9200)),\n",
              "  (0.6668992550192215, tensor(80.8700)),\n",
              "  (0.6708744460943071, tensor(80.5100)),\n",
              "  (0.6683726574782021, tensor(80.9900)),\n",
              "  (0.6733793527886078, tensor(80.6700)),\n",
              "  (0.6718268407456166, tensor(80.8100)),\n",
              "  (0.6727998974163413, tensor(80.8700)),\n",
              "  (0.67225201347, tensor(80.9300)),\n",
              "  (0.6718666979360028, tensor(80.8500)),\n",
              "  (0.6807758413926793, tensor(80.7300)),\n",
              "  (0.6764002102705224, tensor(80.8800)),\n",
              "  (0.683235371331952, tensor(80.5000)),\n",
              "  (0.6780109713067417, tensor(80.8700)),\n",
              "  (0.6799430360904855, tensor(80.8300)),\n",
              "  (0.6808034387339945, tensor(80.7900)),\n",
              "  (0.6809470807219433, tensor(80.8700)),\n",
              "  (0.683159881896407, tensor(80.7800)),\n",
              "  (0.6838313174670669, tensor(80.6600)),\n",
              "  (0.6864890106435276, tensor(80.7700)),\n",
              "  (0.6903666237869945, tensor(80.6000)),\n",
              "  (0.6932236440546121, tensor(80.3500)),\n",
              "  (0.6903421402402949, tensor(80.7400)),\n",
              "  (0.6882358738130957, tensor(80.7600)),\n",
              "  (0.6908499172315432, tensor(80.4700)),\n",
              "  (0.6902932250424199, tensor(80.7300)),\n",
              "  (0.6973988690738583, tensor(80.4300)),\n",
              "  (0.6963526222121431, tensor(80.4000)),\n",
              "  (0.69520333090594, tensor(80.7000)),\n",
              "  (0.6944429423386551, tensor(80.9300)),\n",
              "  (0.6956166840090942, tensor(80.7700)),\n",
              "  (0.6971585081105651, tensor(80.5800)),\n",
              "  (0.7015572789684832, tensor(80.5900)),\n",
              "  (0.7009982696251149, tensor(80.6800)),\n",
              "  (0.6995661949026869, tensor(80.8100)),\n",
              "  (0.7057817820751014, tensor(80.4800)),\n",
              "  (0.7076551760072435, tensor(80.4700)),\n",
              "  (0.7055146946800086, tensor(80.6600)),\n",
              "  (0.7097124049407912, tensor(80.7300)),\n",
              "  (0.7083500165464653, tensor(80.7200)),\n",
              "  (0.7106732957468356, tensor(80.6200)),\n",
              "  (0.7145917258610169, tensor(80.5400)),\n",
              "  (0.7136180992346983, tensor(80.4500)),\n",
              "  (0.7192452354942663, tensor(80.2600)),\n",
              "  (0.7166119320478284, tensor(80.4800)),\n",
              "  (0.7148839747754376, tensor(80.5600)),\n",
              "  (0.7160697945173604, tensor(80.6800)),\n",
              "  (0.7191833536996557, tensor(80.7400)),\n",
              "  (0.7208426479450796, tensor(80.6900)),\n",
              "  (0.7203822597986578, tensor(80.5200)),\n",
              "  (0.7253540857404178, tensor(80.3400)),\n",
              "  (0.7345440310133361, tensor(80.2200)),\n",
              "  (0.7250871096758672, tensor(80.7100)),\n",
              "  (0.7256325531854126, tensor(80.6400)),\n",
              "  (0.7300116080212401, tensor(80.4000)),\n",
              "  (0.7290520074609232, tensor(80.7700)),\n",
              "  (0.7306759697691898, tensor(80.4400)),\n",
              "  (0.7312501358316755, tensor(80.4300)),\n",
              "  (0.7315933367151801, tensor(80.6700)),\n",
              "  (0.7360910045107858, tensor(80.4000)),\n",
              "  (0.7362397827418896, tensor(80.5700)),\n",
              "  (0.7369566987380853, tensor(80.4800)),\n",
              "  (0.7416646506491416, tensor(80.4500)),\n",
              "  (0.7432149059882378, tensor(80.2500)),\n",
              "  (0.7481060337170439, tensor(80.2700)),\n",
              "  (0.7455022363596973, tensor(80.3200)),\n",
              "  (0.7471195738267024, tensor(80.3600)),\n",
              "  (0.7463331998443399, tensor(80.5200)),\n",
              "  (0.746972390369682, tensor(80.5800)),\n",
              "  (0.7499616079097696, tensor(80.5000)),\n",
              "  (0.7569373392373574, tensor(80.2800)),\n",
              "  (0.7533197378089179, tensor(80.4300)),\n",
              "  (0.7514561315853197, tensor(80.6700)),\n",
              "  (0.7539214745875975, tensor(80.5200)),\n",
              "  (0.753692612974585, tensor(80.7500)),\n",
              "  (0.7569246891592077, tensor(80.3800)),\n",
              "  (0.7576983410940376, tensor(80.5200)),\n",
              "  (0.7597122170894479, tensor(80.4800)),\n",
              "  (0.7629433530505072, tensor(80.4400)),\n",
              "  (0.7615175334791379, tensor(80.4400)),\n",
              "  (0.7623337827556138, tensor(80.6000)),\n",
              "  (0.7648304740312547, tensor(80.6600)),\n",
              "  (0.772022846275262, tensor(80.2300)),\n",
              "  (0.7707303743404145, tensor(80.3800)),\n",
              "  (0.7707744977625435, tensor(80.4100)),\n",
              "  (0.7727521080411884, tensor(80.4400)),\n",
              "  (0.777636389797705, tensor(80.2200)),\n",
              "  (0.7800926456540754, tensor(80.3500)),\n",
              "  (0.7783365782442828, tensor(80.4100)),\n",
              "  (0.780136268354042, tensor(80.2900)),\n",
              "  (0.7778135446611499, tensor(80.5300)),\n",
              "  (0.7829151317486757, tensor(80.3800)),\n",
              "  (0.7824045032928283, tensor(80.4300)),\n",
              "  (0.7838363967159643, tensor(80.5200)),\n",
              "  (0.7827920114958454, tensor(80.5500)),\n",
              "  (0.7864484000914355, tensor(80.5000)),\n",
              "  (0.7885609538496087, tensor(80.4400)),\n",
              "  (0.7899483057871323, tensor(80.5600)),\n",
              "  (0.7900933363033257, tensor(80.4700)),\n",
              "  (0.7947767805108421, tensor(80.3100)),\n",
              "  (0.801861510899102, tensor(80.2400)),\n",
              "  (0.7954548737237367, tensor(80.5000)),\n",
              "  (0.7980412456896373, tensor(80.4300)),\n",
              "  (0.7984257818946038, tensor(80.5700)),\n",
              "  (0.8026391811855851, tensor(80.3200)),\n",
              "  (0.8021169280785785, tensor(80.3700)),\n",
              "  (0.8012935760766166, tensor(80.4400)),\n",
              "  (0.8041360728175783, tensor(80.4900)),\n",
              "  (0.8050837435862361, tensor(80.4300)),\n",
              "  (0.8087940217538495, tensor(80.3700)),\n",
              "  (0.8101876322856535, tensor(80.4600)),\n",
              "  (0.8117319773916328, tensor(80.3900)),\n",
              "  (0.8131510278226357, tensor(80.3700)),\n",
              "  (0.8143613064345587, tensor(80.3600)),\n",
              "  (0.8127407115278623, tensor(80.6000)),\n",
              "  (0.8166926493941103, tensor(80.4500)),\n",
              "  (0.8175362653789888, tensor(80.4500)),\n",
              "  (0.8202790337493959, tensor(80.3900)),\n",
              "  (0.8221227564313769, tensor(80.3600)),\n",
              "  (0.8219058167548837, tensor(80.4300)),\n",
              "  (0.8240536129083929, tensor(80.4000)),\n",
              "  (0.8262704648094606, tensor(80.3300)),\n",
              "  (0.8236216803535786, tensor(80.4000)),\n",
              "  (0.8275672074684491, tensor(80.3900)),\n",
              "  (0.8275946263972203, tensor(80.3700)),\n",
              "  (0.8280243953401752, tensor(80.5700)),\n",
              "  (0.8306010951120919, tensor(80.4000)),\n",
              "  (0.8330659773708108, tensor(80.4100)),\n",
              "  (0.8338805665597425, tensor(80.4900)),\n",
              "  (0.8334143010280389, tensor(80.4400)),\n",
              "  (0.8349797654409489, tensor(80.5000)),\n",
              "  (0.835441789551844, tensor(80.5200)),\n",
              "  (0.8376637576940448, tensor(80.4700)),\n",
              "  (0.8399607915422242, tensor(80.5100)),\n",
              "  (0.8407637345640673, tensor(80.4900)),\n",
              "  (0.8432482951503274, tensor(80.3700)),\n",
              "  (0.8485505045087899, tensor(80.3000)),\n",
              "  (0.8464112546589997, tensor(80.5800)),\n",
              "  (0.8460359053109939, tensor(80.5100)),\n",
              "  (0.8480482494450368, tensor(80.4900)),\n",
              "  (0.8489813407893434, tensor(80.5400)),\n",
              "  (0.8531152794680875, tensor(80.3900)),\n",
              "  (0.8511118695797959, tensor(80.5500)),\n",
              "  (0.8528817537941914, tensor(80.4900)),\n",
              "  (0.8545107085322943, tensor(80.5200)),\n",
              "  (0.8561572006389573, tensor(80.4100)),\n",
              "  (0.8557522782923741, tensor(80.5600)),\n",
              "  (0.8615377598744318, tensor(80.3400)),\n",
              "  (0.8608305515769773, tensor(80.4400)),\n",
              "  (0.8622519577682111, tensor(80.4400)),\n",
              "  (0.8673322265945037, tensor(80.3000)),\n",
              "  (0.8652886514974237, tensor(80.4600)),\n",
              "  (0.8677245519549776, tensor(80.3900)),\n",
              "  (0.8662073432695626, tensor(80.6100)),\n",
              "  (0.8689774809548406, tensor(80.4300)),\n",
              "  (0.8691463907140397, tensor(80.4300)),\n",
              "  (0.870157164303965, tensor(80.5100)),\n",
              "  (0.8715902433269787, tensor(80.5500)),\n",
              "  (0.8737891994903139, tensor(80.4500)),\n",
              "  (0.8730057358518992, tensor(80.4800)),\n",
              "  (0.8744907854409152, tensor(80.5500)),\n",
              "  (0.8770702428606599, tensor(80.4300)),\n",
              "  (0.8779281188180211, tensor(80.5100)),\n",
              "  (0.8786796603116269, tensor(80.4600)),\n",
              "  (0.8807729524019815, tensor(80.4500)),\n",
              "  (0.8826815993475827, tensor(80.4500)),\n",
              "  (0.8841471172109665, tensor(80.4400)),\n",
              "  (0.8853064444858306, tensor(80.4900)),\n",
              "  (0.88551423362435, tensor(80.4300)),\n",
              "  (0.8857295708182317, tensor(80.4500)),\n",
              "  (0.8878804279849355, tensor(80.4900)),\n",
              "  (0.8893799124964249, tensor(80.5300)),\n",
              "  (0.8914674008630127, tensor(80.3400)),\n",
              "  (0.8918525810336665, tensor(80.4600)),\n",
              "  (0.8920614806064706, tensor(80.4800)),\n",
              "  (0.8934404857920998, tensor(80.5600)),\n",
              "  (0.8941199964145461, tensor(80.6400)),\n",
              "  (0.8973416711966961, tensor(80.4400)),\n",
              "  (0.8982017007734814, tensor(80.5400)),\n",
              "  (0.8988863628627969, tensor(80.6300)),\n",
              "  (0.9012446887532342, tensor(80.4700)),\n",
              "  (0.9014405130451031, tensor(80.4900))]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    }
  ]
}