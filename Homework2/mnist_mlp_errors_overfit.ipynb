{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "mnist_mlp.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw7SPqRCbLXf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qz6iMBeybLXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.1307,), (0.3081,)),\n",
        "           ])\n",
        "\n",
        "def mnist(batch_size=50, valid=0, shuffle=True, transform=mnist_transform, path='./MNIST_data'):\n",
        "    test_data = datasets.MNIST(path, train=False, download=True, transform=transform)\n",
        "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "    \n",
        "    train_data = datasets.MNIST(path, train=True, download=True, transform=transform)\n",
        "    train_data.data = train_data.data[:1000,:,:]\n",
        "    if valid > 0:\n",
        "        num_train = len(train_data)\n",
        "        indices = list(range(num_train))\n",
        "        split = num_train-valid\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        train_idx, valid_idx = indices[:split], indices[split:]\n",
        "        train_sampler = SubsetRandomSampler(train_idx)\n",
        "        valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "        train_loader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n",
        "        valid_loader = DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler)\n",
        "    \n",
        "        return train_loader, valid_loader, test_loader\n",
        "    else:\n",
        "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=shuffle)\n",
        "        return train_loader, test_loader\n",
        "\n",
        "\n",
        "def plot_mnist(images, shape):\n",
        "    fig = plt.figure(figsize=shape[::-1], dpi=80)\n",
        "    for j in range(1, len(images) + 1):\n",
        "        ax = fig.add_subplot(shape[0], shape[1], j)\n",
        "        ax.matshow(images[j - 1, 0, :, :], cmap = matplotlib.cm.binary)\n",
        "        plt.xticks(np.array([]))\n",
        "        plt.yticks(np.array([]))\n",
        "    plt.show()\n",
        "    \n",
        "def plot_graphs(log, tpe='loss'):\n",
        "    keys = log.keys()\n",
        "    logs = {k:[z for z in zip(*log[k])] for k in keys}\n",
        "    epochs = {k:range(len(log[k])) for k in keys}\n",
        "    \n",
        "    if tpe == 'loss':\n",
        "        handlers, = zip(*[plt.plot(epochs[k], logs[k][0], label=k) for k in keys])\n",
        "        plt.title('errors')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.ylabel('error')\n",
        "        plt.legend(handles=handlers)\n",
        "        plt.show()\n",
        "    elif tpe == 'accuracy':\n",
        "        handlers, = zip(*[plt.plot(epochs[k], logs[k][1], label=k) for k in log.keys()])\n",
        "        plt.title('accuracy')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.ylabel('accuracy')\n",
        "        plt.legend(handles=handlers)\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DhDm90RNw8_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader, test_loader = mnist(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0jvvDrjbLXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, log_softmax=False):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 128)\n",
        "        self.fc2 = nn.Linear(128, 256)\n",
        "        self.fc3 = nn.Linear(256, 512)\n",
        "        self.fc4 = nn.Linear(512, 10)\n",
        "        self.log_softmax = log_softmax\n",
        "        self.optim = optim.SGD(self.parameters(), lr=1e-4)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = torch.sigmoid(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        if self.log_softmax:\n",
        "            x = F.log_softmax(x, dim=1)\n",
        "        else:\n",
        "            x = torch.log(F.softmax(x, dim=1))\n",
        "        return x\n",
        "    \n",
        "    def loss(self, output, target, **kwargs):\n",
        "        self._loss = F.nll_loss(output, target, **kwargs)\n",
        "        return self._loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANxuIpOkbLXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epoch, models):\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        for name, model in models.items():\n",
        "            model.optim.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = model.loss(output, target)\n",
        "            loss.backward()\n",
        "            model.optim.step()\n",
        "            \n",
        "        if batch_idx % 200 == 0:\n",
        "            line = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLosses '.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader))\n",
        "            losses = ' '.join(['{}: {:.6f}'.format(k, m._loss.item()) for k, m in models.items()])\n",
        "            print(line + losses)\n",
        "            \n",
        "    else:\n",
        "        batch_idx += 1\n",
        "        line = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLosses '.format(\n",
        "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "            100. * batch_idx / len(train_loader))\n",
        "        losses = ' '.join(['{}: {:.6f}'.format(k, m._loss.item()) for k, m in models.items()])\n",
        "        print(line + losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbrbWpJNbLXy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models = {'F.softmax': Net(), 'log_softmax': Net(True)}\n",
        "test_log = {k: [] for k in models}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-2EpiW9bLX0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "avg_lambda = lambda l: 'Loss: {:.4f}'.format(l)\n",
        "acc_lambda = lambda c, p: 'Accuracy: {}/{} ({:.0f}%)'.format(c, len(test_loader.dataset), p)\n",
        "line = lambda i, l, c, p: '{}: '.format(i) + avg_lambda(l) + '\\t' + acc_lambda(c, p)\n",
        "\n",
        "def test(models, log=None):\n",
        "    test_size = len(test_loader.sampler)\n",
        "    test_loss = {k: 0. for k in models}\n",
        "    correct = {k: 0. for k in models}\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            for k, m in models.items():\n",
        "                output = m(data)\n",
        "                test_loss[k] += m.loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "                pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "                correct[k] += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "    \n",
        "    for k in models.keys():\n",
        "        test_loss[k] /= test_size\n",
        "    correct_pct = {k: 100. * correct[k] / test_size for k in correct}\n",
        "    lines = '\\n'.join([line(k, test_loss[k], correct[k], correct_pct[k]) for k in models]) + '\\n'\n",
        "    report = 'Test set:\\n' + lines\n",
        "    if log is not None:\n",
        "        for k in models.keys():\n",
        "            log[k].append((test_loss[k], correct_pct[k]))\n",
        "    print(report)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hp2nSaCjbLX2",
        "colab_type": "code",
        "outputId": "6c59ebcd-8e72-45d0-b92f-2d0e4fa81a60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(1, 501):\n",
        "    train(epoch, models)\n",
        "    test(models, test_log)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Train Epoch: 1 [200/1000 (20%)]\tLosses F.softmax: 2.337541 log_softmax: 2.316493\n",
            "Train Epoch: 1 [400/1000 (40%)]\tLosses F.softmax: 2.325866 log_softmax: 2.320865\n",
            "Train Epoch: 1 [600/1000 (60%)]\tLosses F.softmax: 2.331181 log_softmax: 2.306338\n",
            "Train Epoch: 1 [800/1000 (80%)]\tLosses F.softmax: 2.334376 log_softmax: 2.337328\n",
            "Train Epoch: 1 [1000/1000 (100%)]\tLosses F.softmax: 2.221020 log_softmax: 2.328003\n",
            "Test set:\n",
            "F.softmax: Loss: 2.3017\tAccuracy: 954.0/10000 (10%)\n",
            "log_softmax: Loss: 2.3026\tAccuracy: 980.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 2 [0/1000 (0%)]\tLosses F.softmax: 2.230298 log_softmax: 2.323147\n",
            "Train Epoch: 2 [200/1000 (20%)]\tLosses F.softmax: 2.305220 log_softmax: 2.286254\n",
            "Train Epoch: 2 [400/1000 (40%)]\tLosses F.softmax: 2.333586 log_softmax: 2.321257\n",
            "Train Epoch: 2 [600/1000 (60%)]\tLosses F.softmax: 2.267598 log_softmax: 2.306463\n",
            "Train Epoch: 2 [800/1000 (80%)]\tLosses F.softmax: 2.309756 log_softmax: 2.297863\n",
            "Train Epoch: 2 [1000/1000 (100%)]\tLosses F.softmax: 2.312021 log_softmax: 2.315021\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2996\tAccuracy: 922.0/10000 (9%)\n",
            "log_softmax: Loss: 2.3008\tAccuracy: 980.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 3 [0/1000 (0%)]\tLosses F.softmax: 2.324239 log_softmax: 2.297207\n",
            "Train Epoch: 3 [200/1000 (20%)]\tLosses F.softmax: 2.322927 log_softmax: 2.315019\n",
            "Train Epoch: 3 [400/1000 (40%)]\tLosses F.softmax: 2.312798 log_softmax: 2.298451\n",
            "Train Epoch: 3 [600/1000 (60%)]\tLosses F.softmax: 2.308134 log_softmax: 2.293440\n",
            "Train Epoch: 3 [800/1000 (80%)]\tLosses F.softmax: 2.287683 log_softmax: 2.266897\n",
            "Train Epoch: 3 [1000/1000 (100%)]\tLosses F.softmax: 2.301633 log_softmax: 2.354658\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2977\tAccuracy: 856.0/10000 (9%)\n",
            "log_softmax: Loss: 2.2992\tAccuracy: 980.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 4 [0/1000 (0%)]\tLosses F.softmax: 2.300952 log_softmax: 2.303052\n",
            "Train Epoch: 4 [200/1000 (20%)]\tLosses F.softmax: 2.269125 log_softmax: 2.287183\n",
            "Train Epoch: 4 [400/1000 (40%)]\tLosses F.softmax: 2.265100 log_softmax: 2.340850\n",
            "Train Epoch: 4 [600/1000 (60%)]\tLosses F.softmax: 2.348734 log_softmax: 2.356035\n",
            "Train Epoch: 4 [800/1000 (80%)]\tLosses F.softmax: 2.315696 log_softmax: 2.299124\n",
            "Train Epoch: 4 [1000/1000 (100%)]\tLosses F.softmax: 2.264702 log_softmax: 2.259625\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2960\tAccuracy: 960.0/10000 (10%)\n",
            "log_softmax: Loss: 2.2976\tAccuracy: 980.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 5 [0/1000 (0%)]\tLosses F.softmax: 2.321722 log_softmax: 2.179932\n",
            "Train Epoch: 5 [200/1000 (20%)]\tLosses F.softmax: 2.286368 log_softmax: 2.290445\n",
            "Train Epoch: 5 [400/1000 (40%)]\tLosses F.softmax: 2.271893 log_softmax: 2.277807\n",
            "Train Epoch: 5 [600/1000 (60%)]\tLosses F.softmax: 2.338761 log_softmax: 2.338848\n",
            "Train Epoch: 5 [800/1000 (80%)]\tLosses F.softmax: 2.309573 log_softmax: 2.290982\n",
            "Train Epoch: 5 [1000/1000 (100%)]\tLosses F.softmax: 2.312877 log_softmax: 2.339020\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2943\tAccuracy: 1863.0/10000 (19%)\n",
            "log_softmax: Loss: 2.2961\tAccuracy: 980.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 6 [0/1000 (0%)]\tLosses F.softmax: 2.319790 log_softmax: 2.335152\n",
            "Train Epoch: 6 [200/1000 (20%)]\tLosses F.softmax: 2.286731 log_softmax: 2.307475\n",
            "Train Epoch: 6 [400/1000 (40%)]\tLosses F.softmax: 2.312400 log_softmax: 2.349133\n",
            "Train Epoch: 6 [600/1000 (60%)]\tLosses F.softmax: 2.322749 log_softmax: 2.303130\n",
            "Train Epoch: 6 [800/1000 (80%)]\tLosses F.softmax: 2.282326 log_softmax: 2.283964\n",
            "Train Epoch: 6 [1000/1000 (100%)]\tLosses F.softmax: 2.286455 log_softmax: 2.307587\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2928\tAccuracy: 1949.0/10000 (19%)\n",
            "log_softmax: Loss: 2.2947\tAccuracy: 981.0/10000 (10%)\n",
            "\n",
            "Train Epoch: 7 [0/1000 (0%)]\tLosses F.softmax: 2.244828 log_softmax: 2.244709\n",
            "Train Epoch: 7 [200/1000 (20%)]\tLosses F.softmax: 2.241346 log_softmax: 2.254092\n",
            "Train Epoch: 7 [400/1000 (40%)]\tLosses F.softmax: 2.259163 log_softmax: 2.254052\n",
            "Train Epoch: 7 [600/1000 (60%)]\tLosses F.softmax: 2.279235 log_softmax: 2.248107\n",
            "Train Epoch: 7 [800/1000 (80%)]\tLosses F.softmax: 2.298594 log_softmax: 2.199196\n",
            "Train Epoch: 7 [1000/1000 (100%)]\tLosses F.softmax: 2.220808 log_softmax: 2.242824\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2913\tAccuracy: 2002.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2932\tAccuracy: 1128.0/10000 (11%)\n",
            "\n",
            "Train Epoch: 8 [0/1000 (0%)]\tLosses F.softmax: 2.237236 log_softmax: 2.255115\n",
            "Train Epoch: 8 [200/1000 (20%)]\tLosses F.softmax: 2.233254 log_softmax: 2.238804\n",
            "Train Epoch: 8 [400/1000 (40%)]\tLosses F.softmax: 2.222140 log_softmax: 2.237173\n",
            "Train Epoch: 8 [600/1000 (60%)]\tLosses F.softmax: 2.319424 log_softmax: 2.314349\n",
            "Train Epoch: 8 [800/1000 (80%)]\tLosses F.softmax: 2.338212 log_softmax: 2.344726\n",
            "Train Epoch: 8 [1000/1000 (100%)]\tLosses F.softmax: 2.306245 log_softmax: 2.332479\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2899\tAccuracy: 2018.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2918\tAccuracy: 1635.0/10000 (16%)\n",
            "\n",
            "Train Epoch: 9 [0/1000 (0%)]\tLosses F.softmax: 2.321798 log_softmax: 2.315208\n",
            "Train Epoch: 9 [200/1000 (20%)]\tLosses F.softmax: 2.283374 log_softmax: 2.242404\n",
            "Train Epoch: 9 [400/1000 (40%)]\tLosses F.softmax: 2.295084 log_softmax: 2.346658\n",
            "Train Epoch: 9 [600/1000 (60%)]\tLosses F.softmax: 2.333166 log_softmax: 2.348396\n",
            "Train Epoch: 9 [800/1000 (80%)]\tLosses F.softmax: 2.359237 log_softmax: 2.320043\n",
            "Train Epoch: 9 [1000/1000 (100%)]\tLosses F.softmax: 2.316081 log_softmax: 2.347749\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2886\tAccuracy: 2018.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2904\tAccuracy: 2002.0/10000 (20%)\n",
            "\n",
            "Train Epoch: 10 [0/1000 (0%)]\tLosses F.softmax: 2.313888 log_softmax: 2.317897\n",
            "Train Epoch: 10 [200/1000 (20%)]\tLosses F.softmax: 2.313447 log_softmax: 2.305050\n",
            "Train Epoch: 10 [400/1000 (40%)]\tLosses F.softmax: 2.306179 log_softmax: 2.355866\n",
            "Train Epoch: 10 [600/1000 (60%)]\tLosses F.softmax: 2.289397 log_softmax: 2.294707\n",
            "Train Epoch: 10 [800/1000 (80%)]\tLosses F.softmax: 2.289475 log_softmax: 2.294935\n",
            "Train Epoch: 10 [1000/1000 (100%)]\tLosses F.softmax: 2.285074 log_softmax: 2.216894\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2872\tAccuracy: 2029.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2890\tAccuracy: 2100.0/10000 (21%)\n",
            "\n",
            "Train Epoch: 11 [0/1000 (0%)]\tLosses F.softmax: 2.316231 log_softmax: 2.321638\n",
            "Train Epoch: 11 [200/1000 (20%)]\tLosses F.softmax: 2.316224 log_softmax: 2.326501\n",
            "Train Epoch: 11 [400/1000 (40%)]\tLosses F.softmax: 2.290734 log_softmax: 2.281963\n",
            "Train Epoch: 11 [600/1000 (60%)]\tLosses F.softmax: 2.288782 log_softmax: 2.321621\n",
            "Train Epoch: 11 [800/1000 (80%)]\tLosses F.softmax: 2.294694 log_softmax: 2.304507\n",
            "Train Epoch: 11 [1000/1000 (100%)]\tLosses F.softmax: 2.311815 log_softmax: 2.319715\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2859\tAccuracy: 2037.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2876\tAccuracy: 2098.0/10000 (21%)\n",
            "\n",
            "Train Epoch: 12 [0/1000 (0%)]\tLosses F.softmax: 2.289872 log_softmax: 2.311625\n",
            "Train Epoch: 12 [200/1000 (20%)]\tLosses F.softmax: 2.190990 log_softmax: 2.199749\n",
            "Train Epoch: 12 [400/1000 (40%)]\tLosses F.softmax: 2.188420 log_softmax: 2.209657\n",
            "Train Epoch: 12 [600/1000 (60%)]\tLosses F.softmax: 2.190186 log_softmax: 2.216059\n",
            "Train Epoch: 12 [800/1000 (80%)]\tLosses F.softmax: 2.293086 log_softmax: 2.230541\n",
            "Train Epoch: 12 [1000/1000 (100%)]\tLosses F.softmax: 2.213457 log_softmax: 2.215805\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2846\tAccuracy: 2043.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2862\tAccuracy: 2093.0/10000 (21%)\n",
            "\n",
            "Train Epoch: 13 [0/1000 (0%)]\tLosses F.softmax: 2.185646 log_softmax: 2.201679\n",
            "Train Epoch: 13 [200/1000 (20%)]\tLosses F.softmax: 2.307943 log_softmax: 2.309997\n",
            "Train Epoch: 13 [400/1000 (40%)]\tLosses F.softmax: 2.255810 log_softmax: 2.238168\n",
            "Train Epoch: 13 [600/1000 (60%)]\tLosses F.softmax: 2.343829 log_softmax: 2.353015\n",
            "Train Epoch: 13 [800/1000 (80%)]\tLosses F.softmax: 2.168421 log_softmax: 2.186631\n",
            "Train Epoch: 13 [1000/1000 (100%)]\tLosses F.softmax: 2.334788 log_softmax: 2.356823\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2832\tAccuracy: 2041.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2847\tAccuracy: 2067.0/10000 (21%)\n",
            "\n",
            "Train Epoch: 14 [0/1000 (0%)]\tLosses F.softmax: 2.169513 log_softmax: 2.188013\n",
            "Train Epoch: 14 [200/1000 (20%)]\tLosses F.softmax: 2.330951 log_softmax: 2.314710\n",
            "Train Epoch: 14 [400/1000 (40%)]\tLosses F.softmax: 2.294975 log_softmax: 2.294016\n",
            "Train Epoch: 14 [600/1000 (60%)]\tLosses F.softmax: 2.339353 log_softmax: 2.369515\n",
            "Train Epoch: 14 [800/1000 (80%)]\tLosses F.softmax: 2.287638 log_softmax: 2.251216\n",
            "Train Epoch: 14 [1000/1000 (100%)]\tLosses F.softmax: 2.346397 log_softmax: 2.372558\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2818\tAccuracy: 2042.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2832\tAccuracy: 2060.0/10000 (21%)\n",
            "\n",
            "Train Epoch: 15 [0/1000 (0%)]\tLosses F.softmax: 2.188647 log_softmax: 2.199995\n",
            "Train Epoch: 15 [200/1000 (20%)]\tLosses F.softmax: 2.178877 log_softmax: 2.197484\n",
            "Train Epoch: 15 [400/1000 (40%)]\tLosses F.softmax: 2.360480 log_softmax: 2.346761\n",
            "Train Epoch: 15 [600/1000 (60%)]\tLosses F.softmax: 2.178843 log_softmax: 2.198410\n",
            "Train Epoch: 15 [800/1000 (80%)]\tLosses F.softmax: 2.351927 log_softmax: 2.362417\n",
            "Train Epoch: 15 [1000/1000 (100%)]\tLosses F.softmax: 2.186776 log_softmax: 2.192698\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2804\tAccuracy: 2044.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2816\tAccuracy: 2056.0/10000 (21%)\n",
            "\n",
            "Train Epoch: 16 [0/1000 (0%)]\tLosses F.softmax: 2.190572 log_softmax: 2.190361\n",
            "Train Epoch: 16 [200/1000 (20%)]\tLosses F.softmax: 2.164783 log_softmax: 2.175531\n",
            "Train Epoch: 16 [400/1000 (40%)]\tLosses F.softmax: 2.337542 log_softmax: 2.371556\n",
            "Train Epoch: 16 [600/1000 (60%)]\tLosses F.softmax: 2.267648 log_softmax: 2.220416\n",
            "Train Epoch: 16 [800/1000 (80%)]\tLosses F.softmax: 2.164315 log_softmax: 2.181088\n",
            "Train Epoch: 16 [1000/1000 (100%)]\tLosses F.softmax: 2.304132 log_softmax: 2.296243\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2790\tAccuracy: 2048.0/10000 (20%)\n",
            "log_softmax: Loss: 2.2800\tAccuracy: 2048.0/10000 (20%)\n",
            "\n",
            "Train Epoch: 17 [0/1000 (0%)]\tLosses F.softmax: 2.151387 log_softmax: 2.173496\n",
            "Train Epoch: 17 [200/1000 (20%)]\tLosses F.softmax: 2.263807 log_softmax: 2.256598\n",
            "Train Epoch: 17 [400/1000 (40%)]\tLosses F.softmax: 2.148947 log_softmax: 2.178169\n",
            "Train Epoch: 17 [600/1000 (60%)]\tLosses F.softmax: 2.195761 log_softmax: 2.183733\n",
            "Train Epoch: 17 [800/1000 (80%)]\tLosses F.softmax: 2.367646 log_softmax: 2.398553\n",
            "Train Epoch: 17 [1000/1000 (100%)]\tLosses F.softmax: 2.369729 log_softmax: 2.329870\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2775\tAccuracy: 2052.0/10000 (21%)\n",
            "log_softmax: Loss: 2.2783\tAccuracy: 2036.0/10000 (20%)\n",
            "\n",
            "Train Epoch: 18 [0/1000 (0%)]\tLosses F.softmax: 2.158154 log_softmax: 2.172563\n",
            "Train Epoch: 18 [200/1000 (20%)]\tLosses F.softmax: 2.347063 log_softmax: 2.397639\n",
            "Train Epoch: 18 [400/1000 (40%)]\tLosses F.softmax: 2.293190 log_softmax: 2.299763\n",
            "Train Epoch: 18 [600/1000 (60%)]\tLosses F.softmax: 2.240883 log_softmax: 2.228212\n",
            "Train Epoch: 18 [800/1000 (80%)]\tLosses F.softmax: 2.137283 log_softmax: 2.155842\n",
            "Train Epoch: 18 [1000/1000 (100%)]\tLosses F.softmax: 2.231980 log_softmax: 2.217330\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2760\tAccuracy: 2053.0/10000 (21%)\n",
            "log_softmax: Loss: 2.2766\tAccuracy: 2034.0/10000 (20%)\n",
            "\n",
            "Train Epoch: 19 [0/1000 (0%)]\tLosses F.softmax: 2.372044 log_softmax: 2.350484\n",
            "Train Epoch: 19 [200/1000 (20%)]\tLosses F.softmax: 2.124469 log_softmax: 2.158359\n",
            "Train Epoch: 19 [400/1000 (40%)]\tLosses F.softmax: 2.319170 log_softmax: 2.324861\n",
            "Train Epoch: 19 [600/1000 (60%)]\tLosses F.softmax: 2.352624 log_softmax: 2.367779\n",
            "Train Epoch: 19 [800/1000 (80%)]\tLosses F.softmax: 2.295766 log_softmax: 2.283395\n",
            "Train Epoch: 19 [1000/1000 (100%)]\tLosses F.softmax: 2.229714 log_softmax: 2.223638\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2744\tAccuracy: 2057.0/10000 (21%)\n",
            "log_softmax: Loss: 2.2748\tAccuracy: 2024.0/10000 (20%)\n",
            "\n",
            "Train Epoch: 20 [0/1000 (0%)]\tLosses F.softmax: 2.132913 log_softmax: 2.167367\n",
            "Train Epoch: 20 [200/1000 (20%)]\tLosses F.softmax: 2.222241 log_softmax: 2.216137\n",
            "Train Epoch: 20 [400/1000 (40%)]\tLosses F.softmax: 2.286935 log_softmax: 2.282954\n",
            "Train Epoch: 20 [600/1000 (60%)]\tLosses F.softmax: 2.274866 log_softmax: 2.232108\n",
            "Train Epoch: 20 [800/1000 (80%)]\tLosses F.softmax: 2.290657 log_softmax: 2.295598\n",
            "Train Epoch: 20 [1000/1000 (100%)]\tLosses F.softmax: 2.294134 log_softmax: 2.277375\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2727\tAccuracy: 2057.0/10000 (21%)\n",
            "log_softmax: Loss: 2.2729\tAccuracy: 2017.0/10000 (20%)\n",
            "\n",
            "Train Epoch: 21 [0/1000 (0%)]\tLosses F.softmax: 2.317420 log_softmax: 2.304624\n",
            "Train Epoch: 21 [200/1000 (20%)]\tLosses F.softmax: 2.309972 log_softmax: 2.307337\n",
            "Train Epoch: 21 [400/1000 (40%)]\tLosses F.softmax: 2.378659 log_softmax: 2.344413\n",
            "Train Epoch: 21 [600/1000 (60%)]\tLosses F.softmax: 2.373624 log_softmax: 2.321375\n",
            "Train Epoch: 21 [800/1000 (80%)]\tLosses F.softmax: 2.124129 log_softmax: 2.134798\n",
            "Train Epoch: 21 [1000/1000 (100%)]\tLosses F.softmax: 2.388077 log_softmax: 2.386799\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2710\tAccuracy: 2056.0/10000 (21%)\n",
            "log_softmax: Loss: 2.2710\tAccuracy: 2007.0/10000 (20%)\n",
            "\n",
            "Train Epoch: 22 [0/1000 (0%)]\tLosses F.softmax: 2.339925 log_softmax: 2.363132\n",
            "Train Epoch: 22 [200/1000 (20%)]\tLosses F.softmax: 2.363736 log_softmax: 2.370672\n",
            "Train Epoch: 22 [400/1000 (40%)]\tLosses F.softmax: 2.282192 log_softmax: 2.260169\n",
            "Train Epoch: 22 [600/1000 (60%)]\tLosses F.softmax: 2.128212 log_softmax: 2.152274\n",
            "Train Epoch: 22 [800/1000 (80%)]\tLosses F.softmax: 2.364207 log_softmax: 2.378099\n",
            "Train Epoch: 22 [1000/1000 (100%)]\tLosses F.softmax: 2.207584 log_softmax: 2.194005\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2692\tAccuracy: 2056.0/10000 (21%)\n",
            "log_softmax: Loss: 2.2689\tAccuracy: 2003.0/10000 (20%)\n",
            "\n",
            "Train Epoch: 23 [0/1000 (0%)]\tLosses F.softmax: 2.272407 log_softmax: 2.277455\n",
            "Train Epoch: 23 [200/1000 (20%)]\tLosses F.softmax: 2.263006 log_softmax: 2.275266\n",
            "Train Epoch: 23 [400/1000 (40%)]\tLosses F.softmax: 2.355162 log_softmax: 2.341493\n",
            "Train Epoch: 23 [600/1000 (60%)]\tLosses F.softmax: 2.370926 log_softmax: 2.372550\n",
            "Train Epoch: 23 [800/1000 (80%)]\tLosses F.softmax: 2.297955 log_softmax: 2.275039\n",
            "Train Epoch: 23 [1000/1000 (100%)]\tLosses F.softmax: 2.088544 log_softmax: 2.120293\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2673\tAccuracy: 2056.0/10000 (21%)\n",
            "log_softmax: Loss: 2.2667\tAccuracy: 1999.0/10000 (20%)\n",
            "\n",
            "Train Epoch: 24 [0/1000 (0%)]\tLosses F.softmax: 2.092859 log_softmax: 2.122556\n",
            "Train Epoch: 24 [200/1000 (20%)]\tLosses F.softmax: 2.304481 log_softmax: 2.272194\n",
            "Train Epoch: 24 [400/1000 (40%)]\tLosses F.softmax: 2.257024 log_softmax: 2.247700\n",
            "Train Epoch: 24 [600/1000 (60%)]\tLosses F.softmax: 2.395644 log_softmax: 2.341704\n",
            "Train Epoch: 24 [800/1000 (80%)]\tLosses F.softmax: 2.376191 log_softmax: 2.337602\n",
            "Train Epoch: 24 [1000/1000 (100%)]\tLosses F.softmax: 2.169895 log_softmax: 2.159715\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2654\tAccuracy: 2057.0/10000 (21%)\n",
            "log_softmax: Loss: 2.2644\tAccuracy: 2004.0/10000 (20%)\n",
            "\n",
            "Train Epoch: 25 [0/1000 (0%)]\tLosses F.softmax: 2.371387 log_softmax: 2.333394\n",
            "Train Epoch: 25 [200/1000 (20%)]\tLosses F.softmax: 2.211885 log_softmax: 2.211221\n",
            "Train Epoch: 25 [400/1000 (40%)]\tLosses F.softmax: 2.255935 log_softmax: 2.260300\n",
            "Train Epoch: 25 [600/1000 (60%)]\tLosses F.softmax: 2.281429 log_softmax: 2.272370\n",
            "Train Epoch: 25 [800/1000 (80%)]\tLosses F.softmax: 2.242102 log_softmax: 2.237294\n",
            "Train Epoch: 25 [1000/1000 (100%)]\tLosses F.softmax: 2.345159 log_softmax: 2.363742\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2633\tAccuracy: 2056.0/10000 (21%)\n",
            "log_softmax: Loss: 2.2620\tAccuracy: 2013.0/10000 (20%)\n",
            "\n",
            "Train Epoch: 26 [0/1000 (0%)]\tLosses F.softmax: 2.288702 log_softmax: 2.270815\n",
            "Train Epoch: 26 [200/1000 (20%)]\tLosses F.softmax: 2.293921 log_softmax: 2.288117\n",
            "Train Epoch: 26 [400/1000 (40%)]\tLosses F.softmax: 2.253394 log_softmax: 2.203047\n",
            "Train Epoch: 26 [600/1000 (60%)]\tLosses F.softmax: 2.384540 log_softmax: 2.364096\n",
            "Train Epoch: 26 [800/1000 (80%)]\tLosses F.softmax: 2.385616 log_softmax: 2.387446\n",
            "Train Epoch: 26 [1000/1000 (100%)]\tLosses F.softmax: 2.059586 log_softmax: 2.085456\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2612\tAccuracy: 2057.0/10000 (21%)\n",
            "log_softmax: Loss: 2.2595\tAccuracy: 2027.0/10000 (20%)\n",
            "\n",
            "Train Epoch: 27 [0/1000 (0%)]\tLosses F.softmax: 2.283505 log_softmax: 2.244295\n",
            "Train Epoch: 27 [200/1000 (20%)]\tLosses F.softmax: 2.092595 log_softmax: 2.091106\n",
            "Train Epoch: 27 [400/1000 (40%)]\tLosses F.softmax: 2.165108 log_softmax: 2.167886\n",
            "Train Epoch: 27 [600/1000 (60%)]\tLosses F.softmax: 2.192939 log_softmax: 2.149930\n",
            "Train Epoch: 27 [800/1000 (80%)]\tLosses F.softmax: 2.370197 log_softmax: 2.370948\n",
            "Train Epoch: 27 [1000/1000 (100%)]\tLosses F.softmax: 2.296979 log_softmax: 2.270728\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2589\tAccuracy: 2059.0/10000 (21%)\n",
            "log_softmax: Loss: 2.2569\tAccuracy: 2048.0/10000 (20%)\n",
            "\n",
            "Train Epoch: 28 [0/1000 (0%)]\tLosses F.softmax: 2.278973 log_softmax: 2.232124\n",
            "Train Epoch: 28 [200/1000 (20%)]\tLosses F.softmax: 2.386170 log_softmax: 2.383792\n",
            "Train Epoch: 28 [400/1000 (40%)]\tLosses F.softmax: 2.305700 log_softmax: 2.286266\n",
            "Train Epoch: 28 [600/1000 (60%)]\tLosses F.softmax: 2.079298 log_softmax: 2.118389\n",
            "Train Epoch: 28 [800/1000 (80%)]\tLosses F.softmax: 2.379449 log_softmax: 2.389879\n",
            "Train Epoch: 28 [1000/1000 (100%)]\tLosses F.softmax: 2.362364 log_softmax: 2.380605\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2565\tAccuracy: 2060.0/10000 (21%)\n",
            "log_softmax: Loss: 2.2542\tAccuracy: 2074.0/10000 (21%)\n",
            "\n",
            "Train Epoch: 29 [0/1000 (0%)]\tLosses F.softmax: 2.037937 log_softmax: 2.063682\n",
            "Train Epoch: 29 [200/1000 (20%)]\tLosses F.softmax: 2.069546 log_softmax: 2.126100\n",
            "Train Epoch: 29 [400/1000 (40%)]\tLosses F.softmax: 2.361654 log_softmax: 2.365486\n",
            "Train Epoch: 29 [600/1000 (60%)]\tLosses F.softmax: 2.378070 log_softmax: 2.362519\n",
            "Train Epoch: 29 [800/1000 (80%)]\tLosses F.softmax: 2.037956 log_softmax: 2.062151\n",
            "Train Epoch: 29 [1000/1000 (100%)]\tLosses F.softmax: 2.191765 log_softmax: 2.184828\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2540\tAccuracy: 2066.0/10000 (21%)\n",
            "log_softmax: Loss: 2.2513\tAccuracy: 2112.0/10000 (21%)\n",
            "\n",
            "Train Epoch: 30 [0/1000 (0%)]\tLosses F.softmax: 2.345250 log_softmax: 2.277359\n",
            "Train Epoch: 30 [200/1000 (20%)]\tLosses F.softmax: 2.281302 log_softmax: 2.294350\n",
            "Train Epoch: 30 [400/1000 (40%)]\tLosses F.softmax: 2.262240 log_softmax: 2.246001\n",
            "Train Epoch: 30 [600/1000 (60%)]\tLosses F.softmax: 2.047982 log_softmax: 2.106124\n",
            "Train Epoch: 30 [800/1000 (80%)]\tLosses F.softmax: 2.068833 log_softmax: 2.075355\n",
            "Train Epoch: 30 [1000/1000 (100%)]\tLosses F.softmax: 2.242394 log_softmax: 2.237773\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2514\tAccuracy: 2075.0/10000 (21%)\n",
            "log_softmax: Loss: 2.2483\tAccuracy: 2155.0/10000 (22%)\n",
            "\n",
            "Train Epoch: 31 [0/1000 (0%)]\tLosses F.softmax: 2.338297 log_softmax: 2.271638\n",
            "Train Epoch: 31 [200/1000 (20%)]\tLosses F.softmax: 2.338898 log_softmax: 2.283043\n",
            "Train Epoch: 31 [400/1000 (40%)]\tLosses F.softmax: 2.058548 log_softmax: 2.076639\n",
            "Train Epoch: 31 [600/1000 (60%)]\tLosses F.softmax: 2.015746 log_softmax: 2.050155\n",
            "Train Epoch: 31 [800/1000 (80%)]\tLosses F.softmax: 2.275223 log_softmax: 2.274003\n",
            "Train Epoch: 31 [1000/1000 (100%)]\tLosses F.softmax: 2.248700 log_softmax: 2.243224\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2486\tAccuracy: 2088.0/10000 (21%)\n",
            "log_softmax: Loss: 2.2451\tAccuracy: 2219.0/10000 (22%)\n",
            "\n",
            "Train Epoch: 32 [0/1000 (0%)]\tLosses F.softmax: 2.287183 log_softmax: 2.306075\n",
            "Train Epoch: 32 [200/1000 (20%)]\tLosses F.softmax: 2.002080 log_softmax: 2.036501\n",
            "Train Epoch: 32 [400/1000 (40%)]\tLosses F.softmax: 2.271207 log_softmax: 2.233285\n",
            "Train Epoch: 32 [600/1000 (60%)]\tLosses F.softmax: 2.084226 log_softmax: 2.102129\n",
            "Train Epoch: 32 [800/1000 (80%)]\tLosses F.softmax: 2.187615 log_softmax: 2.183320\n",
            "Train Epoch: 32 [1000/1000 (100%)]\tLosses F.softmax: 2.246858 log_softmax: 2.249233\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2456\tAccuracy: 2117.0/10000 (21%)\n",
            "log_softmax: Loss: 2.2418\tAccuracy: 2267.0/10000 (23%)\n",
            "\n",
            "Train Epoch: 33 [0/1000 (0%)]\tLosses F.softmax: 2.399653 log_softmax: 2.409874\n",
            "Train Epoch: 33 [200/1000 (20%)]\tLosses F.softmax: 2.333001 log_softmax: 2.294590\n",
            "Train Epoch: 33 [400/1000 (40%)]\tLosses F.softmax: 2.331310 log_softmax: 2.340639\n",
            "Train Epoch: 33 [600/1000 (60%)]\tLosses F.softmax: 2.052088 log_softmax: 2.093731\n",
            "Train Epoch: 33 [800/1000 (80%)]\tLosses F.softmax: 2.363875 log_softmax: 2.371182\n",
            "Train Epoch: 33 [1000/1000 (100%)]\tLosses F.softmax: 2.017294 log_softmax: 2.046748\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2425\tAccuracy: 2152.0/10000 (22%)\n",
            "log_softmax: Loss: 2.2383\tAccuracy: 2329.0/10000 (23%)\n",
            "\n",
            "Train Epoch: 34 [0/1000 (0%)]\tLosses F.softmax: 2.055554 log_softmax: 2.079208\n",
            "Train Epoch: 34 [200/1000 (20%)]\tLosses F.softmax: 2.022681 log_softmax: 2.046587\n",
            "Train Epoch: 34 [400/1000 (40%)]\tLosses F.softmax: 2.306317 log_softmax: 2.270909\n",
            "Train Epoch: 34 [600/1000 (60%)]\tLosses F.softmax: 2.081882 log_softmax: 2.111513\n",
            "Train Epoch: 34 [800/1000 (80%)]\tLosses F.softmax: 2.351612 log_softmax: 2.358167\n",
            "Train Epoch: 34 [1000/1000 (100%)]\tLosses F.softmax: 2.248462 log_softmax: 2.295396\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2392\tAccuracy: 2191.0/10000 (22%)\n",
            "log_softmax: Loss: 2.2347\tAccuracy: 2382.0/10000 (24%)\n",
            "\n",
            "Train Epoch: 35 [0/1000 (0%)]\tLosses F.softmax: 2.379574 log_softmax: 2.375003\n",
            "Train Epoch: 35 [200/1000 (20%)]\tLosses F.softmax: 2.387366 log_softmax: 2.397293\n",
            "Train Epoch: 35 [400/1000 (40%)]\tLosses F.softmax: 2.062990 log_softmax: 2.106900\n",
            "Train Epoch: 35 [600/1000 (60%)]\tLosses F.softmax: 2.203982 log_softmax: 2.182428\n",
            "Train Epoch: 35 [800/1000 (80%)]\tLosses F.softmax: 2.267364 log_softmax: 2.209107\n",
            "Train Epoch: 35 [1000/1000 (100%)]\tLosses F.softmax: 2.276090 log_softmax: 2.260752\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2357\tAccuracy: 2236.0/10000 (22%)\n",
            "log_softmax: Loss: 2.2308\tAccuracy: 2443.0/10000 (24%)\n",
            "\n",
            "Train Epoch: 36 [0/1000 (0%)]\tLosses F.softmax: 2.359657 log_softmax: 2.357124\n",
            "Train Epoch: 36 [200/1000 (20%)]\tLosses F.softmax: 2.257545 log_softmax: 2.231409\n",
            "Train Epoch: 36 [400/1000 (40%)]\tLosses F.softmax: 2.357418 log_softmax: 2.368336\n",
            "Train Epoch: 36 [600/1000 (60%)]\tLosses F.softmax: 2.155273 log_softmax: 2.129789\n",
            "Train Epoch: 36 [800/1000 (80%)]\tLosses F.softmax: 2.241924 log_softmax: 2.202217\n",
            "Train Epoch: 36 [1000/1000 (100%)]\tLosses F.softmax: 2.210173 log_softmax: 2.188105\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2320\tAccuracy: 2290.0/10000 (23%)\n",
            "log_softmax: Loss: 2.2267\tAccuracy: 2487.0/10000 (25%)\n",
            "\n",
            "Train Epoch: 37 [0/1000 (0%)]\tLosses F.softmax: 2.216924 log_softmax: 2.210818\n",
            "Train Epoch: 37 [200/1000 (20%)]\tLosses F.softmax: 2.364215 log_softmax: 2.365221\n",
            "Train Epoch: 37 [400/1000 (40%)]\tLosses F.softmax: 2.297851 log_softmax: 2.285109\n",
            "Train Epoch: 37 [600/1000 (60%)]\tLosses F.softmax: 2.209626 log_softmax: 2.175426\n",
            "Train Epoch: 37 [800/1000 (80%)]\tLosses F.softmax: 2.074261 log_softmax: 2.101398\n",
            "Train Epoch: 37 [1000/1000 (100%)]\tLosses F.softmax: 1.949996 log_softmax: 1.986094\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2281\tAccuracy: 2352.0/10000 (24%)\n",
            "log_softmax: Loss: 2.2224\tAccuracy: 2539.0/10000 (25%)\n",
            "\n",
            "Train Epoch: 38 [0/1000 (0%)]\tLosses F.softmax: 2.266706 log_softmax: 2.258929\n",
            "Train Epoch: 38 [200/1000 (20%)]\tLosses F.softmax: 1.932396 log_softmax: 1.975754\n",
            "Train Epoch: 38 [400/1000 (40%)]\tLosses F.softmax: 2.137607 log_softmax: 2.089444\n",
            "Train Epoch: 38 [600/1000 (60%)]\tLosses F.softmax: 2.380655 log_softmax: 2.404607\n",
            "Train Epoch: 38 [800/1000 (80%)]\tLosses F.softmax: 2.072712 log_softmax: 2.096553\n",
            "Train Epoch: 38 [1000/1000 (100%)]\tLosses F.softmax: 1.916378 log_softmax: 1.964413\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2239\tAccuracy: 2430.0/10000 (24%)\n",
            "log_softmax: Loss: 2.2178\tAccuracy: 2606.0/10000 (26%)\n",
            "\n",
            "Train Epoch: 39 [0/1000 (0%)]\tLosses F.softmax: 2.322100 log_softmax: 2.327736\n",
            "Train Epoch: 39 [200/1000 (20%)]\tLosses F.softmax: 1.975757 log_softmax: 2.047932\n",
            "Train Epoch: 39 [400/1000 (40%)]\tLosses F.softmax: 2.246233 log_softmax: 2.213497\n",
            "Train Epoch: 39 [600/1000 (60%)]\tLosses F.softmax: 2.416654 log_softmax: 2.335790\n",
            "Train Epoch: 39 [800/1000 (80%)]\tLosses F.softmax: 2.303754 log_softmax: 2.268971\n",
            "Train Epoch: 39 [1000/1000 (100%)]\tLosses F.softmax: 2.245409 log_softmax: 2.225821\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2196\tAccuracy: 2512.0/10000 (25%)\n",
            "log_softmax: Loss: 2.2130\tAccuracy: 2698.0/10000 (27%)\n",
            "\n",
            "Train Epoch: 40 [0/1000 (0%)]\tLosses F.softmax: 2.368426 log_softmax: 2.304437\n",
            "Train Epoch: 40 [200/1000 (20%)]\tLosses F.softmax: 2.047607 log_softmax: 2.069422\n",
            "Train Epoch: 40 [400/1000 (40%)]\tLosses F.softmax: 2.321529 log_softmax: 2.337046\n",
            "Train Epoch: 40 [600/1000 (60%)]\tLosses F.softmax: 2.398275 log_softmax: 2.359060\n",
            "Train Epoch: 40 [800/1000 (80%)]\tLosses F.softmax: 2.284823 log_softmax: 2.197962\n",
            "Train Epoch: 40 [1000/1000 (100%)]\tLosses F.softmax: 2.238171 log_softmax: 2.175401\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2149\tAccuracy: 2586.0/10000 (26%)\n",
            "log_softmax: Loss: 2.2079\tAccuracy: 2791.0/10000 (28%)\n",
            "\n",
            "Train Epoch: 41 [0/1000 (0%)]\tLosses F.softmax: 2.323369 log_softmax: 2.304118\n",
            "Train Epoch: 41 [200/1000 (20%)]\tLosses F.softmax: 1.927413 log_softmax: 1.976088\n",
            "Train Epoch: 41 [400/1000 (40%)]\tLosses F.softmax: 2.146043 log_softmax: 2.084417\n",
            "Train Epoch: 41 [600/1000 (60%)]\tLosses F.softmax: 2.239125 log_softmax: 2.228169\n",
            "Train Epoch: 41 [800/1000 (80%)]\tLosses F.softmax: 2.229666 log_softmax: 2.181259\n",
            "Train Epoch: 41 [1000/1000 (100%)]\tLosses F.softmax: 2.180990 log_softmax: 2.151931\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2100\tAccuracy: 2673.0/10000 (27%)\n",
            "log_softmax: Loss: 2.2024\tAccuracy: 2874.0/10000 (29%)\n",
            "\n",
            "Train Epoch: 42 [0/1000 (0%)]\tLosses F.softmax: 2.302333 log_softmax: 2.286109\n",
            "Train Epoch: 42 [200/1000 (20%)]\tLosses F.softmax: 2.055452 log_softmax: 2.070569\n",
            "Train Epoch: 42 [400/1000 (40%)]\tLosses F.softmax: 2.377340 log_softmax: 2.420958\n",
            "Train Epoch: 42 [600/1000 (60%)]\tLosses F.softmax: 2.218638 log_softmax: 2.183062\n",
            "Train Epoch: 42 [800/1000 (80%)]\tLosses F.softmax: 2.118563 log_softmax: 2.071168\n",
            "Train Epoch: 42 [1000/1000 (100%)]\tLosses F.softmax: 1.888528 log_softmax: 1.937371\n",
            "Test set:\n",
            "F.softmax: Loss: 2.2049\tAccuracy: 2749.0/10000 (27%)\n",
            "log_softmax: Loss: 2.1966\tAccuracy: 2958.0/10000 (30%)\n",
            "\n",
            "Train Epoch: 43 [0/1000 (0%)]\tLosses F.softmax: 2.208169 log_softmax: 2.227023\n",
            "Train Epoch: 43 [200/1000 (20%)]\tLosses F.softmax: 2.096440 log_softmax: 2.111680\n",
            "Train Epoch: 43 [400/1000 (40%)]\tLosses F.softmax: 2.356325 log_softmax: 2.360717\n",
            "Train Epoch: 43 [600/1000 (60%)]\tLosses F.softmax: 1.988843 log_softmax: 2.045375\n",
            "Train Epoch: 43 [800/1000 (80%)]\tLosses F.softmax: 2.176394 log_softmax: 2.142451\n",
            "Train Epoch: 43 [1000/1000 (100%)]\tLosses F.softmax: 2.287896 log_softmax: 2.167181\n",
            "Test set:\n",
            "F.softmax: Loss: 2.1994\tAccuracy: 2830.0/10000 (28%)\n",
            "log_softmax: Loss: 2.1905\tAccuracy: 3046.0/10000 (30%)\n",
            "\n",
            "Train Epoch: 44 [0/1000 (0%)]\tLosses F.softmax: 2.223157 log_softmax: 2.189262\n",
            "Train Epoch: 44 [200/1000 (20%)]\tLosses F.softmax: 2.259521 log_softmax: 2.261431\n",
            "Train Epoch: 44 [400/1000 (40%)]\tLosses F.softmax: 2.309834 log_softmax: 2.235922\n",
            "Train Epoch: 44 [600/1000 (60%)]\tLosses F.softmax: 2.350023 log_softmax: 2.363975\n",
            "Train Epoch: 44 [800/1000 (80%)]\tLosses F.softmax: 2.374827 log_softmax: 2.378363\n",
            "Train Epoch: 44 [1000/1000 (100%)]\tLosses F.softmax: 1.888299 log_softmax: 1.932392\n",
            "Test set:\n",
            "F.softmax: Loss: 2.1936\tAccuracy: 2918.0/10000 (29%)\n",
            "log_softmax: Loss: 2.1839\tAccuracy: 3147.0/10000 (31%)\n",
            "\n",
            "Train Epoch: 45 [0/1000 (0%)]\tLosses F.softmax: 2.126440 log_softmax: 2.080007\n",
            "Train Epoch: 45 [200/1000 (20%)]\tLosses F.softmax: 1.827370 log_softmax: 1.898839\n",
            "Train Epoch: 45 [400/1000 (40%)]\tLosses F.softmax: 2.349100 log_softmax: 2.345094\n",
            "Train Epoch: 45 [600/1000 (60%)]\tLosses F.softmax: 2.292703 log_softmax: 2.173372\n",
            "Train Epoch: 45 [800/1000 (80%)]\tLosses F.softmax: 2.077897 log_softmax: 2.080699\n",
            "Train Epoch: 45 [1000/1000 (100%)]\tLosses F.softmax: 2.036156 log_softmax: 2.060671\n",
            "Test set:\n",
            "F.softmax: Loss: 2.1874\tAccuracy: 3017.0/10000 (30%)\n",
            "log_softmax: Loss: 2.1770\tAccuracy: 3250.0/10000 (32%)\n",
            "\n",
            "Train Epoch: 46 [0/1000 (0%)]\tLosses F.softmax: 2.296263 log_softmax: 2.183126\n",
            "Train Epoch: 46 [200/1000 (20%)]\tLosses F.softmax: 2.237356 log_softmax: 2.136853\n",
            "Train Epoch: 46 [400/1000 (40%)]\tLosses F.softmax: 2.389406 log_softmax: 2.345212\n",
            "Train Epoch: 46 [600/1000 (60%)]\tLosses F.softmax: 2.366669 log_softmax: 2.281811\n",
            "Train Epoch: 46 [800/1000 (80%)]\tLosses F.softmax: 2.332730 log_softmax: 2.226588\n",
            "Train Epoch: 46 [1000/1000 (100%)]\tLosses F.softmax: 2.061638 log_softmax: 2.091387\n",
            "Test set:\n",
            "F.softmax: Loss: 2.1809\tAccuracy: 3094.0/10000 (31%)\n",
            "log_softmax: Loss: 2.1697\tAccuracy: 3365.0/10000 (34%)\n",
            "\n",
            "Train Epoch: 47 [0/1000 (0%)]\tLosses F.softmax: 2.318625 log_softmax: 2.309705\n",
            "Train Epoch: 47 [200/1000 (20%)]\tLosses F.softmax: 2.137023 log_softmax: 2.112945\n",
            "Train Epoch: 47 [400/1000 (40%)]\tLosses F.softmax: 2.194360 log_softmax: 2.096688\n",
            "Train Epoch: 47 [600/1000 (60%)]\tLosses F.softmax: 2.148969 log_softmax: 2.164764\n",
            "Train Epoch: 47 [800/1000 (80%)]\tLosses F.softmax: 2.217946 log_softmax: 2.329351\n",
            "Train Epoch: 47 [1000/1000 (100%)]\tLosses F.softmax: 2.307054 log_softmax: 2.323440\n",
            "Test set:\n",
            "F.softmax: Loss: 2.1740\tAccuracy: 3142.0/10000 (31%)\n",
            "log_softmax: Loss: 2.1619\tAccuracy: 3459.0/10000 (35%)\n",
            "\n",
            "Train Epoch: 48 [0/1000 (0%)]\tLosses F.softmax: 2.224251 log_softmax: 2.151687\n",
            "Train Epoch: 48 [200/1000 (20%)]\tLosses F.softmax: 2.001345 log_softmax: 1.962769\n",
            "Train Epoch: 48 [400/1000 (40%)]\tLosses F.softmax: 2.270854 log_softmax: 2.255092\n",
            "Train Epoch: 48 [600/1000 (60%)]\tLosses F.softmax: 2.382249 log_softmax: 2.340619\n",
            "Train Epoch: 48 [800/1000 (80%)]\tLosses F.softmax: 1.840602 log_softmax: 1.860179\n",
            "Train Epoch: 48 [1000/1000 (100%)]\tLosses F.softmax: 2.107444 log_softmax: 2.145390\n",
            "Test set:\n",
            "F.softmax: Loss: 2.1667\tAccuracy: 3210.0/10000 (32%)\n",
            "log_softmax: Loss: 2.1536\tAccuracy: 3540.0/10000 (35%)\n",
            "\n",
            "Train Epoch: 49 [0/1000 (0%)]\tLosses F.softmax: 2.065626 log_softmax: 1.984997\n",
            "Train Epoch: 49 [200/1000 (20%)]\tLosses F.softmax: 2.236639 log_softmax: 2.233939\n",
            "Train Epoch: 49 [400/1000 (40%)]\tLosses F.softmax: 2.400896 log_softmax: 2.350299\n",
            "Train Epoch: 49 [600/1000 (60%)]\tLosses F.softmax: 2.352548 log_softmax: 2.351048\n",
            "Train Epoch: 49 [800/1000 (80%)]\tLosses F.softmax: 2.320464 log_softmax: 2.332928\n",
            "Train Epoch: 49 [1000/1000 (100%)]\tLosses F.softmax: 2.275499 log_softmax: 2.259362\n",
            "Test set:\n",
            "F.softmax: Loss: 2.1589\tAccuracy: 3292.0/10000 (33%)\n",
            "log_softmax: Loss: 2.1448\tAccuracy: 3641.0/10000 (36%)\n",
            "\n",
            "Train Epoch: 50 [0/1000 (0%)]\tLosses F.softmax: 2.372430 log_softmax: 2.256457\n",
            "Train Epoch: 50 [200/1000 (20%)]\tLosses F.softmax: 2.210092 log_softmax: 2.179966\n",
            "Train Epoch: 50 [400/1000 (40%)]\tLosses F.softmax: 1.882280 log_softmax: 1.972349\n",
            "Train Epoch: 50 [600/1000 (60%)]\tLosses F.softmax: 1.973421 log_softmax: 2.002947\n",
            "Train Epoch: 50 [800/1000 (80%)]\tLosses F.softmax: 2.165330 log_softmax: 2.122607\n",
            "Train Epoch: 50 [1000/1000 (100%)]\tLosses F.softmax: 2.178442 log_softmax: 2.109667\n",
            "Test set:\n",
            "F.softmax: Loss: 2.1506\tAccuracy: 3361.0/10000 (34%)\n",
            "log_softmax: Loss: 2.1354\tAccuracy: 3760.0/10000 (38%)\n",
            "\n",
            "Train Epoch: 51 [0/1000 (0%)]\tLosses F.softmax: 2.366715 log_softmax: 2.314681\n",
            "Train Epoch: 51 [200/1000 (20%)]\tLosses F.softmax: 2.102476 log_softmax: 1.995713\n",
            "Train Epoch: 51 [400/1000 (40%)]\tLosses F.softmax: 2.294686 log_softmax: 2.255237\n",
            "Train Epoch: 51 [600/1000 (60%)]\tLosses F.softmax: 2.138109 log_softmax: 2.117579\n",
            "Train Epoch: 51 [800/1000 (80%)]\tLosses F.softmax: 2.260734 log_softmax: 2.097440\n",
            "Train Epoch: 51 [1000/1000 (100%)]\tLosses F.softmax: 2.286303 log_softmax: 2.214712\n",
            "Test set:\n",
            "F.softmax: Loss: 2.1418\tAccuracy: 3422.0/10000 (34%)\n",
            "log_softmax: Loss: 2.1254\tAccuracy: 3833.0/10000 (38%)\n",
            "\n",
            "Train Epoch: 52 [0/1000 (0%)]\tLosses F.softmax: 2.231389 log_softmax: 2.174524\n",
            "Train Epoch: 52 [200/1000 (20%)]\tLosses F.softmax: 1.806430 log_softmax: 1.917734\n",
            "Train Epoch: 52 [400/1000 (40%)]\tLosses F.softmax: 2.310968 log_softmax: 2.127754\n",
            "Train Epoch: 52 [600/1000 (60%)]\tLosses F.softmax: 2.169102 log_softmax: 2.094970\n",
            "Train Epoch: 52 [800/1000 (80%)]\tLosses F.softmax: 2.050465 log_softmax: 2.001848\n",
            "Train Epoch: 52 [1000/1000 (100%)]\tLosses F.softmax: 2.316436 log_softmax: 2.119425\n",
            "Test set:\n",
            "F.softmax: Loss: 2.1325\tAccuracy: 3479.0/10000 (35%)\n",
            "log_softmax: Loss: 2.1148\tAccuracy: 3936.0/10000 (39%)\n",
            "\n",
            "Train Epoch: 53 [0/1000 (0%)]\tLosses F.softmax: 2.015874 log_softmax: 1.971481\n",
            "Train Epoch: 53 [200/1000 (20%)]\tLosses F.softmax: 2.357995 log_softmax: 2.246014\n",
            "Train Epoch: 53 [400/1000 (40%)]\tLosses F.softmax: 2.376409 log_softmax: 2.351857\n",
            "Train Epoch: 53 [600/1000 (60%)]\tLosses F.softmax: 1.644198 log_softmax: 1.749859\n",
            "Train Epoch: 53 [800/1000 (80%)]\tLosses F.softmax: 1.976943 log_softmax: 2.006548\n",
            "Train Epoch: 53 [1000/1000 (100%)]\tLosses F.softmax: 1.608764 log_softmax: 1.696977\n",
            "Test set:\n",
            "F.softmax: Loss: 2.1225\tAccuracy: 3533.0/10000 (35%)\n",
            "log_softmax: Loss: 2.1035\tAccuracy: 4020.0/10000 (40%)\n",
            "\n",
            "Train Epoch: 54 [0/1000 (0%)]\tLosses F.softmax: 1.618577 log_softmax: 1.705961\n",
            "Train Epoch: 54 [200/1000 (20%)]\tLosses F.softmax: 1.614853 log_softmax: 1.679296\n",
            "Train Epoch: 54 [400/1000 (40%)]\tLosses F.softmax: 1.980374 log_softmax: 1.967258\n",
            "Train Epoch: 54 [600/1000 (60%)]\tLosses F.softmax: 2.184291 log_softmax: 2.084653\n",
            "Train Epoch: 54 [800/1000 (80%)]\tLosses F.softmax: 2.289809 log_softmax: 2.253918\n",
            "Train Epoch: 54 [1000/1000 (100%)]\tLosses F.softmax: 1.840540 log_softmax: 1.894343\n",
            "Test set:\n",
            "F.softmax: Loss: 2.1119\tAccuracy: 3602.0/10000 (36%)\n",
            "log_softmax: Loss: 2.0915\tAccuracy: 4117.0/10000 (41%)\n",
            "\n",
            "Train Epoch: 55 [0/1000 (0%)]\tLosses F.softmax: 2.381279 log_softmax: 2.394084\n",
            "Train Epoch: 55 [200/1000 (20%)]\tLosses F.softmax: 2.403817 log_softmax: 2.322798\n",
            "Train Epoch: 55 [400/1000 (40%)]\tLosses F.softmax: 1.919933 log_softmax: 1.880689\n",
            "Train Epoch: 55 [600/1000 (60%)]\tLosses F.softmax: 1.580637 log_softmax: 1.680063\n",
            "Train Epoch: 55 [800/1000 (80%)]\tLosses F.softmax: 1.948774 log_softmax: 1.823187\n",
            "Train Epoch: 55 [1000/1000 (100%)]\tLosses F.softmax: 1.760264 log_softmax: 1.846319\n",
            "Test set:\n",
            "F.softmax: Loss: 2.1007\tAccuracy: 3641.0/10000 (36%)\n",
            "log_softmax: Loss: 2.0788\tAccuracy: 4188.0/10000 (42%)\n",
            "\n",
            "Train Epoch: 56 [0/1000 (0%)]\tLosses F.softmax: 2.232809 log_softmax: 2.199452\n",
            "Train Epoch: 56 [200/1000 (20%)]\tLosses F.softmax: 2.264880 log_softmax: 2.294314\n",
            "Train Epoch: 56 [400/1000 (40%)]\tLosses F.softmax: 2.243862 log_softmax: 2.212542\n",
            "Train Epoch: 56 [600/1000 (60%)]\tLosses F.softmax: 2.130138 log_softmax: 2.042999\n",
            "Train Epoch: 56 [800/1000 (80%)]\tLosses F.softmax: 2.127875 log_softmax: 2.088855\n",
            "Train Epoch: 56 [1000/1000 (100%)]\tLosses F.softmax: 1.787371 log_softmax: 1.917413\n",
            "Test set:\n",
            "F.softmax: Loss: 2.0888\tAccuracy: 3702.0/10000 (37%)\n",
            "log_softmax: Loss: 2.0653\tAccuracy: 4269.0/10000 (43%)\n",
            "\n",
            "Train Epoch: 57 [0/1000 (0%)]\tLosses F.softmax: 2.137856 log_softmax: 2.028103\n",
            "Train Epoch: 57 [200/1000 (20%)]\tLosses F.softmax: 2.312699 log_softmax: 2.320248\n",
            "Train Epoch: 57 [400/1000 (40%)]\tLosses F.softmax: 2.191011 log_softmax: 2.140691\n",
            "Train Epoch: 57 [600/1000 (60%)]\tLosses F.softmax: 1.948546 log_softmax: 1.863326\n",
            "Train Epoch: 57 [800/1000 (80%)]\tLosses F.softmax: 2.238583 log_softmax: 2.204010\n",
            "Train Epoch: 57 [1000/1000 (100%)]\tLosses F.softmax: 1.821558 log_softmax: 1.904412\n",
            "Test set:\n",
            "F.softmax: Loss: 2.0763\tAccuracy: 3742.0/10000 (37%)\n",
            "log_softmax: Loss: 2.0510\tAccuracy: 4334.0/10000 (43%)\n",
            "\n",
            "Train Epoch: 58 [0/1000 (0%)]\tLosses F.softmax: 2.282670 log_softmax: 2.303756\n",
            "Train Epoch: 58 [200/1000 (20%)]\tLosses F.softmax: 2.218466 log_softmax: 2.141018\n",
            "Train Epoch: 58 [400/1000 (40%)]\tLosses F.softmax: 2.255870 log_softmax: 2.149161\n",
            "Train Epoch: 58 [600/1000 (60%)]\tLosses F.softmax: 1.720345 log_softmax: 1.825211\n",
            "Train Epoch: 58 [800/1000 (80%)]\tLosses F.softmax: 1.439130 log_softmax: 1.587798\n",
            "Train Epoch: 58 [1000/1000 (100%)]\tLosses F.softmax: 1.803623 log_softmax: 1.845173\n",
            "Test set:\n",
            "F.softmax: Loss: 2.0630\tAccuracy: 3769.0/10000 (38%)\n",
            "log_softmax: Loss: 2.0358\tAccuracy: 4383.0/10000 (44%)\n",
            "\n",
            "Train Epoch: 59 [0/1000 (0%)]\tLosses F.softmax: 2.011832 log_softmax: 2.039125\n",
            "Train Epoch: 59 [200/1000 (20%)]\tLosses F.softmax: 2.219104 log_softmax: 2.066176\n",
            "Train Epoch: 59 [400/1000 (40%)]\tLosses F.softmax: 2.317287 log_softmax: 2.316356\n",
            "Train Epoch: 59 [600/1000 (60%)]\tLosses F.softmax: 1.782173 log_softmax: 1.858482\n",
            "Train Epoch: 59 [800/1000 (80%)]\tLosses F.softmax: 2.228353 log_softmax: 2.060114\n",
            "Train Epoch: 59 [1000/1000 (100%)]\tLosses F.softmax: 1.746566 log_softmax: 1.677590\n",
            "Test set:\n",
            "F.softmax: Loss: 2.0490\tAccuracy: 3822.0/10000 (38%)\n",
            "log_softmax: Loss: 2.0198\tAccuracy: 4443.0/10000 (44%)\n",
            "\n",
            "Train Epoch: 60 [0/1000 (0%)]\tLosses F.softmax: 1.446794 log_softmax: 1.576216\n",
            "Train Epoch: 60 [200/1000 (20%)]\tLosses F.softmax: 2.228066 log_softmax: 2.094646\n",
            "Train Epoch: 60 [400/1000 (40%)]\tLosses F.softmax: 1.497016 log_softmax: 1.651742\n",
            "Train Epoch: 60 [600/1000 (60%)]\tLosses F.softmax: 2.175587 log_softmax: 2.089626\n",
            "Train Epoch: 60 [800/1000 (80%)]\tLosses F.softmax: 1.535445 log_softmax: 1.668750\n",
            "Train Epoch: 60 [1000/1000 (100%)]\tLosses F.softmax: 2.240237 log_softmax: 2.298743\n",
            "Test set:\n",
            "F.softmax: Loss: 2.0341\tAccuracy: 3868.0/10000 (39%)\n",
            "log_softmax: Loss: 2.0027\tAccuracy: 4496.0/10000 (45%)\n",
            "\n",
            "Train Epoch: 61 [0/1000 (0%)]\tLosses F.softmax: 1.517041 log_softmax: 1.621382\n",
            "Train Epoch: 61 [200/1000 (20%)]\tLosses F.softmax: 2.332448 log_softmax: 2.324826\n",
            "Train Epoch: 61 [400/1000 (40%)]\tLosses F.softmax: 1.820265 log_softmax: 1.865759\n",
            "Train Epoch: 61 [600/1000 (60%)]\tLosses F.softmax: 1.623884 log_softmax: 1.495998\n",
            "Train Epoch: 61 [800/1000 (80%)]\tLosses F.softmax: 2.119560 log_softmax: 1.975399\n",
            "Train Epoch: 61 [1000/1000 (100%)]\tLosses F.softmax: 2.448322 log_softmax: 2.421518\n",
            "Test set:\n",
            "F.softmax: Loss: 2.0185\tAccuracy: 3922.0/10000 (39%)\n",
            "log_softmax: Loss: 1.9849\tAccuracy: 4591.0/10000 (46%)\n",
            "\n",
            "Train Epoch: 62 [0/1000 (0%)]\tLosses F.softmax: 1.393026 log_softmax: 1.528321\n",
            "Train Epoch: 62 [200/1000 (20%)]\tLosses F.softmax: 2.347486 log_softmax: 2.318917\n",
            "Train Epoch: 62 [400/1000 (40%)]\tLosses F.softmax: 1.658715 log_softmax: 1.507032\n",
            "Train Epoch: 62 [600/1000 (60%)]\tLosses F.softmax: 1.983444 log_softmax: 2.026615\n",
            "Train Epoch: 62 [800/1000 (80%)]\tLosses F.softmax: 2.146210 log_softmax: 2.070542\n",
            "Train Epoch: 62 [1000/1000 (100%)]\tLosses F.softmax: 1.808508 log_softmax: 1.853642\n",
            "Test set:\n",
            "F.softmax: Loss: 2.0022\tAccuracy: 3959.0/10000 (40%)\n",
            "log_softmax: Loss: 1.9663\tAccuracy: 4638.0/10000 (46%)\n",
            "\n",
            "Train Epoch: 63 [0/1000 (0%)]\tLosses F.softmax: 1.944003 log_softmax: 1.980451\n",
            "Train Epoch: 63 [200/1000 (20%)]\tLosses F.softmax: 1.932418 log_softmax: 1.952783\n",
            "Train Epoch: 63 [400/1000 (40%)]\tLosses F.softmax: 2.125962 log_softmax: 1.983528\n",
            "Train Epoch: 63 [600/1000 (60%)]\tLosses F.softmax: 1.825230 log_softmax: 1.858363\n",
            "Train Epoch: 63 [800/1000 (80%)]\tLosses F.softmax: 1.891686 log_softmax: 1.801025\n",
            "Train Epoch: 63 [1000/1000 (100%)]\tLosses F.softmax: 1.531024 log_softmax: 1.673195\n",
            "Test set:\n",
            "F.softmax: Loss: 1.9845\tAccuracy: 4021.0/10000 (40%)\n",
            "log_softmax: Loss: 1.9461\tAccuracy: 4686.0/10000 (47%)\n",
            "\n",
            "Train Epoch: 64 [0/1000 (0%)]\tLosses F.softmax: 2.383232 log_softmax: 2.343528\n",
            "Train Epoch: 64 [200/1000 (20%)]\tLosses F.softmax: 2.033196 log_softmax: 1.946916\n",
            "Train Epoch: 64 [400/1000 (40%)]\tLosses F.softmax: 1.698606 log_softmax: 1.560479\n",
            "Train Epoch: 64 [600/1000 (60%)]\tLosses F.softmax: 1.135180 log_softmax: 1.339842\n",
            "Train Epoch: 64 [800/1000 (80%)]\tLosses F.softmax: 2.264717 log_softmax: 2.044731\n",
            "Train Epoch: 64 [1000/1000 (100%)]\tLosses F.softmax: 2.364845 log_softmax: 2.239195\n",
            "Test set:\n",
            "F.softmax: Loss: 1.9663\tAccuracy: 4055.0/10000 (41%)\n",
            "log_softmax: Loss: 1.9254\tAccuracy: 4714.0/10000 (47%)\n",
            "\n",
            "Train Epoch: 65 [0/1000 (0%)]\tLosses F.softmax: 1.481279 log_softmax: 1.319116\n",
            "Train Epoch: 65 [200/1000 (20%)]\tLosses F.softmax: 2.156860 log_softmax: 2.117381\n",
            "Train Epoch: 65 [400/1000 (40%)]\tLosses F.softmax: 2.313361 log_softmax: 2.301396\n",
            "Train Epoch: 65 [600/1000 (60%)]\tLosses F.softmax: 2.142266 log_softmax: 2.073536\n",
            "Train Epoch: 65 [800/1000 (80%)]\tLosses F.softmax: 2.234075 log_softmax: 2.264724\n",
            "Train Epoch: 65 [1000/1000 (100%)]\tLosses F.softmax: 2.185140 log_softmax: 2.221473\n",
            "Test set:\n",
            "F.softmax: Loss: 1.9475\tAccuracy: 4093.0/10000 (41%)\n",
            "log_softmax: Loss: 1.9041\tAccuracy: 4772.0/10000 (48%)\n",
            "\n",
            "Train Epoch: 66 [0/1000 (0%)]\tLosses F.softmax: 2.232960 log_softmax: 1.961607\n",
            "Train Epoch: 66 [200/1000 (20%)]\tLosses F.softmax: 2.236111 log_softmax: 2.243030\n",
            "Train Epoch: 66 [400/1000 (40%)]\tLosses F.softmax: 1.582983 log_softmax: 1.664240\n",
            "Train Epoch: 66 [600/1000 (60%)]\tLosses F.softmax: 1.743754 log_softmax: 1.620350\n",
            "Train Epoch: 66 [800/1000 (80%)]\tLosses F.softmax: 2.355060 log_softmax: 2.312822\n",
            "Train Epoch: 66 [1000/1000 (100%)]\tLosses F.softmax: 2.033310 log_softmax: 1.991890\n",
            "Test set:\n",
            "F.softmax: Loss: 1.9277\tAccuracy: 4147.0/10000 (41%)\n",
            "log_softmax: Loss: 1.8818\tAccuracy: 4821.0/10000 (48%)\n",
            "\n",
            "Train Epoch: 67 [0/1000 (0%)]\tLosses F.softmax: 2.136249 log_softmax: 1.978613\n",
            "Train Epoch: 67 [200/1000 (20%)]\tLosses F.softmax: 1.947124 log_softmax: 1.814425\n",
            "Train Epoch: 67 [400/1000 (40%)]\tLosses F.softmax: 1.583451 log_softmax: 1.656291\n",
            "Train Epoch: 67 [600/1000 (60%)]\tLosses F.softmax: 1.550332 log_softmax: 1.767233\n",
            "Train Epoch: 67 [800/1000 (80%)]\tLosses F.softmax: 2.306647 log_softmax: 2.233274\n",
            "Train Epoch: 67 [1000/1000 (100%)]\tLosses F.softmax: 1.933099 log_softmax: 1.775676\n",
            "Test set:\n",
            "F.softmax: Loss: 1.9070\tAccuracy: 4207.0/10000 (42%)\n",
            "log_softmax: Loss: 1.8585\tAccuracy: 4871.0/10000 (49%)\n",
            "\n",
            "Train Epoch: 68 [0/1000 (0%)]\tLosses F.softmax: 1.789843 log_softmax: 1.621592\n",
            "Train Epoch: 68 [200/1000 (20%)]\tLosses F.softmax: 1.823812 log_softmax: 1.719998\n",
            "Train Epoch: 68 [400/1000 (40%)]\tLosses F.softmax: 1.540211 log_softmax: 1.625378\n",
            "Train Epoch: 68 [600/1000 (60%)]\tLosses F.softmax: 1.303606 log_softmax: 1.483078\n",
            "Train Epoch: 68 [800/1000 (80%)]\tLosses F.softmax: 2.243295 log_softmax: 2.353118\n",
            "Train Epoch: 68 [1000/1000 (100%)]\tLosses F.softmax: 0.977758 log_softmax: 1.178149\n",
            "Test set:\n",
            "F.softmax: Loss: 1.8860\tAccuracy: 4237.0/10000 (42%)\n",
            "log_softmax: Loss: 1.8351\tAccuracy: 4905.0/10000 (49%)\n",
            "\n",
            "Train Epoch: 69 [0/1000 (0%)]\tLosses F.softmax: 2.153253 log_softmax: 1.995996\n",
            "Train Epoch: 69 [200/1000 (20%)]\tLosses F.softmax: 1.784528 log_softmax: 1.646179\n",
            "Train Epoch: 69 [400/1000 (40%)]\tLosses F.softmax: 2.475358 log_softmax: 2.394352\n",
            "Train Epoch: 69 [600/1000 (60%)]\tLosses F.softmax: 2.335288 log_softmax: 2.056611\n",
            "Train Epoch: 69 [800/1000 (80%)]\tLosses F.softmax: 1.333963 log_softmax: 1.472296\n",
            "Train Epoch: 69 [1000/1000 (100%)]\tLosses F.softmax: 1.993605 log_softmax: 1.834293\n",
            "Test set:\n",
            "F.softmax: Loss: 1.8639\tAccuracy: 4281.0/10000 (43%)\n",
            "log_softmax: Loss: 1.8107\tAccuracy: 4974.0/10000 (50%)\n",
            "\n",
            "Train Epoch: 70 [0/1000 (0%)]\tLosses F.softmax: 1.660317 log_softmax: 1.744839\n",
            "Train Epoch: 70 [200/1000 (20%)]\tLosses F.softmax: 1.963022 log_softmax: 1.855017\n",
            "Train Epoch: 70 [400/1000 (40%)]\tLosses F.softmax: 2.236418 log_softmax: 2.278359\n",
            "Train Epoch: 70 [600/1000 (60%)]\tLosses F.softmax: 1.468047 log_softmax: 1.338182\n",
            "Train Epoch: 70 [800/1000 (80%)]\tLosses F.softmax: 2.104744 log_softmax: 1.922654\n",
            "Train Epoch: 70 [1000/1000 (100%)]\tLosses F.softmax: 1.125755 log_softmax: 0.916745\n",
            "Test set:\n",
            "F.softmax: Loss: 1.8411\tAccuracy: 4302.0/10000 (43%)\n",
            "log_softmax: Loss: 1.7855\tAccuracy: 5022.0/10000 (50%)\n",
            "\n",
            "Train Epoch: 71 [0/1000 (0%)]\tLosses F.softmax: 1.453821 log_softmax: 1.282507\n",
            "Train Epoch: 71 [200/1000 (20%)]\tLosses F.softmax: 2.082500 log_softmax: 1.901153\n",
            "Train Epoch: 71 [400/1000 (40%)]\tLosses F.softmax: 1.652619 log_softmax: 1.545906\n",
            "Train Epoch: 71 [600/1000 (60%)]\tLosses F.softmax: 1.263399 log_softmax: 1.091968\n",
            "Train Epoch: 71 [800/1000 (80%)]\tLosses F.softmax: 2.493654 log_softmax: 2.491155\n",
            "Train Epoch: 71 [1000/1000 (100%)]\tLosses F.softmax: 1.991685 log_softmax: 1.887382\n",
            "Test set:\n",
            "F.softmax: Loss: 1.8179\tAccuracy: 4369.0/10000 (44%)\n",
            "log_softmax: Loss: 1.7604\tAccuracy: 5083.0/10000 (51%)\n",
            "\n",
            "Train Epoch: 72 [0/1000 (0%)]\tLosses F.softmax: 0.788386 log_softmax: 1.019491\n",
            "Train Epoch: 72 [200/1000 (20%)]\tLosses F.softmax: 1.987443 log_softmax: 1.733103\n",
            "Train Epoch: 72 [400/1000 (40%)]\tLosses F.softmax: 1.967587 log_softmax: 1.834715\n",
            "Train Epoch: 72 [600/1000 (60%)]\tLosses F.softmax: 1.806770 log_softmax: 1.685324\n",
            "Train Epoch: 72 [800/1000 (80%)]\tLosses F.softmax: 2.121579 log_softmax: 1.916681\n",
            "Train Epoch: 72 [1000/1000 (100%)]\tLosses F.softmax: 1.373278 log_softmax: 1.540119\n",
            "Test set:\n",
            "F.softmax: Loss: 1.7941\tAccuracy: 4464.0/10000 (45%)\n",
            "log_softmax: Loss: 1.7348\tAccuracy: 5148.0/10000 (51%)\n",
            "\n",
            "Train Epoch: 73 [0/1000 (0%)]\tLosses F.softmax: 0.761321 log_softmax: 0.993782\n",
            "Train Epoch: 73 [200/1000 (20%)]\tLosses F.softmax: 1.298725 log_softmax: 1.510328\n",
            "Train Epoch: 73 [400/1000 (40%)]\tLosses F.softmax: 2.144890 log_softmax: 2.151411\n",
            "Train Epoch: 73 [600/1000 (60%)]\tLosses F.softmax: 2.181069 log_softmax: 2.081445\n",
            "Train Epoch: 73 [800/1000 (80%)]\tLosses F.softmax: 2.098803 log_softmax: 1.879729\n",
            "Train Epoch: 73 [1000/1000 (100%)]\tLosses F.softmax: 2.087482 log_softmax: 2.011300\n",
            "Test set:\n",
            "F.softmax: Loss: 1.7698\tAccuracy: 4531.0/10000 (45%)\n",
            "log_softmax: Loss: 1.7092\tAccuracy: 5204.0/10000 (52%)\n",
            "\n",
            "Train Epoch: 74 [0/1000 (0%)]\tLosses F.softmax: 0.723984 log_softmax: 0.986960\n",
            "Train Epoch: 74 [200/1000 (20%)]\tLosses F.softmax: 1.132798 log_softmax: 0.966370\n",
            "Train Epoch: 74 [400/1000 (40%)]\tLosses F.softmax: 1.973754 log_softmax: 1.904534\n",
            "Train Epoch: 74 [600/1000 (60%)]\tLosses F.softmax: 1.905444 log_softmax: 1.758345\n",
            "Train Epoch: 74 [800/1000 (80%)]\tLosses F.softmax: 1.369609 log_softmax: 1.477800\n",
            "Train Epoch: 74 [1000/1000 (100%)]\tLosses F.softmax: 1.433759 log_softmax: 1.280355\n",
            "Test set:\n",
            "F.softmax: Loss: 1.7451\tAccuracy: 4604.0/10000 (46%)\n",
            "log_softmax: Loss: 1.6832\tAccuracy: 5244.0/10000 (52%)\n",
            "\n",
            "Train Epoch: 75 [0/1000 (0%)]\tLosses F.softmax: 1.937555 log_softmax: 1.877485\n",
            "Train Epoch: 75 [200/1000 (20%)]\tLosses F.softmax: 1.577643 log_softmax: 1.565123\n",
            "Train Epoch: 75 [400/1000 (40%)]\tLosses F.softmax: 1.777609 log_softmax: 1.653264\n",
            "Train Epoch: 75 [600/1000 (60%)]\tLosses F.softmax: 1.830641 log_softmax: 1.694739\n",
            "Train Epoch: 75 [800/1000 (80%)]\tLosses F.softmax: 1.849433 log_softmax: 1.610509\n",
            "Train Epoch: 75 [1000/1000 (100%)]\tLosses F.softmax: 1.195441 log_softmax: 1.322960\n",
            "Test set:\n",
            "F.softmax: Loss: 1.7200\tAccuracy: 4683.0/10000 (47%)\n",
            "log_softmax: Loss: 1.6573\tAccuracy: 5302.0/10000 (53%)\n",
            "\n",
            "Train Epoch: 76 [0/1000 (0%)]\tLosses F.softmax: 1.804543 log_softmax: 1.669616\n",
            "Train Epoch: 76 [200/1000 (20%)]\tLosses F.softmax: 1.161191 log_softmax: 1.376533\n",
            "Train Epoch: 76 [400/1000 (40%)]\tLosses F.softmax: 1.923622 log_softmax: 1.463048\n",
            "Train Epoch: 76 [600/1000 (60%)]\tLosses F.softmax: 1.257031 log_softmax: 1.371886\n",
            "Train Epoch: 76 [800/1000 (80%)]\tLosses F.softmax: 0.805254 log_softmax: 0.907440\n",
            "Train Epoch: 76 [1000/1000 (100%)]\tLosses F.softmax: 0.678300 log_softmax: 0.897132\n",
            "Test set:\n",
            "F.softmax: Loss: 1.6946\tAccuracy: 4746.0/10000 (47%)\n",
            "log_softmax: Loss: 1.6317\tAccuracy: 5331.0/10000 (53%)\n",
            "\n",
            "Train Epoch: 77 [0/1000 (0%)]\tLosses F.softmax: 2.098370 log_softmax: 2.125709\n",
            "Train Epoch: 77 [200/1000 (20%)]\tLosses F.softmax: 1.365329 log_softmax: 1.314749\n",
            "Train Epoch: 77 [400/1000 (40%)]\tLosses F.softmax: 1.650470 log_softmax: 1.586152\n",
            "Train Epoch: 77 [600/1000 (60%)]\tLosses F.softmax: 2.128670 log_softmax: 2.120164\n",
            "Train Epoch: 77 [800/1000 (80%)]\tLosses F.softmax: 0.827450 log_softmax: 1.015648\n",
            "Train Epoch: 77 [1000/1000 (100%)]\tLosses F.softmax: 1.226448 log_softmax: 1.334311\n",
            "Test set:\n",
            "F.softmax: Loss: 1.6689\tAccuracy: 4794.0/10000 (48%)\n",
            "log_softmax: Loss: 1.6060\tAccuracy: 5381.0/10000 (54%)\n",
            "\n",
            "Train Epoch: 78 [0/1000 (0%)]\tLosses F.softmax: 0.601112 log_softmax: 0.760513\n",
            "Train Epoch: 78 [200/1000 (20%)]\tLosses F.softmax: 0.595269 log_softmax: 0.782250\n",
            "Train Epoch: 78 [400/1000 (40%)]\tLosses F.softmax: 2.197141 log_softmax: 1.960818\n",
            "Train Epoch: 78 [600/1000 (60%)]\tLosses F.softmax: 1.378302 log_softmax: 1.243074\n",
            "Train Epoch: 78 [800/1000 (80%)]\tLosses F.softmax: 2.095755 log_softmax: 1.997801\n",
            "Train Epoch: 78 [1000/1000 (100%)]\tLosses F.softmax: 1.209979 log_softmax: 0.957238\n",
            "Test set:\n",
            "F.softmax: Loss: 1.6431\tAccuracy: 4870.0/10000 (49%)\n",
            "log_softmax: Loss: 1.5806\tAccuracy: 5439.0/10000 (54%)\n",
            "\n",
            "Train Epoch: 79 [0/1000 (0%)]\tLosses F.softmax: 1.325362 log_softmax: 1.225337\n",
            "Train Epoch: 79 [200/1000 (20%)]\tLosses F.softmax: 2.201288 log_softmax: 2.302128\n",
            "Train Epoch: 79 [400/1000 (40%)]\tLosses F.softmax: 1.191858 log_softmax: 0.940564\n",
            "Train Epoch: 79 [600/1000 (60%)]\tLosses F.softmax: 1.671638 log_softmax: 1.433844\n",
            "Train Epoch: 79 [800/1000 (80%)]\tLosses F.softmax: 0.521664 log_softmax: 0.734364\n",
            "Train Epoch: 79 [1000/1000 (100%)]\tLosses F.softmax: 1.055005 log_softmax: 0.936327\n",
            "Test set:\n",
            "F.softmax: Loss: 1.6174\tAccuracy: 4982.0/10000 (50%)\n",
            "log_softmax: Loss: 1.5558\tAccuracy: 5504.0/10000 (55%)\n",
            "\n",
            "Train Epoch: 80 [0/1000 (0%)]\tLosses F.softmax: 1.607716 log_softmax: 1.640968\n",
            "Train Epoch: 80 [200/1000 (20%)]\tLosses F.softmax: 1.443583 log_softmax: 1.407697\n",
            "Train Epoch: 80 [400/1000 (40%)]\tLosses F.softmax: 1.458429 log_softmax: 1.311760\n",
            "Train Epoch: 80 [600/1000 (60%)]\tLosses F.softmax: 2.420840 log_softmax: 2.272089\n",
            "Train Epoch: 80 [800/1000 (80%)]\tLosses F.softmax: 1.389467 log_softmax: 1.298469\n",
            "Train Epoch: 80 [1000/1000 (100%)]\tLosses F.softmax: 1.748931 log_softmax: 1.468720\n",
            "Test set:\n",
            "F.softmax: Loss: 1.5918\tAccuracy: 5088.0/10000 (51%)\n",
            "log_softmax: Loss: 1.5321\tAccuracy: 5569.0/10000 (56%)\n",
            "\n",
            "Train Epoch: 81 [0/1000 (0%)]\tLosses F.softmax: 1.811100 log_softmax: 1.725361\n",
            "Train Epoch: 81 [200/1000 (20%)]\tLosses F.softmax: 0.849105 log_softmax: 0.706976\n",
            "Train Epoch: 81 [400/1000 (40%)]\tLosses F.softmax: 0.861646 log_softmax: 1.084459\n",
            "Train Epoch: 81 [600/1000 (60%)]\tLosses F.softmax: 2.066430 log_softmax: 1.827939\n",
            "Train Epoch: 81 [800/1000 (80%)]\tLosses F.softmax: 1.642217 log_softmax: 1.433010\n",
            "Train Epoch: 81 [1000/1000 (100%)]\tLosses F.softmax: 0.569962 log_softmax: 0.765897\n",
            "Test set:\n",
            "F.softmax: Loss: 1.5661\tAccuracy: 5204.0/10000 (52%)\n",
            "log_softmax: Loss: 1.5083\tAccuracy: 5619.0/10000 (56%)\n",
            "\n",
            "Train Epoch: 82 [0/1000 (0%)]\tLosses F.softmax: 2.099455 log_softmax: 1.798194\n",
            "Train Epoch: 82 [200/1000 (20%)]\tLosses F.softmax: 1.845968 log_softmax: 1.878565\n",
            "Train Epoch: 82 [400/1000 (40%)]\tLosses F.softmax: 2.197608 log_softmax: 2.141490\n",
            "Train Epoch: 82 [600/1000 (60%)]\tLosses F.softmax: 1.367123 log_softmax: 1.248987\n",
            "Train Epoch: 82 [800/1000 (80%)]\tLosses F.softmax: 3.313660 log_softmax: 4.043632\n",
            "Train Epoch: 82 [1000/1000 (100%)]\tLosses F.softmax: 1.058631 log_softmax: 1.262855\n",
            "Test set:\n",
            "F.softmax: Loss: 1.5402\tAccuracy: 5271.0/10000 (53%)\n",
            "log_softmax: Loss: 1.4845\tAccuracy: 5660.0/10000 (57%)\n",
            "\n",
            "Train Epoch: 83 [0/1000 (0%)]\tLosses F.softmax: 1.905581 log_softmax: 2.092227\n",
            "Train Epoch: 83 [200/1000 (20%)]\tLosses F.softmax: 2.007130 log_softmax: 1.787103\n",
            "Train Epoch: 83 [400/1000 (40%)]\tLosses F.softmax: 1.948922 log_softmax: 1.560047\n",
            "Train Epoch: 83 [600/1000 (60%)]\tLosses F.softmax: 1.989511 log_softmax: 1.740224\n",
            "Train Epoch: 83 [800/1000 (80%)]\tLosses F.softmax: 1.229840 log_softmax: 1.179594\n",
            "Train Epoch: 83 [1000/1000 (100%)]\tLosses F.softmax: 1.251340 log_softmax: 1.025744\n",
            "Test set:\n",
            "F.softmax: Loss: 1.5151\tAccuracy: 5358.0/10000 (54%)\n",
            "log_softmax: Loss: 1.4621\tAccuracy: 5696.0/10000 (57%)\n",
            "\n",
            "Train Epoch: 84 [0/1000 (0%)]\tLosses F.softmax: 1.753822 log_softmax: 1.432437\n",
            "Train Epoch: 84 [200/1000 (20%)]\tLosses F.softmax: 0.794967 log_softmax: 0.715686\n",
            "Train Epoch: 84 [400/1000 (40%)]\tLosses F.softmax: 1.882550 log_softmax: 1.837397\n",
            "Train Epoch: 84 [600/1000 (60%)]\tLosses F.softmax: 0.626578 log_softmax: 0.847415\n",
            "Train Epoch: 84 [800/1000 (80%)]\tLosses F.softmax: 1.090641 log_softmax: 1.276190\n",
            "Train Epoch: 84 [1000/1000 (100%)]\tLosses F.softmax: 0.445818 log_softmax: 0.635208\n",
            "Test set:\n",
            "F.softmax: Loss: 1.4899\tAccuracy: 5435.0/10000 (54%)\n",
            "log_softmax: Loss: 1.4400\tAccuracy: 5746.0/10000 (57%)\n",
            "\n",
            "Train Epoch: 85 [0/1000 (0%)]\tLosses F.softmax: 2.101595 log_softmax: 2.046210\n",
            "Train Epoch: 85 [200/1000 (20%)]\tLosses F.softmax: 1.367507 log_softmax: 1.162274\n",
            "Train Epoch: 85 [400/1000 (40%)]\tLosses F.softmax: 1.069077 log_softmax: 1.176446\n",
            "Train Epoch: 85 [600/1000 (60%)]\tLosses F.softmax: 1.247120 log_softmax: 1.167686\n",
            "Train Epoch: 85 [800/1000 (80%)]\tLosses F.softmax: 1.073537 log_softmax: 1.019228\n",
            "Train Epoch: 85 [1000/1000 (100%)]\tLosses F.softmax: 2.435284 log_softmax: 2.049313\n",
            "Test set:\n",
            "F.softmax: Loss: 1.4649\tAccuracy: 5514.0/10000 (55%)\n",
            "log_softmax: Loss: 1.4184\tAccuracy: 5796.0/10000 (58%)\n",
            "\n",
            "Train Epoch: 86 [0/1000 (0%)]\tLosses F.softmax: 1.669273 log_softmax: 1.331967\n",
            "Train Epoch: 86 [200/1000 (20%)]\tLosses F.softmax: 1.056193 log_softmax: 1.062539\n",
            "Train Epoch: 86 [400/1000 (40%)]\tLosses F.softmax: 1.182330 log_softmax: 1.129874\n",
            "Train Epoch: 86 [600/1000 (60%)]\tLosses F.softmax: 1.883400 log_softmax: 2.093323\n",
            "Train Epoch: 86 [800/1000 (80%)]\tLosses F.softmax: 0.628714 log_softmax: 0.715365\n",
            "Train Epoch: 86 [1000/1000 (100%)]\tLosses F.softmax: 2.063425 log_softmax: 1.920481\n",
            "Test set:\n",
            "F.softmax: Loss: 1.4398\tAccuracy: 5606.0/10000 (56%)\n",
            "log_softmax: Loss: 1.3964\tAccuracy: 5892.0/10000 (59%)\n",
            "\n",
            "Train Epoch: 87 [0/1000 (0%)]\tLosses F.softmax: 1.451664 log_softmax: 1.393841\n",
            "Train Epoch: 87 [200/1000 (20%)]\tLosses F.softmax: 1.021651 log_softmax: 1.140250\n",
            "Train Epoch: 87 [400/1000 (40%)]\tLosses F.softmax: 2.012593 log_softmax: 1.772594\n",
            "Train Epoch: 87 [600/1000 (60%)]\tLosses F.softmax: 0.744340 log_softmax: 0.976148\n",
            "Train Epoch: 87 [800/1000 (80%)]\tLosses F.softmax: 0.902065 log_softmax: 1.037561\n",
            "Train Epoch: 87 [1000/1000 (100%)]\tLosses F.softmax: 2.309931 log_softmax: 2.104325\n",
            "Test set:\n",
            "F.softmax: Loss: 1.4156\tAccuracy: 5682.0/10000 (57%)\n",
            "log_softmax: Loss: 1.3760\tAccuracy: 5936.0/10000 (59%)\n",
            "\n",
            "Train Epoch: 88 [0/1000 (0%)]\tLosses F.softmax: 1.582552 log_softmax: 1.264703\n",
            "Train Epoch: 88 [200/1000 (20%)]\tLosses F.softmax: 1.947034 log_softmax: 1.952035\n",
            "Train Epoch: 88 [400/1000 (40%)]\tLosses F.softmax: 0.571525 log_softmax: 0.541605\n",
            "Train Epoch: 88 [600/1000 (60%)]\tLosses F.softmax: 0.607866 log_softmax: 0.838373\n",
            "Train Epoch: 88 [800/1000 (80%)]\tLosses F.softmax: 1.489399 log_softmax: 1.534268\n",
            "Train Epoch: 88 [1000/1000 (100%)]\tLosses F.softmax: 2.127626 log_softmax: 2.101326\n",
            "Test set:\n",
            "F.softmax: Loss: 1.3919\tAccuracy: 5770.0/10000 (58%)\n",
            "log_softmax: Loss: 1.3562\tAccuracy: 6005.0/10000 (60%)\n",
            "\n",
            "Train Epoch: 89 [0/1000 (0%)]\tLosses F.softmax: 1.392076 log_softmax: 1.192604\n",
            "Train Epoch: 89 [200/1000 (20%)]\tLosses F.softmax: 1.472387 log_softmax: 1.219719\n",
            "Train Epoch: 89 [400/1000 (40%)]\tLosses F.softmax: 1.312068 log_softmax: 0.921449\n",
            "Train Epoch: 89 [600/1000 (60%)]\tLosses F.softmax: 1.202883 log_softmax: 1.109732\n",
            "Train Epoch: 89 [800/1000 (80%)]\tLosses F.softmax: 1.154069 log_softmax: 1.174038\n",
            "Train Epoch: 89 [1000/1000 (100%)]\tLosses F.softmax: 0.864198 log_softmax: 1.035660\n",
            "Test set:\n",
            "F.softmax: Loss: 1.3691\tAccuracy: 5841.0/10000 (58%)\n",
            "log_softmax: Loss: 1.3372\tAccuracy: 6059.0/10000 (61%)\n",
            "\n",
            "Train Epoch: 90 [0/1000 (0%)]\tLosses F.softmax: 0.400845 log_softmax: 0.502900\n",
            "Train Epoch: 90 [200/1000 (20%)]\tLosses F.softmax: 2.687408 log_softmax: 3.123823\n",
            "Train Epoch: 90 [400/1000 (40%)]\tLosses F.softmax: 1.270494 log_softmax: 1.388400\n",
            "Train Epoch: 90 [600/1000 (60%)]\tLosses F.softmax: 1.299372 log_softmax: 0.924603\n",
            "Train Epoch: 90 [800/1000 (80%)]\tLosses F.softmax: 1.309764 log_softmax: 0.995744\n",
            "Train Epoch: 90 [1000/1000 (100%)]\tLosses F.softmax: 1.108865 log_softmax: 1.069251\n",
            "Test set:\n",
            "F.softmax: Loss: 1.3464\tAccuracy: 5904.0/10000 (59%)\n",
            "log_softmax: Loss: 1.3185\tAccuracy: 6102.0/10000 (61%)\n",
            "\n",
            "Train Epoch: 91 [0/1000 (0%)]\tLosses F.softmax: 2.246775 log_softmax: 2.210113\n",
            "Train Epoch: 91 [200/1000 (20%)]\tLosses F.softmax: 1.511113 log_softmax: 1.278914\n",
            "Train Epoch: 91 [400/1000 (40%)]\tLosses F.softmax: 0.562187 log_softmax: 0.562113\n",
            "Train Epoch: 91 [600/1000 (60%)]\tLosses F.softmax: 0.511931 log_softmax: 0.390364\n",
            "Train Epoch: 91 [800/1000 (80%)]\tLosses F.softmax: 1.356331 log_softmax: 1.053572\n",
            "Train Epoch: 91 [1000/1000 (100%)]\tLosses F.softmax: 1.457374 log_softmax: 1.421512\n",
            "Test set:\n",
            "F.softmax: Loss: 1.3243\tAccuracy: 5978.0/10000 (60%)\n",
            "log_softmax: Loss: 1.3003\tAccuracy: 6165.0/10000 (62%)\n",
            "\n",
            "Train Epoch: 92 [0/1000 (0%)]\tLosses F.softmax: 1.266690 log_softmax: 1.136630\n",
            "Train Epoch: 92 [200/1000 (20%)]\tLosses F.softmax: 2.001573 log_softmax: 2.024933\n",
            "Train Epoch: 92 [400/1000 (40%)]\tLosses F.softmax: 1.240873 log_softmax: 0.872096\n",
            "Train Epoch: 92 [600/1000 (60%)]\tLosses F.softmax: 0.735508 log_softmax: 0.873160\n",
            "Train Epoch: 92 [800/1000 (80%)]\tLosses F.softmax: 0.529650 log_softmax: 0.486903\n",
            "Train Epoch: 92 [1000/1000 (100%)]\tLosses F.softmax: 1.768505 log_softmax: 1.791091\n",
            "Test set:\n",
            "F.softmax: Loss: 1.3030\tAccuracy: 6041.0/10000 (60%)\n",
            "log_softmax: Loss: 1.2828\tAccuracy: 6208.0/10000 (62%)\n",
            "\n",
            "Train Epoch: 93 [0/1000 (0%)]\tLosses F.softmax: 1.104364 log_softmax: 1.090867\n",
            "Train Epoch: 93 [200/1000 (20%)]\tLosses F.softmax: 0.885424 log_softmax: 0.976846\n",
            "Train Epoch: 93 [400/1000 (40%)]\tLosses F.softmax: 1.377190 log_softmax: 1.112865\n",
            "Train Epoch: 93 [600/1000 (60%)]\tLosses F.softmax: 1.441520 log_softmax: 1.609882\n",
            "Train Epoch: 93 [800/1000 (80%)]\tLosses F.softmax: 1.114215 log_softmax: 1.034542\n",
            "Train Epoch: 93 [1000/1000 (100%)]\tLosses F.softmax: 1.549944 log_softmax: 1.246786\n",
            "Test set:\n",
            "F.softmax: Loss: 1.2822\tAccuracy: 6097.0/10000 (61%)\n",
            "log_softmax: Loss: 1.2659\tAccuracy: 6281.0/10000 (63%)\n",
            "\n",
            "Train Epoch: 94 [0/1000 (0%)]\tLosses F.softmax: 1.257911 log_softmax: 1.217310\n",
            "Train Epoch: 94 [200/1000 (20%)]\tLosses F.softmax: 0.385961 log_softmax: 0.497530\n",
            "Train Epoch: 94 [400/1000 (40%)]\tLosses F.softmax: 0.333195 log_softmax: 0.305142\n",
            "Train Epoch: 94 [600/1000 (60%)]\tLosses F.softmax: 1.275829 log_softmax: 1.421195\n",
            "Train Epoch: 94 [800/1000 (80%)]\tLosses F.softmax: 1.941931 log_softmax: 1.926774\n",
            "Train Epoch: 94 [1000/1000 (100%)]\tLosses F.softmax: 0.864256 log_softmax: 1.053275\n",
            "Test set:\n",
            "F.softmax: Loss: 1.2620\tAccuracy: 6140.0/10000 (61%)\n",
            "log_softmax: Loss: 1.2490\tAccuracy: 6319.0/10000 (63%)\n",
            "\n",
            "Train Epoch: 95 [0/1000 (0%)]\tLosses F.softmax: 0.323926 log_softmax: 0.253855\n",
            "Train Epoch: 95 [200/1000 (20%)]\tLosses F.softmax: 0.448946 log_softmax: 0.548827\n",
            "Train Epoch: 95 [400/1000 (40%)]\tLosses F.softmax: 1.477205 log_softmax: 1.537321\n",
            "Train Epoch: 95 [600/1000 (60%)]\tLosses F.softmax: 1.033837 log_softmax: 1.135426\n",
            "Train Epoch: 95 [800/1000 (80%)]\tLosses F.softmax: 0.440679 log_softmax: 0.601782\n",
            "Train Epoch: 95 [1000/1000 (100%)]\tLosses F.softmax: 0.402529 log_softmax: 0.564989\n",
            "Test set:\n",
            "F.softmax: Loss: 1.2419\tAccuracy: 6227.0/10000 (62%)\n",
            "log_softmax: Loss: 1.2321\tAccuracy: 6411.0/10000 (64%)\n",
            "\n",
            "Train Epoch: 96 [0/1000 (0%)]\tLosses F.softmax: 0.651912 log_softmax: 0.818189\n",
            "Train Epoch: 96 [200/1000 (20%)]\tLosses F.softmax: 0.521713 log_softmax: 0.786413\n",
            "Train Epoch: 96 [400/1000 (40%)]\tLosses F.softmax: 1.158586 log_softmax: 0.849347\n",
            "Train Epoch: 96 [600/1000 (60%)]\tLosses F.softmax: 1.259130 log_softmax: 0.906245\n",
            "Train Epoch: 96 [800/1000 (80%)]\tLosses F.softmax: 2.201099 log_softmax: 2.675368\n",
            "Train Epoch: 96 [1000/1000 (100%)]\tLosses F.softmax: 2.128331 log_softmax: 1.642567\n",
            "Test set:\n",
            "F.softmax: Loss: 1.2232\tAccuracy: 6285.0/10000 (63%)\n",
            "log_softmax: Loss: 1.2165\tAccuracy: 6463.0/10000 (65%)\n",
            "\n",
            "Train Epoch: 97 [0/1000 (0%)]\tLosses F.softmax: 1.316917 log_softmax: 1.391053\n",
            "Train Epoch: 97 [200/1000 (20%)]\tLosses F.softmax: 1.472861 log_softmax: 1.438259\n",
            "Train Epoch: 97 [400/1000 (40%)]\tLosses F.softmax: 2.264769 log_softmax: 2.059958\n",
            "Train Epoch: 97 [600/1000 (60%)]\tLosses F.softmax: 0.667786 log_softmax: 0.796061\n",
            "Train Epoch: 97 [800/1000 (80%)]\tLosses F.softmax: 1.202766 log_softmax: 1.029537\n",
            "Train Epoch: 97 [1000/1000 (100%)]\tLosses F.softmax: 1.208834 log_softmax: 1.214573\n",
            "Test set:\n",
            "F.softmax: Loss: 1.2051\tAccuracy: 6310.0/10000 (63%)\n",
            "log_softmax: Loss: 1.2014\tAccuracy: 6464.0/10000 (65%)\n",
            "\n",
            "Train Epoch: 98 [0/1000 (0%)]\tLosses F.softmax: 1.102089 log_softmax: 0.858225\n",
            "Train Epoch: 98 [200/1000 (20%)]\tLosses F.softmax: 0.546576 log_softmax: 0.565736\n",
            "Train Epoch: 98 [400/1000 (40%)]\tLosses F.softmax: 0.462140 log_softmax: 0.414695\n",
            "Train Epoch: 98 [600/1000 (60%)]\tLosses F.softmax: 0.466415 log_softmax: 0.584433\n",
            "Train Epoch: 98 [800/1000 (80%)]\tLosses F.softmax: 0.834211 log_softmax: 0.991808\n",
            "Train Epoch: 98 [1000/1000 (100%)]\tLosses F.softmax: 0.994272 log_softmax: 0.971474\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1870\tAccuracy: 6382.0/10000 (64%)\n",
            "log_softmax: Loss: 1.1862\tAccuracy: 6549.0/10000 (65%)\n",
            "\n",
            "Train Epoch: 99 [0/1000 (0%)]\tLosses F.softmax: 0.714154 log_softmax: 0.732348\n",
            "Train Epoch: 99 [200/1000 (20%)]\tLosses F.softmax: 1.678054 log_softmax: 1.972926\n",
            "Train Epoch: 99 [400/1000 (40%)]\tLosses F.softmax: 1.090324 log_softmax: 0.890187\n",
            "Train Epoch: 99 [600/1000 (60%)]\tLosses F.softmax: 0.309326 log_softmax: 0.429686\n",
            "Train Epoch: 99 [800/1000 (80%)]\tLosses F.softmax: 0.267057 log_softmax: 0.244253\n",
            "Train Epoch: 99 [1000/1000 (100%)]\tLosses F.softmax: 0.390895 log_softmax: 0.289699\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1698\tAccuracy: 6413.0/10000 (64%)\n",
            "log_softmax: Loss: 1.1709\tAccuracy: 6586.0/10000 (66%)\n",
            "\n",
            "Train Epoch: 100 [0/1000 (0%)]\tLosses F.softmax: 1.759342 log_softmax: 1.785158\n",
            "Train Epoch: 100 [200/1000 (20%)]\tLosses F.softmax: 0.358064 log_softmax: 0.408699\n",
            "Train Epoch: 100 [400/1000 (40%)]\tLosses F.softmax: 0.691888 log_softmax: 0.716806\n",
            "Train Epoch: 100 [600/1000 (60%)]\tLosses F.softmax: 0.284385 log_softmax: 0.332236\n",
            "Train Epoch: 100 [800/1000 (80%)]\tLosses F.softmax: 2.210821 log_softmax: 1.899390\n",
            "Train Epoch: 100 [1000/1000 (100%)]\tLosses F.softmax: 0.311538 log_softmax: 0.406288\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1539\tAccuracy: 6436.0/10000 (64%)\n",
            "log_softmax: Loss: 1.1572\tAccuracy: 6616.0/10000 (66%)\n",
            "\n",
            "Train Epoch: 101 [0/1000 (0%)]\tLosses F.softmax: 1.198436 log_softmax: 0.981197\n",
            "Train Epoch: 101 [200/1000 (20%)]\tLosses F.softmax: 0.466502 log_softmax: 0.395779\n",
            "Train Epoch: 101 [400/1000 (40%)]\tLosses F.softmax: 1.406473 log_softmax: 1.756269\n",
            "Train Epoch: 101 [600/1000 (60%)]\tLosses F.softmax: 1.061296 log_softmax: 0.704350\n",
            "Train Epoch: 101 [800/1000 (80%)]\tLosses F.softmax: 1.433518 log_softmax: 1.388854\n",
            "Train Epoch: 101 [1000/1000 (100%)]\tLosses F.softmax: 1.468405 log_softmax: 1.480942\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1379\tAccuracy: 6464.0/10000 (65%)\n",
            "log_softmax: Loss: 1.1434\tAccuracy: 6628.0/10000 (66%)\n",
            "\n",
            "Train Epoch: 102 [0/1000 (0%)]\tLosses F.softmax: 0.626471 log_softmax: 0.792938\n",
            "Train Epoch: 102 [200/1000 (20%)]\tLosses F.softmax: 1.759901 log_softmax: 1.781914\n",
            "Train Epoch: 102 [400/1000 (40%)]\tLosses F.softmax: 1.004261 log_softmax: 1.121042\n",
            "Train Epoch: 102 [600/1000 (60%)]\tLosses F.softmax: 2.055015 log_softmax: 1.697579\n",
            "Train Epoch: 102 [800/1000 (80%)]\tLosses F.softmax: 0.606119 log_softmax: 0.440047\n",
            "Train Epoch: 102 [1000/1000 (100%)]\tLosses F.softmax: 0.334029 log_softmax: 0.291013\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1226\tAccuracy: 6501.0/10000 (65%)\n",
            "log_softmax: Loss: 1.1296\tAccuracy: 6654.0/10000 (67%)\n",
            "\n",
            "Train Epoch: 103 [0/1000 (0%)]\tLosses F.softmax: 0.260600 log_softmax: 0.229997\n",
            "Train Epoch: 103 [200/1000 (20%)]\tLosses F.softmax: 0.643813 log_softmax: 0.759176\n",
            "Train Epoch: 103 [400/1000 (40%)]\tLosses F.softmax: 0.439811 log_softmax: 0.365619\n",
            "Train Epoch: 103 [600/1000 (60%)]\tLosses F.softmax: 0.395632 log_softmax: 0.310033\n",
            "Train Epoch: 103 [800/1000 (80%)]\tLosses F.softmax: 1.083614 log_softmax: 1.317486\n",
            "Train Epoch: 103 [1000/1000 (100%)]\tLosses F.softmax: 2.663242 log_softmax: 2.679391\n",
            "Test set:\n",
            "F.softmax: Loss: 1.1076\tAccuracy: 6548.0/10000 (65%)\n",
            "log_softmax: Loss: 1.1161\tAccuracy: 6716.0/10000 (67%)\n",
            "\n",
            "Train Epoch: 104 [0/1000 (0%)]\tLosses F.softmax: 1.086848 log_softmax: 0.975509\n",
            "Train Epoch: 104 [200/1000 (20%)]\tLosses F.softmax: 0.352758 log_softmax: 0.449061\n",
            "Train Epoch: 104 [400/1000 (40%)]\tLosses F.softmax: 1.634485 log_softmax: 1.917472\n",
            "Train Epoch: 104 [600/1000 (60%)]\tLosses F.softmax: 0.505111 log_softmax: 0.548025\n",
            "Train Epoch: 104 [800/1000 (80%)]\tLosses F.softmax: 0.252558 log_softmax: 0.208561\n",
            "Train Epoch: 104 [1000/1000 (100%)]\tLosses F.softmax: 0.902420 log_softmax: 0.707141\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0920\tAccuracy: 6645.0/10000 (66%)\n",
            "log_softmax: Loss: 1.1024\tAccuracy: 6776.0/10000 (68%)\n",
            "\n",
            "Train Epoch: 105 [0/1000 (0%)]\tLosses F.softmax: 1.868403 log_softmax: 1.885847\n",
            "Train Epoch: 105 [200/1000 (20%)]\tLosses F.softmax: 0.850395 log_softmax: 1.088606\n",
            "Train Epoch: 105 [400/1000 (40%)]\tLosses F.softmax: 0.395741 log_softmax: 0.346584\n",
            "Train Epoch: 105 [600/1000 (60%)]\tLosses F.softmax: 0.809833 log_softmax: 0.599661\n",
            "Train Epoch: 105 [800/1000 (80%)]\tLosses F.softmax: 1.547044 log_softmax: 1.598467\n",
            "Train Epoch: 105 [1000/1000 (100%)]\tLosses F.softmax: 1.454095 log_softmax: 1.365729\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0780\tAccuracy: 6694.0/10000 (67%)\n",
            "log_softmax: Loss: 1.0892\tAccuracy: 6828.0/10000 (68%)\n",
            "\n",
            "Train Epoch: 106 [0/1000 (0%)]\tLosses F.softmax: 1.738880 log_softmax: 1.770979\n",
            "Train Epoch: 106 [200/1000 (20%)]\tLosses F.softmax: 0.238261 log_softmax: 0.299521\n",
            "Train Epoch: 106 [400/1000 (40%)]\tLosses F.softmax: 0.609751 log_softmax: 0.639591\n",
            "Train Epoch: 106 [600/1000 (60%)]\tLosses F.softmax: 0.468321 log_softmax: 0.494956\n",
            "Train Epoch: 106 [800/1000 (80%)]\tLosses F.softmax: 0.220422 log_softmax: 0.179163\n",
            "Train Epoch: 106 [1000/1000 (100%)]\tLosses F.softmax: 1.739595 log_softmax: 1.386891\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0648\tAccuracy: 6724.0/10000 (67%)\n",
            "log_softmax: Loss: 1.0773\tAccuracy: 6831.0/10000 (68%)\n",
            "\n",
            "Train Epoch: 107 [0/1000 (0%)]\tLosses F.softmax: 1.034081 log_softmax: 1.003366\n",
            "Train Epoch: 107 [200/1000 (20%)]\tLosses F.softmax: 0.292756 log_softmax: 0.374890\n",
            "Train Epoch: 107 [400/1000 (40%)]\tLosses F.softmax: 0.748211 log_softmax: 0.499528\n",
            "Train Epoch: 107 [600/1000 (60%)]\tLosses F.softmax: 0.340280 log_softmax: 0.488336\n",
            "Train Epoch: 107 [800/1000 (80%)]\tLosses F.softmax: 0.269302 log_softmax: 0.278281\n",
            "Train Epoch: 107 [1000/1000 (100%)]\tLosses F.softmax: 0.268445 log_softmax: 0.311884\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0525\tAccuracy: 6714.0/10000 (67%)\n",
            "log_softmax: Loss: 1.0652\tAccuracy: 6846.0/10000 (68%)\n",
            "\n",
            "Train Epoch: 108 [0/1000 (0%)]\tLosses F.softmax: 1.126430 log_softmax: 0.925979\n",
            "Train Epoch: 108 [200/1000 (20%)]\tLosses F.softmax: 2.187801 log_softmax: 1.592114\n",
            "Train Epoch: 108 [400/1000 (40%)]\tLosses F.softmax: 0.274280 log_softmax: 0.204172\n",
            "Train Epoch: 108 [600/1000 (60%)]\tLosses F.softmax: 1.990677 log_softmax: 2.632794\n",
            "Train Epoch: 108 [800/1000 (80%)]\tLosses F.softmax: 1.087634 log_softmax: 1.445258\n",
            "Train Epoch: 108 [1000/1000 (100%)]\tLosses F.softmax: 0.323942 log_softmax: 0.321081\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0396\tAccuracy: 6787.0/10000 (68%)\n",
            "log_softmax: Loss: 1.0530\tAccuracy: 6884.0/10000 (69%)\n",
            "\n",
            "Train Epoch: 109 [0/1000 (0%)]\tLosses F.softmax: 1.535772 log_softmax: 1.856749\n",
            "Train Epoch: 109 [200/1000 (20%)]\tLosses F.softmax: 0.901963 log_softmax: 0.722327\n",
            "Train Epoch: 109 [400/1000 (40%)]\tLosses F.softmax: 0.454842 log_softmax: 0.493864\n",
            "Train Epoch: 109 [600/1000 (60%)]\tLosses F.softmax: 0.657620 log_softmax: 0.482864\n",
            "Train Epoch: 109 [800/1000 (80%)]\tLosses F.softmax: 0.861159 log_softmax: 0.625655\n",
            "Train Epoch: 109 [1000/1000 (100%)]\tLosses F.softmax: 0.351910 log_softmax: 0.543499\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0268\tAccuracy: 6854.0/10000 (69%)\n",
            "log_softmax: Loss: 1.0410\tAccuracy: 6935.0/10000 (69%)\n",
            "\n",
            "Train Epoch: 110 [0/1000 (0%)]\tLosses F.softmax: 0.749168 log_softmax: 0.786748\n",
            "Train Epoch: 110 [200/1000 (20%)]\tLosses F.softmax: 0.823906 log_softmax: 0.670678\n",
            "Train Epoch: 110 [400/1000 (40%)]\tLosses F.softmax: 0.935086 log_softmax: 0.822458\n",
            "Train Epoch: 110 [600/1000 (60%)]\tLosses F.softmax: 0.716785 log_softmax: 1.024731\n",
            "Train Epoch: 110 [800/1000 (80%)]\tLosses F.softmax: 0.954672 log_softmax: 1.184622\n",
            "Train Epoch: 110 [1000/1000 (100%)]\tLosses F.softmax: 2.141258 log_softmax: 1.565123\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0152\tAccuracy: 6891.0/10000 (69%)\n",
            "log_softmax: Loss: 1.0293\tAccuracy: 6939.0/10000 (69%)\n",
            "\n",
            "Train Epoch: 111 [0/1000 (0%)]\tLosses F.softmax: 1.112207 log_softmax: 1.204858\n",
            "Train Epoch: 111 [200/1000 (20%)]\tLosses F.softmax: 0.525660 log_softmax: 0.687259\n",
            "Train Epoch: 111 [400/1000 (40%)]\tLosses F.softmax: 3.795326 log_softmax: 4.290601\n",
            "Train Epoch: 111 [600/1000 (60%)]\tLosses F.softmax: 0.220220 log_softmax: 0.240649\n",
            "Train Epoch: 111 [800/1000 (80%)]\tLosses F.softmax: 0.949346 log_softmax: 0.998648\n",
            "Train Epoch: 111 [1000/1000 (100%)]\tLosses F.softmax: 2.162941 log_softmax: 3.428970\n",
            "Test set:\n",
            "F.softmax: Loss: 1.0046\tAccuracy: 6911.0/10000 (69%)\n",
            "log_softmax: Loss: 1.0192\tAccuracy: 6956.0/10000 (70%)\n",
            "\n",
            "Train Epoch: 112 [0/1000 (0%)]\tLosses F.softmax: 0.916515 log_softmax: 0.921936\n",
            "Train Epoch: 112 [200/1000 (20%)]\tLosses F.softmax: 0.737722 log_softmax: 0.587243\n",
            "Train Epoch: 112 [400/1000 (40%)]\tLosses F.softmax: 1.037624 log_softmax: 0.729561\n",
            "Train Epoch: 112 [600/1000 (60%)]\tLosses F.softmax: 0.278033 log_softmax: 0.346450\n",
            "Train Epoch: 112 [800/1000 (80%)]\tLosses F.softmax: 0.837314 log_softmax: 0.659463\n",
            "Train Epoch: 112 [1000/1000 (100%)]\tLosses F.softmax: 3.840786 log_softmax: 2.203966\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9925\tAccuracy: 6977.0/10000 (70%)\n",
            "log_softmax: Loss: 1.0073\tAccuracy: 7016.0/10000 (70%)\n",
            "\n",
            "Train Epoch: 113 [0/1000 (0%)]\tLosses F.softmax: 0.459606 log_softmax: 0.409096\n",
            "Train Epoch: 113 [200/1000 (20%)]\tLosses F.softmax: 1.181132 log_softmax: 1.527689\n",
            "Train Epoch: 113 [400/1000 (40%)]\tLosses F.softmax: 0.686940 log_softmax: 0.779327\n",
            "Train Epoch: 113 [600/1000 (60%)]\tLosses F.softmax: 0.304758 log_softmax: 0.357339\n",
            "Train Epoch: 113 [800/1000 (80%)]\tLosses F.softmax: 0.233446 log_softmax: 0.308323\n",
            "Train Epoch: 113 [1000/1000 (100%)]\tLosses F.softmax: 0.379102 log_softmax: 0.255177\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9821\tAccuracy: 6980.0/10000 (70%)\n",
            "log_softmax: Loss: 0.9966\tAccuracy: 7033.0/10000 (70%)\n",
            "\n",
            "Train Epoch: 114 [0/1000 (0%)]\tLosses F.softmax: 0.577530 log_softmax: 0.446546\n",
            "Train Epoch: 114 [200/1000 (20%)]\tLosses F.softmax: 0.160213 log_softmax: 0.150839\n",
            "Train Epoch: 114 [400/1000 (40%)]\tLosses F.softmax: 2.304606 log_softmax: 2.080154\n",
            "Train Epoch: 114 [600/1000 (60%)]\tLosses F.softmax: 1.091259 log_softmax: 0.914314\n",
            "Train Epoch: 114 [800/1000 (80%)]\tLosses F.softmax: 1.359890 log_softmax: 1.373633\n",
            "Train Epoch: 114 [1000/1000 (100%)]\tLosses F.softmax: 0.393115 log_softmax: 0.523608\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9724\tAccuracy: 7042.0/10000 (70%)\n",
            "log_softmax: Loss: 0.9869\tAccuracy: 7078.0/10000 (71%)\n",
            "\n",
            "Train Epoch: 115 [0/1000 (0%)]\tLosses F.softmax: 0.875155 log_softmax: 1.110828\n",
            "Train Epoch: 115 [200/1000 (20%)]\tLosses F.softmax: 1.469544 log_softmax: 1.566410\n",
            "Train Epoch: 115 [400/1000 (40%)]\tLosses F.softmax: 0.217332 log_softmax: 0.204311\n",
            "Train Epoch: 115 [600/1000 (60%)]\tLosses F.softmax: 0.756164 log_softmax: 0.699045\n",
            "Train Epoch: 115 [800/1000 (80%)]\tLosses F.softmax: 0.228913 log_softmax: 0.271945\n",
            "Train Epoch: 115 [1000/1000 (100%)]\tLosses F.softmax: 0.922145 log_softmax: 1.177851\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9608\tAccuracy: 7093.0/10000 (71%)\n",
            "log_softmax: Loss: 0.9753\tAccuracy: 7103.0/10000 (71%)\n",
            "\n",
            "Train Epoch: 116 [0/1000 (0%)]\tLosses F.softmax: 0.372539 log_softmax: 0.484827\n",
            "Train Epoch: 116 [200/1000 (20%)]\tLosses F.softmax: 0.243868 log_softmax: 0.216951\n",
            "Train Epoch: 116 [400/1000 (40%)]\tLosses F.softmax: 0.891373 log_softmax: 0.893920\n",
            "Train Epoch: 116 [600/1000 (60%)]\tLosses F.softmax: 1.230398 log_softmax: 1.188706\n",
            "Train Epoch: 116 [800/1000 (80%)]\tLosses F.softmax: 0.430830 log_softmax: 0.399554\n",
            "Train Epoch: 116 [1000/1000 (100%)]\tLosses F.softmax: 0.483556 log_softmax: 0.580514\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9513\tAccuracy: 7106.0/10000 (71%)\n",
            "log_softmax: Loss: 0.9655\tAccuracy: 7108.0/10000 (71%)\n",
            "\n",
            "Train Epoch: 117 [0/1000 (0%)]\tLosses F.softmax: 0.246044 log_softmax: 0.295356\n",
            "Train Epoch: 117 [200/1000 (20%)]\tLosses F.softmax: 0.344032 log_softmax: 0.421930\n",
            "Train Epoch: 117 [400/1000 (40%)]\tLosses F.softmax: 1.099422 log_softmax: 1.026867\n",
            "Train Epoch: 117 [600/1000 (60%)]\tLosses F.softmax: 0.235629 log_softmax: 0.313516\n",
            "Train Epoch: 117 [800/1000 (80%)]\tLosses F.softmax: 1.197311 log_softmax: 1.162997\n",
            "Train Epoch: 117 [1000/1000 (100%)]\tLosses F.softmax: 0.634530 log_softmax: 0.743864\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9425\tAccuracy: 7121.0/10000 (71%)\n",
            "log_softmax: Loss: 0.9564\tAccuracy: 7135.0/10000 (71%)\n",
            "\n",
            "Train Epoch: 118 [0/1000 (0%)]\tLosses F.softmax: 1.358615 log_softmax: 1.116373\n",
            "Train Epoch: 118 [200/1000 (20%)]\tLosses F.softmax: 1.100173 log_softmax: 1.099537\n",
            "Train Epoch: 118 [400/1000 (40%)]\tLosses F.softmax: 0.244598 log_softmax: 0.222344\n",
            "Train Epoch: 118 [600/1000 (60%)]\tLosses F.softmax: 1.244805 log_softmax: 1.173800\n",
            "Train Epoch: 118 [800/1000 (80%)]\tLosses F.softmax: 0.122677 log_softmax: 0.096462\n",
            "Train Epoch: 118 [1000/1000 (100%)]\tLosses F.softmax: 0.696320 log_softmax: 0.858026\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9335\tAccuracy: 7148.0/10000 (71%)\n",
            "log_softmax: Loss: 0.9467\tAccuracy: 7153.0/10000 (72%)\n",
            "\n",
            "Train Epoch: 119 [0/1000 (0%)]\tLosses F.softmax: 0.625163 log_softmax: 0.508980\n",
            "Train Epoch: 119 [200/1000 (20%)]\tLosses F.softmax: 1.029071 log_softmax: 1.242783\n",
            "Train Epoch: 119 [400/1000 (40%)]\tLosses F.softmax: 1.314457 log_softmax: 1.485132\n",
            "Train Epoch: 119 [600/1000 (60%)]\tLosses F.softmax: 1.267162 log_softmax: 0.972953\n",
            "Train Epoch: 119 [800/1000 (80%)]\tLosses F.softmax: 1.209265 log_softmax: 1.157495\n",
            "Train Epoch: 119 [1000/1000 (100%)]\tLosses F.softmax: 0.959980 log_softmax: 1.104294\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9244\tAccuracy: 7159.0/10000 (72%)\n",
            "log_softmax: Loss: 0.9368\tAccuracy: 7176.0/10000 (72%)\n",
            "\n",
            "Train Epoch: 120 [0/1000 (0%)]\tLosses F.softmax: 0.390812 log_softmax: 0.475748\n",
            "Train Epoch: 120 [200/1000 (20%)]\tLosses F.softmax: 0.367362 log_softmax: 0.332738\n",
            "Train Epoch: 120 [400/1000 (40%)]\tLosses F.softmax: 0.293803 log_softmax: 0.360019\n",
            "Train Epoch: 120 [600/1000 (60%)]\tLosses F.softmax: 0.532111 log_softmax: 0.408894\n",
            "Train Epoch: 120 [800/1000 (80%)]\tLosses F.softmax: 0.228297 log_softmax: 0.245120\n",
            "Train Epoch: 120 [1000/1000 (100%)]\tLosses F.softmax: 0.663999 log_softmax: 0.812629\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9162\tAccuracy: 7197.0/10000 (72%)\n",
            "log_softmax: Loss: 0.9282\tAccuracy: 7208.0/10000 (72%)\n",
            "\n",
            "Train Epoch: 121 [0/1000 (0%)]\tLosses F.softmax: 0.202355 log_softmax: 0.186905\n",
            "Train Epoch: 121 [200/1000 (20%)]\tLosses F.softmax: 0.506454 log_softmax: 0.382858\n",
            "Train Epoch: 121 [400/1000 (40%)]\tLosses F.softmax: 0.956501 log_softmax: 0.898887\n",
            "Train Epoch: 121 [600/1000 (60%)]\tLosses F.softmax: 1.069253 log_softmax: 0.940413\n",
            "Train Epoch: 121 [800/1000 (80%)]\tLosses F.softmax: 5.060540 log_softmax: 5.478099\n",
            "Train Epoch: 121 [1000/1000 (100%)]\tLosses F.softmax: 0.262496 log_softmax: 0.324537\n",
            "Test set:\n",
            "F.softmax: Loss: 0.9077\tAccuracy: 7260.0/10000 (73%)\n",
            "log_softmax: Loss: 0.9195\tAccuracy: 7240.0/10000 (72%)\n",
            "\n",
            "Train Epoch: 122 [0/1000 (0%)]\tLosses F.softmax: 0.496325 log_softmax: 0.619725\n",
            "Train Epoch: 122 [200/1000 (20%)]\tLosses F.softmax: 0.187172 log_softmax: 0.120281\n",
            "Train Epoch: 122 [400/1000 (40%)]\tLosses F.softmax: 0.742125 log_softmax: 1.218066\n",
            "Train Epoch: 122 [600/1000 (60%)]\tLosses F.softmax: 1.245005 log_softmax: 0.988780\n",
            "Train Epoch: 122 [800/1000 (80%)]\tLosses F.softmax: 1.201928 log_softmax: 1.353047\n",
            "Train Epoch: 122 [1000/1000 (100%)]\tLosses F.softmax: 0.262093 log_softmax: 0.295276\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8992\tAccuracy: 7290.0/10000 (73%)\n",
            "log_softmax: Loss: 0.9107\tAccuracy: 7264.0/10000 (73%)\n",
            "\n",
            "Train Epoch: 123 [0/1000 (0%)]\tLosses F.softmax: 0.937059 log_softmax: 0.705414\n",
            "Train Epoch: 123 [200/1000 (20%)]\tLosses F.softmax: 1.183024 log_softmax: 1.221769\n",
            "Train Epoch: 123 [400/1000 (40%)]\tLosses F.softmax: 3.359137 log_softmax: 2.962749\n",
            "Train Epoch: 123 [600/1000 (60%)]\tLosses F.softmax: 1.073070 log_softmax: 0.843282\n",
            "Train Epoch: 123 [800/1000 (80%)]\tLosses F.softmax: 1.048884 log_softmax: 1.001713\n",
            "Train Epoch: 123 [1000/1000 (100%)]\tLosses F.softmax: 1.804868 log_softmax: 1.523540\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8927\tAccuracy: 7266.0/10000 (73%)\n",
            "log_softmax: Loss: 0.9031\tAccuracy: 7260.0/10000 (73%)\n",
            "\n",
            "Train Epoch: 124 [0/1000 (0%)]\tLosses F.softmax: 0.349501 log_softmax: 0.349194\n",
            "Train Epoch: 124 [200/1000 (20%)]\tLosses F.softmax: 0.710832 log_softmax: 0.826096\n",
            "Train Epoch: 124 [400/1000 (40%)]\tLosses F.softmax: 1.372965 log_softmax: 1.227847\n",
            "Train Epoch: 124 [600/1000 (60%)]\tLosses F.softmax: 1.360166 log_softmax: 1.381003\n",
            "Train Epoch: 124 [800/1000 (80%)]\tLosses F.softmax: 0.248059 log_softmax: 0.238061\n",
            "Train Epoch: 124 [1000/1000 (100%)]\tLosses F.softmax: 1.418029 log_softmax: 1.506360\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8847\tAccuracy: 7320.0/10000 (73%)\n",
            "log_softmax: Loss: 0.8948\tAccuracy: 7294.0/10000 (73%)\n",
            "\n",
            "Train Epoch: 125 [0/1000 (0%)]\tLosses F.softmax: 0.342377 log_softmax: 0.354424\n",
            "Train Epoch: 125 [200/1000 (20%)]\tLosses F.softmax: 0.151793 log_softmax: 0.176318\n",
            "Train Epoch: 125 [400/1000 (40%)]\tLosses F.softmax: 0.896210 log_softmax: 0.731203\n",
            "Train Epoch: 125 [600/1000 (60%)]\tLosses F.softmax: 1.220131 log_softmax: 0.863966\n",
            "Train Epoch: 125 [800/1000 (80%)]\tLosses F.softmax: 0.121652 log_softmax: 0.143230\n",
            "Train Epoch: 125 [1000/1000 (100%)]\tLosses F.softmax: 0.444121 log_softmax: 0.416229\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8757\tAccuracy: 7357.0/10000 (74%)\n",
            "log_softmax: Loss: 0.8856\tAccuracy: 7337.0/10000 (73%)\n",
            "\n",
            "Train Epoch: 126 [0/1000 (0%)]\tLosses F.softmax: 0.247962 log_softmax: 0.339649\n",
            "Train Epoch: 126 [200/1000 (20%)]\tLosses F.softmax: 0.132879 log_softmax: 0.149753\n",
            "Train Epoch: 126 [400/1000 (40%)]\tLosses F.softmax: 0.599960 log_softmax: 0.890608\n",
            "Train Epoch: 126 [600/1000 (60%)]\tLosses F.softmax: 0.785199 log_softmax: 0.686821\n",
            "Train Epoch: 126 [800/1000 (80%)]\tLosses F.softmax: 0.201235 log_softmax: 0.184515\n",
            "Train Epoch: 126 [1000/1000 (100%)]\tLosses F.softmax: 0.127342 log_softmax: 0.119716\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8684\tAccuracy: 7363.0/10000 (74%)\n",
            "log_softmax: Loss: 0.8771\tAccuracy: 7338.0/10000 (73%)\n",
            "\n",
            "Train Epoch: 127 [0/1000 (0%)]\tLosses F.softmax: 1.154681 log_softmax: 1.536741\n",
            "Train Epoch: 127 [200/1000 (20%)]\tLosses F.softmax: 0.665812 log_softmax: 0.809449\n",
            "Train Epoch: 127 [400/1000 (40%)]\tLosses F.softmax: 1.089696 log_softmax: 0.968438\n",
            "Train Epoch: 127 [600/1000 (60%)]\tLosses F.softmax: 0.725192 log_softmax: 0.689064\n",
            "Train Epoch: 127 [800/1000 (80%)]\tLosses F.softmax: 0.741204 log_softmax: 0.751200\n",
            "Train Epoch: 127 [1000/1000 (100%)]\tLosses F.softmax: 0.855503 log_softmax: 0.638239\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8614\tAccuracy: 7398.0/10000 (74%)\n",
            "log_softmax: Loss: 0.8698\tAccuracy: 7354.0/10000 (74%)\n",
            "\n",
            "Train Epoch: 128 [0/1000 (0%)]\tLosses F.softmax: 0.758341 log_softmax: 0.659710\n",
            "Train Epoch: 128 [200/1000 (20%)]\tLosses F.softmax: 1.029346 log_softmax: 0.844146\n",
            "Train Epoch: 128 [400/1000 (40%)]\tLosses F.softmax: 1.224429 log_softmax: 1.214651\n",
            "Train Epoch: 128 [600/1000 (60%)]\tLosses F.softmax: 1.385644 log_softmax: 1.669781\n",
            "Train Epoch: 128 [800/1000 (80%)]\tLosses F.softmax: 1.324577 log_softmax: 1.153438\n",
            "Train Epoch: 128 [1000/1000 (100%)]\tLosses F.softmax: 0.164024 log_softmax: 0.131214\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8539\tAccuracy: 7424.0/10000 (74%)\n",
            "log_softmax: Loss: 0.8617\tAccuracy: 7381.0/10000 (74%)\n",
            "\n",
            "Train Epoch: 129 [0/1000 (0%)]\tLosses F.softmax: 0.318748 log_softmax: 0.289417\n",
            "Train Epoch: 129 [200/1000 (20%)]\tLosses F.softmax: 0.757166 log_softmax: 0.887916\n",
            "Train Epoch: 129 [400/1000 (40%)]\tLosses F.softmax: 1.133356 log_softmax: 1.159010\n",
            "Train Epoch: 129 [600/1000 (60%)]\tLosses F.softmax: 0.241158 log_softmax: 0.301789\n",
            "Train Epoch: 129 [800/1000 (80%)]\tLosses F.softmax: 0.879083 log_softmax: 1.008608\n",
            "Train Epoch: 129 [1000/1000 (100%)]\tLosses F.softmax: 0.226535 log_softmax: 0.372407\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8480\tAccuracy: 7443.0/10000 (74%)\n",
            "log_softmax: Loss: 0.8552\tAccuracy: 7398.0/10000 (74%)\n",
            "\n",
            "Train Epoch: 130 [0/1000 (0%)]\tLosses F.softmax: 1.121252 log_softmax: 1.512590\n",
            "Train Epoch: 130 [200/1000 (20%)]\tLosses F.softmax: 0.171031 log_softmax: 0.153591\n",
            "Train Epoch: 130 [400/1000 (40%)]\tLosses F.softmax: 0.289961 log_softmax: 0.299266\n",
            "Train Epoch: 130 [600/1000 (60%)]\tLosses F.softmax: 0.445935 log_softmax: 0.739570\n",
            "Train Epoch: 130 [800/1000 (80%)]\tLosses F.softmax: 0.520540 log_softmax: 0.363016\n",
            "Train Epoch: 130 [1000/1000 (100%)]\tLosses F.softmax: 0.266051 log_softmax: 0.221047\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8394\tAccuracy: 7460.0/10000 (75%)\n",
            "log_softmax: Loss: 0.8458\tAccuracy: 7419.0/10000 (74%)\n",
            "\n",
            "Train Epoch: 131 [0/1000 (0%)]\tLosses F.softmax: 1.094497 log_softmax: 1.192851\n",
            "Train Epoch: 131 [200/1000 (20%)]\tLosses F.softmax: 0.513236 log_softmax: 0.576422\n",
            "Train Epoch: 131 [400/1000 (40%)]\tLosses F.softmax: 0.757309 log_softmax: 0.808063\n",
            "Train Epoch: 131 [600/1000 (60%)]\tLosses F.softmax: 0.133496 log_softmax: 0.150189\n",
            "Train Epoch: 131 [800/1000 (80%)]\tLosses F.softmax: 0.193931 log_softmax: 0.210689\n",
            "Train Epoch: 131 [1000/1000 (100%)]\tLosses F.softmax: 1.023337 log_softmax: 0.957315\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8331\tAccuracy: 7511.0/10000 (75%)\n",
            "log_softmax: Loss: 0.8389\tAccuracy: 7433.0/10000 (74%)\n",
            "\n",
            "Train Epoch: 132 [0/1000 (0%)]\tLosses F.softmax: 1.623070 log_softmax: 1.746985\n",
            "Train Epoch: 132 [200/1000 (20%)]\tLosses F.softmax: 0.445821 log_softmax: 0.455554\n",
            "Train Epoch: 132 [400/1000 (40%)]\tLosses F.softmax: 0.435621 log_softmax: 0.665443\n",
            "Train Epoch: 132 [600/1000 (60%)]\tLosses F.softmax: 1.068178 log_softmax: 0.951600\n",
            "Train Epoch: 132 [800/1000 (80%)]\tLosses F.softmax: 0.925868 log_softmax: 1.228145\n",
            "Train Epoch: 132 [1000/1000 (100%)]\tLosses F.softmax: 0.091170 log_softmax: 0.061125\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8272\tAccuracy: 7511.0/10000 (75%)\n",
            "log_softmax: Loss: 0.8328\tAccuracy: 7452.0/10000 (75%)\n",
            "\n",
            "Train Epoch: 133 [0/1000 (0%)]\tLosses F.softmax: 0.355624 log_softmax: 0.253416\n",
            "Train Epoch: 133 [200/1000 (20%)]\tLosses F.softmax: 0.043303 log_softmax: 0.036159\n",
            "Train Epoch: 133 [400/1000 (40%)]\tLosses F.softmax: 0.684046 log_softmax: 0.746409\n",
            "Train Epoch: 133 [600/1000 (60%)]\tLosses F.softmax: 0.542059 log_softmax: 0.445874\n",
            "Train Epoch: 133 [800/1000 (80%)]\tLosses F.softmax: 0.687358 log_softmax: 0.545399\n",
            "Train Epoch: 133 [1000/1000 (100%)]\tLosses F.softmax: 0.710577 log_softmax: 0.828226\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8211\tAccuracy: 7523.0/10000 (75%)\n",
            "log_softmax: Loss: 0.8262\tAccuracy: 7476.0/10000 (75%)\n",
            "\n",
            "Train Epoch: 134 [0/1000 (0%)]\tLosses F.softmax: 0.487093 log_softmax: 0.410991\n",
            "Train Epoch: 134 [200/1000 (20%)]\tLosses F.softmax: 0.602621 log_softmax: 0.820601\n",
            "Train Epoch: 134 [400/1000 (40%)]\tLosses F.softmax: 0.502459 log_softmax: 0.410331\n",
            "Train Epoch: 134 [600/1000 (60%)]\tLosses F.softmax: 1.701076 log_softmax: 1.674497\n",
            "Train Epoch: 134 [800/1000 (80%)]\tLosses F.softmax: 0.162381 log_softmax: 0.233026\n",
            "Train Epoch: 134 [1000/1000 (100%)]\tLosses F.softmax: 0.433354 log_softmax: 0.332646\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8157\tAccuracy: 7527.0/10000 (75%)\n",
            "log_softmax: Loss: 0.8195\tAccuracy: 7475.0/10000 (75%)\n",
            "\n",
            "Train Epoch: 135 [0/1000 (0%)]\tLosses F.softmax: 0.678505 log_softmax: 0.558849\n",
            "Train Epoch: 135 [200/1000 (20%)]\tLosses F.softmax: 0.913364 log_softmax: 0.976198\n",
            "Train Epoch: 135 [400/1000 (40%)]\tLosses F.softmax: 0.175698 log_softmax: 0.190029\n",
            "Train Epoch: 135 [600/1000 (60%)]\tLosses F.softmax: 0.131123 log_softmax: 0.151116\n",
            "Train Epoch: 135 [800/1000 (80%)]\tLosses F.softmax: 1.276395 log_softmax: 1.183987\n",
            "Train Epoch: 135 [1000/1000 (100%)]\tLosses F.softmax: 0.105017 log_softmax: 0.065921\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8098\tAccuracy: 7544.0/10000 (75%)\n",
            "log_softmax: Loss: 0.8132\tAccuracy: 7511.0/10000 (75%)\n",
            "\n",
            "Train Epoch: 136 [0/1000 (0%)]\tLosses F.softmax: 1.330777 log_softmax: 1.422156\n",
            "Train Epoch: 136 [200/1000 (20%)]\tLosses F.softmax: 0.129800 log_softmax: 0.209832\n",
            "Train Epoch: 136 [400/1000 (40%)]\tLosses F.softmax: 0.250392 log_softmax: 0.296110\n",
            "Train Epoch: 136 [600/1000 (60%)]\tLosses F.softmax: 0.128570 log_softmax: 0.157653\n",
            "Train Epoch: 136 [800/1000 (80%)]\tLosses F.softmax: 1.039701 log_softmax: 1.239298\n",
            "Train Epoch: 136 [1000/1000 (100%)]\tLosses F.softmax: 3.335229 log_softmax: 3.202976\n",
            "Test set:\n",
            "F.softmax: Loss: 0.8030\tAccuracy: 7549.0/10000 (75%)\n",
            "log_softmax: Loss: 0.8058\tAccuracy: 7526.0/10000 (75%)\n",
            "\n",
            "Train Epoch: 137 [0/1000 (0%)]\tLosses F.softmax: 0.104739 log_softmax: 0.114648\n",
            "Train Epoch: 137 [200/1000 (20%)]\tLosses F.softmax: 0.152414 log_softmax: 0.212369\n",
            "Train Epoch: 137 [400/1000 (40%)]\tLosses F.softmax: 0.216260 log_softmax: 0.175561\n",
            "Train Epoch: 137 [600/1000 (60%)]\tLosses F.softmax: 1.879453 log_softmax: 2.058445\n",
            "Train Epoch: 137 [800/1000 (80%)]\tLosses F.softmax: 0.071851 log_softmax: 0.064873\n",
            "Train Epoch: 137 [1000/1000 (100%)]\tLosses F.softmax: 0.176044 log_softmax: 0.233737\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7971\tAccuracy: 7597.0/10000 (76%)\n",
            "log_softmax: Loss: 0.7999\tAccuracy: 7559.0/10000 (76%)\n",
            "\n",
            "Train Epoch: 138 [0/1000 (0%)]\tLosses F.softmax: 0.341007 log_softmax: 0.331719\n",
            "Train Epoch: 138 [200/1000 (20%)]\tLosses F.softmax: 0.661729 log_softmax: 0.686963\n",
            "Train Epoch: 138 [400/1000 (40%)]\tLosses F.softmax: 0.874431 log_softmax: 0.766633\n",
            "Train Epoch: 138 [600/1000 (60%)]\tLosses F.softmax: 0.625847 log_softmax: 0.557592\n",
            "Train Epoch: 138 [800/1000 (80%)]\tLosses F.softmax: 0.144859 log_softmax: 0.135609\n",
            "Train Epoch: 138 [1000/1000 (100%)]\tLosses F.softmax: 0.924742 log_softmax: 0.997683\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7926\tAccuracy: 7585.0/10000 (76%)\n",
            "log_softmax: Loss: 0.7945\tAccuracy: 7544.0/10000 (75%)\n",
            "\n",
            "Train Epoch: 139 [0/1000 (0%)]\tLosses F.softmax: 0.511984 log_softmax: 0.486591\n",
            "Train Epoch: 139 [200/1000 (20%)]\tLosses F.softmax: 0.099680 log_softmax: 0.106034\n",
            "Train Epoch: 139 [400/1000 (40%)]\tLosses F.softmax: 0.068375 log_softmax: 0.052591\n",
            "Train Epoch: 139 [600/1000 (60%)]\tLosses F.softmax: 0.415560 log_softmax: 0.312610\n",
            "Train Epoch: 139 [800/1000 (80%)]\tLosses F.softmax: 1.026598 log_softmax: 1.285960\n",
            "Train Epoch: 139 [1000/1000 (100%)]\tLosses F.softmax: 0.740169 log_softmax: 0.745152\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7854\tAccuracy: 7600.0/10000 (76%)\n",
            "log_softmax: Loss: 0.7865\tAccuracy: 7583.0/10000 (76%)\n",
            "\n",
            "Train Epoch: 140 [0/1000 (0%)]\tLosses F.softmax: 0.572922 log_softmax: 0.485995\n",
            "Train Epoch: 140 [200/1000 (20%)]\tLosses F.softmax: 0.228979 log_softmax: 0.309970\n",
            "Train Epoch: 140 [400/1000 (40%)]\tLosses F.softmax: 0.603776 log_softmax: 0.701339\n",
            "Train Epoch: 140 [600/1000 (60%)]\tLosses F.softmax: 0.068085 log_softmax: 0.079726\n",
            "Train Epoch: 140 [800/1000 (80%)]\tLosses F.softmax: 0.109770 log_softmax: 0.106515\n",
            "Train Epoch: 140 [1000/1000 (100%)]\tLosses F.softmax: 0.683629 log_softmax: 0.726819\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7793\tAccuracy: 7632.0/10000 (76%)\n",
            "log_softmax: Loss: 0.7809\tAccuracy: 7576.0/10000 (76%)\n",
            "\n",
            "Train Epoch: 141 [0/1000 (0%)]\tLosses F.softmax: 0.541742 log_softmax: 0.752576\n",
            "Train Epoch: 141 [200/1000 (20%)]\tLosses F.softmax: 0.550020 log_softmax: 0.563771\n",
            "Train Epoch: 141 [400/1000 (40%)]\tLosses F.softmax: 0.459154 log_softmax: 0.504151\n",
            "Train Epoch: 141 [600/1000 (60%)]\tLosses F.softmax: 0.994002 log_softmax: 1.067476\n",
            "Train Epoch: 141 [800/1000 (80%)]\tLosses F.softmax: 1.447842 log_softmax: 1.558096\n",
            "Train Epoch: 141 [1000/1000 (100%)]\tLosses F.softmax: 0.101578 log_softmax: 0.092124\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7739\tAccuracy: 7636.0/10000 (76%)\n",
            "log_softmax: Loss: 0.7746\tAccuracy: 7615.0/10000 (76%)\n",
            "\n",
            "Train Epoch: 142 [0/1000 (0%)]\tLosses F.softmax: 0.686301 log_softmax: 0.651712\n",
            "Train Epoch: 142 [200/1000 (20%)]\tLosses F.softmax: 0.105611 log_softmax: 0.130718\n",
            "Train Epoch: 142 [400/1000 (40%)]\tLosses F.softmax: 1.086980 log_softmax: 1.008518\n",
            "Train Epoch: 142 [600/1000 (60%)]\tLosses F.softmax: 0.145757 log_softmax: 0.159098\n",
            "Train Epoch: 142 [800/1000 (80%)]\tLosses F.softmax: 1.476952 log_softmax: 1.420132\n",
            "Train Epoch: 142 [1000/1000 (100%)]\tLosses F.softmax: 0.376143 log_softmax: 0.324677\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7695\tAccuracy: 7678.0/10000 (77%)\n",
            "log_softmax: Loss: 0.7698\tAccuracy: 7626.0/10000 (76%)\n",
            "\n",
            "Train Epoch: 143 [0/1000 (0%)]\tLosses F.softmax: 0.719100 log_softmax: 1.026960\n",
            "Train Epoch: 143 [200/1000 (20%)]\tLosses F.softmax: 0.954897 log_softmax: 0.859519\n",
            "Train Epoch: 143 [400/1000 (40%)]\tLosses F.softmax: 0.519767 log_softmax: 0.470775\n",
            "Train Epoch: 143 [600/1000 (60%)]\tLosses F.softmax: 0.371749 log_softmax: 0.337656\n",
            "Train Epoch: 143 [800/1000 (80%)]\tLosses F.softmax: 0.610056 log_softmax: 0.616081\n",
            "Train Epoch: 143 [1000/1000 (100%)]\tLosses F.softmax: 0.150061 log_softmax: 0.140122\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7642\tAccuracy: 7677.0/10000 (77%)\n",
            "log_softmax: Loss: 0.7641\tAccuracy: 7652.0/10000 (77%)\n",
            "\n",
            "Train Epoch: 144 [0/1000 (0%)]\tLosses F.softmax: 0.388662 log_softmax: 0.305456\n",
            "Train Epoch: 144 [200/1000 (20%)]\tLosses F.softmax: 0.976844 log_softmax: 1.064506\n",
            "Train Epoch: 144 [400/1000 (40%)]\tLosses F.softmax: 0.732440 log_softmax: 0.978281\n",
            "Train Epoch: 144 [600/1000 (60%)]\tLosses F.softmax: 0.666155 log_softmax: 0.468818\n",
            "Train Epoch: 144 [800/1000 (80%)]\tLosses F.softmax: 0.517278 log_softmax: 0.589260\n",
            "Train Epoch: 144 [1000/1000 (100%)]\tLosses F.softmax: 0.235035 log_softmax: 0.218564\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7590\tAccuracy: 7702.0/10000 (77%)\n",
            "log_softmax: Loss: 0.7589\tAccuracy: 7660.0/10000 (77%)\n",
            "\n",
            "Train Epoch: 145 [0/1000 (0%)]\tLosses F.softmax: 0.100715 log_softmax: 0.114280\n",
            "Train Epoch: 145 [200/1000 (20%)]\tLosses F.softmax: 0.147363 log_softmax: 0.117176\n",
            "Train Epoch: 145 [400/1000 (40%)]\tLosses F.softmax: 0.755045 log_softmax: 0.816646\n",
            "Train Epoch: 145 [600/1000 (60%)]\tLosses F.softmax: 0.297620 log_softmax: 0.530953\n",
            "Train Epoch: 145 [800/1000 (80%)]\tLosses F.softmax: 0.430751 log_softmax: 0.380925\n",
            "Train Epoch: 145 [1000/1000 (100%)]\tLosses F.softmax: 0.464186 log_softmax: 0.523326\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7533\tAccuracy: 7727.0/10000 (77%)\n",
            "log_softmax: Loss: 0.7525\tAccuracy: 7684.0/10000 (77%)\n",
            "\n",
            "Train Epoch: 146 [0/1000 (0%)]\tLosses F.softmax: 0.444963 log_softmax: 0.394732\n",
            "Train Epoch: 146 [200/1000 (20%)]\tLosses F.softmax: 0.867351 log_softmax: 0.631975\n",
            "Train Epoch: 146 [400/1000 (40%)]\tLosses F.softmax: 0.525461 log_softmax: 0.566579\n",
            "Train Epoch: 146 [600/1000 (60%)]\tLosses F.softmax: 0.435132 log_softmax: 0.664430\n",
            "Train Epoch: 146 [800/1000 (80%)]\tLosses F.softmax: 0.740930 log_softmax: 0.659290\n",
            "Train Epoch: 146 [1000/1000 (100%)]\tLosses F.softmax: 0.983713 log_softmax: 0.990728\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7494\tAccuracy: 7718.0/10000 (77%)\n",
            "log_softmax: Loss: 0.7481\tAccuracy: 7699.0/10000 (77%)\n",
            "\n",
            "Train Epoch: 147 [0/1000 (0%)]\tLosses F.softmax: 1.773396 log_softmax: 1.721382\n",
            "Train Epoch: 147 [200/1000 (20%)]\tLosses F.softmax: 0.410759 log_softmax: 0.564855\n",
            "Train Epoch: 147 [400/1000 (40%)]\tLosses F.softmax: 0.909443 log_softmax: 0.914154\n",
            "Train Epoch: 147 [600/1000 (60%)]\tLosses F.softmax: 0.219625 log_softmax: 0.179476\n",
            "Train Epoch: 147 [800/1000 (80%)]\tLosses F.softmax: 0.750700 log_softmax: 0.708661\n",
            "Train Epoch: 147 [1000/1000 (100%)]\tLosses F.softmax: 0.658185 log_softmax: 0.641612\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7445\tAccuracy: 7757.0/10000 (78%)\n",
            "log_softmax: Loss: 0.7434\tAccuracy: 7711.0/10000 (77%)\n",
            "\n",
            "Train Epoch: 148 [0/1000 (0%)]\tLosses F.softmax: 0.517285 log_softmax: 0.496382\n",
            "Train Epoch: 148 [200/1000 (20%)]\tLosses F.softmax: 0.897968 log_softmax: 0.936264\n",
            "Train Epoch: 148 [400/1000 (40%)]\tLosses F.softmax: 0.282920 log_softmax: 0.200296\n",
            "Train Epoch: 148 [600/1000 (60%)]\tLosses F.softmax: 0.417839 log_softmax: 0.418983\n",
            "Train Epoch: 148 [800/1000 (80%)]\tLosses F.softmax: 0.213066 log_softmax: 0.178420\n",
            "Train Epoch: 148 [1000/1000 (100%)]\tLosses F.softmax: 1.942634 log_softmax: 2.373373\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7393\tAccuracy: 7763.0/10000 (78%)\n",
            "log_softmax: Loss: 0.7371\tAccuracy: 7722.0/10000 (77%)\n",
            "\n",
            "Train Epoch: 149 [0/1000 (0%)]\tLosses F.softmax: 0.854351 log_softmax: 0.762859\n",
            "Train Epoch: 149 [200/1000 (20%)]\tLosses F.softmax: 0.242830 log_softmax: 0.334100\n",
            "Train Epoch: 149 [400/1000 (40%)]\tLosses F.softmax: 0.172499 log_softmax: 0.289329\n",
            "Train Epoch: 149 [600/1000 (60%)]\tLosses F.softmax: 0.572728 log_softmax: 0.759663\n",
            "Train Epoch: 149 [800/1000 (80%)]\tLosses F.softmax: 0.097722 log_softmax: 0.137713\n",
            "Train Epoch: 149 [1000/1000 (100%)]\tLosses F.softmax: 0.039783 log_softmax: 0.035095\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7347\tAccuracy: 7781.0/10000 (78%)\n",
            "log_softmax: Loss: 0.7326\tAccuracy: 7745.0/10000 (77%)\n",
            "\n",
            "Train Epoch: 150 [0/1000 (0%)]\tLosses F.softmax: 0.271897 log_softmax: 0.164743\n",
            "Train Epoch: 150 [200/1000 (20%)]\tLosses F.softmax: 0.896055 log_softmax: 0.900425\n",
            "Train Epoch: 150 [400/1000 (40%)]\tLosses F.softmax: 1.150116 log_softmax: 0.888178\n",
            "Train Epoch: 150 [600/1000 (60%)]\tLosses F.softmax: 0.463440 log_softmax: 0.513007\n",
            "Train Epoch: 150 [800/1000 (80%)]\tLosses F.softmax: 0.715512 log_softmax: 0.691297\n",
            "Train Epoch: 150 [1000/1000 (100%)]\tLosses F.softmax: 0.335017 log_softmax: 0.423740\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7297\tAccuracy: 7790.0/10000 (78%)\n",
            "log_softmax: Loss: 0.7269\tAccuracy: 7753.0/10000 (78%)\n",
            "\n",
            "Train Epoch: 151 [0/1000 (0%)]\tLosses F.softmax: 0.102689 log_softmax: 0.062878\n",
            "Train Epoch: 151 [200/1000 (20%)]\tLosses F.softmax: 0.755095 log_softmax: 0.728283\n",
            "Train Epoch: 151 [400/1000 (40%)]\tLosses F.softmax: 0.607882 log_softmax: 0.637182\n",
            "Train Epoch: 151 [600/1000 (60%)]\tLosses F.softmax: 2.285200 log_softmax: 2.225346\n",
            "Train Epoch: 151 [800/1000 (80%)]\tLosses F.softmax: 0.211105 log_softmax: 0.343832\n",
            "Train Epoch: 151 [1000/1000 (100%)]\tLosses F.softmax: 0.063271 log_softmax: 0.083306\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7252\tAccuracy: 7794.0/10000 (78%)\n",
            "log_softmax: Loss: 0.7221\tAccuracy: 7762.0/10000 (78%)\n",
            "\n",
            "Train Epoch: 152 [0/1000 (0%)]\tLosses F.softmax: 0.053590 log_softmax: 0.050583\n",
            "Train Epoch: 152 [200/1000 (20%)]\tLosses F.softmax: 0.634925 log_softmax: 0.477138\n",
            "Train Epoch: 152 [400/1000 (40%)]\tLosses F.softmax: 0.224860 log_softmax: 0.198791\n",
            "Train Epoch: 152 [600/1000 (60%)]\tLosses F.softmax: 0.055870 log_softmax: 0.068103\n",
            "Train Epoch: 152 [800/1000 (80%)]\tLosses F.softmax: 2.673708 log_softmax: 3.060848\n",
            "Train Epoch: 152 [1000/1000 (100%)]\tLosses F.softmax: 1.468024 log_softmax: 1.327992\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7203\tAccuracy: 7833.0/10000 (78%)\n",
            "log_softmax: Loss: 0.7176\tAccuracy: 7794.0/10000 (78%)\n",
            "\n",
            "Train Epoch: 153 [0/1000 (0%)]\tLosses F.softmax: 0.624700 log_softmax: 0.720259\n",
            "Train Epoch: 153 [200/1000 (20%)]\tLosses F.softmax: 0.122556 log_softmax: 0.148760\n",
            "Train Epoch: 153 [400/1000 (40%)]\tLosses F.softmax: 0.056968 log_softmax: 0.078457\n",
            "Train Epoch: 153 [600/1000 (60%)]\tLosses F.softmax: 0.409095 log_softmax: 0.273501\n",
            "Train Epoch: 153 [800/1000 (80%)]\tLosses F.softmax: 0.026976 log_softmax: 0.017311\n",
            "Train Epoch: 153 [1000/1000 (100%)]\tLosses F.softmax: 0.527632 log_softmax: 0.503496\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7159\tAccuracy: 7839.0/10000 (78%)\n",
            "log_softmax: Loss: 0.7123\tAccuracy: 7798.0/10000 (78%)\n",
            "\n",
            "Train Epoch: 154 [0/1000 (0%)]\tLosses F.softmax: 1.004997 log_softmax: 0.919571\n",
            "Train Epoch: 154 [200/1000 (20%)]\tLosses F.softmax: 0.902920 log_softmax: 0.744903\n",
            "Train Epoch: 154 [400/1000 (40%)]\tLosses F.softmax: 0.588252 log_softmax: 0.835613\n",
            "Train Epoch: 154 [600/1000 (60%)]\tLosses F.softmax: 0.394607 log_softmax: 0.279885\n",
            "Train Epoch: 154 [800/1000 (80%)]\tLosses F.softmax: 0.322337 log_softmax: 0.382864\n",
            "Train Epoch: 154 [1000/1000 (100%)]\tLosses F.softmax: 0.303119 log_softmax: 0.250645\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7113\tAccuracy: 7848.0/10000 (78%)\n",
            "log_softmax: Loss: 0.7075\tAccuracy: 7796.0/10000 (78%)\n",
            "\n",
            "Train Epoch: 155 [0/1000 (0%)]\tLosses F.softmax: 0.312246 log_softmax: 0.400170\n",
            "Train Epoch: 155 [200/1000 (20%)]\tLosses F.softmax: 0.528562 log_softmax: 0.329314\n",
            "Train Epoch: 155 [400/1000 (40%)]\tLosses F.softmax: 0.211180 log_softmax: 0.139328\n",
            "Train Epoch: 155 [600/1000 (60%)]\tLosses F.softmax: 1.457295 log_softmax: 1.364387\n",
            "Train Epoch: 155 [800/1000 (80%)]\tLosses F.softmax: 0.076300 log_softmax: 0.081554\n",
            "Train Epoch: 155 [1000/1000 (100%)]\tLosses F.softmax: 0.583362 log_softmax: 0.653490\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7072\tAccuracy: 7870.0/10000 (79%)\n",
            "log_softmax: Loss: 0.7032\tAccuracy: 7815.0/10000 (78%)\n",
            "\n",
            "Train Epoch: 156 [0/1000 (0%)]\tLosses F.softmax: 0.345824 log_softmax: 0.356411\n",
            "Train Epoch: 156 [200/1000 (20%)]\tLosses F.softmax: 0.324364 log_softmax: 0.276326\n",
            "Train Epoch: 156 [400/1000 (40%)]\tLosses F.softmax: 0.087652 log_softmax: 0.062823\n",
            "Train Epoch: 156 [600/1000 (60%)]\tLosses F.softmax: 0.064940 log_softmax: 0.093210\n",
            "Train Epoch: 156 [800/1000 (80%)]\tLosses F.softmax: 0.072074 log_softmax: 0.052277\n",
            "Train Epoch: 156 [1000/1000 (100%)]\tLosses F.softmax: 0.603278 log_softmax: 0.455500\n",
            "Test set:\n",
            "F.softmax: Loss: 0.7027\tAccuracy: 7885.0/10000 (79%)\n",
            "log_softmax: Loss: 0.6986\tAccuracy: 7834.0/10000 (78%)\n",
            "\n",
            "Train Epoch: 157 [0/1000 (0%)]\tLosses F.softmax: 0.980782 log_softmax: 1.195263\n",
            "Train Epoch: 157 [200/1000 (20%)]\tLosses F.softmax: 0.047565 log_softmax: 0.064474\n",
            "Train Epoch: 157 [400/1000 (40%)]\tLosses F.softmax: 0.135409 log_softmax: 0.133084\n",
            "Train Epoch: 157 [600/1000 (60%)]\tLosses F.softmax: 0.532897 log_softmax: 0.388201\n",
            "Train Epoch: 157 [800/1000 (80%)]\tLosses F.softmax: 0.731436 log_softmax: 0.657054\n",
            "Train Epoch: 157 [1000/1000 (100%)]\tLosses F.softmax: 0.155583 log_softmax: 0.141493\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6990\tAccuracy: 7894.0/10000 (79%)\n",
            "log_softmax: Loss: 0.6938\tAccuracy: 7846.0/10000 (78%)\n",
            "\n",
            "Train Epoch: 158 [0/1000 (0%)]\tLosses F.softmax: 0.831069 log_softmax: 0.806219\n",
            "Train Epoch: 158 [200/1000 (20%)]\tLosses F.softmax: 0.113494 log_softmax: 0.232726\n",
            "Train Epoch: 158 [400/1000 (40%)]\tLosses F.softmax: 0.105104 log_softmax: 0.107337\n",
            "Train Epoch: 158 [600/1000 (60%)]\tLosses F.softmax: 0.123600 log_softmax: 0.114690\n",
            "Train Epoch: 158 [800/1000 (80%)]\tLosses F.softmax: 0.156030 log_softmax: 0.227516\n",
            "Train Epoch: 158 [1000/1000 (100%)]\tLosses F.softmax: 0.486651 log_softmax: 0.446047\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6958\tAccuracy: 7888.0/10000 (79%)\n",
            "log_softmax: Loss: 0.6909\tAccuracy: 7856.0/10000 (79%)\n",
            "\n",
            "Train Epoch: 159 [0/1000 (0%)]\tLosses F.softmax: 0.407918 log_softmax: 0.359662\n",
            "Train Epoch: 159 [200/1000 (20%)]\tLosses F.softmax: 0.163095 log_softmax: 0.301721\n",
            "Train Epoch: 159 [400/1000 (40%)]\tLosses F.softmax: 1.579671 log_softmax: 1.062574\n",
            "Train Epoch: 159 [600/1000 (60%)]\tLosses F.softmax: 0.079163 log_softmax: 0.069118\n",
            "Train Epoch: 159 [800/1000 (80%)]\tLosses F.softmax: 0.357483 log_softmax: 0.319274\n",
            "Train Epoch: 159 [1000/1000 (100%)]\tLosses F.softmax: 0.421454 log_softmax: 0.309712\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6904\tAccuracy: 7900.0/10000 (79%)\n",
            "log_softmax: Loss: 0.6858\tAccuracy: 7861.0/10000 (79%)\n",
            "\n",
            "Train Epoch: 160 [0/1000 (0%)]\tLosses F.softmax: 0.125977 log_softmax: 0.202323\n",
            "Train Epoch: 160 [200/1000 (20%)]\tLosses F.softmax: 3.282414 log_softmax: 3.668394\n",
            "Train Epoch: 160 [400/1000 (40%)]\tLosses F.softmax: 0.390374 log_softmax: 0.390779\n",
            "Train Epoch: 160 [600/1000 (60%)]\tLosses F.softmax: 0.228225 log_softmax: 0.326874\n",
            "Train Epoch: 160 [800/1000 (80%)]\tLosses F.softmax: 0.182283 log_softmax: 0.141873\n",
            "Train Epoch: 160 [1000/1000 (100%)]\tLosses F.softmax: 1.386657 log_softmax: 1.343345\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6860\tAccuracy: 7923.0/10000 (79%)\n",
            "log_softmax: Loss: 0.6807\tAccuracy: 7894.0/10000 (79%)\n",
            "\n",
            "Train Epoch: 161 [0/1000 (0%)]\tLosses F.softmax: 0.174564 log_softmax: 0.187300\n",
            "Train Epoch: 161 [200/1000 (20%)]\tLosses F.softmax: 0.668350 log_softmax: 0.583162\n",
            "Train Epoch: 161 [400/1000 (40%)]\tLosses F.softmax: 0.368338 log_softmax: 0.265204\n",
            "Train Epoch: 161 [600/1000 (60%)]\tLosses F.softmax: 0.318887 log_softmax: 0.399428\n",
            "Train Epoch: 161 [800/1000 (80%)]\tLosses F.softmax: 0.046081 log_softmax: 0.042815\n",
            "Train Epoch: 161 [1000/1000 (100%)]\tLosses F.softmax: 0.484276 log_softmax: 0.299459\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6816\tAccuracy: 7925.0/10000 (79%)\n",
            "log_softmax: Loss: 0.6763\tAccuracy: 7886.0/10000 (79%)\n",
            "\n",
            "Train Epoch: 162 [0/1000 (0%)]\tLosses F.softmax: 0.064423 log_softmax: 0.047242\n",
            "Train Epoch: 162 [200/1000 (20%)]\tLosses F.softmax: 0.837237 log_softmax: 1.177425\n",
            "Train Epoch: 162 [400/1000 (40%)]\tLosses F.softmax: 2.473236 log_softmax: 1.632208\n",
            "Train Epoch: 162 [600/1000 (60%)]\tLosses F.softmax: 0.307671 log_softmax: 0.303119\n",
            "Train Epoch: 162 [800/1000 (80%)]\tLosses F.softmax: 0.115710 log_softmax: 0.112024\n",
            "Train Epoch: 162 [1000/1000 (100%)]\tLosses F.softmax: 0.082444 log_softmax: 0.115474\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6784\tAccuracy: 7929.0/10000 (79%)\n",
            "log_softmax: Loss: 0.6730\tAccuracy: 7896.0/10000 (79%)\n",
            "\n",
            "Train Epoch: 163 [0/1000 (0%)]\tLosses F.softmax: 0.047341 log_softmax: 0.068013\n",
            "Train Epoch: 163 [200/1000 (20%)]\tLosses F.softmax: 0.992180 log_softmax: 1.207164\n",
            "Train Epoch: 163 [400/1000 (40%)]\tLosses F.softmax: 0.529923 log_softmax: 0.502680\n",
            "Train Epoch: 163 [600/1000 (60%)]\tLosses F.softmax: 2.417726 log_softmax: 1.540870\n",
            "Train Epoch: 163 [800/1000 (80%)]\tLosses F.softmax: 0.032432 log_softmax: 0.028420\n",
            "Train Epoch: 163 [1000/1000 (100%)]\tLosses F.softmax: 1.842331 log_softmax: 1.889178\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6742\tAccuracy: 7942.0/10000 (79%)\n",
            "log_softmax: Loss: 0.6681\tAccuracy: 7911.0/10000 (79%)\n",
            "\n",
            "Train Epoch: 164 [0/1000 (0%)]\tLosses F.softmax: 0.538523 log_softmax: 0.605453\n",
            "Train Epoch: 164 [200/1000 (20%)]\tLosses F.softmax: 0.574312 log_softmax: 0.577975\n",
            "Train Epoch: 164 [400/1000 (40%)]\tLosses F.softmax: 0.177552 log_softmax: 0.175006\n",
            "Train Epoch: 164 [600/1000 (60%)]\tLosses F.softmax: 0.108581 log_softmax: 0.104647\n",
            "Train Epoch: 164 [800/1000 (80%)]\tLosses F.softmax: 0.029649 log_softmax: 0.041698\n",
            "Train Epoch: 164 [1000/1000 (100%)]\tLosses F.softmax: 0.324231 log_softmax: 0.312795\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6708\tAccuracy: 7958.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6650\tAccuracy: 7913.0/10000 (79%)\n",
            "\n",
            "Train Epoch: 165 [0/1000 (0%)]\tLosses F.softmax: 1.295383 log_softmax: 0.994457\n",
            "Train Epoch: 165 [200/1000 (20%)]\tLosses F.softmax: 0.397210 log_softmax: 0.374826\n",
            "Train Epoch: 165 [400/1000 (40%)]\tLosses F.softmax: 0.186881 log_softmax: 0.112990\n",
            "Train Epoch: 165 [600/1000 (60%)]\tLosses F.softmax: 0.068300 log_softmax: 0.068559\n",
            "Train Epoch: 165 [800/1000 (80%)]\tLosses F.softmax: 0.038783 log_softmax: 0.050007\n",
            "Train Epoch: 165 [1000/1000 (100%)]\tLosses F.softmax: 0.757018 log_softmax: 0.425032\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6667\tAccuracy: 7978.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6611\tAccuracy: 7932.0/10000 (79%)\n",
            "\n",
            "Train Epoch: 166 [0/1000 (0%)]\tLosses F.softmax: 0.942000 log_softmax: 1.084736\n",
            "Train Epoch: 166 [200/1000 (20%)]\tLosses F.softmax: 0.215975 log_softmax: 0.187800\n",
            "Train Epoch: 166 [400/1000 (40%)]\tLosses F.softmax: 0.881865 log_softmax: 0.737410\n",
            "Train Epoch: 166 [600/1000 (60%)]\tLosses F.softmax: 0.050722 log_softmax: 0.043850\n",
            "Train Epoch: 166 [800/1000 (80%)]\tLosses F.softmax: 0.260285 log_softmax: 0.181573\n",
            "Train Epoch: 166 [1000/1000 (100%)]\tLosses F.softmax: 0.051312 log_softmax: 0.056194\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6635\tAccuracy: 7985.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6573\tAccuracy: 7951.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 167 [0/1000 (0%)]\tLosses F.softmax: 0.613520 log_softmax: 0.906941\n",
            "Train Epoch: 167 [200/1000 (20%)]\tLosses F.softmax: 0.279147 log_softmax: 0.314057\n",
            "Train Epoch: 167 [400/1000 (40%)]\tLosses F.softmax: 0.456268 log_softmax: 0.433860\n",
            "Train Epoch: 167 [600/1000 (60%)]\tLosses F.softmax: 0.117627 log_softmax: 0.243075\n",
            "Train Epoch: 167 [800/1000 (80%)]\tLosses F.softmax: 0.022750 log_softmax: 0.049822\n",
            "Train Epoch: 167 [1000/1000 (100%)]\tLosses F.softmax: 0.111334 log_softmax: 0.141116\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6598\tAccuracy: 8006.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6537\tAccuracy: 7971.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 168 [0/1000 (0%)]\tLosses F.softmax: 0.155354 log_softmax: 0.365999\n",
            "Train Epoch: 168 [200/1000 (20%)]\tLosses F.softmax: 0.188882 log_softmax: 0.198439\n",
            "Train Epoch: 168 [400/1000 (40%)]\tLosses F.softmax: 0.120137 log_softmax: 0.110926\n",
            "Train Epoch: 168 [600/1000 (60%)]\tLosses F.softmax: 0.608592 log_softmax: 0.335115\n",
            "Train Epoch: 168 [800/1000 (80%)]\tLosses F.softmax: 0.488905 log_softmax: 0.271427\n",
            "Train Epoch: 168 [1000/1000 (100%)]\tLosses F.softmax: 0.102937 log_softmax: 0.056116\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6573\tAccuracy: 7996.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6511\tAccuracy: 7964.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 169 [0/1000 (0%)]\tLosses F.softmax: 0.779323 log_softmax: 0.735973\n",
            "Train Epoch: 169 [200/1000 (20%)]\tLosses F.softmax: 0.129062 log_softmax: 0.148749\n",
            "Train Epoch: 169 [400/1000 (40%)]\tLosses F.softmax: 0.087781 log_softmax: 0.171214\n",
            "Train Epoch: 169 [600/1000 (60%)]\tLosses F.softmax: 0.736441 log_softmax: 0.642800\n",
            "Train Epoch: 169 [800/1000 (80%)]\tLosses F.softmax: 0.409237 log_softmax: 0.412405\n",
            "Train Epoch: 169 [1000/1000 (100%)]\tLosses F.softmax: 0.062970 log_softmax: 0.065658\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6532\tAccuracy: 8003.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6466\tAccuracy: 7965.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 170 [0/1000 (0%)]\tLosses F.softmax: 0.736355 log_softmax: 0.653647\n",
            "Train Epoch: 170 [200/1000 (20%)]\tLosses F.softmax: 0.134070 log_softmax: 0.102300\n",
            "Train Epoch: 170 [400/1000 (40%)]\tLosses F.softmax: 1.061426 log_softmax: 0.686863\n",
            "Train Epoch: 170 [600/1000 (60%)]\tLosses F.softmax: 0.108512 log_softmax: 0.169605\n",
            "Train Epoch: 170 [800/1000 (80%)]\tLosses F.softmax: 0.024597 log_softmax: 0.038208\n",
            "Train Epoch: 170 [1000/1000 (100%)]\tLosses F.softmax: 0.634420 log_softmax: 0.522562\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6478\tAccuracy: 8044.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6413\tAccuracy: 8017.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 171 [0/1000 (0%)]\tLosses F.softmax: 0.170288 log_softmax: 0.129818\n",
            "Train Epoch: 171 [200/1000 (20%)]\tLosses F.softmax: 0.092416 log_softmax: 0.101367\n",
            "Train Epoch: 171 [400/1000 (40%)]\tLosses F.softmax: 0.362059 log_softmax: 0.612698\n",
            "Train Epoch: 171 [600/1000 (60%)]\tLosses F.softmax: 0.096215 log_softmax: 0.060404\n",
            "Train Epoch: 171 [800/1000 (80%)]\tLosses F.softmax: 0.628242 log_softmax: 0.499422\n",
            "Train Epoch: 171 [1000/1000 (100%)]\tLosses F.softmax: 0.082304 log_softmax: 0.095176\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6466\tAccuracy: 8040.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6399\tAccuracy: 8016.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 172 [0/1000 (0%)]\tLosses F.softmax: 0.058975 log_softmax: 0.068354\n",
            "Train Epoch: 172 [200/1000 (20%)]\tLosses F.softmax: 0.049229 log_softmax: 0.067689\n",
            "Train Epoch: 172 [400/1000 (40%)]\tLosses F.softmax: 2.454007 log_softmax: 2.470119\n",
            "Train Epoch: 172 [600/1000 (60%)]\tLosses F.softmax: 0.096163 log_softmax: 0.152317\n",
            "Train Epoch: 172 [800/1000 (80%)]\tLosses F.softmax: 0.280543 log_softmax: 0.313200\n",
            "Train Epoch: 172 [1000/1000 (100%)]\tLosses F.softmax: 0.088899 log_softmax: 0.101241\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6423\tAccuracy: 8045.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6355\tAccuracy: 8021.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 173 [0/1000 (0%)]\tLosses F.softmax: 1.197041 log_softmax: 2.014566\n",
            "Train Epoch: 173 [200/1000 (20%)]\tLosses F.softmax: 0.070846 log_softmax: 0.044607\n",
            "Train Epoch: 173 [400/1000 (40%)]\tLosses F.softmax: 0.752488 log_softmax: 0.703878\n",
            "Train Epoch: 173 [600/1000 (60%)]\tLosses F.softmax: 0.068780 log_softmax: 0.043121\n",
            "Train Epoch: 173 [800/1000 (80%)]\tLosses F.softmax: 0.738553 log_softmax: 0.879924\n",
            "Train Epoch: 173 [1000/1000 (100%)]\tLosses F.softmax: 0.099424 log_softmax: 0.145956\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6399\tAccuracy: 8046.0/10000 (80%)\n",
            "log_softmax: Loss: 0.6326\tAccuracy: 8021.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 174 [0/1000 (0%)]\tLosses F.softmax: 0.738415 log_softmax: 0.654337\n",
            "Train Epoch: 174 [200/1000 (20%)]\tLosses F.softmax: 0.312137 log_softmax: 0.411418\n",
            "Train Epoch: 174 [400/1000 (40%)]\tLosses F.softmax: 0.388659 log_softmax: 0.318424\n",
            "Train Epoch: 174 [600/1000 (60%)]\tLosses F.softmax: 1.410969 log_softmax: 1.128235\n",
            "Train Epoch: 174 [800/1000 (80%)]\tLosses F.softmax: 0.138988 log_softmax: 0.065189\n",
            "Train Epoch: 174 [1000/1000 (100%)]\tLosses F.softmax: 0.073954 log_softmax: 0.163716\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6368\tAccuracy: 8065.0/10000 (81%)\n",
            "log_softmax: Loss: 0.6292\tAccuracy: 8050.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 175 [0/1000 (0%)]\tLosses F.softmax: 0.058598 log_softmax: 0.034432\n",
            "Train Epoch: 175 [200/1000 (20%)]\tLosses F.softmax: 0.222034 log_softmax: 0.182773\n",
            "Train Epoch: 175 [400/1000 (40%)]\tLosses F.softmax: 0.570495 log_softmax: 0.677775\n",
            "Train Epoch: 175 [600/1000 (60%)]\tLosses F.softmax: 0.939710 log_softmax: 0.950203\n",
            "Train Epoch: 175 [800/1000 (80%)]\tLosses F.softmax: 0.120709 log_softmax: 0.135004\n",
            "Train Epoch: 175 [1000/1000 (100%)]\tLosses F.softmax: 0.055955 log_softmax: 0.034483\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6346\tAccuracy: 8071.0/10000 (81%)\n",
            "log_softmax: Loss: 0.6270\tAccuracy: 8055.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 176 [0/1000 (0%)]\tLosses F.softmax: 0.135305 log_softmax: 0.107922\n",
            "Train Epoch: 176 [200/1000 (20%)]\tLosses F.softmax: 1.464173 log_softmax: 1.785782\n",
            "Train Epoch: 176 [400/1000 (40%)]\tLosses F.softmax: 0.276841 log_softmax: 0.211982\n",
            "Train Epoch: 176 [600/1000 (60%)]\tLosses F.softmax: 0.139280 log_softmax: 0.132214\n",
            "Train Epoch: 176 [800/1000 (80%)]\tLosses F.softmax: 0.039506 log_softmax: 0.047183\n",
            "Train Epoch: 176 [1000/1000 (100%)]\tLosses F.softmax: 0.634368 log_softmax: 0.499770\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6319\tAccuracy: 8075.0/10000 (81%)\n",
            "log_softmax: Loss: 0.6245\tAccuracy: 8048.0/10000 (80%)\n",
            "\n",
            "Train Epoch: 177 [0/1000 (0%)]\tLosses F.softmax: 1.949444 log_softmax: 1.965605\n",
            "Train Epoch: 177 [200/1000 (20%)]\tLosses F.softmax: 0.369236 log_softmax: 0.327977\n",
            "Train Epoch: 177 [400/1000 (40%)]\tLosses F.softmax: 0.022156 log_softmax: 0.019842\n",
            "Train Epoch: 177 [600/1000 (60%)]\tLosses F.softmax: 0.542608 log_softmax: 0.515251\n",
            "Train Epoch: 177 [800/1000 (80%)]\tLosses F.softmax: 0.153184 log_softmax: 0.275499\n",
            "Train Epoch: 177 [1000/1000 (100%)]\tLosses F.softmax: 0.059662 log_softmax: 0.037073\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6269\tAccuracy: 8085.0/10000 (81%)\n",
            "log_softmax: Loss: 0.6194\tAccuracy: 8093.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 178 [0/1000 (0%)]\tLosses F.softmax: 2.160933 log_softmax: 1.229290\n",
            "Train Epoch: 178 [200/1000 (20%)]\tLosses F.softmax: 0.389268 log_softmax: 0.312467\n",
            "Train Epoch: 178 [400/1000 (40%)]\tLosses F.softmax: 0.098181 log_softmax: 0.097131\n",
            "Train Epoch: 178 [600/1000 (60%)]\tLosses F.softmax: 0.235468 log_softmax: 0.300031\n",
            "Train Epoch: 178 [800/1000 (80%)]\tLosses F.softmax: 0.708301 log_softmax: 0.864856\n",
            "Train Epoch: 178 [1000/1000 (100%)]\tLosses F.softmax: 0.345664 log_softmax: 0.293562\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6254\tAccuracy: 8084.0/10000 (81%)\n",
            "log_softmax: Loss: 0.6183\tAccuracy: 8081.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 179 [0/1000 (0%)]\tLosses F.softmax: 1.692682 log_softmax: 1.600351\n",
            "Train Epoch: 179 [200/1000 (20%)]\tLosses F.softmax: 1.963161 log_softmax: 1.871833\n",
            "Train Epoch: 179 [400/1000 (40%)]\tLosses F.softmax: 0.620870 log_softmax: 0.702612\n",
            "Train Epoch: 179 [600/1000 (60%)]\tLosses F.softmax: 0.423233 log_softmax: 0.340540\n",
            "Train Epoch: 179 [800/1000 (80%)]\tLosses F.softmax: 0.144216 log_softmax: 0.141024\n",
            "Train Epoch: 179 [1000/1000 (100%)]\tLosses F.softmax: 0.263879 log_softmax: 0.452911\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6230\tAccuracy: 8086.0/10000 (81%)\n",
            "log_softmax: Loss: 0.6157\tAccuracy: 8088.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 180 [0/1000 (0%)]\tLosses F.softmax: 0.332904 log_softmax: 0.313226\n",
            "Train Epoch: 180 [200/1000 (20%)]\tLosses F.softmax: 0.075283 log_softmax: 0.082906\n",
            "Train Epoch: 180 [400/1000 (40%)]\tLosses F.softmax: 0.889779 log_softmax: 1.017816\n",
            "Train Epoch: 180 [600/1000 (60%)]\tLosses F.softmax: 0.022490 log_softmax: 0.038976\n",
            "Train Epoch: 180 [800/1000 (80%)]\tLosses F.softmax: 0.152619 log_softmax: 0.112051\n",
            "Train Epoch: 180 [1000/1000 (100%)]\tLosses F.softmax: 0.105653 log_softmax: 0.073819\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6193\tAccuracy: 8109.0/10000 (81%)\n",
            "log_softmax: Loss: 0.6117\tAccuracy: 8106.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 181 [0/1000 (0%)]\tLosses F.softmax: 0.078636 log_softmax: 0.078052\n",
            "Train Epoch: 181 [200/1000 (20%)]\tLosses F.softmax: 0.650715 log_softmax: 0.714799\n",
            "Train Epoch: 181 [400/1000 (40%)]\tLosses F.softmax: 2.534438 log_softmax: 2.873120\n",
            "Train Epoch: 181 [600/1000 (60%)]\tLosses F.softmax: 0.141531 log_softmax: 0.103104\n",
            "Train Epoch: 181 [800/1000 (80%)]\tLosses F.softmax: 0.011917 log_softmax: 0.008381\n",
            "Train Epoch: 181 [1000/1000 (100%)]\tLosses F.softmax: 0.608313 log_softmax: 0.818007\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6155\tAccuracy: 8131.0/10000 (81%)\n",
            "log_softmax: Loss: 0.6073\tAccuracy: 8139.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 182 [0/1000 (0%)]\tLosses F.softmax: 0.447862 log_softmax: 0.466760\n",
            "Train Epoch: 182 [200/1000 (20%)]\tLosses F.softmax: 0.681676 log_softmax: 0.714731\n",
            "Train Epoch: 182 [400/1000 (40%)]\tLosses F.softmax: 1.170094 log_softmax: 0.883593\n",
            "Train Epoch: 182 [600/1000 (60%)]\tLosses F.softmax: 0.112421 log_softmax: 0.105784\n",
            "Train Epoch: 182 [800/1000 (80%)]\tLosses F.softmax: 0.103054 log_softmax: 0.130775\n",
            "Train Epoch: 182 [1000/1000 (100%)]\tLosses F.softmax: 0.792943 log_softmax: 0.528218\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6140\tAccuracy: 8134.0/10000 (81%)\n",
            "log_softmax: Loss: 0.6061\tAccuracy: 8134.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 183 [0/1000 (0%)]\tLosses F.softmax: 0.021767 log_softmax: 0.020647\n",
            "Train Epoch: 183 [200/1000 (20%)]\tLosses F.softmax: 0.601959 log_softmax: 0.943954\n",
            "Train Epoch: 183 [400/1000 (40%)]\tLosses F.softmax: 0.160114 log_softmax: 0.205417\n",
            "Train Epoch: 183 [600/1000 (60%)]\tLosses F.softmax: 0.027838 log_softmax: 0.035005\n",
            "Train Epoch: 183 [800/1000 (80%)]\tLosses F.softmax: 0.510128 log_softmax: 0.409481\n",
            "Train Epoch: 183 [1000/1000 (100%)]\tLosses F.softmax: 0.172668 log_softmax: 0.278750\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6118\tAccuracy: 8125.0/10000 (81%)\n",
            "log_softmax: Loss: 0.6039\tAccuracy: 8120.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 184 [0/1000 (0%)]\tLosses F.softmax: 0.443991 log_softmax: 0.434426\n",
            "Train Epoch: 184 [200/1000 (20%)]\tLosses F.softmax: 0.161473 log_softmax: 0.231308\n",
            "Train Epoch: 184 [400/1000 (40%)]\tLosses F.softmax: 1.659530 log_softmax: 1.754946\n",
            "Train Epoch: 184 [600/1000 (60%)]\tLosses F.softmax: 0.060176 log_softmax: 0.067354\n",
            "Train Epoch: 184 [800/1000 (80%)]\tLosses F.softmax: 0.028394 log_softmax: 0.030909\n",
            "Train Epoch: 184 [1000/1000 (100%)]\tLosses F.softmax: 0.507933 log_softmax: 0.446490\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6081\tAccuracy: 8141.0/10000 (81%)\n",
            "log_softmax: Loss: 0.6002\tAccuracy: 8146.0/10000 (81%)\n",
            "\n",
            "Train Epoch: 185 [0/1000 (0%)]\tLosses F.softmax: 0.028732 log_softmax: 0.044432\n",
            "Train Epoch: 185 [200/1000 (20%)]\tLosses F.softmax: 0.112225 log_softmax: 0.103137\n",
            "Train Epoch: 185 [400/1000 (40%)]\tLosses F.softmax: 1.058352 log_softmax: 0.805068\n",
            "Train Epoch: 185 [600/1000 (60%)]\tLosses F.softmax: 0.277625 log_softmax: 0.372075\n",
            "Train Epoch: 185 [800/1000 (80%)]\tLosses F.softmax: 0.056924 log_softmax: 0.061958\n",
            "Train Epoch: 185 [1000/1000 (100%)]\tLosses F.softmax: 0.096940 log_softmax: 0.112118\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6058\tAccuracy: 8146.0/10000 (81%)\n",
            "log_softmax: Loss: 0.5974\tAccuracy: 8159.0/10000 (82%)\n",
            "\n",
            "Train Epoch: 186 [0/1000 (0%)]\tLosses F.softmax: 0.133509 log_softmax: 0.122547\n",
            "Train Epoch: 186 [200/1000 (20%)]\tLosses F.softmax: 0.123045 log_softmax: 0.098623\n",
            "Train Epoch: 186 [400/1000 (40%)]\tLosses F.softmax: 0.074640 log_softmax: 0.117905\n",
            "Train Epoch: 186 [600/1000 (60%)]\tLosses F.softmax: 0.057632 log_softmax: 0.086550\n",
            "Train Epoch: 186 [800/1000 (80%)]\tLosses F.softmax: 0.679059 log_softmax: 0.529876\n",
            "Train Epoch: 186 [1000/1000 (100%)]\tLosses F.softmax: 1.830925 log_softmax: 1.941120\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6036\tAccuracy: 8154.0/10000 (82%)\n",
            "log_softmax: Loss: 0.5954\tAccuracy: 8176.0/10000 (82%)\n",
            "\n",
            "Train Epoch: 187 [0/1000 (0%)]\tLosses F.softmax: 0.151007 log_softmax: 0.106152\n",
            "Train Epoch: 187 [200/1000 (20%)]\tLosses F.softmax: 0.038771 log_softmax: 0.046079\n",
            "Train Epoch: 187 [400/1000 (40%)]\tLosses F.softmax: 0.411296 log_softmax: 0.366921\n",
            "Train Epoch: 187 [600/1000 (60%)]\tLosses F.softmax: 0.051230 log_softmax: 0.065339\n",
            "Train Epoch: 187 [800/1000 (80%)]\tLosses F.softmax: 0.451532 log_softmax: 0.341839\n",
            "Train Epoch: 187 [1000/1000 (100%)]\tLosses F.softmax: 0.246842 log_softmax: 0.256476\n",
            "Test set:\n",
            "F.softmax: Loss: 0.6004\tAccuracy: 8165.0/10000 (82%)\n",
            "log_softmax: Loss: 0.5923\tAccuracy: 8166.0/10000 (82%)\n",
            "\n",
            "Train Epoch: 188 [0/1000 (0%)]\tLosses F.softmax: 0.039521 log_softmax: 0.094304\n",
            "Train Epoch: 188 [200/1000 (20%)]\tLosses F.softmax: 0.190251 log_softmax: 0.251042\n",
            "Train Epoch: 188 [400/1000 (40%)]\tLosses F.softmax: 0.848819 log_softmax: 0.739565\n",
            "Train Epoch: 188 [600/1000 (60%)]\tLosses F.softmax: 0.045214 log_softmax: 0.129746\n",
            "Train Epoch: 188 [800/1000 (80%)]\tLosses F.softmax: 0.573308 log_softmax: 0.390072\n",
            "Train Epoch: 188 [1000/1000 (100%)]\tLosses F.softmax: 0.024985 log_softmax: 0.039735\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5987\tAccuracy: 8171.0/10000 (82%)\n",
            "log_softmax: Loss: 0.5903\tAccuracy: 8173.0/10000 (82%)\n",
            "\n",
            "Train Epoch: 189 [0/1000 (0%)]\tLosses F.softmax: 0.190186 log_softmax: 0.173547\n",
            "Train Epoch: 189 [200/1000 (20%)]\tLosses F.softmax: 0.294438 log_softmax: 0.255677\n",
            "Train Epoch: 189 [400/1000 (40%)]\tLosses F.softmax: 0.155742 log_softmax: 0.363327\n",
            "Train Epoch: 189 [600/1000 (60%)]\tLosses F.softmax: 2.255480 log_softmax: 2.690164\n",
            "Train Epoch: 189 [800/1000 (80%)]\tLosses F.softmax: 0.352345 log_softmax: 0.492254\n",
            "Train Epoch: 189 [1000/1000 (100%)]\tLosses F.softmax: 1.013966 log_softmax: 1.336561\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5973\tAccuracy: 8168.0/10000 (82%)\n",
            "log_softmax: Loss: 0.5893\tAccuracy: 8159.0/10000 (82%)\n",
            "\n",
            "Train Epoch: 190 [0/1000 (0%)]\tLosses F.softmax: 0.565191 log_softmax: 0.315688\n",
            "Train Epoch: 190 [200/1000 (20%)]\tLosses F.softmax: 0.249413 log_softmax: 0.134239\n",
            "Train Epoch: 190 [400/1000 (40%)]\tLosses F.softmax: 0.105958 log_softmax: 0.142517\n",
            "Train Epoch: 190 [600/1000 (60%)]\tLosses F.softmax: 0.348199 log_softmax: 0.478077\n",
            "Train Epoch: 190 [800/1000 (80%)]\tLosses F.softmax: 0.141573 log_softmax: 0.074663\n",
            "Train Epoch: 190 [1000/1000 (100%)]\tLosses F.softmax: 0.293051 log_softmax: 0.651204\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5937\tAccuracy: 8179.0/10000 (82%)\n",
            "log_softmax: Loss: 0.5856\tAccuracy: 8176.0/10000 (82%)\n",
            "\n",
            "Train Epoch: 191 [0/1000 (0%)]\tLosses F.softmax: 0.603094 log_softmax: 0.645648\n",
            "Train Epoch: 191 [200/1000 (20%)]\tLosses F.softmax: 0.180602 log_softmax: 0.108100\n",
            "Train Epoch: 191 [400/1000 (40%)]\tLosses F.softmax: 0.018427 log_softmax: 0.029313\n",
            "Train Epoch: 191 [600/1000 (60%)]\tLosses F.softmax: 0.144734 log_softmax: 0.132540\n",
            "Train Epoch: 191 [800/1000 (80%)]\tLosses F.softmax: 0.869407 log_softmax: 1.022890\n",
            "Train Epoch: 191 [1000/1000 (100%)]\tLosses F.softmax: 0.247591 log_softmax: 0.195796\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5922\tAccuracy: 8195.0/10000 (82%)\n",
            "log_softmax: Loss: 0.5837\tAccuracy: 8208.0/10000 (82%)\n",
            "\n",
            "Train Epoch: 192 [0/1000 (0%)]\tLosses F.softmax: 0.630847 log_softmax: 0.247523\n",
            "Train Epoch: 192 [200/1000 (20%)]\tLosses F.softmax: 0.054124 log_softmax: 0.077078\n",
            "Train Epoch: 192 [400/1000 (40%)]\tLosses F.softmax: 0.150659 log_softmax: 0.229111\n",
            "Train Epoch: 192 [600/1000 (60%)]\tLosses F.softmax: 0.032052 log_softmax: 0.044551\n",
            "Train Epoch: 192 [800/1000 (80%)]\tLosses F.softmax: 2.264379 log_softmax: 1.296774\n",
            "Train Epoch: 192 [1000/1000 (100%)]\tLosses F.softmax: 1.324814 log_softmax: 1.258457\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5912\tAccuracy: 8185.0/10000 (82%)\n",
            "log_softmax: Loss: 0.5827\tAccuracy: 8213.0/10000 (82%)\n",
            "\n",
            "Train Epoch: 193 [0/1000 (0%)]\tLosses F.softmax: 0.897216 log_softmax: 1.363002\n",
            "Train Epoch: 193 [200/1000 (20%)]\tLosses F.softmax: 0.039965 log_softmax: 0.065808\n",
            "Train Epoch: 193 [400/1000 (40%)]\tLosses F.softmax: 0.174554 log_softmax: 0.145787\n",
            "Train Epoch: 193 [600/1000 (60%)]\tLosses F.softmax: 0.198055 log_softmax: 0.232983\n",
            "Train Epoch: 193 [800/1000 (80%)]\tLosses F.softmax: 0.099505 log_softmax: 0.093719\n",
            "Train Epoch: 193 [1000/1000 (100%)]\tLosses F.softmax: 0.038293 log_softmax: 0.119882\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5881\tAccuracy: 8214.0/10000 (82%)\n",
            "log_softmax: Loss: 0.5794\tAccuracy: 8187.0/10000 (82%)\n",
            "\n",
            "Train Epoch: 194 [0/1000 (0%)]\tLosses F.softmax: 3.457420 log_softmax: 3.737833\n",
            "Train Epoch: 194 [200/1000 (20%)]\tLosses F.softmax: 0.020418 log_softmax: 0.040570\n",
            "Train Epoch: 194 [400/1000 (40%)]\tLosses F.softmax: 2.343024 log_softmax: 2.051187\n",
            "Train Epoch: 194 [600/1000 (60%)]\tLosses F.softmax: 0.373716 log_softmax: 0.202590\n",
            "Train Epoch: 194 [800/1000 (80%)]\tLosses F.softmax: 0.396238 log_softmax: 0.339395\n",
            "Train Epoch: 194 [1000/1000 (100%)]\tLosses F.softmax: 0.023165 log_softmax: 0.017506\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5864\tAccuracy: 8212.0/10000 (82%)\n",
            "log_softmax: Loss: 0.5778\tAccuracy: 8231.0/10000 (82%)\n",
            "\n",
            "Train Epoch: 195 [0/1000 (0%)]\tLosses F.softmax: 0.045306 log_softmax: 0.054408\n",
            "Train Epoch: 195 [200/1000 (20%)]\tLosses F.softmax: 0.223950 log_softmax: 0.239826\n",
            "Train Epoch: 195 [400/1000 (40%)]\tLosses F.softmax: 0.622543 log_softmax: 0.610714\n",
            "Train Epoch: 195 [600/1000 (60%)]\tLosses F.softmax: 0.274252 log_softmax: 0.217456\n",
            "Train Epoch: 195 [800/1000 (80%)]\tLosses F.softmax: 1.426267 log_softmax: 1.465422\n",
            "Train Epoch: 195 [1000/1000 (100%)]\tLosses F.softmax: 0.147631 log_softmax: 0.162838\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5840\tAccuracy: 8215.0/10000 (82%)\n",
            "log_softmax: Loss: 0.5755\tAccuracy: 8244.0/10000 (82%)\n",
            "\n",
            "Train Epoch: 196 [0/1000 (0%)]\tLosses F.softmax: 0.022849 log_softmax: 0.020245\n",
            "Train Epoch: 196 [200/1000 (20%)]\tLosses F.softmax: 0.070046 log_softmax: 0.068344\n",
            "Train Epoch: 196 [400/1000 (40%)]\tLosses F.softmax: 0.027433 log_softmax: 0.037863\n",
            "Train Epoch: 196 [600/1000 (60%)]\tLosses F.softmax: 0.010594 log_softmax: 0.012459\n",
            "Train Epoch: 196 [800/1000 (80%)]\tLosses F.softmax: 1.521016 log_softmax: 1.996272\n",
            "Train Epoch: 196 [1000/1000 (100%)]\tLosses F.softmax: 0.794202 log_softmax: 0.753946\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5817\tAccuracy: 8219.0/10000 (82%)\n",
            "log_softmax: Loss: 0.5732\tAccuracy: 8230.0/10000 (82%)\n",
            "\n",
            "Train Epoch: 197 [0/1000 (0%)]\tLosses F.softmax: 0.143610 log_softmax: 0.215709\n",
            "Train Epoch: 197 [200/1000 (20%)]\tLosses F.softmax: 0.908810 log_softmax: 0.895558\n",
            "Train Epoch: 197 [400/1000 (40%)]\tLosses F.softmax: 0.410001 log_softmax: 0.564275\n",
            "Train Epoch: 197 [600/1000 (60%)]\tLosses F.softmax: 0.351468 log_softmax: 0.294721\n",
            "Train Epoch: 197 [800/1000 (80%)]\tLosses F.softmax: 0.525893 log_softmax: 0.253655\n",
            "Train Epoch: 197 [1000/1000 (100%)]\tLosses F.softmax: 0.073275 log_softmax: 0.049853\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5802\tAccuracy: 8226.0/10000 (82%)\n",
            "log_softmax: Loss: 0.5714\tAccuracy: 8235.0/10000 (82%)\n",
            "\n",
            "Train Epoch: 198 [0/1000 (0%)]\tLosses F.softmax: 0.121890 log_softmax: 0.145399\n",
            "Train Epoch: 198 [200/1000 (20%)]\tLosses F.softmax: 0.450864 log_softmax: 0.166098\n",
            "Train Epoch: 198 [400/1000 (40%)]\tLosses F.softmax: 0.642821 log_softmax: 0.527997\n",
            "Train Epoch: 198 [600/1000 (60%)]\tLosses F.softmax: 0.070956 log_softmax: 0.065282\n",
            "Train Epoch: 198 [800/1000 (80%)]\tLosses F.softmax: 0.068729 log_softmax: 0.078680\n",
            "Train Epoch: 198 [1000/1000 (100%)]\tLosses F.softmax: 0.826889 log_softmax: 0.842437\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5799\tAccuracy: 8231.0/10000 (82%)\n",
            "log_softmax: Loss: 0.5714\tAccuracy: 8234.0/10000 (82%)\n",
            "\n",
            "Train Epoch: 199 [0/1000 (0%)]\tLosses F.softmax: 1.247642 log_softmax: 1.281421\n",
            "Train Epoch: 199 [200/1000 (20%)]\tLosses F.softmax: 0.070497 log_softmax: 0.049902\n",
            "Train Epoch: 199 [400/1000 (40%)]\tLosses F.softmax: 0.057813 log_softmax: 0.048730\n",
            "Train Epoch: 199 [600/1000 (60%)]\tLosses F.softmax: 0.142105 log_softmax: 0.092205\n",
            "Train Epoch: 199 [800/1000 (80%)]\tLosses F.softmax: 0.076610 log_softmax: 0.098649\n",
            "Train Epoch: 199 [1000/1000 (100%)]\tLosses F.softmax: 0.126785 log_softmax: 0.078756\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5770\tAccuracy: 8245.0/10000 (82%)\n",
            "log_softmax: Loss: 0.5685\tAccuracy: 8245.0/10000 (82%)\n",
            "\n",
            "Train Epoch: 200 [0/1000 (0%)]\tLosses F.softmax: 0.013890 log_softmax: 0.014152\n",
            "Train Epoch: 200 [200/1000 (20%)]\tLosses F.softmax: 0.840771 log_softmax: 0.985073\n",
            "Train Epoch: 200 [400/1000 (40%)]\tLosses F.softmax: 0.384847 log_softmax: 0.163515\n",
            "Train Epoch: 200 [600/1000 (60%)]\tLosses F.softmax: 0.080202 log_softmax: 0.071633\n",
            "Train Epoch: 200 [800/1000 (80%)]\tLosses F.softmax: 0.033085 log_softmax: 0.021477\n",
            "Train Epoch: 200 [1000/1000 (100%)]\tLosses F.softmax: 0.074994 log_softmax: 0.064159\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5748\tAccuracy: 8254.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5660\tAccuracy: 8257.0/10000 (83%)\n",
            "\n",
            "Train Epoch: 201 [0/1000 (0%)]\tLosses F.softmax: 0.034197 log_softmax: 0.030093\n",
            "Train Epoch: 201 [200/1000 (20%)]\tLosses F.softmax: 0.572122 log_softmax: 0.399856\n",
            "Train Epoch: 201 [400/1000 (40%)]\tLosses F.softmax: 0.018087 log_softmax: 0.027733\n",
            "Train Epoch: 201 [600/1000 (60%)]\tLosses F.softmax: 0.052043 log_softmax: 0.051551\n",
            "Train Epoch: 201 [800/1000 (80%)]\tLosses F.softmax: 0.146947 log_softmax: 0.156843\n",
            "Train Epoch: 201 [1000/1000 (100%)]\tLosses F.softmax: 0.017501 log_softmax: 0.018070\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5724\tAccuracy: 8266.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5635\tAccuracy: 8238.0/10000 (82%)\n",
            "\n",
            "Train Epoch: 202 [0/1000 (0%)]\tLosses F.softmax: 0.256508 log_softmax: 0.214399\n",
            "Train Epoch: 202 [200/1000 (20%)]\tLosses F.softmax: 0.034026 log_softmax: 0.025269\n",
            "Train Epoch: 202 [400/1000 (40%)]\tLosses F.softmax: 0.012532 log_softmax: 0.019179\n",
            "Train Epoch: 202 [600/1000 (60%)]\tLosses F.softmax: 0.130255 log_softmax: 0.223860\n",
            "Train Epoch: 202 [800/1000 (80%)]\tLosses F.softmax: 0.985904 log_softmax: 0.686140\n",
            "Train Epoch: 202 [1000/1000 (100%)]\tLosses F.softmax: 0.205700 log_softmax: 0.122558\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5701\tAccuracy: 8258.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5612\tAccuracy: 8278.0/10000 (83%)\n",
            "\n",
            "Train Epoch: 203 [0/1000 (0%)]\tLosses F.softmax: 0.366942 log_softmax: 0.108871\n",
            "Train Epoch: 203 [200/1000 (20%)]\tLosses F.softmax: 0.151146 log_softmax: 0.233433\n",
            "Train Epoch: 203 [400/1000 (40%)]\tLosses F.softmax: 0.022752 log_softmax: 0.045717\n",
            "Train Epoch: 203 [600/1000 (60%)]\tLosses F.softmax: 1.490175 log_softmax: 1.104318\n",
            "Train Epoch: 203 [800/1000 (80%)]\tLosses F.softmax: 0.186523 log_softmax: 0.220672\n",
            "Train Epoch: 203 [1000/1000 (100%)]\tLosses F.softmax: 0.006149 log_softmax: 0.014665\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5692\tAccuracy: 8266.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5601\tAccuracy: 8283.0/10000 (83%)\n",
            "\n",
            "Train Epoch: 204 [0/1000 (0%)]\tLosses F.softmax: 0.064783 log_softmax: 0.052593\n",
            "Train Epoch: 204 [200/1000 (20%)]\tLosses F.softmax: 0.005539 log_softmax: 0.018605\n",
            "Train Epoch: 204 [400/1000 (40%)]\tLosses F.softmax: 0.320519 log_softmax: 0.135297\n",
            "Train Epoch: 204 [600/1000 (60%)]\tLosses F.softmax: 0.043294 log_softmax: 0.050414\n",
            "Train Epoch: 204 [800/1000 (80%)]\tLosses F.softmax: 0.063477 log_softmax: 0.064391\n",
            "Train Epoch: 204 [1000/1000 (100%)]\tLosses F.softmax: 0.277238 log_softmax: 0.349012\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5677\tAccuracy: 8282.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5586\tAccuracy: 8265.0/10000 (83%)\n",
            "\n",
            "Train Epoch: 205 [0/1000 (0%)]\tLosses F.softmax: 0.211933 log_softmax: 0.294765\n",
            "Train Epoch: 205 [200/1000 (20%)]\tLosses F.softmax: 0.023224 log_softmax: 0.047695\n",
            "Train Epoch: 205 [400/1000 (40%)]\tLosses F.softmax: 0.061060 log_softmax: 0.034286\n",
            "Train Epoch: 205 [600/1000 (60%)]\tLosses F.softmax: 0.068959 log_softmax: 0.056036\n",
            "Train Epoch: 205 [800/1000 (80%)]\tLosses F.softmax: 0.274818 log_softmax: 0.292138\n",
            "Train Epoch: 205 [1000/1000 (100%)]\tLosses F.softmax: 0.104859 log_softmax: 0.092874\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5651\tAccuracy: 8268.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5561\tAccuracy: 8290.0/10000 (83%)\n",
            "\n",
            "Train Epoch: 206 [0/1000 (0%)]\tLosses F.softmax: 0.208872 log_softmax: 0.206349\n",
            "Train Epoch: 206 [200/1000 (20%)]\tLosses F.softmax: 0.544515 log_softmax: 1.084025\n",
            "Train Epoch: 206 [400/1000 (40%)]\tLosses F.softmax: 0.057670 log_softmax: 0.050055\n",
            "Train Epoch: 206 [600/1000 (60%)]\tLosses F.softmax: 0.026743 log_softmax: 0.044355\n",
            "Train Epoch: 206 [800/1000 (80%)]\tLosses F.softmax: 0.391962 log_softmax: 0.347289\n",
            "Train Epoch: 206 [1000/1000 (100%)]\tLosses F.softmax: 0.030507 log_softmax: 0.035145\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5648\tAccuracy: 8271.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5557\tAccuracy: 8291.0/10000 (83%)\n",
            "\n",
            "Train Epoch: 207 [0/1000 (0%)]\tLosses F.softmax: 0.196560 log_softmax: 0.118449\n",
            "Train Epoch: 207 [200/1000 (20%)]\tLosses F.softmax: 0.200291 log_softmax: 0.145876\n",
            "Train Epoch: 207 [400/1000 (40%)]\tLosses F.softmax: 0.028512 log_softmax: 0.034606\n",
            "Train Epoch: 207 [600/1000 (60%)]\tLosses F.softmax: 0.811745 log_softmax: 1.043770\n",
            "Train Epoch: 207 [800/1000 (80%)]\tLosses F.softmax: 0.008100 log_softmax: 0.021685\n",
            "Train Epoch: 207 [1000/1000 (100%)]\tLosses F.softmax: 0.232824 log_softmax: 0.157946\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5641\tAccuracy: 8267.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5549\tAccuracy: 8291.0/10000 (83%)\n",
            "\n",
            "Train Epoch: 208 [0/1000 (0%)]\tLosses F.softmax: 0.559319 log_softmax: 0.634439\n",
            "Train Epoch: 208 [200/1000 (20%)]\tLosses F.softmax: 0.755816 log_softmax: 0.571534\n",
            "Train Epoch: 208 [400/1000 (40%)]\tLosses F.softmax: 0.007462 log_softmax: 0.020355\n",
            "Train Epoch: 208 [600/1000 (60%)]\tLosses F.softmax: 0.022133 log_softmax: 0.014745\n",
            "Train Epoch: 208 [800/1000 (80%)]\tLosses F.softmax: 0.015761 log_softmax: 0.009405\n",
            "Train Epoch: 208 [1000/1000 (100%)]\tLosses F.softmax: 0.110165 log_softmax: 0.065141\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5620\tAccuracy: 8278.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5528\tAccuracy: 8300.0/10000 (83%)\n",
            "\n",
            "Train Epoch: 209 [0/1000 (0%)]\tLosses F.softmax: 0.062254 log_softmax: 0.061844\n",
            "Train Epoch: 209 [200/1000 (20%)]\tLosses F.softmax: 0.066865 log_softmax: 0.105743\n",
            "Train Epoch: 209 [400/1000 (40%)]\tLosses F.softmax: 0.029585 log_softmax: 0.033184\n",
            "Train Epoch: 209 [600/1000 (60%)]\tLosses F.softmax: 0.108130 log_softmax: 0.080527\n",
            "Train Epoch: 209 [800/1000 (80%)]\tLosses F.softmax: 0.237068 log_softmax: 0.252109\n",
            "Train Epoch: 209 [1000/1000 (100%)]\tLosses F.softmax: 0.752501 log_softmax: 0.791514\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5611\tAccuracy: 8296.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5517\tAccuracy: 8291.0/10000 (83%)\n",
            "\n",
            "Train Epoch: 210 [0/1000 (0%)]\tLosses F.softmax: 0.054299 log_softmax: 0.035598\n",
            "Train Epoch: 210 [200/1000 (20%)]\tLosses F.softmax: 0.037428 log_softmax: 0.041978\n",
            "Train Epoch: 210 [400/1000 (40%)]\tLosses F.softmax: 0.174252 log_softmax: 0.198110\n",
            "Train Epoch: 210 [600/1000 (60%)]\tLosses F.softmax: 0.681088 log_softmax: 0.528955\n",
            "Train Epoch: 210 [800/1000 (80%)]\tLosses F.softmax: 0.565038 log_softmax: 0.572163\n",
            "Train Epoch: 210 [1000/1000 (100%)]\tLosses F.softmax: 0.251552 log_softmax: 0.222864\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5592\tAccuracy: 8278.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5501\tAccuracy: 8309.0/10000 (83%)\n",
            "\n",
            "Train Epoch: 211 [0/1000 (0%)]\tLosses F.softmax: 0.014934 log_softmax: 0.016010\n",
            "Train Epoch: 211 [200/1000 (20%)]\tLosses F.softmax: 0.518457 log_softmax: 1.239233\n",
            "Train Epoch: 211 [400/1000 (40%)]\tLosses F.softmax: 0.744471 log_softmax: 0.597337\n",
            "Train Epoch: 211 [600/1000 (60%)]\tLosses F.softmax: 0.023681 log_softmax: 0.027407\n",
            "Train Epoch: 211 [800/1000 (80%)]\tLosses F.softmax: 0.127822 log_softmax: 0.074219\n",
            "Train Epoch: 211 [1000/1000 (100%)]\tLosses F.softmax: 0.084897 log_softmax: 0.075329\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5579\tAccuracy: 8288.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5487\tAccuracy: 8291.0/10000 (83%)\n",
            "\n",
            "Train Epoch: 212 [0/1000 (0%)]\tLosses F.softmax: 0.087062 log_softmax: 0.102946\n",
            "Train Epoch: 212 [200/1000 (20%)]\tLosses F.softmax: 0.033154 log_softmax: 0.040311\n",
            "Train Epoch: 212 [400/1000 (40%)]\tLosses F.softmax: 0.478030 log_softmax: 0.341311\n",
            "Train Epoch: 212 [600/1000 (60%)]\tLosses F.softmax: 0.258466 log_softmax: 0.267299\n",
            "Train Epoch: 212 [800/1000 (80%)]\tLosses F.softmax: 0.015370 log_softmax: 0.015503\n",
            "Train Epoch: 212 [1000/1000 (100%)]\tLosses F.softmax: 0.068179 log_softmax: 0.079937\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5556\tAccuracy: 8294.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5467\tAccuracy: 8296.0/10000 (83%)\n",
            "\n",
            "Train Epoch: 213 [0/1000 (0%)]\tLosses F.softmax: 0.283719 log_softmax: 0.222077\n",
            "Train Epoch: 213 [200/1000 (20%)]\tLosses F.softmax: 0.019603 log_softmax: 0.018139\n",
            "Train Epoch: 213 [400/1000 (40%)]\tLosses F.softmax: 0.314511 log_softmax: 0.357684\n",
            "Train Epoch: 213 [600/1000 (60%)]\tLosses F.softmax: 0.022605 log_softmax: 0.021920\n",
            "Train Epoch: 213 [800/1000 (80%)]\tLosses F.softmax: 0.050590 log_softmax: 0.048950\n",
            "Train Epoch: 213 [1000/1000 (100%)]\tLosses F.softmax: 0.076230 log_softmax: 0.056438\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5551\tAccuracy: 8300.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5458\tAccuracy: 8295.0/10000 (83%)\n",
            "\n",
            "Train Epoch: 214 [0/1000 (0%)]\tLosses F.softmax: 0.009629 log_softmax: 0.014888\n",
            "Train Epoch: 214 [200/1000 (20%)]\tLosses F.softmax: 0.148568 log_softmax: 0.102002\n",
            "Train Epoch: 214 [400/1000 (40%)]\tLosses F.softmax: 0.047910 log_softmax: 0.051265\n",
            "Train Epoch: 214 [600/1000 (60%)]\tLosses F.softmax: 0.026586 log_softmax: 0.025354\n",
            "Train Epoch: 214 [800/1000 (80%)]\tLosses F.softmax: 0.124960 log_softmax: 0.091438\n",
            "Train Epoch: 214 [1000/1000 (100%)]\tLosses F.softmax: 0.195203 log_softmax: 0.460243\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5548\tAccuracy: 8297.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5456\tAccuracy: 8299.0/10000 (83%)\n",
            "\n",
            "Train Epoch: 215 [0/1000 (0%)]\tLosses F.softmax: 0.007572 log_softmax: 0.009923\n",
            "Train Epoch: 215 [200/1000 (20%)]\tLosses F.softmax: 2.873102 log_softmax: 4.073834\n",
            "Train Epoch: 215 [400/1000 (40%)]\tLosses F.softmax: 0.053955 log_softmax: 0.104859\n",
            "Train Epoch: 215 [600/1000 (60%)]\tLosses F.softmax: 0.031949 log_softmax: 0.037075\n",
            "Train Epoch: 215 [800/1000 (80%)]\tLosses F.softmax: 0.484974 log_softmax: 0.476424\n",
            "Train Epoch: 215 [1000/1000 (100%)]\tLosses F.softmax: 2.966354 log_softmax: 2.842375\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5524\tAccuracy: 8302.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5429\tAccuracy: 8302.0/10000 (83%)\n",
            "\n",
            "Train Epoch: 216 [0/1000 (0%)]\tLosses F.softmax: 0.679229 log_softmax: 0.397496\n",
            "Train Epoch: 216 [200/1000 (20%)]\tLosses F.softmax: 0.030367 log_softmax: 0.037457\n",
            "Train Epoch: 216 [400/1000 (40%)]\tLosses F.softmax: 0.030519 log_softmax: 0.029750\n",
            "Train Epoch: 216 [600/1000 (60%)]\tLosses F.softmax: 1.439375 log_softmax: 1.213705\n",
            "Train Epoch: 216 [800/1000 (80%)]\tLosses F.softmax: 0.019322 log_softmax: 0.033374\n",
            "Train Epoch: 216 [1000/1000 (100%)]\tLosses F.softmax: 0.118671 log_softmax: 0.161166\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5510\tAccuracy: 8308.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5415\tAccuracy: 8319.0/10000 (83%)\n",
            "\n",
            "Train Epoch: 217 [0/1000 (0%)]\tLosses F.softmax: 0.017282 log_softmax: 0.047754\n",
            "Train Epoch: 217 [200/1000 (20%)]\tLosses F.softmax: 0.020594 log_softmax: 0.018839\n",
            "Train Epoch: 217 [400/1000 (40%)]\tLosses F.softmax: 0.044476 log_softmax: 0.040970\n",
            "Train Epoch: 217 [600/1000 (60%)]\tLosses F.softmax: 0.321947 log_softmax: 0.314792\n",
            "Train Epoch: 217 [800/1000 (80%)]\tLosses F.softmax: 0.321168 log_softmax: 0.339316\n",
            "Train Epoch: 217 [1000/1000 (100%)]\tLosses F.softmax: 0.535439 log_softmax: 0.498003\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5490\tAccuracy: 8305.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5395\tAccuracy: 8322.0/10000 (83%)\n",
            "\n",
            "Train Epoch: 218 [0/1000 (0%)]\tLosses F.softmax: 0.089670 log_softmax: 0.131245\n",
            "Train Epoch: 218 [200/1000 (20%)]\tLosses F.softmax: 0.138930 log_softmax: 0.162120\n",
            "Train Epoch: 218 [400/1000 (40%)]\tLosses F.softmax: 0.444720 log_softmax: 0.338457\n",
            "Train Epoch: 218 [600/1000 (60%)]\tLosses F.softmax: 0.974428 log_softmax: 1.023839\n",
            "Train Epoch: 218 [800/1000 (80%)]\tLosses F.softmax: 0.015687 log_softmax: 0.074616\n",
            "Train Epoch: 218 [1000/1000 (100%)]\tLosses F.softmax: 2.387277 log_softmax: 1.765512\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5488\tAccuracy: 8304.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5400\tAccuracy: 8315.0/10000 (83%)\n",
            "\n",
            "Train Epoch: 219 [0/1000 (0%)]\tLosses F.softmax: 0.176390 log_softmax: 0.150919\n",
            "Train Epoch: 219 [200/1000 (20%)]\tLosses F.softmax: 0.329189 log_softmax: 0.331273\n",
            "Train Epoch: 219 [400/1000 (40%)]\tLosses F.softmax: 0.008745 log_softmax: 0.009908\n",
            "Train Epoch: 219 [600/1000 (60%)]\tLosses F.softmax: 4.712240 log_softmax: 4.484411\n",
            "Train Epoch: 219 [800/1000 (80%)]\tLosses F.softmax: 0.030990 log_softmax: 0.035365\n",
            "Train Epoch: 219 [1000/1000 (100%)]\tLosses F.softmax: 0.068740 log_softmax: 0.080962\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5482\tAccuracy: 8301.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5391\tAccuracy: 8331.0/10000 (83%)\n",
            "\n",
            "Train Epoch: 220 [0/1000 (0%)]\tLosses F.softmax: 1.289307 log_softmax: 1.388680\n",
            "Train Epoch: 220 [200/1000 (20%)]\tLosses F.softmax: 0.735243 log_softmax: 0.425224\n",
            "Train Epoch: 220 [400/1000 (40%)]\tLosses F.softmax: 0.014125 log_softmax: 0.030365\n",
            "Train Epoch: 220 [600/1000 (60%)]\tLosses F.softmax: 0.076755 log_softmax: 0.079855\n",
            "Train Epoch: 220 [800/1000 (80%)]\tLosses F.softmax: 0.119955 log_softmax: 0.129937\n",
            "Train Epoch: 220 [1000/1000 (100%)]\tLosses F.softmax: 0.341340 log_softmax: 0.305909\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5463\tAccuracy: 8305.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5371\tAccuracy: 8313.0/10000 (83%)\n",
            "\n",
            "Train Epoch: 221 [0/1000 (0%)]\tLosses F.softmax: 0.031293 log_softmax: 0.024821\n",
            "Train Epoch: 221 [200/1000 (20%)]\tLosses F.softmax: 0.044670 log_softmax: 0.092602\n",
            "Train Epoch: 221 [400/1000 (40%)]\tLosses F.softmax: 0.608024 log_softmax: 0.479149\n",
            "Train Epoch: 221 [600/1000 (60%)]\tLosses F.softmax: 0.100187 log_softmax: 0.096746\n",
            "Train Epoch: 221 [800/1000 (80%)]\tLosses F.softmax: 0.300866 log_softmax: 0.368533\n",
            "Train Epoch: 221 [1000/1000 (100%)]\tLosses F.softmax: 0.611088 log_softmax: 0.727200\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5455\tAccuracy: 8326.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5362\tAccuracy: 8332.0/10000 (83%)\n",
            "\n",
            "Train Epoch: 222 [0/1000 (0%)]\tLosses F.softmax: 0.019381 log_softmax: 0.033928\n",
            "Train Epoch: 222 [200/1000 (20%)]\tLosses F.softmax: 0.414056 log_softmax: 0.266606\n",
            "Train Epoch: 222 [400/1000 (40%)]\tLosses F.softmax: 1.393803 log_softmax: 1.512962\n",
            "Train Epoch: 222 [600/1000 (60%)]\tLosses F.softmax: 0.054916 log_softmax: 0.044017\n",
            "Train Epoch: 222 [800/1000 (80%)]\tLosses F.softmax: 0.119991 log_softmax: 0.233454\n",
            "Train Epoch: 222 [1000/1000 (100%)]\tLosses F.softmax: 0.021736 log_softmax: 0.053635\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5439\tAccuracy: 8330.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5341\tAccuracy: 8331.0/10000 (83%)\n",
            "\n",
            "Train Epoch: 223 [0/1000 (0%)]\tLosses F.softmax: 0.034173 log_softmax: 0.044783\n",
            "Train Epoch: 223 [200/1000 (20%)]\tLosses F.softmax: 1.172235 log_softmax: 1.014743\n",
            "Train Epoch: 223 [400/1000 (40%)]\tLosses F.softmax: 0.045402 log_softmax: 0.055253\n",
            "Train Epoch: 223 [600/1000 (60%)]\tLosses F.softmax: 0.121546 log_softmax: 0.074241\n",
            "Train Epoch: 223 [800/1000 (80%)]\tLosses F.softmax: 0.642484 log_softmax: 0.358268\n",
            "Train Epoch: 223 [1000/1000 (100%)]\tLosses F.softmax: 0.240790 log_softmax: 0.164605\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5438\tAccuracy: 8319.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5345\tAccuracy: 8329.0/10000 (83%)\n",
            "\n",
            "Train Epoch: 224 [0/1000 (0%)]\tLosses F.softmax: 0.287313 log_softmax: 0.216040\n",
            "Train Epoch: 224 [200/1000 (20%)]\tLosses F.softmax: 0.639943 log_softmax: 0.573511\n",
            "Train Epoch: 224 [400/1000 (40%)]\tLosses F.softmax: 0.017403 log_softmax: 0.035296\n",
            "Train Epoch: 224 [600/1000 (60%)]\tLosses F.softmax: 0.168925 log_softmax: 0.066894\n",
            "Train Epoch: 224 [800/1000 (80%)]\tLosses F.softmax: 0.188721 log_softmax: 0.121177\n",
            "Train Epoch: 224 [1000/1000 (100%)]\tLosses F.softmax: 0.019334 log_softmax: 0.049062\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5436\tAccuracy: 8321.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5341\tAccuracy: 8349.0/10000 (83%)\n",
            "\n",
            "Train Epoch: 225 [0/1000 (0%)]\tLosses F.softmax: 0.158934 log_softmax: 0.097910\n",
            "Train Epoch: 225 [200/1000 (20%)]\tLosses F.softmax: 0.023059 log_softmax: 0.032626\n",
            "Train Epoch: 225 [400/1000 (40%)]\tLosses F.softmax: 0.042251 log_softmax: 0.040918\n",
            "Train Epoch: 225 [600/1000 (60%)]\tLosses F.softmax: 2.129829 log_softmax: 1.266984\n",
            "Train Epoch: 225 [800/1000 (80%)]\tLosses F.softmax: 0.049445 log_softmax: 0.049623\n",
            "Train Epoch: 225 [1000/1000 (100%)]\tLosses F.softmax: 0.030158 log_softmax: 0.038591\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5422\tAccuracy: 8312.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5333\tAccuracy: 8318.0/10000 (83%)\n",
            "\n",
            "Train Epoch: 226 [0/1000 (0%)]\tLosses F.softmax: 0.227683 log_softmax: 0.315371\n",
            "Train Epoch: 226 [200/1000 (20%)]\tLosses F.softmax: 0.301471 log_softmax: 0.140703\n",
            "Train Epoch: 226 [400/1000 (40%)]\tLosses F.softmax: 0.174330 log_softmax: 0.160853\n",
            "Train Epoch: 226 [600/1000 (60%)]\tLosses F.softmax: 1.103469 log_softmax: 1.095399\n",
            "Train Epoch: 226 [800/1000 (80%)]\tLosses F.softmax: 0.120671 log_softmax: 0.137724\n",
            "Train Epoch: 226 [1000/1000 (100%)]\tLosses F.softmax: 0.246400 log_softmax: 0.205432\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5410\tAccuracy: 8309.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5318\tAccuracy: 8345.0/10000 (83%)\n",
            "\n",
            "Train Epoch: 227 [0/1000 (0%)]\tLosses F.softmax: 0.031386 log_softmax: 0.038689\n",
            "Train Epoch: 227 [200/1000 (20%)]\tLosses F.softmax: 0.451049 log_softmax: 0.404234\n",
            "Train Epoch: 227 [400/1000 (40%)]\tLosses F.softmax: 0.089594 log_softmax: 0.077171\n",
            "Train Epoch: 227 [600/1000 (60%)]\tLosses F.softmax: 0.019622 log_softmax: 0.014983\n",
            "Train Epoch: 227 [800/1000 (80%)]\tLosses F.softmax: 0.344863 log_softmax: 0.350046\n",
            "Train Epoch: 227 [1000/1000 (100%)]\tLosses F.softmax: 0.010266 log_softmax: 0.016862\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5394\tAccuracy: 8331.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5303\tAccuracy: 8337.0/10000 (83%)\n",
            "\n",
            "Train Epoch: 228 [0/1000 (0%)]\tLosses F.softmax: 0.174367 log_softmax: 0.121625\n",
            "Train Epoch: 228 [200/1000 (20%)]\tLosses F.softmax: 0.072270 log_softmax: 0.074950\n",
            "Train Epoch: 228 [400/1000 (40%)]\tLosses F.softmax: 0.395059 log_softmax: 0.382654\n",
            "Train Epoch: 228 [600/1000 (60%)]\tLosses F.softmax: 0.132349 log_softmax: 0.156624\n",
            "Train Epoch: 228 [800/1000 (80%)]\tLosses F.softmax: 0.345904 log_softmax: 0.445146\n",
            "Train Epoch: 228 [1000/1000 (100%)]\tLosses F.softmax: 0.085373 log_softmax: 0.123548\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5387\tAccuracy: 8333.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5296\tAccuracy: 8342.0/10000 (83%)\n",
            "\n",
            "Train Epoch: 229 [0/1000 (0%)]\tLosses F.softmax: 0.040362 log_softmax: 0.033253\n",
            "Train Epoch: 229 [200/1000 (20%)]\tLosses F.softmax: 0.428224 log_softmax: 0.381746\n",
            "Train Epoch: 229 [400/1000 (40%)]\tLosses F.softmax: 0.034638 log_softmax: 0.084002\n",
            "Train Epoch: 229 [600/1000 (60%)]\tLosses F.softmax: 0.020801 log_softmax: 0.024772\n",
            "Train Epoch: 229 [800/1000 (80%)]\tLosses F.softmax: 0.076504 log_softmax: 0.061127\n",
            "Train Epoch: 229 [1000/1000 (100%)]\tLosses F.softmax: 1.264161 log_softmax: 0.938180\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5374\tAccuracy: 8338.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5280\tAccuracy: 8356.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 230 [0/1000 (0%)]\tLosses F.softmax: 0.061209 log_softmax: 0.074897\n",
            "Train Epoch: 230 [200/1000 (20%)]\tLosses F.softmax: 0.009567 log_softmax: 0.010574\n",
            "Train Epoch: 230 [400/1000 (40%)]\tLosses F.softmax: 0.060953 log_softmax: 0.058552\n",
            "Train Epoch: 230 [600/1000 (60%)]\tLosses F.softmax: 0.096538 log_softmax: 0.132318\n",
            "Train Epoch: 230 [800/1000 (80%)]\tLosses F.softmax: 0.068897 log_softmax: 0.082703\n",
            "Train Epoch: 230 [1000/1000 (100%)]\tLosses F.softmax: 0.024895 log_softmax: 0.035698\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5364\tAccuracy: 8327.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5270\tAccuracy: 8354.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 231 [0/1000 (0%)]\tLosses F.softmax: 0.073724 log_softmax: 0.082570\n",
            "Train Epoch: 231 [200/1000 (20%)]\tLosses F.softmax: 0.008255 log_softmax: 0.003672\n",
            "Train Epoch: 231 [400/1000 (40%)]\tLosses F.softmax: 1.053248 log_softmax: 0.946640\n",
            "Train Epoch: 231 [600/1000 (60%)]\tLosses F.softmax: 0.012484 log_softmax: 0.009913\n",
            "Train Epoch: 231 [800/1000 (80%)]\tLosses F.softmax: 0.941798 log_softmax: 0.972362\n",
            "Train Epoch: 231 [1000/1000 (100%)]\tLosses F.softmax: 0.639220 log_softmax: 0.545932\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5357\tAccuracy: 8339.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5264\tAccuracy: 8376.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 232 [0/1000 (0%)]\tLosses F.softmax: 0.017457 log_softmax: 0.015271\n",
            "Train Epoch: 232 [200/1000 (20%)]\tLosses F.softmax: 0.132624 log_softmax: 0.075480\n",
            "Train Epoch: 232 [400/1000 (40%)]\tLosses F.softmax: 0.016152 log_softmax: 0.018252\n",
            "Train Epoch: 232 [600/1000 (60%)]\tLosses F.softmax: 0.044103 log_softmax: 0.053188\n",
            "Train Epoch: 232 [800/1000 (80%)]\tLosses F.softmax: 0.080619 log_softmax: 0.128919\n",
            "Train Epoch: 232 [1000/1000 (100%)]\tLosses F.softmax: 0.402266 log_softmax: 0.405558\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5360\tAccuracy: 8334.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5267\tAccuracy: 8357.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 233 [0/1000 (0%)]\tLosses F.softmax: 0.251945 log_softmax: 0.124412\n",
            "Train Epoch: 233 [200/1000 (20%)]\tLosses F.softmax: 1.349545 log_softmax: 1.055317\n",
            "Train Epoch: 233 [400/1000 (40%)]\tLosses F.softmax: 0.194554 log_softmax: 0.192489\n",
            "Train Epoch: 233 [600/1000 (60%)]\tLosses F.softmax: 0.080712 log_softmax: 0.204891\n",
            "Train Epoch: 233 [800/1000 (80%)]\tLosses F.softmax: 0.391794 log_softmax: 0.547423\n",
            "Train Epoch: 233 [1000/1000 (100%)]\tLosses F.softmax: 0.066598 log_softmax: 0.054744\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5356\tAccuracy: 8330.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5259\tAccuracy: 8350.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 234 [0/1000 (0%)]\tLosses F.softmax: 0.244289 log_softmax: 0.153192\n",
            "Train Epoch: 234 [200/1000 (20%)]\tLosses F.softmax: 0.517034 log_softmax: 0.362596\n",
            "Train Epoch: 234 [400/1000 (40%)]\tLosses F.softmax: 0.308022 log_softmax: 0.456981\n",
            "Train Epoch: 234 [600/1000 (60%)]\tLosses F.softmax: 0.014972 log_softmax: 0.027389\n",
            "Train Epoch: 234 [800/1000 (80%)]\tLosses F.softmax: 0.055315 log_softmax: 0.047141\n",
            "Train Epoch: 234 [1000/1000 (100%)]\tLosses F.softmax: 0.183918 log_softmax: 0.151389\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5331\tAccuracy: 8329.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5236\tAccuracy: 8371.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 235 [0/1000 (0%)]\tLosses F.softmax: 0.020112 log_softmax: 0.024010\n",
            "Train Epoch: 235 [200/1000 (20%)]\tLosses F.softmax: 0.172065 log_softmax: 0.165252\n",
            "Train Epoch: 235 [400/1000 (40%)]\tLosses F.softmax: 0.036232 log_softmax: 0.038960\n",
            "Train Epoch: 235 [600/1000 (60%)]\tLosses F.softmax: 0.593257 log_softmax: 0.553883\n",
            "Train Epoch: 235 [800/1000 (80%)]\tLosses F.softmax: 0.011159 log_softmax: 0.018653\n",
            "Train Epoch: 235 [1000/1000 (100%)]\tLosses F.softmax: 0.121631 log_softmax: 0.078171\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5331\tAccuracy: 8342.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5238\tAccuracy: 8366.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 236 [0/1000 (0%)]\tLosses F.softmax: 0.012369 log_softmax: 0.016197\n",
            "Train Epoch: 236 [200/1000 (20%)]\tLosses F.softmax: 0.272327 log_softmax: 0.202428\n",
            "Train Epoch: 236 [400/1000 (40%)]\tLosses F.softmax: 0.340681 log_softmax: 0.334053\n",
            "Train Epoch: 236 [600/1000 (60%)]\tLosses F.softmax: 0.219256 log_softmax: 0.212659\n",
            "Train Epoch: 236 [800/1000 (80%)]\tLosses F.softmax: 0.010726 log_softmax: 0.010420\n",
            "Train Epoch: 236 [1000/1000 (100%)]\tLosses F.softmax: 0.010293 log_softmax: 0.022150\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5314\tAccuracy: 8334.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5221\tAccuracy: 8371.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 237 [0/1000 (0%)]\tLosses F.softmax: 0.004305 log_softmax: 0.003828\n",
            "Train Epoch: 237 [200/1000 (20%)]\tLosses F.softmax: 0.050863 log_softmax: 0.046381\n",
            "Train Epoch: 237 [400/1000 (40%)]\tLosses F.softmax: 0.006287 log_softmax: 0.014034\n",
            "Train Epoch: 237 [600/1000 (60%)]\tLosses F.softmax: 0.014071 log_softmax: 0.018928\n",
            "Train Epoch: 237 [800/1000 (80%)]\tLosses F.softmax: 0.107346 log_softmax: 0.109291\n",
            "Train Epoch: 237 [1000/1000 (100%)]\tLosses F.softmax: 1.690296 log_softmax: 1.763493\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5313\tAccuracy: 8342.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5218\tAccuracy: 8374.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 238 [0/1000 (0%)]\tLosses F.softmax: 0.048940 log_softmax: 0.038070\n",
            "Train Epoch: 238 [200/1000 (20%)]\tLosses F.softmax: 0.257226 log_softmax: 0.279612\n",
            "Train Epoch: 238 [400/1000 (40%)]\tLosses F.softmax: 0.087813 log_softmax: 0.155984\n",
            "Train Epoch: 238 [600/1000 (60%)]\tLosses F.softmax: 0.032525 log_softmax: 0.032200\n",
            "Train Epoch: 238 [800/1000 (80%)]\tLosses F.softmax: 0.074599 log_softmax: 0.107245\n",
            "Train Epoch: 238 [1000/1000 (100%)]\tLosses F.softmax: 0.061308 log_softmax: 0.084262\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5302\tAccuracy: 8345.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5207\tAccuracy: 8390.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 239 [0/1000 (0%)]\tLosses F.softmax: 0.196399 log_softmax: 0.238970\n",
            "Train Epoch: 239 [200/1000 (20%)]\tLosses F.softmax: 0.004729 log_softmax: 0.007694\n",
            "Train Epoch: 239 [400/1000 (40%)]\tLosses F.softmax: 0.100270 log_softmax: 0.068883\n",
            "Train Epoch: 239 [600/1000 (60%)]\tLosses F.softmax: 0.015430 log_softmax: 0.012358\n",
            "Train Epoch: 239 [800/1000 (80%)]\tLosses F.softmax: 0.051568 log_softmax: 0.077276\n",
            "Train Epoch: 239 [1000/1000 (100%)]\tLosses F.softmax: 1.275830 log_softmax: 1.162697\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5303\tAccuracy: 8344.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5209\tAccuracy: 8382.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 240 [0/1000 (0%)]\tLosses F.softmax: 0.065684 log_softmax: 0.059996\n",
            "Train Epoch: 240 [200/1000 (20%)]\tLosses F.softmax: 0.096519 log_softmax: 0.079220\n",
            "Train Epoch: 240 [400/1000 (40%)]\tLosses F.softmax: 0.003637 log_softmax: 0.003269\n",
            "Train Epoch: 240 [600/1000 (60%)]\tLosses F.softmax: 0.596060 log_softmax: 0.466151\n",
            "Train Epoch: 240 [800/1000 (80%)]\tLosses F.softmax: 1.231882 log_softmax: 1.385891\n",
            "Train Epoch: 240 [1000/1000 (100%)]\tLosses F.softmax: 0.044490 log_softmax: 0.028394\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5293\tAccuracy: 8349.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5199\tAccuracy: 8386.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 241 [0/1000 (0%)]\tLosses F.softmax: 0.135299 log_softmax: 0.171155\n",
            "Train Epoch: 241 [200/1000 (20%)]\tLosses F.softmax: 0.287943 log_softmax: 0.404970\n",
            "Train Epoch: 241 [400/1000 (40%)]\tLosses F.softmax: 1.323738 log_softmax: 1.415307\n",
            "Train Epoch: 241 [600/1000 (60%)]\tLosses F.softmax: 0.039366 log_softmax: 0.025462\n",
            "Train Epoch: 241 [800/1000 (80%)]\tLosses F.softmax: 0.019432 log_softmax: 0.015429\n",
            "Train Epoch: 241 [1000/1000 (100%)]\tLosses F.softmax: 0.014182 log_softmax: 0.023120\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5290\tAccuracy: 8347.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5196\tAccuracy: 8379.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 242 [0/1000 (0%)]\tLosses F.softmax: 0.029264 log_softmax: 0.048393\n",
            "Train Epoch: 242 [200/1000 (20%)]\tLosses F.softmax: 0.078252 log_softmax: 0.047849\n",
            "Train Epoch: 242 [400/1000 (40%)]\tLosses F.softmax: 0.092719 log_softmax: 0.074075\n",
            "Train Epoch: 242 [600/1000 (60%)]\tLosses F.softmax: 0.387747 log_softmax: 0.826403\n",
            "Train Epoch: 242 [800/1000 (80%)]\tLosses F.softmax: 0.189964 log_softmax: 0.126559\n",
            "Train Epoch: 242 [1000/1000 (100%)]\tLosses F.softmax: 0.069599 log_softmax: 0.062259\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5287\tAccuracy: 8352.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5196\tAccuracy: 8382.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 243 [0/1000 (0%)]\tLosses F.softmax: 0.034254 log_softmax: 0.027972\n",
            "Train Epoch: 243 [200/1000 (20%)]\tLosses F.softmax: 0.028119 log_softmax: 0.017359\n",
            "Train Epoch: 243 [400/1000 (40%)]\tLosses F.softmax: 0.051370 log_softmax: 0.073742\n",
            "Train Epoch: 243 [600/1000 (60%)]\tLosses F.softmax: 0.671575 log_softmax: 0.694077\n",
            "Train Epoch: 243 [800/1000 (80%)]\tLosses F.softmax: 0.823650 log_softmax: 0.843345\n",
            "Train Epoch: 243 [1000/1000 (100%)]\tLosses F.softmax: 0.048101 log_softmax: 0.042418\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5277\tAccuracy: 8347.0/10000 (83%)\n",
            "log_softmax: Loss: 0.5184\tAccuracy: 8390.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 244 [0/1000 (0%)]\tLosses F.softmax: 0.002002 log_softmax: 0.002552\n",
            "Train Epoch: 244 [200/1000 (20%)]\tLosses F.softmax: 1.528855 log_softmax: 2.064239\n",
            "Train Epoch: 244 [400/1000 (40%)]\tLosses F.softmax: 0.043237 log_softmax: 0.054014\n",
            "Train Epoch: 244 [600/1000 (60%)]\tLosses F.softmax: 0.017737 log_softmax: 0.047408\n",
            "Train Epoch: 244 [800/1000 (80%)]\tLosses F.softmax: 0.187582 log_softmax: 0.188497\n",
            "Train Epoch: 244 [1000/1000 (100%)]\tLosses F.softmax: 0.016423 log_softmax: 0.068981\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5273\tAccuracy: 8351.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5180\tAccuracy: 8390.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 245 [0/1000 (0%)]\tLosses F.softmax: 0.007809 log_softmax: 0.003418\n",
            "Train Epoch: 245 [200/1000 (20%)]\tLosses F.softmax: 0.312193 log_softmax: 0.422234\n",
            "Train Epoch: 245 [400/1000 (40%)]\tLosses F.softmax: 0.150360 log_softmax: 0.077701\n",
            "Train Epoch: 245 [600/1000 (60%)]\tLosses F.softmax: 0.002491 log_softmax: 0.006300\n",
            "Train Epoch: 245 [800/1000 (80%)]\tLosses F.softmax: 0.024245 log_softmax: 0.032604\n",
            "Train Epoch: 245 [1000/1000 (100%)]\tLosses F.softmax: 0.178147 log_softmax: 0.152698\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5273\tAccuracy: 8350.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5179\tAccuracy: 8392.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 246 [0/1000 (0%)]\tLosses F.softmax: 0.301395 log_softmax: 0.238226\n",
            "Train Epoch: 246 [200/1000 (20%)]\tLosses F.softmax: 0.105251 log_softmax: 0.088011\n",
            "Train Epoch: 246 [400/1000 (40%)]\tLosses F.softmax: 0.341248 log_softmax: 0.744799\n",
            "Train Epoch: 246 [600/1000 (60%)]\tLosses F.softmax: 0.262432 log_softmax: 0.164016\n",
            "Train Epoch: 246 [800/1000 (80%)]\tLosses F.softmax: 1.009149 log_softmax: 1.501086\n",
            "Train Epoch: 246 [1000/1000 (100%)]\tLosses F.softmax: 0.089646 log_softmax: 0.094392\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5260\tAccuracy: 8350.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5163\tAccuracy: 8396.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 247 [0/1000 (0%)]\tLosses F.softmax: 0.269642 log_softmax: 0.206955\n",
            "Train Epoch: 247 [200/1000 (20%)]\tLosses F.softmax: 0.113275 log_softmax: 0.114405\n",
            "Train Epoch: 247 [400/1000 (40%)]\tLosses F.softmax: 0.006204 log_softmax: 0.006943\n",
            "Train Epoch: 247 [600/1000 (60%)]\tLosses F.softmax: 0.005719 log_softmax: 0.007310\n",
            "Train Epoch: 247 [800/1000 (80%)]\tLosses F.softmax: 0.032902 log_softmax: 0.037453\n",
            "Train Epoch: 247 [1000/1000 (100%)]\tLosses F.softmax: 0.052354 log_softmax: 0.083651\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5252\tAccuracy: 8351.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5162\tAccuracy: 8401.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 248 [0/1000 (0%)]\tLosses F.softmax: 0.541946 log_softmax: 0.438602\n",
            "Train Epoch: 248 [200/1000 (20%)]\tLosses F.softmax: 0.030581 log_softmax: 0.042994\n",
            "Train Epoch: 248 [400/1000 (40%)]\tLosses F.softmax: 0.069061 log_softmax: 0.061683\n",
            "Train Epoch: 248 [600/1000 (60%)]\tLosses F.softmax: 0.071122 log_softmax: 0.067726\n",
            "Train Epoch: 248 [800/1000 (80%)]\tLosses F.softmax: 0.025234 log_softmax: 0.032496\n",
            "Train Epoch: 248 [1000/1000 (100%)]\tLosses F.softmax: 0.902639 log_softmax: 0.546427\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5247\tAccuracy: 8361.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5156\tAccuracy: 8406.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 249 [0/1000 (0%)]\tLosses F.softmax: 0.009871 log_softmax: 0.022806\n",
            "Train Epoch: 249 [200/1000 (20%)]\tLosses F.softmax: 0.259191 log_softmax: 0.215230\n",
            "Train Epoch: 249 [400/1000 (40%)]\tLosses F.softmax: 0.071691 log_softmax: 0.142342\n",
            "Train Epoch: 249 [600/1000 (60%)]\tLosses F.softmax: 0.183702 log_softmax: 0.110145\n",
            "Train Epoch: 249 [800/1000 (80%)]\tLosses F.softmax: 0.063668 log_softmax: 0.059033\n",
            "Train Epoch: 249 [1000/1000 (100%)]\tLosses F.softmax: 0.160260 log_softmax: 0.138853\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5238\tAccuracy: 8361.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5146\tAccuracy: 8404.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 250 [0/1000 (0%)]\tLosses F.softmax: 1.625530 log_softmax: 1.025214\n",
            "Train Epoch: 250 [200/1000 (20%)]\tLosses F.softmax: 0.030599 log_softmax: 0.034824\n",
            "Train Epoch: 250 [400/1000 (40%)]\tLosses F.softmax: 0.006916 log_softmax: 0.008378\n",
            "Train Epoch: 250 [600/1000 (60%)]\tLosses F.softmax: 0.297927 log_softmax: 0.201564\n",
            "Train Epoch: 250 [800/1000 (80%)]\tLosses F.softmax: 0.015424 log_softmax: 0.016189\n",
            "Train Epoch: 250 [1000/1000 (100%)]\tLosses F.softmax: 0.191782 log_softmax: 0.158419\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5246\tAccuracy: 8352.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5151\tAccuracy: 8397.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 251 [0/1000 (0%)]\tLosses F.softmax: 0.158073 log_softmax: 0.330695\n",
            "Train Epoch: 251 [200/1000 (20%)]\tLosses F.softmax: 0.238383 log_softmax: 0.140938\n",
            "Train Epoch: 251 [400/1000 (40%)]\tLosses F.softmax: 2.682393 log_softmax: 2.514983\n",
            "Train Epoch: 251 [600/1000 (60%)]\tLosses F.softmax: 0.021655 log_softmax: 0.021703\n",
            "Train Epoch: 251 [800/1000 (80%)]\tLosses F.softmax: 0.056798 log_softmax: 0.051720\n",
            "Train Epoch: 251 [1000/1000 (100%)]\tLosses F.softmax: 0.459833 log_softmax: 0.344577\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5238\tAccuracy: 8352.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5140\tAccuracy: 8419.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 252 [0/1000 (0%)]\tLosses F.softmax: 0.028214 log_softmax: 0.028101\n",
            "Train Epoch: 252 [200/1000 (20%)]\tLosses F.softmax: 0.016506 log_softmax: 0.022643\n",
            "Train Epoch: 252 [400/1000 (40%)]\tLosses F.softmax: 0.015651 log_softmax: 0.015105\n",
            "Train Epoch: 252 [600/1000 (60%)]\tLosses F.softmax: 0.251423 log_softmax: 0.261064\n",
            "Train Epoch: 252 [800/1000 (80%)]\tLosses F.softmax: 0.014808 log_softmax: 0.016579\n",
            "Train Epoch: 252 [1000/1000 (100%)]\tLosses F.softmax: 0.132130 log_softmax: 0.069446\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5236\tAccuracy: 8353.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5140\tAccuracy: 8406.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 253 [0/1000 (0%)]\tLosses F.softmax: 0.136163 log_softmax: 0.130227\n",
            "Train Epoch: 253 [200/1000 (20%)]\tLosses F.softmax: 0.040807 log_softmax: 0.034557\n",
            "Train Epoch: 253 [400/1000 (40%)]\tLosses F.softmax: 0.036222 log_softmax: 0.030972\n",
            "Train Epoch: 253 [600/1000 (60%)]\tLosses F.softmax: 0.125385 log_softmax: 0.132144\n",
            "Train Epoch: 253 [800/1000 (80%)]\tLosses F.softmax: 0.216469 log_softmax: 0.295889\n",
            "Train Epoch: 253 [1000/1000 (100%)]\tLosses F.softmax: 0.040491 log_softmax: 0.055962\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5221\tAccuracy: 8368.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5127\tAccuracy: 8414.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 254 [0/1000 (0%)]\tLosses F.softmax: 0.051116 log_softmax: 0.049950\n",
            "Train Epoch: 254 [200/1000 (20%)]\tLosses F.softmax: 0.417053 log_softmax: 0.474076\n",
            "Train Epoch: 254 [400/1000 (40%)]\tLosses F.softmax: 0.038492 log_softmax: 0.035473\n",
            "Train Epoch: 254 [600/1000 (60%)]\tLosses F.softmax: 0.034470 log_softmax: 0.032368\n",
            "Train Epoch: 254 [800/1000 (80%)]\tLosses F.softmax: 0.079893 log_softmax: 0.064136\n",
            "Train Epoch: 254 [1000/1000 (100%)]\tLosses F.softmax: 0.088225 log_softmax: 0.083608\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5206\tAccuracy: 8367.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5108\tAccuracy: 8427.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 255 [0/1000 (0%)]\tLosses F.softmax: 0.007182 log_softmax: 0.012094\n",
            "Train Epoch: 255 [200/1000 (20%)]\tLosses F.softmax: 0.096129 log_softmax: 0.114136\n",
            "Train Epoch: 255 [400/1000 (40%)]\tLosses F.softmax: 0.054053 log_softmax: 0.050715\n",
            "Train Epoch: 255 [600/1000 (60%)]\tLosses F.softmax: 0.134939 log_softmax: 0.092940\n",
            "Train Epoch: 255 [800/1000 (80%)]\tLosses F.softmax: 0.034052 log_softmax: 0.046014\n",
            "Train Epoch: 255 [1000/1000 (100%)]\tLosses F.softmax: 0.008504 log_softmax: 0.010746\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5223\tAccuracy: 8369.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5123\tAccuracy: 8421.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 256 [0/1000 (0%)]\tLosses F.softmax: 0.006527 log_softmax: 0.006257\n",
            "Train Epoch: 256 [200/1000 (20%)]\tLosses F.softmax: 0.018079 log_softmax: 0.023285\n",
            "Train Epoch: 256 [400/1000 (40%)]\tLosses F.softmax: 0.031404 log_softmax: 0.040538\n",
            "Train Epoch: 256 [600/1000 (60%)]\tLosses F.softmax: 1.047917 log_softmax: 0.843726\n",
            "Train Epoch: 256 [800/1000 (80%)]\tLosses F.softmax: 0.273197 log_softmax: 0.287450\n",
            "Train Epoch: 256 [1000/1000 (100%)]\tLosses F.softmax: 0.092339 log_softmax: 0.051015\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5221\tAccuracy: 8357.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5124\tAccuracy: 8415.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 257 [0/1000 (0%)]\tLosses F.softmax: 0.028810 log_softmax: 0.016989\n",
            "Train Epoch: 257 [200/1000 (20%)]\tLosses F.softmax: 0.039175 log_softmax: 0.063798\n",
            "Train Epoch: 257 [400/1000 (40%)]\tLosses F.softmax: 0.066408 log_softmax: 0.029793\n",
            "Train Epoch: 257 [600/1000 (60%)]\tLosses F.softmax: 0.037382 log_softmax: 0.048122\n",
            "Train Epoch: 257 [800/1000 (80%)]\tLosses F.softmax: 0.005353 log_softmax: 0.005111\n",
            "Train Epoch: 257 [1000/1000 (100%)]\tLosses F.softmax: 0.146333 log_softmax: 0.163748\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5215\tAccuracy: 8367.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5120\tAccuracy: 8415.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 258 [0/1000 (0%)]\tLosses F.softmax: 0.045225 log_softmax: 0.047977\n",
            "Train Epoch: 258 [200/1000 (20%)]\tLosses F.softmax: 1.191795 log_softmax: 0.621895\n",
            "Train Epoch: 258 [400/1000 (40%)]\tLosses F.softmax: 0.155188 log_softmax: 0.092678\n",
            "Train Epoch: 258 [600/1000 (60%)]\tLosses F.softmax: 0.006817 log_softmax: 0.015102\n",
            "Train Epoch: 258 [800/1000 (80%)]\tLosses F.softmax: 0.113168 log_softmax: 0.160493\n",
            "Train Epoch: 258 [1000/1000 (100%)]\tLosses F.softmax: 0.553393 log_softmax: 0.370280\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5220\tAccuracy: 8364.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5122\tAccuracy: 8414.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 259 [0/1000 (0%)]\tLosses F.softmax: 0.006191 log_softmax: 0.005875\n",
            "Train Epoch: 259 [200/1000 (20%)]\tLosses F.softmax: 0.026954 log_softmax: 0.051778\n",
            "Train Epoch: 259 [400/1000 (40%)]\tLosses F.softmax: 0.632280 log_softmax: 0.392812\n",
            "Train Epoch: 259 [600/1000 (60%)]\tLosses F.softmax: 0.009864 log_softmax: 0.018495\n",
            "Train Epoch: 259 [800/1000 (80%)]\tLosses F.softmax: 0.045131 log_softmax: 0.061022\n",
            "Train Epoch: 259 [1000/1000 (100%)]\tLosses F.softmax: 0.025142 log_softmax: 0.034529\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5196\tAccuracy: 8385.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5105\tAccuracy: 8420.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 260 [0/1000 (0%)]\tLosses F.softmax: 0.081689 log_softmax: 0.171471\n",
            "Train Epoch: 260 [200/1000 (20%)]\tLosses F.softmax: 2.507160 log_softmax: 2.049641\n",
            "Train Epoch: 260 [400/1000 (40%)]\tLosses F.softmax: 0.009327 log_softmax: 0.017386\n",
            "Train Epoch: 260 [600/1000 (60%)]\tLosses F.softmax: 0.091580 log_softmax: 0.110149\n",
            "Train Epoch: 260 [800/1000 (80%)]\tLosses F.softmax: 0.019833 log_softmax: 0.023024\n",
            "Train Epoch: 260 [1000/1000 (100%)]\tLosses F.softmax: 0.039714 log_softmax: 0.053977\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5190\tAccuracy: 8388.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5097\tAccuracy: 8440.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 261 [0/1000 (0%)]\tLosses F.softmax: 0.018717 log_softmax: 0.022348\n",
            "Train Epoch: 261 [200/1000 (20%)]\tLosses F.softmax: 0.852844 log_softmax: 0.724806\n",
            "Train Epoch: 261 [400/1000 (40%)]\tLosses F.softmax: 0.014087 log_softmax: 0.018336\n",
            "Train Epoch: 261 [600/1000 (60%)]\tLosses F.softmax: 3.309500 log_softmax: 3.830503\n",
            "Train Epoch: 261 [800/1000 (80%)]\tLosses F.softmax: 0.029174 log_softmax: 0.022126\n",
            "Train Epoch: 261 [1000/1000 (100%)]\tLosses F.softmax: 0.063795 log_softmax: 0.042400\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5190\tAccuracy: 8384.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5100\tAccuracy: 8422.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 262 [0/1000 (0%)]\tLosses F.softmax: 0.702697 log_softmax: 0.467242\n",
            "Train Epoch: 262 [200/1000 (20%)]\tLosses F.softmax: 0.086995 log_softmax: 0.048823\n",
            "Train Epoch: 262 [400/1000 (40%)]\tLosses F.softmax: 0.008800 log_softmax: 0.011908\n",
            "Train Epoch: 262 [600/1000 (60%)]\tLosses F.softmax: 0.008286 log_softmax: 0.015569\n",
            "Train Epoch: 262 [800/1000 (80%)]\tLosses F.softmax: 0.023274 log_softmax: 0.015761\n",
            "Train Epoch: 262 [1000/1000 (100%)]\tLosses F.softmax: 0.086598 log_softmax: 0.068064\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5195\tAccuracy: 8383.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5100\tAccuracy: 8426.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 263 [0/1000 (0%)]\tLosses F.softmax: 0.333416 log_softmax: 0.135225\n",
            "Train Epoch: 263 [200/1000 (20%)]\tLosses F.softmax: 0.642927 log_softmax: 0.450529\n",
            "Train Epoch: 263 [400/1000 (40%)]\tLosses F.softmax: 0.199637 log_softmax: 0.126164\n",
            "Train Epoch: 263 [600/1000 (60%)]\tLosses F.softmax: 0.184631 log_softmax: 0.177870\n",
            "Train Epoch: 263 [800/1000 (80%)]\tLosses F.softmax: 0.015407 log_softmax: 0.009625\n",
            "Train Epoch: 263 [1000/1000 (100%)]\tLosses F.softmax: 1.314316 log_softmax: 1.565415\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5182\tAccuracy: 8394.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5089\tAccuracy: 8437.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 264 [0/1000 (0%)]\tLosses F.softmax: 0.011787 log_softmax: 0.005078\n",
            "Train Epoch: 264 [200/1000 (20%)]\tLosses F.softmax: 1.154888 log_softmax: 1.033086\n",
            "Train Epoch: 264 [400/1000 (40%)]\tLosses F.softmax: 0.003592 log_softmax: 0.005368\n",
            "Train Epoch: 264 [600/1000 (60%)]\tLosses F.softmax: 0.043803 log_softmax: 0.028230\n",
            "Train Epoch: 264 [800/1000 (80%)]\tLosses F.softmax: 0.938685 log_softmax: 0.697365\n",
            "Train Epoch: 264 [1000/1000 (100%)]\tLosses F.softmax: 0.523893 log_softmax: 0.911369\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5189\tAccuracy: 8382.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5097\tAccuracy: 8423.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 265 [0/1000 (0%)]\tLosses F.softmax: 0.130397 log_softmax: 0.108656\n",
            "Train Epoch: 265 [200/1000 (20%)]\tLosses F.softmax: 0.085758 log_softmax: 0.065218\n",
            "Train Epoch: 265 [400/1000 (40%)]\tLosses F.softmax: 0.080320 log_softmax: 0.101901\n",
            "Train Epoch: 265 [600/1000 (60%)]\tLosses F.softmax: 0.308601 log_softmax: 0.285001\n",
            "Train Epoch: 265 [800/1000 (80%)]\tLosses F.softmax: 0.685739 log_softmax: 0.772274\n",
            "Train Epoch: 265 [1000/1000 (100%)]\tLosses F.softmax: 0.021168 log_softmax: 0.019592\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5193\tAccuracy: 8377.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5098\tAccuracy: 8426.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 266 [0/1000 (0%)]\tLosses F.softmax: 2.588753 log_softmax: 1.784156\n",
            "Train Epoch: 266 [200/1000 (20%)]\tLosses F.softmax: 0.006955 log_softmax: 0.006918\n",
            "Train Epoch: 266 [400/1000 (40%)]\tLosses F.softmax: 0.055268 log_softmax: 0.040692\n",
            "Train Epoch: 266 [600/1000 (60%)]\tLosses F.softmax: 0.021561 log_softmax: 0.020284\n",
            "Train Epoch: 266 [800/1000 (80%)]\tLosses F.softmax: 0.411425 log_softmax: 0.522012\n",
            "Train Epoch: 266 [1000/1000 (100%)]\tLosses F.softmax: 0.294585 log_softmax: 0.239431\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5184\tAccuracy: 8382.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5088\tAccuracy: 8427.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 267 [0/1000 (0%)]\tLosses F.softmax: 0.102502 log_softmax: 0.113024\n",
            "Train Epoch: 267 [200/1000 (20%)]\tLosses F.softmax: 0.001609 log_softmax: 0.007648\n",
            "Train Epoch: 267 [400/1000 (40%)]\tLosses F.softmax: 0.150921 log_softmax: 0.122574\n",
            "Train Epoch: 267 [600/1000 (60%)]\tLosses F.softmax: 0.562461 log_softmax: 0.387960\n",
            "Train Epoch: 267 [800/1000 (80%)]\tLosses F.softmax: 0.064687 log_softmax: 0.044647\n",
            "Train Epoch: 267 [1000/1000 (100%)]\tLosses F.softmax: 0.085163 log_softmax: 0.121931\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5175\tAccuracy: 8394.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5081\tAccuracy: 8435.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 268 [0/1000 (0%)]\tLosses F.softmax: 0.022583 log_softmax: 0.027492\n",
            "Train Epoch: 268 [200/1000 (20%)]\tLosses F.softmax: 0.024471 log_softmax: 0.013064\n",
            "Train Epoch: 268 [400/1000 (40%)]\tLosses F.softmax: 0.064426 log_softmax: 0.057292\n",
            "Train Epoch: 268 [600/1000 (60%)]\tLosses F.softmax: 0.029576 log_softmax: 0.028901\n",
            "Train Epoch: 268 [800/1000 (80%)]\tLosses F.softmax: 0.028834 log_softmax: 0.040997\n",
            "Train Epoch: 268 [1000/1000 (100%)]\tLosses F.softmax: 0.014515 log_softmax: 0.013115\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5179\tAccuracy: 8389.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5085\tAccuracy: 8430.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 269 [0/1000 (0%)]\tLosses F.softmax: 0.185950 log_softmax: 0.205717\n",
            "Train Epoch: 269 [200/1000 (20%)]\tLosses F.softmax: 0.191198 log_softmax: 0.195834\n",
            "Train Epoch: 269 [400/1000 (40%)]\tLosses F.softmax: 0.172592 log_softmax: 0.147724\n",
            "Train Epoch: 269 [600/1000 (60%)]\tLosses F.softmax: 0.007105 log_softmax: 0.007865\n",
            "Train Epoch: 269 [800/1000 (80%)]\tLosses F.softmax: 0.061430 log_softmax: 0.052318\n",
            "Train Epoch: 269 [1000/1000 (100%)]\tLosses F.softmax: 0.004517 log_softmax: 0.004558\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5178\tAccuracy: 8389.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5085\tAccuracy: 8435.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 270 [0/1000 (0%)]\tLosses F.softmax: 0.017226 log_softmax: 0.017417\n",
            "Train Epoch: 270 [200/1000 (20%)]\tLosses F.softmax: 0.454099 log_softmax: 0.408561\n",
            "Train Epoch: 270 [400/1000 (40%)]\tLosses F.softmax: 0.468269 log_softmax: 0.765188\n",
            "Train Epoch: 270 [600/1000 (60%)]\tLosses F.softmax: 0.741201 log_softmax: 0.437551\n",
            "Train Epoch: 270 [800/1000 (80%)]\tLosses F.softmax: 0.147963 log_softmax: 0.132008\n",
            "Train Epoch: 270 [1000/1000 (100%)]\tLosses F.softmax: 0.737029 log_softmax: 0.427897\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5175\tAccuracy: 8386.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5082\tAccuracy: 8436.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 271 [0/1000 (0%)]\tLosses F.softmax: 0.019396 log_softmax: 0.017868\n",
            "Train Epoch: 271 [200/1000 (20%)]\tLosses F.softmax: 0.024331 log_softmax: 0.013861\n",
            "Train Epoch: 271 [400/1000 (40%)]\tLosses F.softmax: 1.529088 log_softmax: 1.386560\n",
            "Train Epoch: 271 [600/1000 (60%)]\tLosses F.softmax: 0.982541 log_softmax: 0.780797\n",
            "Train Epoch: 271 [800/1000 (80%)]\tLosses F.softmax: 0.398085 log_softmax: 0.405042\n",
            "Train Epoch: 271 [1000/1000 (100%)]\tLosses F.softmax: 0.008995 log_softmax: 0.010412\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5162\tAccuracy: 8395.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5072\tAccuracy: 8442.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 272 [0/1000 (0%)]\tLosses F.softmax: 0.058018 log_softmax: 0.044553\n",
            "Train Epoch: 272 [200/1000 (20%)]\tLosses F.softmax: 0.002151 log_softmax: 0.002005\n",
            "Train Epoch: 272 [400/1000 (40%)]\tLosses F.softmax: 0.025614 log_softmax: 0.022315\n",
            "Train Epoch: 272 [600/1000 (60%)]\tLosses F.softmax: 0.081149 log_softmax: 0.076175\n",
            "Train Epoch: 272 [800/1000 (80%)]\tLosses F.softmax: 0.022115 log_softmax: 0.019300\n",
            "Train Epoch: 272 [1000/1000 (100%)]\tLosses F.softmax: 1.293078 log_softmax: 1.086306\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5162\tAccuracy: 8391.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5069\tAccuracy: 8444.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 273 [0/1000 (0%)]\tLosses F.softmax: 0.145954 log_softmax: 0.076231\n",
            "Train Epoch: 273 [200/1000 (20%)]\tLosses F.softmax: 0.038865 log_softmax: 0.045715\n",
            "Train Epoch: 273 [400/1000 (40%)]\tLosses F.softmax: 0.008995 log_softmax: 0.017545\n",
            "Train Epoch: 273 [600/1000 (60%)]\tLosses F.softmax: 0.063171 log_softmax: 0.054998\n",
            "Train Epoch: 273 [800/1000 (80%)]\tLosses F.softmax: 0.068052 log_softmax: 0.055812\n",
            "Train Epoch: 273 [1000/1000 (100%)]\tLosses F.softmax: 0.072662 log_softmax: 0.055101\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5172\tAccuracy: 8397.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5076\tAccuracy: 8436.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 274 [0/1000 (0%)]\tLosses F.softmax: 0.043064 log_softmax: 0.032287\n",
            "Train Epoch: 274 [200/1000 (20%)]\tLosses F.softmax: 0.076175 log_softmax: 0.037492\n",
            "Train Epoch: 274 [400/1000 (40%)]\tLosses F.softmax: 0.211270 log_softmax: 0.196017\n",
            "Train Epoch: 274 [600/1000 (60%)]\tLosses F.softmax: 0.035993 log_softmax: 0.039336\n",
            "Train Epoch: 274 [800/1000 (80%)]\tLosses F.softmax: 0.015383 log_softmax: 0.008956\n",
            "Train Epoch: 274 [1000/1000 (100%)]\tLosses F.softmax: 0.441195 log_softmax: 0.504642\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5173\tAccuracy: 8392.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5075\tAccuracy: 8450.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 275 [0/1000 (0%)]\tLosses F.softmax: 0.002909 log_softmax: 0.002856\n",
            "Train Epoch: 275 [200/1000 (20%)]\tLosses F.softmax: 0.041608 log_softmax: 0.035671\n",
            "Train Epoch: 275 [400/1000 (40%)]\tLosses F.softmax: 0.012084 log_softmax: 0.011523\n",
            "Train Epoch: 275 [600/1000 (60%)]\tLosses F.softmax: 0.094388 log_softmax: 0.085283\n",
            "Train Epoch: 275 [800/1000 (80%)]\tLosses F.softmax: 0.002062 log_softmax: 0.001922\n",
            "Train Epoch: 275 [1000/1000 (100%)]\tLosses F.softmax: 0.024561 log_softmax: 0.028121\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5172\tAccuracy: 8389.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5073\tAccuracy: 8445.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 276 [0/1000 (0%)]\tLosses F.softmax: 1.134790 log_softmax: 0.968228\n",
            "Train Epoch: 276 [200/1000 (20%)]\tLosses F.softmax: 0.056164 log_softmax: 0.105202\n",
            "Train Epoch: 276 [400/1000 (40%)]\tLosses F.softmax: 0.173418 log_softmax: 0.089194\n",
            "Train Epoch: 276 [600/1000 (60%)]\tLosses F.softmax: 0.332619 log_softmax: 0.380906\n",
            "Train Epoch: 276 [800/1000 (80%)]\tLosses F.softmax: 0.237177 log_softmax: 0.172194\n",
            "Train Epoch: 276 [1000/1000 (100%)]\tLosses F.softmax: 0.343530 log_softmax: 0.333853\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5166\tAccuracy: 8397.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5068\tAccuracy: 8448.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 277 [0/1000 (0%)]\tLosses F.softmax: 0.013163 log_softmax: 0.024935\n",
            "Train Epoch: 277 [200/1000 (20%)]\tLosses F.softmax: 0.083998 log_softmax: 0.061336\n",
            "Train Epoch: 277 [400/1000 (40%)]\tLosses F.softmax: 0.018794 log_softmax: 0.072528\n",
            "Train Epoch: 277 [600/1000 (60%)]\tLosses F.softmax: 0.050920 log_softmax: 0.018854\n",
            "Train Epoch: 277 [800/1000 (80%)]\tLosses F.softmax: 0.038499 log_softmax: 0.068348\n",
            "Train Epoch: 277 [1000/1000 (100%)]\tLosses F.softmax: 0.143509 log_softmax: 0.215342\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5174\tAccuracy: 8393.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5082\tAccuracy: 8438.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 278 [0/1000 (0%)]\tLosses F.softmax: 0.007509 log_softmax: 0.015886\n",
            "Train Epoch: 278 [200/1000 (20%)]\tLosses F.softmax: 0.021170 log_softmax: 0.027402\n",
            "Train Epoch: 278 [400/1000 (40%)]\tLosses F.softmax: 0.517649 log_softmax: 0.480109\n",
            "Train Epoch: 278 [600/1000 (60%)]\tLosses F.softmax: 0.008349 log_softmax: 0.009394\n",
            "Train Epoch: 278 [800/1000 (80%)]\tLosses F.softmax: 1.920012 log_softmax: 1.956333\n",
            "Train Epoch: 278 [1000/1000 (100%)]\tLosses F.softmax: 0.061603 log_softmax: 0.027194\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5163\tAccuracy: 8398.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5070\tAccuracy: 8443.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 279 [0/1000 (0%)]\tLosses F.softmax: 0.140823 log_softmax: 0.186402\n",
            "Train Epoch: 279 [200/1000 (20%)]\tLosses F.softmax: 0.011665 log_softmax: 0.016846\n",
            "Train Epoch: 279 [400/1000 (40%)]\tLosses F.softmax: 0.067996 log_softmax: 0.071838\n",
            "Train Epoch: 279 [600/1000 (60%)]\tLosses F.softmax: 0.039335 log_softmax: 0.045143\n",
            "Train Epoch: 279 [800/1000 (80%)]\tLosses F.softmax: 0.099564 log_softmax: 0.104450\n",
            "Train Epoch: 279 [1000/1000 (100%)]\tLosses F.softmax: 0.249498 log_softmax: 0.206144\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5163\tAccuracy: 8391.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5072\tAccuracy: 8439.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 280 [0/1000 (0%)]\tLosses F.softmax: 0.045836 log_softmax: 0.044531\n",
            "Train Epoch: 280 [200/1000 (20%)]\tLosses F.softmax: 0.005177 log_softmax: 0.011355\n",
            "Train Epoch: 280 [400/1000 (40%)]\tLosses F.softmax: 0.197191 log_softmax: 0.142912\n",
            "Train Epoch: 280 [600/1000 (60%)]\tLosses F.softmax: 0.248501 log_softmax: 0.209050\n",
            "Train Epoch: 280 [800/1000 (80%)]\tLosses F.softmax: 0.048607 log_softmax: 0.124386\n",
            "Train Epoch: 280 [1000/1000 (100%)]\tLosses F.softmax: 0.016216 log_softmax: 0.014956\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5157\tAccuracy: 8402.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5063\tAccuracy: 8446.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 281 [0/1000 (0%)]\tLosses F.softmax: 0.005112 log_softmax: 0.009788\n",
            "Train Epoch: 281 [200/1000 (20%)]\tLosses F.softmax: 0.072815 log_softmax: 0.048537\n",
            "Train Epoch: 281 [400/1000 (40%)]\tLosses F.softmax: 0.033014 log_softmax: 0.032618\n",
            "Train Epoch: 281 [600/1000 (60%)]\tLosses F.softmax: 0.091332 log_softmax: 0.049511\n",
            "Train Epoch: 281 [800/1000 (80%)]\tLosses F.softmax: 0.326691 log_softmax: 0.411092\n",
            "Train Epoch: 281 [1000/1000 (100%)]\tLosses F.softmax: 0.023675 log_softmax: 0.018077\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5155\tAccuracy: 8398.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5065\tAccuracy: 8447.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 282 [0/1000 (0%)]\tLosses F.softmax: 0.230728 log_softmax: 0.227399\n",
            "Train Epoch: 282 [200/1000 (20%)]\tLosses F.softmax: 0.196726 log_softmax: 0.343677\n",
            "Train Epoch: 282 [400/1000 (40%)]\tLosses F.softmax: 0.042256 log_softmax: 0.055201\n",
            "Train Epoch: 282 [600/1000 (60%)]\tLosses F.softmax: 0.132410 log_softmax: 0.154599\n",
            "Train Epoch: 282 [800/1000 (80%)]\tLosses F.softmax: 0.036056 log_softmax: 0.022119\n",
            "Train Epoch: 282 [1000/1000 (100%)]\tLosses F.softmax: 0.071208 log_softmax: 0.085718\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5159\tAccuracy: 8406.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5066\tAccuracy: 8446.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 283 [0/1000 (0%)]\tLosses F.softmax: 0.067786 log_softmax: 0.050178\n",
            "Train Epoch: 283 [200/1000 (20%)]\tLosses F.softmax: 0.273281 log_softmax: 0.246591\n",
            "Train Epoch: 283 [400/1000 (40%)]\tLosses F.softmax: 0.018769 log_softmax: 0.021837\n",
            "Train Epoch: 283 [600/1000 (60%)]\tLosses F.softmax: 0.544182 log_softmax: 0.399300\n",
            "Train Epoch: 283 [800/1000 (80%)]\tLosses F.softmax: 0.043097 log_softmax: 0.029172\n",
            "Train Epoch: 283 [1000/1000 (100%)]\tLosses F.softmax: 0.170560 log_softmax: 0.175567\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5158\tAccuracy: 8406.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5065\tAccuracy: 8441.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 284 [0/1000 (0%)]\tLosses F.softmax: 0.001499 log_softmax: 0.002745\n",
            "Train Epoch: 284 [200/1000 (20%)]\tLosses F.softmax: 0.316964 log_softmax: 0.272591\n",
            "Train Epoch: 284 [400/1000 (40%)]\tLosses F.softmax: 0.165593 log_softmax: 0.101735\n",
            "Train Epoch: 284 [600/1000 (60%)]\tLosses F.softmax: 0.351342 log_softmax: 0.278299\n",
            "Train Epoch: 284 [800/1000 (80%)]\tLosses F.softmax: 0.157217 log_softmax: 0.405978\n",
            "Train Epoch: 284 [1000/1000 (100%)]\tLosses F.softmax: 0.843051 log_softmax: 1.078492\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5154\tAccuracy: 8414.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5056\tAccuracy: 8448.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 285 [0/1000 (0%)]\tLosses F.softmax: 0.125141 log_softmax: 0.095149\n",
            "Train Epoch: 285 [200/1000 (20%)]\tLosses F.softmax: 0.008545 log_softmax: 0.007917\n",
            "Train Epoch: 285 [400/1000 (40%)]\tLosses F.softmax: 0.032015 log_softmax: 0.050140\n",
            "Train Epoch: 285 [600/1000 (60%)]\tLosses F.softmax: 0.150538 log_softmax: 0.189423\n",
            "Train Epoch: 285 [800/1000 (80%)]\tLosses F.softmax: 0.784868 log_softmax: 0.583670\n",
            "Train Epoch: 285 [1000/1000 (100%)]\tLosses F.softmax: 0.225584 log_softmax: 0.219884\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5145\tAccuracy: 8407.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5046\tAccuracy: 8450.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 286 [0/1000 (0%)]\tLosses F.softmax: 0.841475 log_softmax: 0.495329\n",
            "Train Epoch: 286 [200/1000 (20%)]\tLosses F.softmax: 0.223447 log_softmax: 0.347668\n",
            "Train Epoch: 286 [400/1000 (40%)]\tLosses F.softmax: 0.205369 log_softmax: 0.123152\n",
            "Train Epoch: 286 [600/1000 (60%)]\tLosses F.softmax: 0.158124 log_softmax: 0.195985\n",
            "Train Epoch: 286 [800/1000 (80%)]\tLosses F.softmax: 0.021508 log_softmax: 0.022263\n",
            "Train Epoch: 286 [1000/1000 (100%)]\tLosses F.softmax: 0.015649 log_softmax: 0.014558\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5145\tAccuracy: 8407.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5048\tAccuracy: 8461.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 287 [0/1000 (0%)]\tLosses F.softmax: 0.014477 log_softmax: 0.009943\n",
            "Train Epoch: 287 [200/1000 (20%)]\tLosses F.softmax: 0.035930 log_softmax: 0.070154\n",
            "Train Epoch: 287 [400/1000 (40%)]\tLosses F.softmax: 0.008515 log_softmax: 0.006738\n",
            "Train Epoch: 287 [600/1000 (60%)]\tLosses F.softmax: 0.005862 log_softmax: 0.005568\n",
            "Train Epoch: 287 [800/1000 (80%)]\tLosses F.softmax: 0.048337 log_softmax: 0.042485\n",
            "Train Epoch: 287 [1000/1000 (100%)]\tLosses F.softmax: 0.573288 log_softmax: 1.002874\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5143\tAccuracy: 8414.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5043\tAccuracy: 8456.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 288 [0/1000 (0%)]\tLosses F.softmax: 0.042718 log_softmax: 0.047637\n",
            "Train Epoch: 288 [200/1000 (20%)]\tLosses F.softmax: 0.002448 log_softmax: 0.002615\n",
            "Train Epoch: 288 [400/1000 (40%)]\tLosses F.softmax: 0.003420 log_softmax: 0.003692\n",
            "Train Epoch: 288 [600/1000 (60%)]\tLosses F.softmax: 0.029138 log_softmax: 0.051671\n",
            "Train Epoch: 288 [800/1000 (80%)]\tLosses F.softmax: 0.046386 log_softmax: 0.041243\n",
            "Train Epoch: 288 [1000/1000 (100%)]\tLosses F.softmax: 0.018126 log_softmax: 0.022471\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5143\tAccuracy: 8410.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5046\tAccuracy: 8451.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 289 [0/1000 (0%)]\tLosses F.softmax: 0.319354 log_softmax: 0.206465\n",
            "Train Epoch: 289 [200/1000 (20%)]\tLosses F.softmax: 1.790838 log_softmax: 1.768940\n",
            "Train Epoch: 289 [400/1000 (40%)]\tLosses F.softmax: 0.026148 log_softmax: 0.021382\n",
            "Train Epoch: 289 [600/1000 (60%)]\tLosses F.softmax: 0.037639 log_softmax: 0.029961\n",
            "Train Epoch: 289 [800/1000 (80%)]\tLosses F.softmax: 0.573830 log_softmax: 0.967508\n",
            "Train Epoch: 289 [1000/1000 (100%)]\tLosses F.softmax: 0.038331 log_softmax: 0.027205\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5149\tAccuracy: 8416.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5051\tAccuracy: 8451.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 290 [0/1000 (0%)]\tLosses F.softmax: 0.023389 log_softmax: 0.021566\n",
            "Train Epoch: 290 [200/1000 (20%)]\tLosses F.softmax: 0.446407 log_softmax: 0.350515\n",
            "Train Epoch: 290 [400/1000 (40%)]\tLosses F.softmax: 0.094014 log_softmax: 0.081574\n",
            "Train Epoch: 290 [600/1000 (60%)]\tLosses F.softmax: 0.044868 log_softmax: 0.039592\n",
            "Train Epoch: 290 [800/1000 (80%)]\tLosses F.softmax: 0.137586 log_softmax: 0.101955\n",
            "Train Epoch: 290 [1000/1000 (100%)]\tLosses F.softmax: 0.027185 log_softmax: 0.040860\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5159\tAccuracy: 8396.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5061\tAccuracy: 8449.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 291 [0/1000 (0%)]\tLosses F.softmax: 0.007177 log_softmax: 0.007113\n",
            "Train Epoch: 291 [200/1000 (20%)]\tLosses F.softmax: 0.040511 log_softmax: 0.053910\n",
            "Train Epoch: 291 [400/1000 (40%)]\tLosses F.softmax: 0.022453 log_softmax: 0.058360\n",
            "Train Epoch: 291 [600/1000 (60%)]\tLosses F.softmax: 0.022542 log_softmax: 0.026777\n",
            "Train Epoch: 291 [800/1000 (80%)]\tLosses F.softmax: 0.257740 log_softmax: 0.169337\n",
            "Train Epoch: 291 [1000/1000 (100%)]\tLosses F.softmax: 0.033742 log_softmax: 0.024175\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5152\tAccuracy: 8408.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5051\tAccuracy: 8449.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 292 [0/1000 (0%)]\tLosses F.softmax: 0.052718 log_softmax: 0.025043\n",
            "Train Epoch: 292 [200/1000 (20%)]\tLosses F.softmax: 0.019338 log_softmax: 0.010297\n",
            "Train Epoch: 292 [400/1000 (40%)]\tLosses F.softmax: 0.022134 log_softmax: 0.036948\n",
            "Train Epoch: 292 [600/1000 (60%)]\tLosses F.softmax: 0.124527 log_softmax: 0.138611\n",
            "Train Epoch: 292 [800/1000 (80%)]\tLosses F.softmax: 0.091548 log_softmax: 0.058584\n",
            "Train Epoch: 292 [1000/1000 (100%)]\tLosses F.softmax: 0.772114 log_softmax: 0.476459\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5150\tAccuracy: 8414.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5046\tAccuracy: 8454.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 293 [0/1000 (0%)]\tLosses F.softmax: 0.037197 log_softmax: 0.030521\n",
            "Train Epoch: 293 [200/1000 (20%)]\tLosses F.softmax: 0.036226 log_softmax: 0.035593\n",
            "Train Epoch: 293 [400/1000 (40%)]\tLosses F.softmax: 0.905459 log_softmax: 1.372434\n",
            "Train Epoch: 293 [600/1000 (60%)]\tLosses F.softmax: 0.045136 log_softmax: 0.050841\n",
            "Train Epoch: 293 [800/1000 (80%)]\tLosses F.softmax: 0.002590 log_softmax: 0.004884\n",
            "Train Epoch: 293 [1000/1000 (100%)]\tLosses F.softmax: 0.075890 log_softmax: 0.085906\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5152\tAccuracy: 8402.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5050\tAccuracy: 8454.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 294 [0/1000 (0%)]\tLosses F.softmax: 0.118218 log_softmax: 0.089486\n",
            "Train Epoch: 294 [200/1000 (20%)]\tLosses F.softmax: 0.036463 log_softmax: 0.029617\n",
            "Train Epoch: 294 [400/1000 (40%)]\tLosses F.softmax: 0.052780 log_softmax: 0.096160\n",
            "Train Epoch: 294 [600/1000 (60%)]\tLosses F.softmax: 0.055883 log_softmax: 0.048336\n",
            "Train Epoch: 294 [800/1000 (80%)]\tLosses F.softmax: 0.142784 log_softmax: 0.104496\n",
            "Train Epoch: 294 [1000/1000 (100%)]\tLosses F.softmax: 0.005372 log_softmax: 0.008011\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5153\tAccuracy: 8397.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5057\tAccuracy: 8446.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 295 [0/1000 (0%)]\tLosses F.softmax: 0.017255 log_softmax: 0.020715\n",
            "Train Epoch: 295 [200/1000 (20%)]\tLosses F.softmax: 0.021156 log_softmax: 0.026874\n",
            "Train Epoch: 295 [400/1000 (40%)]\tLosses F.softmax: 0.014375 log_softmax: 0.018759\n",
            "Train Epoch: 295 [600/1000 (60%)]\tLosses F.softmax: 0.323991 log_softmax: 0.531550\n",
            "Train Epoch: 295 [800/1000 (80%)]\tLosses F.softmax: 0.721907 log_softmax: 1.226698\n",
            "Train Epoch: 295 [1000/1000 (100%)]\tLosses F.softmax: 0.013793 log_softmax: 0.011892\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5149\tAccuracy: 8412.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5054\tAccuracy: 8447.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 296 [0/1000 (0%)]\tLosses F.softmax: 0.004767 log_softmax: 0.009864\n",
            "Train Epoch: 296 [200/1000 (20%)]\tLosses F.softmax: 0.089445 log_softmax: 0.098862\n",
            "Train Epoch: 296 [400/1000 (40%)]\tLosses F.softmax: 0.018326 log_softmax: 0.011226\n",
            "Train Epoch: 296 [600/1000 (60%)]\tLosses F.softmax: 0.086130 log_softmax: 0.069403\n",
            "Train Epoch: 296 [800/1000 (80%)]\tLosses F.softmax: 0.717200 log_softmax: 0.535057\n",
            "Train Epoch: 296 [1000/1000 (100%)]\tLosses F.softmax: 0.448192 log_softmax: 0.372317\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5145\tAccuracy: 8417.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5046\tAccuracy: 8451.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 297 [0/1000 (0%)]\tLosses F.softmax: 0.015176 log_softmax: 0.021801\n",
            "Train Epoch: 297 [200/1000 (20%)]\tLosses F.softmax: 0.020550 log_softmax: 0.024611\n",
            "Train Epoch: 297 [400/1000 (40%)]\tLosses F.softmax: 0.028529 log_softmax: 0.035732\n",
            "Train Epoch: 297 [600/1000 (60%)]\tLosses F.softmax: 0.052733 log_softmax: 0.028383\n",
            "Train Epoch: 297 [800/1000 (80%)]\tLosses F.softmax: 0.090494 log_softmax: 0.111416\n",
            "Train Epoch: 297 [1000/1000 (100%)]\tLosses F.softmax: 0.484834 log_softmax: 0.328236\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5151\tAccuracy: 8414.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5050\tAccuracy: 8455.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 298 [0/1000 (0%)]\tLosses F.softmax: 0.072710 log_softmax: 0.066793\n",
            "Train Epoch: 298 [200/1000 (20%)]\tLosses F.softmax: 0.026758 log_softmax: 0.016254\n",
            "Train Epoch: 298 [400/1000 (40%)]\tLosses F.softmax: 0.727063 log_softmax: 0.937771\n",
            "Train Epoch: 298 [600/1000 (60%)]\tLosses F.softmax: 0.106774 log_softmax: 0.054884\n",
            "Train Epoch: 298 [800/1000 (80%)]\tLosses F.softmax: 0.001001 log_softmax: 0.002940\n",
            "Train Epoch: 298 [1000/1000 (100%)]\tLosses F.softmax: 0.032839 log_softmax: 0.052262\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5141\tAccuracy: 8413.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5046\tAccuracy: 8447.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 299 [0/1000 (0%)]\tLosses F.softmax: 0.091579 log_softmax: 0.092711\n",
            "Train Epoch: 299 [200/1000 (20%)]\tLosses F.softmax: 0.015417 log_softmax: 0.013587\n",
            "Train Epoch: 299 [400/1000 (40%)]\tLosses F.softmax: 0.019000 log_softmax: 0.026940\n",
            "Train Epoch: 299 [600/1000 (60%)]\tLosses F.softmax: 0.094045 log_softmax: 0.067328\n",
            "Train Epoch: 299 [800/1000 (80%)]\tLosses F.softmax: 0.016129 log_softmax: 0.022830\n",
            "Train Epoch: 299 [1000/1000 (100%)]\tLosses F.softmax: 0.096978 log_softmax: 0.058251\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5151\tAccuracy: 8408.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5057\tAccuracy: 8441.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 300 [0/1000 (0%)]\tLosses F.softmax: 0.015167 log_softmax: 0.063487\n",
            "Train Epoch: 300 [200/1000 (20%)]\tLosses F.softmax: 0.120946 log_softmax: 0.087122\n",
            "Train Epoch: 300 [400/1000 (40%)]\tLosses F.softmax: 0.005862 log_softmax: 0.004870\n",
            "Train Epoch: 300 [600/1000 (60%)]\tLosses F.softmax: 0.007457 log_softmax: 0.008428\n",
            "Train Epoch: 300 [800/1000 (80%)]\tLosses F.softmax: 0.027875 log_softmax: 0.020239\n",
            "Train Epoch: 300 [1000/1000 (100%)]\tLosses F.softmax: 0.016895 log_softmax: 0.018536\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5155\tAccuracy: 8413.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5050\tAccuracy: 8453.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 301 [0/1000 (0%)]\tLosses F.softmax: 0.132048 log_softmax: 0.192568\n",
            "Train Epoch: 301 [200/1000 (20%)]\tLosses F.softmax: 0.003382 log_softmax: 0.002768\n",
            "Train Epoch: 301 [400/1000 (40%)]\tLosses F.softmax: 0.017847 log_softmax: 0.016392\n",
            "Train Epoch: 301 [600/1000 (60%)]\tLosses F.softmax: 0.028739 log_softmax: 0.028490\n",
            "Train Epoch: 301 [800/1000 (80%)]\tLosses F.softmax: 0.021998 log_softmax: 0.018512\n",
            "Train Epoch: 301 [1000/1000 (100%)]\tLosses F.softmax: 0.071495 log_softmax: 0.057256\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5151\tAccuracy: 8412.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5050\tAccuracy: 8449.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 302 [0/1000 (0%)]\tLosses F.softmax: 0.003234 log_softmax: 0.004925\n",
            "Train Epoch: 302 [200/1000 (20%)]\tLosses F.softmax: 0.021265 log_softmax: 0.039363\n",
            "Train Epoch: 302 [400/1000 (40%)]\tLosses F.softmax: 0.017127 log_softmax: 0.016100\n",
            "Train Epoch: 302 [600/1000 (60%)]\tLosses F.softmax: 0.014218 log_softmax: 0.021536\n",
            "Train Epoch: 302 [800/1000 (80%)]\tLosses F.softmax: 0.013091 log_softmax: 0.026619\n",
            "Train Epoch: 302 [1000/1000 (100%)]\tLosses F.softmax: 1.330896 log_softmax: 2.601311\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5153\tAccuracy: 8413.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5051\tAccuracy: 8450.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 303 [0/1000 (0%)]\tLosses F.softmax: 1.106064 log_softmax: 0.547823\n",
            "Train Epoch: 303 [200/1000 (20%)]\tLosses F.softmax: 0.134540 log_softmax: 0.187773\n",
            "Train Epoch: 303 [400/1000 (40%)]\tLosses F.softmax: 0.022185 log_softmax: 0.035015\n",
            "Train Epoch: 303 [600/1000 (60%)]\tLosses F.softmax: 0.344089 log_softmax: 0.277146\n",
            "Train Epoch: 303 [800/1000 (80%)]\tLosses F.softmax: 0.036852 log_softmax: 0.020317\n",
            "Train Epoch: 303 [1000/1000 (100%)]\tLosses F.softmax: 0.213676 log_softmax: 0.187778\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5153\tAccuracy: 8407.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5052\tAccuracy: 8444.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 304 [0/1000 (0%)]\tLosses F.softmax: 0.004036 log_softmax: 0.011640\n",
            "Train Epoch: 304 [200/1000 (20%)]\tLosses F.softmax: 0.024169 log_softmax: 0.033980\n",
            "Train Epoch: 304 [400/1000 (40%)]\tLosses F.softmax: 0.386975 log_softmax: 0.358761\n",
            "Train Epoch: 304 [600/1000 (60%)]\tLosses F.softmax: 1.913813 log_softmax: 1.196444\n",
            "Train Epoch: 304 [800/1000 (80%)]\tLosses F.softmax: 0.023167 log_softmax: 0.021027\n",
            "Train Epoch: 304 [1000/1000 (100%)]\tLosses F.softmax: 0.015893 log_softmax: 0.016432\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5153\tAccuracy: 8417.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5058\tAccuracy: 8442.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 305 [0/1000 (0%)]\tLosses F.softmax: 0.938299 log_softmax: 1.571509\n",
            "Train Epoch: 305 [200/1000 (20%)]\tLosses F.softmax: 2.419484 log_softmax: 2.221868\n",
            "Train Epoch: 305 [400/1000 (40%)]\tLosses F.softmax: 0.114005 log_softmax: 0.222421\n",
            "Train Epoch: 305 [600/1000 (60%)]\tLosses F.softmax: 0.023148 log_softmax: 0.032745\n",
            "Train Epoch: 305 [800/1000 (80%)]\tLosses F.softmax: 0.170606 log_softmax: 0.168879\n",
            "Train Epoch: 305 [1000/1000 (100%)]\tLosses F.softmax: 0.006380 log_softmax: 0.014036\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5152\tAccuracy: 8414.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5055\tAccuracy: 8443.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 306 [0/1000 (0%)]\tLosses F.softmax: 0.016299 log_softmax: 0.016576\n",
            "Train Epoch: 306 [200/1000 (20%)]\tLosses F.softmax: 0.059057 log_softmax: 0.024204\n",
            "Train Epoch: 306 [400/1000 (40%)]\tLosses F.softmax: 0.001951 log_softmax: 0.002653\n",
            "Train Epoch: 306 [600/1000 (60%)]\tLosses F.softmax: 0.014301 log_softmax: 0.013984\n",
            "Train Epoch: 306 [800/1000 (80%)]\tLosses F.softmax: 0.009034 log_softmax: 0.006507\n",
            "Train Epoch: 306 [1000/1000 (100%)]\tLosses F.softmax: 0.050720 log_softmax: 0.107151\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5168\tAccuracy: 8408.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5067\tAccuracy: 8445.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 307 [0/1000 (0%)]\tLosses F.softmax: 0.012388 log_softmax: 0.010846\n",
            "Train Epoch: 307 [200/1000 (20%)]\tLosses F.softmax: 0.285587 log_softmax: 0.233970\n",
            "Train Epoch: 307 [400/1000 (40%)]\tLosses F.softmax: 0.011659 log_softmax: 0.007078\n",
            "Train Epoch: 307 [600/1000 (60%)]\tLosses F.softmax: 0.023597 log_softmax: 0.018468\n",
            "Train Epoch: 307 [800/1000 (80%)]\tLosses F.softmax: 0.043783 log_softmax: 0.096755\n",
            "Train Epoch: 307 [1000/1000 (100%)]\tLosses F.softmax: 0.049091 log_softmax: 0.021943\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5163\tAccuracy: 8411.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5058\tAccuracy: 8444.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 308 [0/1000 (0%)]\tLosses F.softmax: 0.011808 log_softmax: 0.013542\n",
            "Train Epoch: 308 [200/1000 (20%)]\tLosses F.softmax: 0.030449 log_softmax: 0.040213\n",
            "Train Epoch: 308 [400/1000 (40%)]\tLosses F.softmax: 0.669638 log_softmax: 0.747357\n",
            "Train Epoch: 308 [600/1000 (60%)]\tLosses F.softmax: 0.037326 log_softmax: 0.029290\n",
            "Train Epoch: 308 [800/1000 (80%)]\tLosses F.softmax: 0.105982 log_softmax: 0.125346\n",
            "Train Epoch: 308 [1000/1000 (100%)]\tLosses F.softmax: 0.003933 log_softmax: 0.006375\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5163\tAccuracy: 8408.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5056\tAccuracy: 8450.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 309 [0/1000 (0%)]\tLosses F.softmax: 0.000326 log_softmax: 0.000787\n",
            "Train Epoch: 309 [200/1000 (20%)]\tLosses F.softmax: 0.002260 log_softmax: 0.002832\n",
            "Train Epoch: 309 [400/1000 (40%)]\tLosses F.softmax: 0.287739 log_softmax: 0.187510\n",
            "Train Epoch: 309 [600/1000 (60%)]\tLosses F.softmax: 0.106225 log_softmax: 0.113231\n",
            "Train Epoch: 309 [800/1000 (80%)]\tLosses F.softmax: 0.004861 log_softmax: 0.007033\n",
            "Train Epoch: 309 [1000/1000 (100%)]\tLosses F.softmax: 0.002011 log_softmax: 0.002082\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5149\tAccuracy: 8423.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5051\tAccuracy: 8447.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 310 [0/1000 (0%)]\tLosses F.softmax: 0.001368 log_softmax: 0.002066\n",
            "Train Epoch: 310 [200/1000 (20%)]\tLosses F.softmax: 0.053276 log_softmax: 0.049615\n",
            "Train Epoch: 310 [400/1000 (40%)]\tLosses F.softmax: 0.070605 log_softmax: 0.046190\n",
            "Train Epoch: 310 [600/1000 (60%)]\tLosses F.softmax: 0.202482 log_softmax: 0.105524\n",
            "Train Epoch: 310 [800/1000 (80%)]\tLosses F.softmax: 0.014665 log_softmax: 0.013258\n",
            "Train Epoch: 310 [1000/1000 (100%)]\tLosses F.softmax: 0.186219 log_softmax: 0.223946\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5151\tAccuracy: 8415.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5048\tAccuracy: 8451.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 311 [0/1000 (0%)]\tLosses F.softmax: 0.210286 log_softmax: 0.213695\n",
            "Train Epoch: 311 [200/1000 (20%)]\tLosses F.softmax: 0.671426 log_softmax: 0.532702\n",
            "Train Epoch: 311 [400/1000 (40%)]\tLosses F.softmax: 0.043862 log_softmax: 0.048078\n",
            "Train Epoch: 311 [600/1000 (60%)]\tLosses F.softmax: 0.072555 log_softmax: 0.054339\n",
            "Train Epoch: 311 [800/1000 (80%)]\tLosses F.softmax: 0.045827 log_softmax: 0.030217\n",
            "Train Epoch: 311 [1000/1000 (100%)]\tLosses F.softmax: 0.031351 log_softmax: 0.026594\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5165\tAccuracy: 8406.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5061\tAccuracy: 8462.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 312 [0/1000 (0%)]\tLosses F.softmax: 0.061800 log_softmax: 0.127506\n",
            "Train Epoch: 312 [200/1000 (20%)]\tLosses F.softmax: 0.050183 log_softmax: 0.034876\n",
            "Train Epoch: 312 [400/1000 (40%)]\tLosses F.softmax: 0.001522 log_softmax: 0.002167\n",
            "Train Epoch: 312 [600/1000 (60%)]\tLosses F.softmax: 0.003804 log_softmax: 0.005057\n",
            "Train Epoch: 312 [800/1000 (80%)]\tLosses F.softmax: 1.171023 log_softmax: 1.112085\n",
            "Train Epoch: 312 [1000/1000 (100%)]\tLosses F.softmax: 0.023931 log_softmax: 0.020791\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5155\tAccuracy: 8419.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5053\tAccuracy: 8457.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 313 [0/1000 (0%)]\tLosses F.softmax: 0.020228 log_softmax: 0.021437\n",
            "Train Epoch: 313 [200/1000 (20%)]\tLosses F.softmax: 0.289943 log_softmax: 0.284134\n",
            "Train Epoch: 313 [400/1000 (40%)]\tLosses F.softmax: 0.017982 log_softmax: 0.016573\n",
            "Train Epoch: 313 [600/1000 (60%)]\tLosses F.softmax: 0.305367 log_softmax: 0.161867\n",
            "Train Epoch: 313 [800/1000 (80%)]\tLosses F.softmax: 0.016301 log_softmax: 0.013893\n",
            "Train Epoch: 313 [1000/1000 (100%)]\tLosses F.softmax: 0.178368 log_softmax: 0.144347\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5166\tAccuracy: 8416.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5064\tAccuracy: 8446.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 314 [0/1000 (0%)]\tLosses F.softmax: 0.005737 log_softmax: 0.013327\n",
            "Train Epoch: 314 [200/1000 (20%)]\tLosses F.softmax: 0.013614 log_softmax: 0.012648\n",
            "Train Epoch: 314 [400/1000 (40%)]\tLosses F.softmax: 0.025008 log_softmax: 0.028693\n",
            "Train Epoch: 314 [600/1000 (60%)]\tLosses F.softmax: 0.405675 log_softmax: 0.810907\n",
            "Train Epoch: 314 [800/1000 (80%)]\tLosses F.softmax: 0.037036 log_softmax: 0.072477\n",
            "Train Epoch: 314 [1000/1000 (100%)]\tLosses F.softmax: 0.212594 log_softmax: 0.251483\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5162\tAccuracy: 8413.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5062\tAccuracy: 8455.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 315 [0/1000 (0%)]\tLosses F.softmax: 0.316322 log_softmax: 0.415346\n",
            "Train Epoch: 315 [200/1000 (20%)]\tLosses F.softmax: 0.025771 log_softmax: 0.066259\n",
            "Train Epoch: 315 [400/1000 (40%)]\tLosses F.softmax: 0.152159 log_softmax: 0.082501\n",
            "Train Epoch: 315 [600/1000 (60%)]\tLosses F.softmax: 0.012476 log_softmax: 0.035309\n",
            "Train Epoch: 315 [800/1000 (80%)]\tLosses F.softmax: 0.004870 log_softmax: 0.011104\n",
            "Train Epoch: 315 [1000/1000 (100%)]\tLosses F.softmax: 0.044603 log_softmax: 0.057516\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5182\tAccuracy: 8412.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5079\tAccuracy: 8447.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 316 [0/1000 (0%)]\tLosses F.softmax: 0.062552 log_softmax: 0.029548\n",
            "Train Epoch: 316 [200/1000 (20%)]\tLosses F.softmax: 0.004578 log_softmax: 0.004751\n",
            "Train Epoch: 316 [400/1000 (40%)]\tLosses F.softmax: 0.304670 log_softmax: 0.333055\n",
            "Train Epoch: 316 [600/1000 (60%)]\tLosses F.softmax: 0.085108 log_softmax: 0.049693\n",
            "Train Epoch: 316 [800/1000 (80%)]\tLosses F.softmax: 0.147715 log_softmax: 0.085276\n",
            "Train Epoch: 316 [1000/1000 (100%)]\tLosses F.softmax: 0.031295 log_softmax: 0.024126\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5174\tAccuracy: 8413.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5071\tAccuracy: 8447.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 317 [0/1000 (0%)]\tLosses F.softmax: 0.004288 log_softmax: 0.003284\n",
            "Train Epoch: 317 [200/1000 (20%)]\tLosses F.softmax: 0.126529 log_softmax: 0.092385\n",
            "Train Epoch: 317 [400/1000 (40%)]\tLosses F.softmax: 0.007832 log_softmax: 0.007426\n",
            "Train Epoch: 317 [600/1000 (60%)]\tLosses F.softmax: 0.013622 log_softmax: 0.012780\n",
            "Train Epoch: 317 [800/1000 (80%)]\tLosses F.softmax: 0.014309 log_softmax: 0.016061\n",
            "Train Epoch: 317 [1000/1000 (100%)]\tLosses F.softmax: 0.034280 log_softmax: 0.032119\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5177\tAccuracy: 8407.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5074\tAccuracy: 8456.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 318 [0/1000 (0%)]\tLosses F.softmax: 1.263680 log_softmax: 0.846687\n",
            "Train Epoch: 318 [200/1000 (20%)]\tLosses F.softmax: 0.753997 log_softmax: 0.684924\n",
            "Train Epoch: 318 [400/1000 (40%)]\tLosses F.softmax: 0.014810 log_softmax: 0.017559\n",
            "Train Epoch: 318 [600/1000 (60%)]\tLosses F.softmax: 0.222645 log_softmax: 0.235077\n",
            "Train Epoch: 318 [800/1000 (80%)]\tLosses F.softmax: 0.070938 log_softmax: 0.048316\n",
            "Train Epoch: 318 [1000/1000 (100%)]\tLosses F.softmax: 0.168535 log_softmax: 0.258521\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5159\tAccuracy: 8418.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5058\tAccuracy: 8461.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 319 [0/1000 (0%)]\tLosses F.softmax: 0.035714 log_softmax: 0.070076\n",
            "Train Epoch: 319 [200/1000 (20%)]\tLosses F.softmax: 0.038464 log_softmax: 0.030263\n",
            "Train Epoch: 319 [400/1000 (40%)]\tLosses F.softmax: 0.031338 log_softmax: 0.038732\n",
            "Train Epoch: 319 [600/1000 (60%)]\tLosses F.softmax: 0.096207 log_softmax: 0.096936\n",
            "Train Epoch: 319 [800/1000 (80%)]\tLosses F.softmax: 0.204630 log_softmax: 0.221464\n",
            "Train Epoch: 319 [1000/1000 (100%)]\tLosses F.softmax: 0.001495 log_softmax: 0.003458\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5172\tAccuracy: 8409.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5064\tAccuracy: 8457.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 320 [0/1000 (0%)]\tLosses F.softmax: 0.084674 log_softmax: 0.151362\n",
            "Train Epoch: 320 [200/1000 (20%)]\tLosses F.softmax: 0.025638 log_softmax: 0.020650\n",
            "Train Epoch: 320 [400/1000 (40%)]\tLosses F.softmax: 0.181549 log_softmax: 0.096914\n",
            "Train Epoch: 320 [600/1000 (60%)]\tLosses F.softmax: 0.533319 log_softmax: 0.774707\n",
            "Train Epoch: 320 [800/1000 (80%)]\tLosses F.softmax: 0.244701 log_softmax: 0.243058\n",
            "Train Epoch: 320 [1000/1000 (100%)]\tLosses F.softmax: 0.056224 log_softmax: 0.031577\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5180\tAccuracy: 8406.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5074\tAccuracy: 8444.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 321 [0/1000 (0%)]\tLosses F.softmax: 0.037148 log_softmax: 0.023878\n",
            "Train Epoch: 321 [200/1000 (20%)]\tLosses F.softmax: 0.083029 log_softmax: 0.062281\n",
            "Train Epoch: 321 [400/1000 (40%)]\tLosses F.softmax: 0.058400 log_softmax: 0.031883\n",
            "Train Epoch: 321 [600/1000 (60%)]\tLosses F.softmax: 0.152761 log_softmax: 0.081875\n",
            "Train Epoch: 321 [800/1000 (80%)]\tLosses F.softmax: 0.013240 log_softmax: 0.014010\n",
            "Train Epoch: 321 [1000/1000 (100%)]\tLosses F.softmax: 1.526136 log_softmax: 2.021735\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5174\tAccuracy: 8410.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5071\tAccuracy: 8460.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 322 [0/1000 (0%)]\tLosses F.softmax: 0.045995 log_softmax: 0.018529\n",
            "Train Epoch: 322 [200/1000 (20%)]\tLosses F.softmax: 0.212364 log_softmax: 0.211782\n",
            "Train Epoch: 322 [400/1000 (40%)]\tLosses F.softmax: 0.012069 log_softmax: 0.011051\n",
            "Train Epoch: 322 [600/1000 (60%)]\tLosses F.softmax: 0.024819 log_softmax: 0.029851\n",
            "Train Epoch: 322 [800/1000 (80%)]\tLosses F.softmax: 0.036162 log_softmax: 0.027620\n",
            "Train Epoch: 322 [1000/1000 (100%)]\tLosses F.softmax: 0.378796 log_softmax: 0.293817\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5181\tAccuracy: 8410.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5076\tAccuracy: 8456.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 323 [0/1000 (0%)]\tLosses F.softmax: 0.376661 log_softmax: 0.725356\n",
            "Train Epoch: 323 [200/1000 (20%)]\tLosses F.softmax: 0.007339 log_softmax: 0.007732\n",
            "Train Epoch: 323 [400/1000 (40%)]\tLosses F.softmax: 0.002937 log_softmax: 0.004210\n",
            "Train Epoch: 323 [600/1000 (60%)]\tLosses F.softmax: 0.058804 log_softmax: 0.033924\n",
            "Train Epoch: 323 [800/1000 (80%)]\tLosses F.softmax: 0.033562 log_softmax: 0.048146\n",
            "Train Epoch: 323 [1000/1000 (100%)]\tLosses F.softmax: 0.013429 log_softmax: 0.010260\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5183\tAccuracy: 8411.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5075\tAccuracy: 8455.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 324 [0/1000 (0%)]\tLosses F.softmax: 0.300749 log_softmax: 0.213071\n",
            "Train Epoch: 324 [200/1000 (20%)]\tLosses F.softmax: 0.069659 log_softmax: 0.049117\n",
            "Train Epoch: 324 [400/1000 (40%)]\tLosses F.softmax: 0.027510 log_softmax: 0.015391\n",
            "Train Epoch: 324 [600/1000 (60%)]\tLosses F.softmax: 0.016779 log_softmax: 0.015975\n",
            "Train Epoch: 324 [800/1000 (80%)]\tLosses F.softmax: 0.046344 log_softmax: 0.032995\n",
            "Train Epoch: 324 [1000/1000 (100%)]\tLosses F.softmax: 0.064094 log_softmax: 0.070455\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5179\tAccuracy: 8407.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5074\tAccuracy: 8458.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 325 [0/1000 (0%)]\tLosses F.softmax: 0.449193 log_softmax: 0.275261\n",
            "Train Epoch: 325 [200/1000 (20%)]\tLosses F.softmax: 0.673596 log_softmax: 0.784109\n",
            "Train Epoch: 325 [400/1000 (40%)]\tLosses F.softmax: 0.005798 log_softmax: 0.014477\n",
            "Train Epoch: 325 [600/1000 (60%)]\tLosses F.softmax: 0.013168 log_softmax: 0.008523\n",
            "Train Epoch: 325 [800/1000 (80%)]\tLosses F.softmax: 0.009757 log_softmax: 0.009188\n",
            "Train Epoch: 325 [1000/1000 (100%)]\tLosses F.softmax: 0.047701 log_softmax: 0.043851\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5180\tAccuracy: 8411.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5071\tAccuracy: 8451.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 326 [0/1000 (0%)]\tLosses F.softmax: 0.002860 log_softmax: 0.004496\n",
            "Train Epoch: 326 [200/1000 (20%)]\tLosses F.softmax: 0.005802 log_softmax: 0.012491\n",
            "Train Epoch: 326 [400/1000 (40%)]\tLosses F.softmax: 0.127837 log_softmax: 0.071901\n",
            "Train Epoch: 326 [600/1000 (60%)]\tLosses F.softmax: 0.017213 log_softmax: 0.011900\n",
            "Train Epoch: 326 [800/1000 (80%)]\tLosses F.softmax: 0.012786 log_softmax: 0.011002\n",
            "Train Epoch: 326 [1000/1000 (100%)]\tLosses F.softmax: 0.079257 log_softmax: 0.107249\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5190\tAccuracy: 8406.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5081\tAccuracy: 8456.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 327 [0/1000 (0%)]\tLosses F.softmax: 0.003904 log_softmax: 0.003137\n",
            "Train Epoch: 327 [200/1000 (20%)]\tLosses F.softmax: 0.004821 log_softmax: 0.007078\n",
            "Train Epoch: 327 [400/1000 (40%)]\tLosses F.softmax: 0.005868 log_softmax: 0.013365\n",
            "Train Epoch: 327 [600/1000 (60%)]\tLosses F.softmax: 0.010124 log_softmax: 0.026764\n",
            "Train Epoch: 327 [800/1000 (80%)]\tLosses F.softmax: 0.007936 log_softmax: 0.011046\n",
            "Train Epoch: 327 [1000/1000 (100%)]\tLosses F.softmax: 0.006382 log_softmax: 0.006555\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5175\tAccuracy: 8415.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5070\tAccuracy: 8466.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 328 [0/1000 (0%)]\tLosses F.softmax: 2.365298 log_softmax: 1.497621\n",
            "Train Epoch: 328 [200/1000 (20%)]\tLosses F.softmax: 0.463182 log_softmax: 0.909473\n",
            "Train Epoch: 328 [400/1000 (40%)]\tLosses F.softmax: 0.004872 log_softmax: 0.016828\n",
            "Train Epoch: 328 [600/1000 (60%)]\tLosses F.softmax: 0.019584 log_softmax: 0.020144\n",
            "Train Epoch: 328 [800/1000 (80%)]\tLosses F.softmax: 0.468391 log_softmax: 0.679533\n",
            "Train Epoch: 328 [1000/1000 (100%)]\tLosses F.softmax: 0.002967 log_softmax: 0.005884\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5180\tAccuracy: 8415.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5074\tAccuracy: 8458.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 329 [0/1000 (0%)]\tLosses F.softmax: 0.006962 log_softmax: 0.004861\n",
            "Train Epoch: 329 [200/1000 (20%)]\tLosses F.softmax: 0.015699 log_softmax: 0.011159\n",
            "Train Epoch: 329 [400/1000 (40%)]\tLosses F.softmax: 0.171228 log_softmax: 0.135096\n",
            "Train Epoch: 329 [600/1000 (60%)]\tLosses F.softmax: 0.003231 log_softmax: 0.004317\n",
            "Train Epoch: 329 [800/1000 (80%)]\tLosses F.softmax: 0.016794 log_softmax: 0.011204\n",
            "Train Epoch: 329 [1000/1000 (100%)]\tLosses F.softmax: 1.003035 log_softmax: 0.894122\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5199\tAccuracy: 8409.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5086\tAccuracy: 8441.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 330 [0/1000 (0%)]\tLosses F.softmax: 0.024289 log_softmax: 0.051254\n",
            "Train Epoch: 330 [200/1000 (20%)]\tLosses F.softmax: 0.128942 log_softmax: 0.091545\n",
            "Train Epoch: 330 [400/1000 (40%)]\tLosses F.softmax: 0.053407 log_softmax: 0.036818\n",
            "Train Epoch: 330 [600/1000 (60%)]\tLosses F.softmax: 0.097888 log_softmax: 0.084891\n",
            "Train Epoch: 330 [800/1000 (80%)]\tLosses F.softmax: 0.470731 log_softmax: 0.967714\n",
            "Train Epoch: 330 [1000/1000 (100%)]\tLosses F.softmax: 0.102754 log_softmax: 0.081884\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5189\tAccuracy: 8415.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5079\tAccuracy: 8460.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 331 [0/1000 (0%)]\tLosses F.softmax: 0.372040 log_softmax: 0.472229\n",
            "Train Epoch: 331 [200/1000 (20%)]\tLosses F.softmax: 0.033979 log_softmax: 0.028367\n",
            "Train Epoch: 331 [400/1000 (40%)]\tLosses F.softmax: 0.224951 log_softmax: 0.365283\n",
            "Train Epoch: 331 [600/1000 (60%)]\tLosses F.softmax: 0.153286 log_softmax: 0.141135\n",
            "Train Epoch: 331 [800/1000 (80%)]\tLosses F.softmax: 0.016729 log_softmax: 0.015234\n",
            "Train Epoch: 331 [1000/1000 (100%)]\tLosses F.softmax: 0.054801 log_softmax: 0.039111\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5194\tAccuracy: 8408.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5088\tAccuracy: 8455.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 332 [0/1000 (0%)]\tLosses F.softmax: 0.071722 log_softmax: 0.087143\n",
            "Train Epoch: 332 [200/1000 (20%)]\tLosses F.softmax: 0.353265 log_softmax: 0.280140\n",
            "Train Epoch: 332 [400/1000 (40%)]\tLosses F.softmax: 0.003985 log_softmax: 0.011270\n",
            "Train Epoch: 332 [600/1000 (60%)]\tLosses F.softmax: 0.010225 log_softmax: 0.007817\n",
            "Train Epoch: 332 [800/1000 (80%)]\tLosses F.softmax: 0.203543 log_softmax: 0.202972\n",
            "Train Epoch: 332 [1000/1000 (100%)]\tLosses F.softmax: 0.137138 log_softmax: 0.143426\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5205\tAccuracy: 8407.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5095\tAccuracy: 8451.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 333 [0/1000 (0%)]\tLosses F.softmax: 0.013485 log_softmax: 0.010440\n",
            "Train Epoch: 333 [200/1000 (20%)]\tLosses F.softmax: 0.029889 log_softmax: 0.024479\n",
            "Train Epoch: 333 [400/1000 (40%)]\tLosses F.softmax: 0.005481 log_softmax: 0.005336\n",
            "Train Epoch: 333 [600/1000 (60%)]\tLosses F.softmax: 0.001442 log_softmax: 0.001612\n",
            "Train Epoch: 333 [800/1000 (80%)]\tLosses F.softmax: 0.112667 log_softmax: 0.022468\n",
            "Train Epoch: 333 [1000/1000 (100%)]\tLosses F.softmax: 0.193159 log_softmax: 0.143710\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5218\tAccuracy: 8409.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5112\tAccuracy: 8443.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 334 [0/1000 (0%)]\tLosses F.softmax: 0.153589 log_softmax: 0.156618\n",
            "Train Epoch: 334 [200/1000 (20%)]\tLosses F.softmax: 0.007101 log_softmax: 0.005028\n",
            "Train Epoch: 334 [400/1000 (40%)]\tLosses F.softmax: 0.038707 log_softmax: 0.026254\n",
            "Train Epoch: 334 [600/1000 (60%)]\tLosses F.softmax: 0.008077 log_softmax: 0.007673\n",
            "Train Epoch: 334 [800/1000 (80%)]\tLosses F.softmax: 0.013528 log_softmax: 0.026349\n",
            "Train Epoch: 334 [1000/1000 (100%)]\tLosses F.softmax: 0.019095 log_softmax: 0.021563\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5208\tAccuracy: 8415.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5106\tAccuracy: 8447.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 335 [0/1000 (0%)]\tLosses F.softmax: 0.229901 log_softmax: 0.195027\n",
            "Train Epoch: 335 [200/1000 (20%)]\tLosses F.softmax: 0.093717 log_softmax: 0.066171\n",
            "Train Epoch: 335 [400/1000 (40%)]\tLosses F.softmax: 0.240861 log_softmax: 0.318169\n",
            "Train Epoch: 335 [600/1000 (60%)]\tLosses F.softmax: 0.125836 log_softmax: 0.073638\n",
            "Train Epoch: 335 [800/1000 (80%)]\tLosses F.softmax: 0.000415 log_softmax: 0.001551\n",
            "Train Epoch: 335 [1000/1000 (100%)]\tLosses F.softmax: 0.157908 log_softmax: 0.303845\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5216\tAccuracy: 8410.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5107\tAccuracy: 8455.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 336 [0/1000 (0%)]\tLosses F.softmax: 0.012242 log_softmax: 0.006561\n",
            "Train Epoch: 336 [200/1000 (20%)]\tLosses F.softmax: 0.966686 log_softmax: 0.460544\n",
            "Train Epoch: 336 [400/1000 (40%)]\tLosses F.softmax: 0.004482 log_softmax: 0.005332\n",
            "Train Epoch: 336 [600/1000 (60%)]\tLosses F.softmax: 0.060856 log_softmax: 0.044763\n",
            "Train Epoch: 336 [800/1000 (80%)]\tLosses F.softmax: 0.129945 log_softmax: 0.084367\n",
            "Train Epoch: 336 [1000/1000 (100%)]\tLosses F.softmax: 0.005354 log_softmax: 0.008183\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5201\tAccuracy: 8418.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5089\tAccuracy: 8457.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 337 [0/1000 (0%)]\tLosses F.softmax: 0.050581 log_softmax: 0.049151\n",
            "Train Epoch: 337 [200/1000 (20%)]\tLosses F.softmax: 1.155469 log_softmax: 0.659321\n",
            "Train Epoch: 337 [400/1000 (40%)]\tLosses F.softmax: 0.086360 log_softmax: 0.101889\n",
            "Train Epoch: 337 [600/1000 (60%)]\tLosses F.softmax: 0.001968 log_softmax: 0.002806\n",
            "Train Epoch: 337 [800/1000 (80%)]\tLosses F.softmax: 0.079324 log_softmax: 0.040393\n",
            "Train Epoch: 337 [1000/1000 (100%)]\tLosses F.softmax: 0.080999 log_softmax: 0.087485\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5214\tAccuracy: 8410.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5098\tAccuracy: 8457.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 338 [0/1000 (0%)]\tLosses F.softmax: 0.012803 log_softmax: 0.022402\n",
            "Train Epoch: 338 [200/1000 (20%)]\tLosses F.softmax: 0.327740 log_softmax: 0.238041\n",
            "Train Epoch: 338 [400/1000 (40%)]\tLosses F.softmax: 0.003321 log_softmax: 0.004293\n",
            "Train Epoch: 338 [600/1000 (60%)]\tLosses F.softmax: 0.017714 log_softmax: 0.018203\n",
            "Train Epoch: 338 [800/1000 (80%)]\tLosses F.softmax: 0.200725 log_softmax: 0.130384\n",
            "Train Epoch: 338 [1000/1000 (100%)]\tLosses F.softmax: 0.011269 log_softmax: 0.017754\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5220\tAccuracy: 8408.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5107\tAccuracy: 8452.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 339 [0/1000 (0%)]\tLosses F.softmax: 0.043410 log_softmax: 0.025961\n",
            "Train Epoch: 339 [200/1000 (20%)]\tLosses F.softmax: 0.022820 log_softmax: 0.022099\n",
            "Train Epoch: 339 [400/1000 (40%)]\tLosses F.softmax: 0.000828 log_softmax: 0.000812\n",
            "Train Epoch: 339 [600/1000 (60%)]\tLosses F.softmax: 0.003601 log_softmax: 0.003545\n",
            "Train Epoch: 339 [800/1000 (80%)]\tLosses F.softmax: 0.002957 log_softmax: 0.002786\n",
            "Train Epoch: 339 [1000/1000 (100%)]\tLosses F.softmax: 0.005773 log_softmax: 0.008297\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5231\tAccuracy: 8406.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5117\tAccuracy: 8447.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 340 [0/1000 (0%)]\tLosses F.softmax: 0.079606 log_softmax: 0.110353\n",
            "Train Epoch: 340 [200/1000 (20%)]\tLosses F.softmax: 0.289599 log_softmax: 0.219212\n",
            "Train Epoch: 340 [400/1000 (40%)]\tLosses F.softmax: 0.036508 log_softmax: 0.030548\n",
            "Train Epoch: 340 [600/1000 (60%)]\tLosses F.softmax: 0.084821 log_softmax: 0.066544\n",
            "Train Epoch: 340 [800/1000 (80%)]\tLosses F.softmax: 0.127942 log_softmax: 0.209164\n",
            "Train Epoch: 340 [1000/1000 (100%)]\tLosses F.softmax: 0.004524 log_softmax: 0.010342\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5233\tAccuracy: 8405.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5121\tAccuracy: 8444.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 341 [0/1000 (0%)]\tLosses F.softmax: 0.001224 log_softmax: 0.001308\n",
            "Train Epoch: 341 [200/1000 (20%)]\tLosses F.softmax: 0.048217 log_softmax: 0.027488\n",
            "Train Epoch: 341 [400/1000 (40%)]\tLosses F.softmax: 0.040779 log_softmax: 0.049016\n",
            "Train Epoch: 341 [600/1000 (60%)]\tLosses F.softmax: 0.099644 log_softmax: 0.079018\n",
            "Train Epoch: 341 [800/1000 (80%)]\tLosses F.softmax: 1.152887 log_softmax: 1.250092\n",
            "Train Epoch: 341 [1000/1000 (100%)]\tLosses F.softmax: 0.077130 log_softmax: 0.084485\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5223\tAccuracy: 8413.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5104\tAccuracy: 8462.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 342 [0/1000 (0%)]\tLosses F.softmax: 0.008049 log_softmax: 0.003914\n",
            "Train Epoch: 342 [200/1000 (20%)]\tLosses F.softmax: 1.314309 log_softmax: 1.586294\n",
            "Train Epoch: 342 [400/1000 (40%)]\tLosses F.softmax: 0.583611 log_softmax: 0.755377\n",
            "Train Epoch: 342 [600/1000 (60%)]\tLosses F.softmax: 0.043171 log_softmax: 0.021253\n",
            "Train Epoch: 342 [800/1000 (80%)]\tLosses F.softmax: 0.331423 log_softmax: 0.368490\n",
            "Train Epoch: 342 [1000/1000 (100%)]\tLosses F.softmax: 0.037812 log_softmax: 0.011332\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5227\tAccuracy: 8409.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5111\tAccuracy: 8458.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 343 [0/1000 (0%)]\tLosses F.softmax: 0.045683 log_softmax: 0.067442\n",
            "Train Epoch: 343 [200/1000 (20%)]\tLosses F.softmax: 0.014156 log_softmax: 0.025158\n",
            "Train Epoch: 343 [400/1000 (40%)]\tLosses F.softmax: 0.035444 log_softmax: 0.013786\n",
            "Train Epoch: 343 [600/1000 (60%)]\tLosses F.softmax: 0.030824 log_softmax: 0.030997\n",
            "Train Epoch: 343 [800/1000 (80%)]\tLosses F.softmax: 0.083888 log_softmax: 0.072735\n",
            "Train Epoch: 343 [1000/1000 (100%)]\tLosses F.softmax: 0.352699 log_softmax: 0.454385\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5226\tAccuracy: 8412.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5112\tAccuracy: 8450.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 344 [0/1000 (0%)]\tLosses F.softmax: 0.338442 log_softmax: 0.446059\n",
            "Train Epoch: 344 [200/1000 (20%)]\tLosses F.softmax: 0.012614 log_softmax: 0.022291\n",
            "Train Epoch: 344 [400/1000 (40%)]\tLosses F.softmax: 0.117919 log_softmax: 0.069907\n",
            "Train Epoch: 344 [600/1000 (60%)]\tLosses F.softmax: 0.010099 log_softmax: 0.007069\n",
            "Train Epoch: 344 [800/1000 (80%)]\tLosses F.softmax: 0.734042 log_softmax: 0.462032\n",
            "Train Epoch: 344 [1000/1000 (100%)]\tLosses F.softmax: 1.130764 log_softmax: 0.767384\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5226\tAccuracy: 8415.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5111\tAccuracy: 8457.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 345 [0/1000 (0%)]\tLosses F.softmax: 0.058074 log_softmax: 0.067207\n",
            "Train Epoch: 345 [200/1000 (20%)]\tLosses F.softmax: 0.037236 log_softmax: 0.021274\n",
            "Train Epoch: 345 [400/1000 (40%)]\tLosses F.softmax: 0.396283 log_softmax: 0.449576\n",
            "Train Epoch: 345 [600/1000 (60%)]\tLosses F.softmax: 0.070441 log_softmax: 0.074345\n",
            "Train Epoch: 345 [800/1000 (80%)]\tLosses F.softmax: 0.006925 log_softmax: 0.019869\n",
            "Train Epoch: 345 [1000/1000 (100%)]\tLosses F.softmax: 0.111394 log_softmax: 0.174946\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5233\tAccuracy: 8414.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5112\tAccuracy: 8454.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 346 [0/1000 (0%)]\tLosses F.softmax: 0.090285 log_softmax: 0.043774\n",
            "Train Epoch: 346 [200/1000 (20%)]\tLosses F.softmax: 0.432689 log_softmax: 0.380067\n",
            "Train Epoch: 346 [400/1000 (40%)]\tLosses F.softmax: 0.000149 log_softmax: 0.000324\n",
            "Train Epoch: 346 [600/1000 (60%)]\tLosses F.softmax: 0.700041 log_softmax: 0.586645\n",
            "Train Epoch: 346 [800/1000 (80%)]\tLosses F.softmax: 0.040850 log_softmax: 0.029195\n",
            "Train Epoch: 346 [1000/1000 (100%)]\tLosses F.softmax: 0.020157 log_softmax: 0.038616\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5241\tAccuracy: 8411.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5122\tAccuracy: 8446.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 347 [0/1000 (0%)]\tLosses F.softmax: 0.356198 log_softmax: 0.696021\n",
            "Train Epoch: 347 [200/1000 (20%)]\tLosses F.softmax: 0.003149 log_softmax: 0.005164\n",
            "Train Epoch: 347 [400/1000 (40%)]\tLosses F.softmax: 0.135220 log_softmax: 0.109625\n",
            "Train Epoch: 347 [600/1000 (60%)]\tLosses F.softmax: 0.018149 log_softmax: 0.030508\n",
            "Train Epoch: 347 [800/1000 (80%)]\tLosses F.softmax: 0.182285 log_softmax: 0.134279\n",
            "Train Epoch: 347 [1000/1000 (100%)]\tLosses F.softmax: 0.145422 log_softmax: 0.152953\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5244\tAccuracy: 8413.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5127\tAccuracy: 8448.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 348 [0/1000 (0%)]\tLosses F.softmax: 0.003731 log_softmax: 0.003658\n",
            "Train Epoch: 348 [200/1000 (20%)]\tLosses F.softmax: 0.032036 log_softmax: 0.021275\n",
            "Train Epoch: 348 [400/1000 (40%)]\tLosses F.softmax: 0.419821 log_softmax: 0.396404\n",
            "Train Epoch: 348 [600/1000 (60%)]\tLosses F.softmax: 0.547393 log_softmax: 0.472985\n",
            "Train Epoch: 348 [800/1000 (80%)]\tLosses F.softmax: 0.001865 log_softmax: 0.003059\n",
            "Train Epoch: 348 [1000/1000 (100%)]\tLosses F.softmax: 0.206512 log_softmax: 0.191188\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5251\tAccuracy: 8406.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5130\tAccuracy: 8443.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 349 [0/1000 (0%)]\tLosses F.softmax: 0.003643 log_softmax: 0.005363\n",
            "Train Epoch: 349 [200/1000 (20%)]\tLosses F.softmax: 0.000956 log_softmax: 0.001799\n",
            "Train Epoch: 349 [400/1000 (40%)]\tLosses F.softmax: 0.005074 log_softmax: 0.004505\n",
            "Train Epoch: 349 [600/1000 (60%)]\tLosses F.softmax: 0.178614 log_softmax: 0.145721\n",
            "Train Epoch: 349 [800/1000 (80%)]\tLosses F.softmax: 0.002934 log_softmax: 0.004903\n",
            "Train Epoch: 349 [1000/1000 (100%)]\tLosses F.softmax: 0.064939 log_softmax: 0.086495\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5256\tAccuracy: 8408.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5132\tAccuracy: 8445.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 350 [0/1000 (0%)]\tLosses F.softmax: 0.008218 log_softmax: 0.020758\n",
            "Train Epoch: 350 [200/1000 (20%)]\tLosses F.softmax: 0.183276 log_softmax: 0.116006\n",
            "Train Epoch: 350 [400/1000 (40%)]\tLosses F.softmax: 0.006403 log_softmax: 0.006694\n",
            "Train Epoch: 350 [600/1000 (60%)]\tLosses F.softmax: 0.047466 log_softmax: 0.033064\n",
            "Train Epoch: 350 [800/1000 (80%)]\tLosses F.softmax: 0.001322 log_softmax: 0.002300\n",
            "Train Epoch: 350 [1000/1000 (100%)]\tLosses F.softmax: 0.012049 log_softmax: 0.024388\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5256\tAccuracy: 8406.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5135\tAccuracy: 8446.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 351 [0/1000 (0%)]\tLosses F.softmax: 0.152924 log_softmax: 0.102418\n",
            "Train Epoch: 351 [200/1000 (20%)]\tLosses F.softmax: 0.076141 log_softmax: 0.033080\n",
            "Train Epoch: 351 [400/1000 (40%)]\tLosses F.softmax: 0.007580 log_softmax: 0.008798\n",
            "Train Epoch: 351 [600/1000 (60%)]\tLosses F.softmax: 0.005373 log_softmax: 0.004097\n",
            "Train Epoch: 351 [800/1000 (80%)]\tLosses F.softmax: 0.111779 log_softmax: 0.084824\n",
            "Train Epoch: 351 [1000/1000 (100%)]\tLosses F.softmax: 0.005044 log_softmax: 0.011607\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5248\tAccuracy: 8422.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5133\tAccuracy: 8448.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 352 [0/1000 (0%)]\tLosses F.softmax: 0.022514 log_softmax: 0.006225\n",
            "Train Epoch: 352 [200/1000 (20%)]\tLosses F.softmax: 0.461381 log_softmax: 0.415770\n",
            "Train Epoch: 352 [400/1000 (40%)]\tLosses F.softmax: 2.283620 log_softmax: 2.420753\n",
            "Train Epoch: 352 [600/1000 (60%)]\tLosses F.softmax: 0.011582 log_softmax: 0.026653\n",
            "Train Epoch: 352 [800/1000 (80%)]\tLosses F.softmax: 1.977729 log_softmax: 1.422674\n",
            "Train Epoch: 352 [1000/1000 (100%)]\tLosses F.softmax: 0.012755 log_softmax: 0.009860\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5250\tAccuracy: 8419.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5129\tAccuracy: 8453.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 353 [0/1000 (0%)]\tLosses F.softmax: 0.175448 log_softmax: 0.112517\n",
            "Train Epoch: 353 [200/1000 (20%)]\tLosses F.softmax: 0.003219 log_softmax: 0.005160\n",
            "Train Epoch: 353 [400/1000 (40%)]\tLosses F.softmax: 0.054749 log_softmax: 0.036116\n",
            "Train Epoch: 353 [600/1000 (60%)]\tLosses F.softmax: 0.007530 log_softmax: 0.009480\n",
            "Train Epoch: 353 [800/1000 (80%)]\tLosses F.softmax: 0.028547 log_softmax: 0.027468\n",
            "Train Epoch: 353 [1000/1000 (100%)]\tLosses F.softmax: 0.104879 log_softmax: 0.086012\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5246\tAccuracy: 8426.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5129\tAccuracy: 8457.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 354 [0/1000 (0%)]\tLosses F.softmax: 0.084368 log_softmax: 0.093001\n",
            "Train Epoch: 354 [200/1000 (20%)]\tLosses F.softmax: 0.090706 log_softmax: 0.069324\n",
            "Train Epoch: 354 [400/1000 (40%)]\tLosses F.softmax: 0.106210 log_softmax: 0.071815\n",
            "Train Epoch: 354 [600/1000 (60%)]\tLosses F.softmax: 0.171991 log_softmax: 0.123202\n",
            "Train Epoch: 354 [800/1000 (80%)]\tLosses F.softmax: 0.014845 log_softmax: 0.022588\n",
            "Train Epoch: 354 [1000/1000 (100%)]\tLosses F.softmax: 0.170636 log_softmax: 0.187573\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5261\tAccuracy: 8422.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5143\tAccuracy: 8442.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 355 [0/1000 (0%)]\tLosses F.softmax: 0.008811 log_softmax: 0.007309\n",
            "Train Epoch: 355 [200/1000 (20%)]\tLosses F.softmax: 0.613812 log_softmax: 0.448384\n",
            "Train Epoch: 355 [400/1000 (40%)]\tLosses F.softmax: 0.041365 log_softmax: 0.033411\n",
            "Train Epoch: 355 [600/1000 (60%)]\tLosses F.softmax: 0.057264 log_softmax: 0.073787\n",
            "Train Epoch: 355 [800/1000 (80%)]\tLosses F.softmax: 0.002620 log_softmax: 0.003780\n",
            "Train Epoch: 355 [1000/1000 (100%)]\tLosses F.softmax: 0.008688 log_softmax: 0.010842\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5267\tAccuracy: 8410.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5147\tAccuracy: 8453.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 356 [0/1000 (0%)]\tLosses F.softmax: 0.002549 log_softmax: 0.001997\n",
            "Train Epoch: 356 [200/1000 (20%)]\tLosses F.softmax: 0.009317 log_softmax: 0.012658\n",
            "Train Epoch: 356 [400/1000 (40%)]\tLosses F.softmax: 0.010579 log_softmax: 0.007996\n",
            "Train Epoch: 356 [600/1000 (60%)]\tLosses F.softmax: 0.009207 log_softmax: 0.010042\n",
            "Train Epoch: 356 [800/1000 (80%)]\tLosses F.softmax: 0.090301 log_softmax: 0.080087\n",
            "Train Epoch: 356 [1000/1000 (100%)]\tLosses F.softmax: 0.001908 log_softmax: 0.002107\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5256\tAccuracy: 8426.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5139\tAccuracy: 8447.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 357 [0/1000 (0%)]\tLosses F.softmax: 0.023223 log_softmax: 0.015511\n",
            "Train Epoch: 357 [200/1000 (20%)]\tLosses F.softmax: 0.174589 log_softmax: 0.080692\n",
            "Train Epoch: 357 [400/1000 (40%)]\tLosses F.softmax: 0.008994 log_softmax: 0.007168\n",
            "Train Epoch: 357 [600/1000 (60%)]\tLosses F.softmax: 0.370501 log_softmax: 0.282104\n",
            "Train Epoch: 357 [800/1000 (80%)]\tLosses F.softmax: 0.014854 log_softmax: 0.007464\n",
            "Train Epoch: 357 [1000/1000 (100%)]\tLosses F.softmax: 0.009307 log_softmax: 0.005781\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5263\tAccuracy: 8420.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5143\tAccuracy: 8447.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 358 [0/1000 (0%)]\tLosses F.softmax: 0.691494 log_softmax: 0.595705\n",
            "Train Epoch: 358 [200/1000 (20%)]\tLosses F.softmax: 0.037655 log_softmax: 0.039200\n",
            "Train Epoch: 358 [400/1000 (40%)]\tLosses F.softmax: 0.012891 log_softmax: 0.020574\n",
            "Train Epoch: 358 [600/1000 (60%)]\tLosses F.softmax: 0.002175 log_softmax: 0.001956\n",
            "Train Epoch: 358 [800/1000 (80%)]\tLosses F.softmax: 0.329403 log_softmax: 0.433296\n",
            "Train Epoch: 358 [1000/1000 (100%)]\tLosses F.softmax: 0.121544 log_softmax: 0.130783\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5261\tAccuracy: 8425.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5142\tAccuracy: 8450.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 359 [0/1000 (0%)]\tLosses F.softmax: 0.105820 log_softmax: 0.094384\n",
            "Train Epoch: 359 [200/1000 (20%)]\tLosses F.softmax: 0.807113 log_softmax: 1.222028\n",
            "Train Epoch: 359 [400/1000 (40%)]\tLosses F.softmax: 0.004125 log_softmax: 0.004301\n",
            "Train Epoch: 359 [600/1000 (60%)]\tLosses F.softmax: 0.079295 log_softmax: 0.060699\n",
            "Train Epoch: 359 [800/1000 (80%)]\tLosses F.softmax: 0.004550 log_softmax: 0.006424\n",
            "Train Epoch: 359 [1000/1000 (100%)]\tLosses F.softmax: 0.000994 log_softmax: 0.001919\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5273\tAccuracy: 8429.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5152\tAccuracy: 8448.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 360 [0/1000 (0%)]\tLosses F.softmax: 0.263563 log_softmax: 0.198844\n",
            "Train Epoch: 360 [200/1000 (20%)]\tLosses F.softmax: 0.056679 log_softmax: 0.015683\n",
            "Train Epoch: 360 [400/1000 (40%)]\tLosses F.softmax: 0.563326 log_softmax: 0.907862\n",
            "Train Epoch: 360 [600/1000 (60%)]\tLosses F.softmax: 0.137678 log_softmax: 0.097474\n",
            "Train Epoch: 360 [800/1000 (80%)]\tLosses F.softmax: 0.016191 log_softmax: 0.011929\n",
            "Train Epoch: 360 [1000/1000 (100%)]\tLosses F.softmax: 0.003763 log_softmax: 0.003793\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5279\tAccuracy: 8421.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5158\tAccuracy: 8446.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 361 [0/1000 (0%)]\tLosses F.softmax: 0.021435 log_softmax: 0.029135\n",
            "Train Epoch: 361 [200/1000 (20%)]\tLosses F.softmax: 0.002481 log_softmax: 0.001880\n",
            "Train Epoch: 361 [400/1000 (40%)]\tLosses F.softmax: 0.457803 log_softmax: 0.552760\n",
            "Train Epoch: 361 [600/1000 (60%)]\tLosses F.softmax: 0.012773 log_softmax: 0.017975\n",
            "Train Epoch: 361 [800/1000 (80%)]\tLosses F.softmax: 0.063250 log_softmax: 0.032001\n",
            "Train Epoch: 361 [1000/1000 (100%)]\tLosses F.softmax: 0.008010 log_softmax: 0.006609\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5291\tAccuracy: 8412.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5166\tAccuracy: 8451.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 362 [0/1000 (0%)]\tLosses F.softmax: 0.012711 log_softmax: 0.004675\n",
            "Train Epoch: 362 [200/1000 (20%)]\tLosses F.softmax: 0.047685 log_softmax: 0.069662\n",
            "Train Epoch: 362 [400/1000 (40%)]\tLosses F.softmax: 0.001620 log_softmax: 0.005114\n",
            "Train Epoch: 362 [600/1000 (60%)]\tLosses F.softmax: 0.154122 log_softmax: 0.079668\n",
            "Train Epoch: 362 [800/1000 (80%)]\tLosses F.softmax: 0.002285 log_softmax: 0.003058\n",
            "Train Epoch: 362 [1000/1000 (100%)]\tLosses F.softmax: 0.320800 log_softmax: 0.247167\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5293\tAccuracy: 8414.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5169\tAccuracy: 8443.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 363 [0/1000 (0%)]\tLosses F.softmax: 0.023438 log_softmax: 0.048191\n",
            "Train Epoch: 363 [200/1000 (20%)]\tLosses F.softmax: 0.116567 log_softmax: 0.235420\n",
            "Train Epoch: 363 [400/1000 (40%)]\tLosses F.softmax: 0.004205 log_softmax: 0.005810\n",
            "Train Epoch: 363 [600/1000 (60%)]\tLosses F.softmax: 0.056188 log_softmax: 0.078578\n",
            "Train Epoch: 363 [800/1000 (80%)]\tLosses F.softmax: 0.000683 log_softmax: 0.000645\n",
            "Train Epoch: 363 [1000/1000 (100%)]\tLosses F.softmax: 0.006978 log_softmax: 0.008131\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5288\tAccuracy: 8429.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5164\tAccuracy: 8444.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 364 [0/1000 (0%)]\tLosses F.softmax: 0.035678 log_softmax: 0.067309\n",
            "Train Epoch: 364 [200/1000 (20%)]\tLosses F.softmax: 0.004057 log_softmax: 0.002558\n",
            "Train Epoch: 364 [400/1000 (40%)]\tLosses F.softmax: 0.008113 log_softmax: 0.012331\n",
            "Train Epoch: 364 [600/1000 (60%)]\tLosses F.softmax: 0.038011 log_softmax: 0.044360\n",
            "Train Epoch: 364 [800/1000 (80%)]\tLosses F.softmax: 0.137480 log_softmax: 0.248907\n",
            "Train Epoch: 364 [1000/1000 (100%)]\tLosses F.softmax: 0.001865 log_softmax: 0.002366\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5294\tAccuracy: 8423.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5172\tAccuracy: 8454.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 365 [0/1000 (0%)]\tLosses F.softmax: 0.200545 log_softmax: 0.336947\n",
            "Train Epoch: 365 [200/1000 (20%)]\tLosses F.softmax: 0.004674 log_softmax: 0.003444\n",
            "Train Epoch: 365 [400/1000 (40%)]\tLosses F.softmax: 0.025665 log_softmax: 0.029316\n",
            "Train Epoch: 365 [600/1000 (60%)]\tLosses F.softmax: 0.032759 log_softmax: 0.018980\n",
            "Train Epoch: 365 [800/1000 (80%)]\tLosses F.softmax: 0.026249 log_softmax: 0.018904\n",
            "Train Epoch: 365 [1000/1000 (100%)]\tLosses F.softmax: 0.686613 log_softmax: 0.591999\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5293\tAccuracy: 8424.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5167\tAccuracy: 8452.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 366 [0/1000 (0%)]\tLosses F.softmax: 0.144196 log_softmax: 0.081647\n",
            "Train Epoch: 366 [200/1000 (20%)]\tLosses F.softmax: 0.012133 log_softmax: 0.026170\n",
            "Train Epoch: 366 [400/1000 (40%)]\tLosses F.softmax: 3.211107 log_softmax: 2.821165\n",
            "Train Epoch: 366 [600/1000 (60%)]\tLosses F.softmax: 0.196936 log_softmax: 0.127062\n",
            "Train Epoch: 366 [800/1000 (80%)]\tLosses F.softmax: 0.125555 log_softmax: 0.124197\n",
            "Train Epoch: 366 [1000/1000 (100%)]\tLosses F.softmax: 0.000846 log_softmax: 0.001425\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5293\tAccuracy: 8420.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5174\tAccuracy: 8451.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 367 [0/1000 (0%)]\tLosses F.softmax: 0.008173 log_softmax: 0.006017\n",
            "Train Epoch: 367 [200/1000 (20%)]\tLosses F.softmax: 0.003323 log_softmax: 0.003705\n",
            "Train Epoch: 367 [400/1000 (40%)]\tLosses F.softmax: 0.010283 log_softmax: 0.013859\n",
            "Train Epoch: 367 [600/1000 (60%)]\tLosses F.softmax: 0.014985 log_softmax: 0.022608\n",
            "Train Epoch: 367 [800/1000 (80%)]\tLosses F.softmax: 0.050016 log_softmax: 0.033499\n",
            "Train Epoch: 367 [1000/1000 (100%)]\tLosses F.softmax: 0.486155 log_softmax: 0.493045\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5308\tAccuracy: 8423.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5185\tAccuracy: 8453.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 368 [0/1000 (0%)]\tLosses F.softmax: 0.066283 log_softmax: 0.087517\n",
            "Train Epoch: 368 [200/1000 (20%)]\tLosses F.softmax: 0.531252 log_softmax: 0.497755\n",
            "Train Epoch: 368 [400/1000 (40%)]\tLosses F.softmax: 0.294708 log_softmax: 0.279344\n",
            "Train Epoch: 368 [600/1000 (60%)]\tLosses F.softmax: 0.017502 log_softmax: 0.025672\n",
            "Train Epoch: 368 [800/1000 (80%)]\tLosses F.softmax: 0.021115 log_softmax: 0.017567\n",
            "Train Epoch: 368 [1000/1000 (100%)]\tLosses F.softmax: 0.010058 log_softmax: 0.024281\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5321\tAccuracy: 8415.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5200\tAccuracy: 8451.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 369 [0/1000 (0%)]\tLosses F.softmax: 0.103811 log_softmax: 0.170127\n",
            "Train Epoch: 369 [200/1000 (20%)]\tLosses F.softmax: 0.000981 log_softmax: 0.003240\n",
            "Train Epoch: 369 [400/1000 (40%)]\tLosses F.softmax: 0.356559 log_softmax: 0.286087\n",
            "Train Epoch: 369 [600/1000 (60%)]\tLosses F.softmax: 0.002326 log_softmax: 0.003939\n",
            "Train Epoch: 369 [800/1000 (80%)]\tLosses F.softmax: 0.039518 log_softmax: 0.024388\n",
            "Train Epoch: 369 [1000/1000 (100%)]\tLosses F.softmax: 0.002279 log_softmax: 0.004479\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5311\tAccuracy: 8422.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5187\tAccuracy: 8448.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 370 [0/1000 (0%)]\tLosses F.softmax: 0.431818 log_softmax: 0.314581\n",
            "Train Epoch: 370 [200/1000 (20%)]\tLosses F.softmax: 0.176339 log_softmax: 0.240479\n",
            "Train Epoch: 370 [400/1000 (40%)]\tLosses F.softmax: 0.021802 log_softmax: 0.015144\n",
            "Train Epoch: 370 [600/1000 (60%)]\tLosses F.softmax: 0.562658 log_softmax: 0.639907\n",
            "Train Epoch: 370 [800/1000 (80%)]\tLosses F.softmax: 1.580670 log_softmax: 1.194206\n",
            "Train Epoch: 370 [1000/1000 (100%)]\tLosses F.softmax: 0.023166 log_softmax: 0.023728\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5320\tAccuracy: 8419.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5191\tAccuracy: 8447.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 371 [0/1000 (0%)]\tLosses F.softmax: 0.004897 log_softmax: 0.008372\n",
            "Train Epoch: 371 [200/1000 (20%)]\tLosses F.softmax: 0.033526 log_softmax: 0.047433\n",
            "Train Epoch: 371 [400/1000 (40%)]\tLosses F.softmax: 0.015696 log_softmax: 0.016848\n",
            "Train Epoch: 371 [600/1000 (60%)]\tLosses F.softmax: 0.001202 log_softmax: 0.000504\n",
            "Train Epoch: 371 [800/1000 (80%)]\tLosses F.softmax: 0.026623 log_softmax: 0.026535\n",
            "Train Epoch: 371 [1000/1000 (100%)]\tLosses F.softmax: 0.033313 log_softmax: 0.011625\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5317\tAccuracy: 8419.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5188\tAccuracy: 8451.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 372 [0/1000 (0%)]\tLosses F.softmax: 0.490221 log_softmax: 0.346962\n",
            "Train Epoch: 372 [200/1000 (20%)]\tLosses F.softmax: 0.048006 log_softmax: 0.050595\n",
            "Train Epoch: 372 [400/1000 (40%)]\tLosses F.softmax: 0.251792 log_softmax: 0.200478\n",
            "Train Epoch: 372 [600/1000 (60%)]\tLosses F.softmax: 0.008261 log_softmax: 0.011389\n",
            "Train Epoch: 372 [800/1000 (80%)]\tLosses F.softmax: 0.051806 log_softmax: 0.046120\n",
            "Train Epoch: 372 [1000/1000 (100%)]\tLosses F.softmax: 0.087177 log_softmax: 0.059742\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5325\tAccuracy: 8423.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5199\tAccuracy: 8445.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 373 [0/1000 (0%)]\tLosses F.softmax: 0.006662 log_softmax: 0.016640\n",
            "Train Epoch: 373 [200/1000 (20%)]\tLosses F.softmax: 0.002349 log_softmax: 0.002579\n",
            "Train Epoch: 373 [400/1000 (40%)]\tLosses F.softmax: 0.000778 log_softmax: 0.000684\n",
            "Train Epoch: 373 [600/1000 (60%)]\tLosses F.softmax: 1.129212 log_softmax: 1.269248\n",
            "Train Epoch: 373 [800/1000 (80%)]\tLosses F.softmax: 0.802111 log_softmax: 1.256083\n",
            "Train Epoch: 373 [1000/1000 (100%)]\tLosses F.softmax: 0.025475 log_softmax: 0.035209\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5333\tAccuracy: 8421.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5202\tAccuracy: 8448.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 374 [0/1000 (0%)]\tLosses F.softmax: 0.023003 log_softmax: 0.015120\n",
            "Train Epoch: 374 [200/1000 (20%)]\tLosses F.softmax: 0.008117 log_softmax: 0.011147\n",
            "Train Epoch: 374 [400/1000 (40%)]\tLosses F.softmax: 0.005879 log_softmax: 0.008176\n",
            "Train Epoch: 374 [600/1000 (60%)]\tLosses F.softmax: 0.416331 log_softmax: 0.453287\n",
            "Train Epoch: 374 [800/1000 (80%)]\tLosses F.softmax: 0.347884 log_softmax: 0.277683\n",
            "Train Epoch: 374 [1000/1000 (100%)]\tLosses F.softmax: 0.949116 log_softmax: 1.028744\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5336\tAccuracy: 8427.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5210\tAccuracy: 8449.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 375 [0/1000 (0%)]\tLosses F.softmax: 0.238387 log_softmax: 0.215474\n",
            "Train Epoch: 375 [200/1000 (20%)]\tLosses F.softmax: 0.031771 log_softmax: 0.023441\n",
            "Train Epoch: 375 [400/1000 (40%)]\tLosses F.softmax: 0.001059 log_softmax: 0.002045\n",
            "Train Epoch: 375 [600/1000 (60%)]\tLosses F.softmax: 0.033727 log_softmax: 0.036793\n",
            "Train Epoch: 375 [800/1000 (80%)]\tLosses F.softmax: 0.075121 log_softmax: 0.063422\n",
            "Train Epoch: 375 [1000/1000 (100%)]\tLosses F.softmax: 0.010344 log_softmax: 0.017193\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5345\tAccuracy: 8414.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5221\tAccuracy: 8444.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 376 [0/1000 (0%)]\tLosses F.softmax: 0.040833 log_softmax: 0.013432\n",
            "Train Epoch: 376 [200/1000 (20%)]\tLosses F.softmax: 0.000204 log_softmax: 0.000836\n",
            "Train Epoch: 376 [400/1000 (40%)]\tLosses F.softmax: 0.063408 log_softmax: 0.068008\n",
            "Train Epoch: 376 [600/1000 (60%)]\tLosses F.softmax: 0.394574 log_softmax: 0.388500\n",
            "Train Epoch: 376 [800/1000 (80%)]\tLosses F.softmax: 0.006426 log_softmax: 0.007537\n",
            "Train Epoch: 376 [1000/1000 (100%)]\tLosses F.softmax: 0.010847 log_softmax: 0.005117\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5331\tAccuracy: 8426.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5208\tAccuracy: 8450.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 377 [0/1000 (0%)]\tLosses F.softmax: 0.400748 log_softmax: 0.300930\n",
            "Train Epoch: 377 [200/1000 (20%)]\tLosses F.softmax: 0.018846 log_softmax: 0.020425\n",
            "Train Epoch: 377 [400/1000 (40%)]\tLosses F.softmax: 0.953313 log_softmax: 1.026421\n",
            "Train Epoch: 377 [600/1000 (60%)]\tLosses F.softmax: 0.424978 log_softmax: 0.468034\n",
            "Train Epoch: 377 [800/1000 (80%)]\tLosses F.softmax: 0.005407 log_softmax: 0.006930\n",
            "Train Epoch: 377 [1000/1000 (100%)]\tLosses F.softmax: 0.057061 log_softmax: 0.057410\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5335\tAccuracy: 8423.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5211\tAccuracy: 8453.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 378 [0/1000 (0%)]\tLosses F.softmax: 0.161738 log_softmax: 0.098308\n",
            "Train Epoch: 378 [200/1000 (20%)]\tLosses F.softmax: 0.000454 log_softmax: 0.000579\n",
            "Train Epoch: 378 [400/1000 (40%)]\tLosses F.softmax: 0.000619 log_softmax: 0.001143\n",
            "Train Epoch: 378 [600/1000 (60%)]\tLosses F.softmax: 0.001054 log_softmax: 0.001517\n",
            "Train Epoch: 378 [800/1000 (80%)]\tLosses F.softmax: 0.173257 log_softmax: 0.172709\n",
            "Train Epoch: 378 [1000/1000 (100%)]\tLosses F.softmax: 0.122838 log_softmax: 0.125554\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5346\tAccuracy: 8418.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5217\tAccuracy: 8447.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 379 [0/1000 (0%)]\tLosses F.softmax: 0.008200 log_softmax: 0.007961\n",
            "Train Epoch: 379 [200/1000 (20%)]\tLosses F.softmax: 0.055406 log_softmax: 0.076247\n",
            "Train Epoch: 379 [400/1000 (40%)]\tLosses F.softmax: 0.002617 log_softmax: 0.004267\n",
            "Train Epoch: 379 [600/1000 (60%)]\tLosses F.softmax: 0.031950 log_softmax: 0.035094\n",
            "Train Epoch: 379 [800/1000 (80%)]\tLosses F.softmax: 0.230350 log_softmax: 0.231973\n",
            "Train Epoch: 379 [1000/1000 (100%)]\tLosses F.softmax: 0.102570 log_softmax: 0.057867\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5340\tAccuracy: 8437.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5210\tAccuracy: 8448.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 380 [0/1000 (0%)]\tLosses F.softmax: 0.198368 log_softmax: 0.215469\n",
            "Train Epoch: 380 [200/1000 (20%)]\tLosses F.softmax: 0.012931 log_softmax: 0.009836\n",
            "Train Epoch: 380 [400/1000 (40%)]\tLosses F.softmax: 0.001742 log_softmax: 0.001361\n",
            "Train Epoch: 380 [600/1000 (60%)]\tLosses F.softmax: 0.055595 log_softmax: 0.116616\n",
            "Train Epoch: 380 [800/1000 (80%)]\tLosses F.softmax: 0.000435 log_softmax: 0.000655\n",
            "Train Epoch: 380 [1000/1000 (100%)]\tLosses F.softmax: 0.039640 log_softmax: 0.055707\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5361\tAccuracy: 8415.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5228\tAccuracy: 8445.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 381 [0/1000 (0%)]\tLosses F.softmax: 0.013230 log_softmax: 0.004756\n",
            "Train Epoch: 381 [200/1000 (20%)]\tLosses F.softmax: 0.195134 log_softmax: 0.190485\n",
            "Train Epoch: 381 [400/1000 (40%)]\tLosses F.softmax: 0.327518 log_softmax: 0.627224\n",
            "Train Epoch: 381 [600/1000 (60%)]\tLosses F.softmax: 0.023039 log_softmax: 0.051130\n",
            "Train Epoch: 381 [800/1000 (80%)]\tLosses F.softmax: 0.067488 log_softmax: 0.060065\n",
            "Train Epoch: 381 [1000/1000 (100%)]\tLosses F.softmax: 0.000717 log_softmax: 0.000463\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5351\tAccuracy: 8424.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5220\tAccuracy: 8452.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 382 [0/1000 (0%)]\tLosses F.softmax: 0.025723 log_softmax: 0.044229\n",
            "Train Epoch: 382 [200/1000 (20%)]\tLosses F.softmax: 0.002692 log_softmax: 0.004107\n",
            "Train Epoch: 382 [400/1000 (40%)]\tLosses F.softmax: 0.032149 log_softmax: 0.010165\n",
            "Train Epoch: 382 [600/1000 (60%)]\tLosses F.softmax: 0.429487 log_softmax: 0.364442\n",
            "Train Epoch: 382 [800/1000 (80%)]\tLosses F.softmax: 0.193758 log_softmax: 0.156225\n",
            "Train Epoch: 382 [1000/1000 (100%)]\tLosses F.softmax: 0.075927 log_softmax: 0.095258\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5359\tAccuracy: 8424.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5234\tAccuracy: 8454.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 383 [0/1000 (0%)]\tLosses F.softmax: 0.041599 log_softmax: 0.060022\n",
            "Train Epoch: 383 [200/1000 (20%)]\tLosses F.softmax: 0.009900 log_softmax: 0.019750\n",
            "Train Epoch: 383 [400/1000 (40%)]\tLosses F.softmax: 0.029287 log_softmax: 0.020739\n",
            "Train Epoch: 383 [600/1000 (60%)]\tLosses F.softmax: 0.005894 log_softmax: 0.006975\n",
            "Train Epoch: 383 [800/1000 (80%)]\tLosses F.softmax: 0.059066 log_softmax: 0.069369\n",
            "Train Epoch: 383 [1000/1000 (100%)]\tLosses F.softmax: 0.001094 log_softmax: 0.001604\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5364\tAccuracy: 8424.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5233\tAccuracy: 8449.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 384 [0/1000 (0%)]\tLosses F.softmax: 0.001010 log_softmax: 0.002525\n",
            "Train Epoch: 384 [200/1000 (20%)]\tLosses F.softmax: 0.001466 log_softmax: 0.004526\n",
            "Train Epoch: 384 [400/1000 (40%)]\tLosses F.softmax: 0.044519 log_softmax: 0.089388\n",
            "Train Epoch: 384 [600/1000 (60%)]\tLosses F.softmax: 0.018868 log_softmax: 0.039638\n",
            "Train Epoch: 384 [800/1000 (80%)]\tLosses F.softmax: 0.022695 log_softmax: 0.013581\n",
            "Train Epoch: 384 [1000/1000 (100%)]\tLosses F.softmax: 0.124778 log_softmax: 0.100657\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5357\tAccuracy: 8435.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5223\tAccuracy: 8463.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 385 [0/1000 (0%)]\tLosses F.softmax: 0.870405 log_softmax: 1.250441\n",
            "Train Epoch: 385 [200/1000 (20%)]\tLosses F.softmax: 0.861743 log_softmax: 0.749452\n",
            "Train Epoch: 385 [400/1000 (40%)]\tLosses F.softmax: 0.001873 log_softmax: 0.001548\n",
            "Train Epoch: 385 [600/1000 (60%)]\tLosses F.softmax: 0.130439 log_softmax: 0.065640\n",
            "Train Epoch: 385 [800/1000 (80%)]\tLosses F.softmax: 0.021101 log_softmax: 0.021850\n",
            "Train Epoch: 385 [1000/1000 (100%)]\tLosses F.softmax: 0.006120 log_softmax: 0.006201\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5379\tAccuracy: 8435.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5244\tAccuracy: 8455.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 386 [0/1000 (0%)]\tLosses F.softmax: 0.015146 log_softmax: 0.010917\n",
            "Train Epoch: 386 [200/1000 (20%)]\tLosses F.softmax: 0.129303 log_softmax: 0.062178\n",
            "Train Epoch: 386 [400/1000 (40%)]\tLosses F.softmax: 1.707060 log_softmax: 1.191735\n",
            "Train Epoch: 386 [600/1000 (60%)]\tLosses F.softmax: 0.662667 log_softmax: 0.644659\n",
            "Train Epoch: 386 [800/1000 (80%)]\tLosses F.softmax: 0.005453 log_softmax: 0.005699\n",
            "Train Epoch: 386 [1000/1000 (100%)]\tLosses F.softmax: 0.112449 log_softmax: 0.068389\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5377\tAccuracy: 8428.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5243\tAccuracy: 8459.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 387 [0/1000 (0%)]\tLosses F.softmax: 0.029632 log_softmax: 0.017258\n",
            "Train Epoch: 387 [200/1000 (20%)]\tLosses F.softmax: 0.031924 log_softmax: 0.045543\n",
            "Train Epoch: 387 [400/1000 (40%)]\tLosses F.softmax: 0.005868 log_softmax: 0.007873\n",
            "Train Epoch: 387 [600/1000 (60%)]\tLosses F.softmax: 0.190958 log_softmax: 0.163962\n",
            "Train Epoch: 387 [800/1000 (80%)]\tLosses F.softmax: 0.045494 log_softmax: 0.031666\n",
            "Train Epoch: 387 [1000/1000 (100%)]\tLosses F.softmax: 0.120892 log_softmax: 0.173269\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5394\tAccuracy: 8430.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5260\tAccuracy: 8451.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 388 [0/1000 (0%)]\tLosses F.softmax: 0.204333 log_softmax: 0.072488\n",
            "Train Epoch: 388 [200/1000 (20%)]\tLosses F.softmax: 0.008232 log_softmax: 0.007277\n",
            "Train Epoch: 388 [400/1000 (40%)]\tLosses F.softmax: 0.023007 log_softmax: 0.014600\n",
            "Train Epoch: 388 [600/1000 (60%)]\tLosses F.softmax: 0.732117 log_softmax: 0.492254\n",
            "Train Epoch: 388 [800/1000 (80%)]\tLosses F.softmax: 0.001654 log_softmax: 0.001437\n",
            "Train Epoch: 388 [1000/1000 (100%)]\tLosses F.softmax: 0.313275 log_softmax: 0.343623\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5393\tAccuracy: 8425.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5258\tAccuracy: 8455.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 389 [0/1000 (0%)]\tLosses F.softmax: 0.036560 log_softmax: 0.054020\n",
            "Train Epoch: 389 [200/1000 (20%)]\tLosses F.softmax: 0.014221 log_softmax: 0.023626\n",
            "Train Epoch: 389 [400/1000 (40%)]\tLosses F.softmax: 0.027436 log_softmax: 0.016485\n",
            "Train Epoch: 389 [600/1000 (60%)]\tLosses F.softmax: 0.000456 log_softmax: 0.000671\n",
            "Train Epoch: 389 [800/1000 (80%)]\tLosses F.softmax: 0.276141 log_softmax: 0.375236\n",
            "Train Epoch: 389 [1000/1000 (100%)]\tLosses F.softmax: 0.052887 log_softmax: 0.036475\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5384\tAccuracy: 8432.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5253\tAccuracy: 8454.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 390 [0/1000 (0%)]\tLosses F.softmax: 0.009707 log_softmax: 0.009397\n",
            "Train Epoch: 390 [200/1000 (20%)]\tLosses F.softmax: 0.008107 log_softmax: 0.007627\n",
            "Train Epoch: 390 [400/1000 (40%)]\tLosses F.softmax: 0.021204 log_softmax: 0.012409\n",
            "Train Epoch: 390 [600/1000 (60%)]\tLosses F.softmax: 0.132641 log_softmax: 0.113249\n",
            "Train Epoch: 390 [800/1000 (80%)]\tLosses F.softmax: 0.016820 log_softmax: 0.028962\n",
            "Train Epoch: 390 [1000/1000 (100%)]\tLosses F.softmax: 0.014356 log_softmax: 0.021172\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5399\tAccuracy: 8421.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5269\tAccuracy: 8452.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 391 [0/1000 (0%)]\tLosses F.softmax: 0.206326 log_softmax: 0.077296\n",
            "Train Epoch: 391 [200/1000 (20%)]\tLosses F.softmax: 0.003815 log_softmax: 0.003772\n",
            "Train Epoch: 391 [400/1000 (40%)]\tLosses F.softmax: 0.060349 log_softmax: 0.034596\n",
            "Train Epoch: 391 [600/1000 (60%)]\tLosses F.softmax: 0.001426 log_softmax: 0.001666\n",
            "Train Epoch: 391 [800/1000 (80%)]\tLosses F.softmax: 0.071927 log_softmax: 0.035272\n",
            "Train Epoch: 391 [1000/1000 (100%)]\tLosses F.softmax: 0.000503 log_softmax: 0.000908\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5397\tAccuracy: 8426.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5260\tAccuracy: 8455.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 392 [0/1000 (0%)]\tLosses F.softmax: 0.002271 log_softmax: 0.004422\n",
            "Train Epoch: 392 [200/1000 (20%)]\tLosses F.softmax: 0.047738 log_softmax: 0.026958\n",
            "Train Epoch: 392 [400/1000 (40%)]\tLosses F.softmax: 0.026612 log_softmax: 0.037620\n",
            "Train Epoch: 392 [600/1000 (60%)]\tLosses F.softmax: 0.874863 log_softmax: 0.927523\n",
            "Train Epoch: 392 [800/1000 (80%)]\tLosses F.softmax: 0.135191 log_softmax: 0.101734\n",
            "Train Epoch: 392 [1000/1000 (100%)]\tLosses F.softmax: 0.009344 log_softmax: 0.017068\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5395\tAccuracy: 8431.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5258\tAccuracy: 8460.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 393 [0/1000 (0%)]\tLosses F.softmax: 0.411662 log_softmax: 0.484549\n",
            "Train Epoch: 393 [200/1000 (20%)]\tLosses F.softmax: 0.011607 log_softmax: 0.055099\n",
            "Train Epoch: 393 [400/1000 (40%)]\tLosses F.softmax: 0.087821 log_softmax: 0.062722\n",
            "Train Epoch: 393 [600/1000 (60%)]\tLosses F.softmax: 0.097771 log_softmax: 0.052774\n",
            "Train Epoch: 393 [800/1000 (80%)]\tLosses F.softmax: 0.015615 log_softmax: 0.011502\n",
            "Train Epoch: 393 [1000/1000 (100%)]\tLosses F.softmax: 0.415375 log_softmax: 0.374789\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5398\tAccuracy: 8433.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5269\tAccuracy: 8460.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 394 [0/1000 (0%)]\tLosses F.softmax: 0.000430 log_softmax: 0.000626\n",
            "Train Epoch: 394 [200/1000 (20%)]\tLosses F.softmax: 0.121841 log_softmax: 0.189903\n",
            "Train Epoch: 394 [400/1000 (40%)]\tLosses F.softmax: 0.003390 log_softmax: 0.004484\n",
            "Train Epoch: 394 [600/1000 (60%)]\tLosses F.softmax: 0.002797 log_softmax: 0.007449\n",
            "Train Epoch: 394 [800/1000 (80%)]\tLosses F.softmax: 0.002689 log_softmax: 0.002119\n",
            "Train Epoch: 394 [1000/1000 (100%)]\tLosses F.softmax: 0.953212 log_softmax: 1.283437\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5412\tAccuracy: 8428.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5277\tAccuracy: 8447.0/10000 (84%)\n",
            "\n",
            "Train Epoch: 395 [0/1000 (0%)]\tLosses F.softmax: 0.219894 log_softmax: 0.246427\n",
            "Train Epoch: 395 [200/1000 (20%)]\tLosses F.softmax: 0.007739 log_softmax: 0.006085\n",
            "Train Epoch: 395 [400/1000 (40%)]\tLosses F.softmax: 0.010567 log_softmax: 0.007587\n",
            "Train Epoch: 395 [600/1000 (60%)]\tLosses F.softmax: 0.014285 log_softmax: 0.020613\n",
            "Train Epoch: 395 [800/1000 (80%)]\tLosses F.softmax: 0.000537 log_softmax: 0.001319\n",
            "Train Epoch: 395 [1000/1000 (100%)]\tLosses F.softmax: 0.002674 log_softmax: 0.001353\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5415\tAccuracy: 8429.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5280\tAccuracy: 8453.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 396 [0/1000 (0%)]\tLosses F.softmax: 0.008668 log_softmax: 0.009754\n",
            "Train Epoch: 396 [200/1000 (20%)]\tLosses F.softmax: 0.019601 log_softmax: 0.009912\n",
            "Train Epoch: 396 [400/1000 (40%)]\tLosses F.softmax: 0.002029 log_softmax: 0.003332\n",
            "Train Epoch: 396 [600/1000 (60%)]\tLosses F.softmax: 0.078246 log_softmax: 0.077782\n",
            "Train Epoch: 396 [800/1000 (80%)]\tLosses F.softmax: 0.001473 log_softmax: 0.002273\n",
            "Train Epoch: 396 [1000/1000 (100%)]\tLosses F.softmax: 0.003370 log_softmax: 0.002403\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5409\tAccuracy: 8434.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5274\tAccuracy: 8457.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 397 [0/1000 (0%)]\tLosses F.softmax: 0.230144 log_softmax: 0.280468\n",
            "Train Epoch: 397 [200/1000 (20%)]\tLosses F.softmax: 0.235736 log_softmax: 0.199777\n",
            "Train Epoch: 397 [400/1000 (40%)]\tLosses F.softmax: 0.300598 log_softmax: 0.218780\n",
            "Train Epoch: 397 [600/1000 (60%)]\tLosses F.softmax: 2.756279 log_softmax: 2.466556\n",
            "Train Epoch: 397 [800/1000 (80%)]\tLosses F.softmax: 0.004047 log_softmax: 0.003034\n",
            "Train Epoch: 397 [1000/1000 (100%)]\tLosses F.softmax: 0.020162 log_softmax: 0.031795\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5415\tAccuracy: 8435.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5279\tAccuracy: 8461.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 398 [0/1000 (0%)]\tLosses F.softmax: 0.058908 log_softmax: 0.060085\n",
            "Train Epoch: 398 [200/1000 (20%)]\tLosses F.softmax: 0.031379 log_softmax: 0.047425\n",
            "Train Epoch: 398 [400/1000 (40%)]\tLosses F.softmax: 0.173382 log_softmax: 0.156262\n",
            "Train Epoch: 398 [600/1000 (60%)]\tLosses F.softmax: 0.047926 log_softmax: 0.032789\n",
            "Train Epoch: 398 [800/1000 (80%)]\tLosses F.softmax: 0.025618 log_softmax: 0.027168\n",
            "Train Epoch: 398 [1000/1000 (100%)]\tLosses F.softmax: 0.029875 log_softmax: 0.028132\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5419\tAccuracy: 8438.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5283\tAccuracy: 8465.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 399 [0/1000 (0%)]\tLosses F.softmax: 0.007131 log_softmax: 0.017433\n",
            "Train Epoch: 399 [200/1000 (20%)]\tLosses F.softmax: 0.030451 log_softmax: 0.042144\n",
            "Train Epoch: 399 [400/1000 (40%)]\tLosses F.softmax: 0.044102 log_softmax: 0.083064\n",
            "Train Epoch: 399 [600/1000 (60%)]\tLosses F.softmax: 0.011182 log_softmax: 0.016002\n",
            "Train Epoch: 399 [800/1000 (80%)]\tLosses F.softmax: 0.000759 log_softmax: 0.000776\n",
            "Train Epoch: 399 [1000/1000 (100%)]\tLosses F.softmax: 0.001382 log_softmax: 0.001581\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5422\tAccuracy: 8445.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5292\tAccuracy: 8463.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 400 [0/1000 (0%)]\tLosses F.softmax: 0.003753 log_softmax: 0.006818\n",
            "Train Epoch: 400 [200/1000 (20%)]\tLosses F.softmax: 0.018137 log_softmax: 0.018345\n",
            "Train Epoch: 400 [400/1000 (40%)]\tLosses F.softmax: 0.186639 log_softmax: 0.137955\n",
            "Train Epoch: 400 [600/1000 (60%)]\tLosses F.softmax: 0.242548 log_softmax: 0.322398\n",
            "Train Epoch: 400 [800/1000 (80%)]\tLosses F.softmax: 0.176591 log_softmax: 0.211518\n",
            "Train Epoch: 400 [1000/1000 (100%)]\tLosses F.softmax: 0.003225 log_softmax: 0.002276\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5426\tAccuracy: 8440.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5295\tAccuracy: 8457.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 401 [0/1000 (0%)]\tLosses F.softmax: 0.087009 log_softmax: 0.135508\n",
            "Train Epoch: 401 [200/1000 (20%)]\tLosses F.softmax: 0.016096 log_softmax: 0.012715\n",
            "Train Epoch: 401 [400/1000 (40%)]\tLosses F.softmax: 0.017935 log_softmax: 0.005027\n",
            "Train Epoch: 401 [600/1000 (60%)]\tLosses F.softmax: 0.000854 log_softmax: 0.000855\n",
            "Train Epoch: 401 [800/1000 (80%)]\tLosses F.softmax: 0.015719 log_softmax: 0.010357\n",
            "Train Epoch: 401 [1000/1000 (100%)]\tLosses F.softmax: 0.540728 log_softmax: 0.465556\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5445\tAccuracy: 8431.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5310\tAccuracy: 8451.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 402 [0/1000 (0%)]\tLosses F.softmax: 0.022530 log_softmax: 0.007875\n",
            "Train Epoch: 402 [200/1000 (20%)]\tLosses F.softmax: 0.014960 log_softmax: 0.024296\n",
            "Train Epoch: 402 [400/1000 (40%)]\tLosses F.softmax: 0.008620 log_softmax: 0.006645\n",
            "Train Epoch: 402 [600/1000 (60%)]\tLosses F.softmax: 0.010145 log_softmax: 0.014856\n",
            "Train Epoch: 402 [800/1000 (80%)]\tLosses F.softmax: 0.002443 log_softmax: 0.002984\n",
            "Train Epoch: 402 [1000/1000 (100%)]\tLosses F.softmax: 0.009852 log_softmax: 0.016046\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5441\tAccuracy: 8438.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5306\tAccuracy: 8459.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 403 [0/1000 (0%)]\tLosses F.softmax: 0.313297 log_softmax: 0.404820\n",
            "Train Epoch: 403 [200/1000 (20%)]\tLosses F.softmax: 0.005057 log_softmax: 0.003614\n",
            "Train Epoch: 403 [400/1000 (40%)]\tLosses F.softmax: 0.047406 log_softmax: 0.027838\n",
            "Train Epoch: 403 [600/1000 (60%)]\tLosses F.softmax: 0.040686 log_softmax: 0.025036\n",
            "Train Epoch: 403 [800/1000 (80%)]\tLosses F.softmax: 0.003969 log_softmax: 0.003832\n",
            "Train Epoch: 403 [1000/1000 (100%)]\tLosses F.softmax: 0.038410 log_softmax: 0.036869\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5437\tAccuracy: 8445.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5304\tAccuracy: 8466.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 404 [0/1000 (0%)]\tLosses F.softmax: 0.048172 log_softmax: 0.068202\n",
            "Train Epoch: 404 [200/1000 (20%)]\tLosses F.softmax: 0.002308 log_softmax: 0.006839\n",
            "Train Epoch: 404 [400/1000 (40%)]\tLosses F.softmax: 0.005658 log_softmax: 0.007868\n",
            "Train Epoch: 404 [600/1000 (60%)]\tLosses F.softmax: 0.007655 log_softmax: 0.010369\n",
            "Train Epoch: 404 [800/1000 (80%)]\tLosses F.softmax: 0.020552 log_softmax: 0.024874\n",
            "Train Epoch: 404 [1000/1000 (100%)]\tLosses F.softmax: 0.299071 log_softmax: 0.158357\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5441\tAccuracy: 8441.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5309\tAccuracy: 8459.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 405 [0/1000 (0%)]\tLosses F.softmax: 0.012857 log_softmax: 0.009144\n",
            "Train Epoch: 405 [200/1000 (20%)]\tLosses F.softmax: 0.003079 log_softmax: 0.008656\n",
            "Train Epoch: 405 [400/1000 (40%)]\tLosses F.softmax: 0.001673 log_softmax: 0.002849\n",
            "Train Epoch: 405 [600/1000 (60%)]\tLosses F.softmax: 0.253106 log_softmax: 0.182945\n",
            "Train Epoch: 405 [800/1000 (80%)]\tLosses F.softmax: 0.014803 log_softmax: 0.030226\n",
            "Train Epoch: 405 [1000/1000 (100%)]\tLosses F.softmax: 0.043975 log_softmax: 0.062036\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5444\tAccuracy: 8443.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5309\tAccuracy: 8461.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 406 [0/1000 (0%)]\tLosses F.softmax: 0.008908 log_softmax: 0.005965\n",
            "Train Epoch: 406 [200/1000 (20%)]\tLosses F.softmax: 0.011158 log_softmax: 0.008116\n",
            "Train Epoch: 406 [400/1000 (40%)]\tLosses F.softmax: 0.001205 log_softmax: 0.000895\n",
            "Train Epoch: 406 [600/1000 (60%)]\tLosses F.softmax: 0.003577 log_softmax: 0.003080\n",
            "Train Epoch: 406 [800/1000 (80%)]\tLosses F.softmax: 0.011331 log_softmax: 0.007654\n",
            "Train Epoch: 406 [1000/1000 (100%)]\tLosses F.softmax: 0.001795 log_softmax: 0.001914\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5445\tAccuracy: 8438.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5310\tAccuracy: 8461.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 407 [0/1000 (0%)]\tLosses F.softmax: 0.163484 log_softmax: 0.153664\n",
            "Train Epoch: 407 [200/1000 (20%)]\tLosses F.softmax: 0.423101 log_softmax: 0.701387\n",
            "Train Epoch: 407 [400/1000 (40%)]\tLosses F.softmax: 0.011737 log_softmax: 0.011360\n",
            "Train Epoch: 407 [600/1000 (60%)]\tLosses F.softmax: 0.001392 log_softmax: 0.002052\n",
            "Train Epoch: 407 [800/1000 (80%)]\tLosses F.softmax: 0.174944 log_softmax: 0.144215\n",
            "Train Epoch: 407 [1000/1000 (100%)]\tLosses F.softmax: 0.025742 log_softmax: 0.033655\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5466\tAccuracy: 8442.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5327\tAccuracy: 8457.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 408 [0/1000 (0%)]\tLosses F.softmax: 0.004348 log_softmax: 0.005702\n",
            "Train Epoch: 408 [200/1000 (20%)]\tLosses F.softmax: 0.029001 log_softmax: 0.039793\n",
            "Train Epoch: 408 [400/1000 (40%)]\tLosses F.softmax: 0.000810 log_softmax: 0.002037\n",
            "Train Epoch: 408 [600/1000 (60%)]\tLosses F.softmax: 0.034380 log_softmax: 0.011042\n",
            "Train Epoch: 408 [800/1000 (80%)]\tLosses F.softmax: 0.048087 log_softmax: 0.044184\n",
            "Train Epoch: 408 [1000/1000 (100%)]\tLosses F.softmax: 0.142586 log_softmax: 0.195097\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5478\tAccuracy: 8431.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5340\tAccuracy: 8457.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 409 [0/1000 (0%)]\tLosses F.softmax: 0.000691 log_softmax: 0.000916\n",
            "Train Epoch: 409 [200/1000 (20%)]\tLosses F.softmax: 0.001754 log_softmax: 0.002464\n",
            "Train Epoch: 409 [400/1000 (40%)]\tLosses F.softmax: 0.189404 log_softmax: 0.201836\n",
            "Train Epoch: 409 [600/1000 (60%)]\tLosses F.softmax: 0.021390 log_softmax: 0.011358\n",
            "Train Epoch: 409 [800/1000 (80%)]\tLosses F.softmax: 0.005806 log_softmax: 0.004812\n",
            "Train Epoch: 409 [1000/1000 (100%)]\tLosses F.softmax: 0.079474 log_softmax: 0.125287\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5483\tAccuracy: 8435.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5341\tAccuracy: 8455.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 410 [0/1000 (0%)]\tLosses F.softmax: 0.002650 log_softmax: 0.006224\n",
            "Train Epoch: 410 [200/1000 (20%)]\tLosses F.softmax: 0.013624 log_softmax: 0.028405\n",
            "Train Epoch: 410 [400/1000 (40%)]\tLosses F.softmax: 0.007721 log_softmax: 0.007095\n",
            "Train Epoch: 410 [600/1000 (60%)]\tLosses F.softmax: 0.002781 log_softmax: 0.002337\n",
            "Train Epoch: 410 [800/1000 (80%)]\tLosses F.softmax: 0.080080 log_softmax: 0.065390\n",
            "Train Epoch: 410 [1000/1000 (100%)]\tLosses F.softmax: 0.001533 log_softmax: 0.000641\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5488\tAccuracy: 8430.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5346\tAccuracy: 8459.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 411 [0/1000 (0%)]\tLosses F.softmax: 0.003696 log_softmax: 0.004410\n",
            "Train Epoch: 411 [200/1000 (20%)]\tLosses F.softmax: 0.022575 log_softmax: 0.030429\n",
            "Train Epoch: 411 [400/1000 (40%)]\tLosses F.softmax: 0.080885 log_softmax: 0.043044\n",
            "Train Epoch: 411 [600/1000 (60%)]\tLosses F.softmax: 0.001818 log_softmax: 0.002841\n",
            "Train Epoch: 411 [800/1000 (80%)]\tLosses F.softmax: 0.345445 log_softmax: 0.265779\n",
            "Train Epoch: 411 [1000/1000 (100%)]\tLosses F.softmax: 0.167281 log_softmax: 0.219245\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5473\tAccuracy: 8443.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5340\tAccuracy: 8460.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 412 [0/1000 (0%)]\tLosses F.softmax: 0.082992 log_softmax: 0.087945\n",
            "Train Epoch: 412 [200/1000 (20%)]\tLosses F.softmax: 0.017166 log_softmax: 0.029731\n",
            "Train Epoch: 412 [400/1000 (40%)]\tLosses F.softmax: 0.000625 log_softmax: 0.000952\n",
            "Train Epoch: 412 [600/1000 (60%)]\tLosses F.softmax: 0.002122 log_softmax: 0.005594\n",
            "Train Epoch: 412 [800/1000 (80%)]\tLosses F.softmax: 0.005490 log_softmax: 0.004637\n",
            "Train Epoch: 412 [1000/1000 (100%)]\tLosses F.softmax: 0.040404 log_softmax: 0.024802\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5482\tAccuracy: 8441.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5345\tAccuracy: 8461.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 413 [0/1000 (0%)]\tLosses F.softmax: 0.061962 log_softmax: 0.090159\n",
            "Train Epoch: 413 [200/1000 (20%)]\tLosses F.softmax: 0.013721 log_softmax: 0.014158\n",
            "Train Epoch: 413 [400/1000 (40%)]\tLosses F.softmax: 0.098498 log_softmax: 0.146039\n",
            "Train Epoch: 413 [600/1000 (60%)]\tLosses F.softmax: 0.070601 log_softmax: 0.053622\n",
            "Train Epoch: 413 [800/1000 (80%)]\tLosses F.softmax: 0.063481 log_softmax: 0.087825\n",
            "Train Epoch: 413 [1000/1000 (100%)]\tLosses F.softmax: 0.072987 log_softmax: 0.055768\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5493\tAccuracy: 8438.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5354\tAccuracy: 8457.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 414 [0/1000 (0%)]\tLosses F.softmax: 0.740958 log_softmax: 0.713738\n",
            "Train Epoch: 414 [200/1000 (20%)]\tLosses F.softmax: 0.621581 log_softmax: 0.549551\n",
            "Train Epoch: 414 [400/1000 (40%)]\tLosses F.softmax: 0.004836 log_softmax: 0.005335\n",
            "Train Epoch: 414 [600/1000 (60%)]\tLosses F.softmax: 0.008186 log_softmax: 0.012882\n",
            "Train Epoch: 414 [800/1000 (80%)]\tLosses F.softmax: 0.020450 log_softmax: 0.011939\n",
            "Train Epoch: 414 [1000/1000 (100%)]\tLosses F.softmax: 0.128228 log_softmax: 0.183120\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5495\tAccuracy: 8445.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5354\tAccuracy: 8461.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 415 [0/1000 (0%)]\tLosses F.softmax: 0.162228 log_softmax: 0.164175\n",
            "Train Epoch: 415 [200/1000 (20%)]\tLosses F.softmax: 0.001109 log_softmax: 0.001015\n",
            "Train Epoch: 415 [400/1000 (40%)]\tLosses F.softmax: 0.007170 log_softmax: 0.018674\n",
            "Train Epoch: 415 [600/1000 (60%)]\tLosses F.softmax: 0.001151 log_softmax: 0.000618\n",
            "Train Epoch: 415 [800/1000 (80%)]\tLosses F.softmax: 0.003904 log_softmax: 0.002028\n",
            "Train Epoch: 415 [1000/1000 (100%)]\tLosses F.softmax: 0.009775 log_softmax: 0.005972\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5498\tAccuracy: 8451.0/10000 (85%)\n",
            "log_softmax: Loss: 0.5357\tAccuracy: 8460.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 416 [0/1000 (0%)]\tLosses F.softmax: 0.061413 log_softmax: 0.039099\n",
            "Train Epoch: 416 [200/1000 (20%)]\tLosses F.softmax: 0.001702 log_softmax: 0.002458\n",
            "Train Epoch: 416 [400/1000 (40%)]\tLosses F.softmax: 0.026710 log_softmax: 0.039711\n",
            "Train Epoch: 416 [600/1000 (60%)]\tLosses F.softmax: 0.078312 log_softmax: 0.071861\n",
            "Train Epoch: 416 [800/1000 (80%)]\tLosses F.softmax: 0.091734 log_softmax: 0.061957\n",
            "Train Epoch: 416 [1000/1000 (100%)]\tLosses F.softmax: 0.005273 log_softmax: 0.004713\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5503\tAccuracy: 8442.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5364\tAccuracy: 8458.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 417 [0/1000 (0%)]\tLosses F.softmax: 0.241979 log_softmax: 0.173754\n",
            "Train Epoch: 417 [200/1000 (20%)]\tLosses F.softmax: 0.384935 log_softmax: 0.264139\n",
            "Train Epoch: 417 [400/1000 (40%)]\tLosses F.softmax: 0.060467 log_softmax: 0.040497\n",
            "Train Epoch: 417 [600/1000 (60%)]\tLosses F.softmax: 0.007417 log_softmax: 0.006006\n",
            "Train Epoch: 417 [800/1000 (80%)]\tLosses F.softmax: 0.002368 log_softmax: 0.003275\n",
            "Train Epoch: 417 [1000/1000 (100%)]\tLosses F.softmax: 0.003708 log_softmax: 0.005564\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5496\tAccuracy: 8451.0/10000 (85%)\n",
            "log_softmax: Loss: 0.5361\tAccuracy: 8466.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 418 [0/1000 (0%)]\tLosses F.softmax: 0.025278 log_softmax: 0.028660\n",
            "Train Epoch: 418 [200/1000 (20%)]\tLosses F.softmax: 0.029133 log_softmax: 0.027664\n",
            "Train Epoch: 418 [400/1000 (40%)]\tLosses F.softmax: 0.378663 log_softmax: 0.594457\n",
            "Train Epoch: 418 [600/1000 (60%)]\tLosses F.softmax: 0.002474 log_softmax: 0.001504\n",
            "Train Epoch: 418 [800/1000 (80%)]\tLosses F.softmax: 0.045496 log_softmax: 0.042188\n",
            "Train Epoch: 418 [1000/1000 (100%)]\tLosses F.softmax: 0.043419 log_softmax: 0.035893\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5503\tAccuracy: 8451.0/10000 (85%)\n",
            "log_softmax: Loss: 0.5368\tAccuracy: 8463.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 419 [0/1000 (0%)]\tLosses F.softmax: 0.102467 log_softmax: 0.150319\n",
            "Train Epoch: 419 [200/1000 (20%)]\tLosses F.softmax: 0.008725 log_softmax: 0.018860\n",
            "Train Epoch: 419 [400/1000 (40%)]\tLosses F.softmax: 0.123481 log_softmax: 0.048462\n",
            "Train Epoch: 419 [600/1000 (60%)]\tLosses F.softmax: 0.001341 log_softmax: 0.002712\n",
            "Train Epoch: 419 [800/1000 (80%)]\tLosses F.softmax: 0.145154 log_softmax: 0.138996\n",
            "Train Epoch: 419 [1000/1000 (100%)]\tLosses F.softmax: 0.260869 log_softmax: 0.143109\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5516\tAccuracy: 8442.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5375\tAccuracy: 8459.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 420 [0/1000 (0%)]\tLosses F.softmax: 1.344767 log_softmax: 1.106668\n",
            "Train Epoch: 420 [200/1000 (20%)]\tLosses F.softmax: 1.093315 log_softmax: 1.060759\n",
            "Train Epoch: 420 [400/1000 (40%)]\tLosses F.softmax: 0.059975 log_softmax: 0.026722\n",
            "Train Epoch: 420 [600/1000 (60%)]\tLosses F.softmax: 0.165012 log_softmax: 0.203029\n",
            "Train Epoch: 420 [800/1000 (80%)]\tLosses F.softmax: 0.815289 log_softmax: 0.940284\n",
            "Train Epoch: 420 [1000/1000 (100%)]\tLosses F.softmax: 0.000766 log_softmax: 0.000721\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5519\tAccuracy: 8441.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5377\tAccuracy: 8464.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 421 [0/1000 (0%)]\tLosses F.softmax: 0.000098 log_softmax: 0.000330\n",
            "Train Epoch: 421 [200/1000 (20%)]\tLosses F.softmax: 0.181355 log_softmax: 0.139145\n",
            "Train Epoch: 421 [400/1000 (40%)]\tLosses F.softmax: 0.008103 log_softmax: 0.007975\n",
            "Train Epoch: 421 [600/1000 (60%)]\tLosses F.softmax: 0.016593 log_softmax: 0.017054\n",
            "Train Epoch: 421 [800/1000 (80%)]\tLosses F.softmax: 0.144038 log_softmax: 0.200752\n",
            "Train Epoch: 421 [1000/1000 (100%)]\tLosses F.softmax: 0.050178 log_softmax: 0.021225\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5516\tAccuracy: 8450.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5376\tAccuracy: 8460.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 422 [0/1000 (0%)]\tLosses F.softmax: 0.895682 log_softmax: 1.003095\n",
            "Train Epoch: 422 [200/1000 (20%)]\tLosses F.softmax: 0.016251 log_softmax: 0.038648\n",
            "Train Epoch: 422 [400/1000 (40%)]\tLosses F.softmax: 0.069090 log_softmax: 0.113721\n",
            "Train Epoch: 422 [600/1000 (60%)]\tLosses F.softmax: 0.013199 log_softmax: 0.008402\n",
            "Train Epoch: 422 [800/1000 (80%)]\tLosses F.softmax: 0.056611 log_softmax: 0.050841\n",
            "Train Epoch: 422 [1000/1000 (100%)]\tLosses F.softmax: 0.005344 log_softmax: 0.006083\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5523\tAccuracy: 8442.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5384\tAccuracy: 8461.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 423 [0/1000 (0%)]\tLosses F.softmax: 0.411500 log_softmax: 0.358422\n",
            "Train Epoch: 423 [200/1000 (20%)]\tLosses F.softmax: 0.014406 log_softmax: 0.025245\n",
            "Train Epoch: 423 [400/1000 (40%)]\tLosses F.softmax: 0.021578 log_softmax: 0.010921\n",
            "Train Epoch: 423 [600/1000 (60%)]\tLosses F.softmax: 0.002050 log_softmax: 0.002147\n",
            "Train Epoch: 423 [800/1000 (80%)]\tLosses F.softmax: 0.088099 log_softmax: 0.089308\n",
            "Train Epoch: 423 [1000/1000 (100%)]\tLosses F.softmax: 0.001756 log_softmax: 0.001517\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5520\tAccuracy: 8453.0/10000 (85%)\n",
            "log_softmax: Loss: 0.5379\tAccuracy: 8462.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 424 [0/1000 (0%)]\tLosses F.softmax: 0.002484 log_softmax: 0.011134\n",
            "Train Epoch: 424 [200/1000 (20%)]\tLosses F.softmax: 0.133637 log_softmax: 0.178973\n",
            "Train Epoch: 424 [400/1000 (40%)]\tLosses F.softmax: 0.068127 log_softmax: 0.058922\n",
            "Train Epoch: 424 [600/1000 (60%)]\tLosses F.softmax: 0.002830 log_softmax: 0.002866\n",
            "Train Epoch: 424 [800/1000 (80%)]\tLosses F.softmax: 0.121190 log_softmax: 0.114245\n",
            "Train Epoch: 424 [1000/1000 (100%)]\tLosses F.softmax: 0.008714 log_softmax: 0.008109\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5528\tAccuracy: 8443.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5389\tAccuracy: 8465.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 425 [0/1000 (0%)]\tLosses F.softmax: 0.266977 log_softmax: 0.152314\n",
            "Train Epoch: 425 [200/1000 (20%)]\tLosses F.softmax: 0.018123 log_softmax: 0.019111\n",
            "Train Epoch: 425 [400/1000 (40%)]\tLosses F.softmax: 0.359893 log_softmax: 0.422049\n",
            "Train Epoch: 425 [600/1000 (60%)]\tLosses F.softmax: 0.021371 log_softmax: 0.020591\n",
            "Train Epoch: 425 [800/1000 (80%)]\tLosses F.softmax: 0.004400 log_softmax: 0.012717\n",
            "Train Epoch: 425 [1000/1000 (100%)]\tLosses F.softmax: 0.000153 log_softmax: 0.000434\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5540\tAccuracy: 8441.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5399\tAccuracy: 8461.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 426 [0/1000 (0%)]\tLosses F.softmax: 0.053509 log_softmax: 0.044331\n",
            "Train Epoch: 426 [200/1000 (20%)]\tLosses F.softmax: 0.003391 log_softmax: 0.002487\n",
            "Train Epoch: 426 [400/1000 (40%)]\tLosses F.softmax: 0.008853 log_softmax: 0.021892\n",
            "Train Epoch: 426 [600/1000 (60%)]\tLosses F.softmax: 0.032497 log_softmax: 0.014044\n",
            "Train Epoch: 426 [800/1000 (80%)]\tLosses F.softmax: 0.168540 log_softmax: 0.176300\n",
            "Train Epoch: 426 [1000/1000 (100%)]\tLosses F.softmax: 0.080202 log_softmax: 0.040578\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5547\tAccuracy: 8440.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5406\tAccuracy: 8455.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 427 [0/1000 (0%)]\tLosses F.softmax: 0.001531 log_softmax: 0.002302\n",
            "Train Epoch: 427 [200/1000 (20%)]\tLosses F.softmax: 0.004204 log_softmax: 0.011789\n",
            "Train Epoch: 427 [400/1000 (40%)]\tLosses F.softmax: 0.091310 log_softmax: 0.097238\n",
            "Train Epoch: 427 [600/1000 (60%)]\tLosses F.softmax: 0.002983 log_softmax: 0.005281\n",
            "Train Epoch: 427 [800/1000 (80%)]\tLosses F.softmax: 0.000367 log_softmax: 0.000752\n",
            "Train Epoch: 427 [1000/1000 (100%)]\tLosses F.softmax: 0.001082 log_softmax: 0.001812\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5557\tAccuracy: 8437.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5411\tAccuracy: 8456.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 428 [0/1000 (0%)]\tLosses F.softmax: 0.006268 log_softmax: 0.002834\n",
            "Train Epoch: 428 [200/1000 (20%)]\tLosses F.softmax: 0.083605 log_softmax: 0.066643\n",
            "Train Epoch: 428 [400/1000 (40%)]\tLosses F.softmax: 0.012011 log_softmax: 0.017832\n",
            "Train Epoch: 428 [600/1000 (60%)]\tLosses F.softmax: 0.001128 log_softmax: 0.000849\n",
            "Train Epoch: 428 [800/1000 (80%)]\tLosses F.softmax: 0.006055 log_softmax: 0.010995\n",
            "Train Epoch: 428 [1000/1000 (100%)]\tLosses F.softmax: 0.000322 log_softmax: 0.000593\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5552\tAccuracy: 8446.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5407\tAccuracy: 8461.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 429 [0/1000 (0%)]\tLosses F.softmax: 0.011253 log_softmax: 0.007737\n",
            "Train Epoch: 429 [200/1000 (20%)]\tLosses F.softmax: 0.042347 log_softmax: 0.067905\n",
            "Train Epoch: 429 [400/1000 (40%)]\tLosses F.softmax: 0.068802 log_softmax: 0.060343\n",
            "Train Epoch: 429 [600/1000 (60%)]\tLosses F.softmax: 0.001099 log_softmax: 0.000558\n",
            "Train Epoch: 429 [800/1000 (80%)]\tLosses F.softmax: 0.000670 log_softmax: 0.000251\n",
            "Train Epoch: 429 [1000/1000 (100%)]\tLosses F.softmax: 0.001727 log_softmax: 0.000848\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5551\tAccuracy: 8449.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5409\tAccuracy: 8462.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 430 [0/1000 (0%)]\tLosses F.softmax: 0.039029 log_softmax: 0.017523\n",
            "Train Epoch: 430 [200/1000 (20%)]\tLosses F.softmax: 0.122415 log_softmax: 0.131081\n",
            "Train Epoch: 430 [400/1000 (40%)]\tLosses F.softmax: 0.001731 log_softmax: 0.000857\n",
            "Train Epoch: 430 [600/1000 (60%)]\tLosses F.softmax: 0.036618 log_softmax: 0.081299\n",
            "Train Epoch: 430 [800/1000 (80%)]\tLosses F.softmax: 0.001592 log_softmax: 0.002300\n",
            "Train Epoch: 430 [1000/1000 (100%)]\tLosses F.softmax: 0.000803 log_softmax: 0.001904\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5552\tAccuracy: 8448.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5410\tAccuracy: 8463.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 431 [0/1000 (0%)]\tLosses F.softmax: 0.000190 log_softmax: 0.000742\n",
            "Train Epoch: 431 [200/1000 (20%)]\tLosses F.softmax: 0.309768 log_softmax: 0.248800\n",
            "Train Epoch: 431 [400/1000 (40%)]\tLosses F.softmax: 0.001125 log_softmax: 0.001155\n",
            "Train Epoch: 431 [600/1000 (60%)]\tLosses F.softmax: 0.003125 log_softmax: 0.004358\n",
            "Train Epoch: 431 [800/1000 (80%)]\tLosses F.softmax: 0.002424 log_softmax: 0.002006\n",
            "Train Epoch: 431 [1000/1000 (100%)]\tLosses F.softmax: 0.001187 log_softmax: 0.000882\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5567\tAccuracy: 8443.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5425\tAccuracy: 8464.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 432 [0/1000 (0%)]\tLosses F.softmax: 0.113220 log_softmax: 0.107794\n",
            "Train Epoch: 432 [200/1000 (20%)]\tLosses F.softmax: 0.020670 log_softmax: 0.017265\n",
            "Train Epoch: 432 [400/1000 (40%)]\tLosses F.softmax: 0.028036 log_softmax: 0.113055\n",
            "Train Epoch: 432 [600/1000 (60%)]\tLosses F.softmax: 0.001763 log_softmax: 0.001943\n",
            "Train Epoch: 432 [800/1000 (80%)]\tLosses F.softmax: 0.091336 log_softmax: 0.128160\n",
            "Train Epoch: 432 [1000/1000 (100%)]\tLosses F.softmax: 0.000684 log_softmax: 0.000631\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5577\tAccuracy: 8446.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5434\tAccuracy: 8460.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 433 [0/1000 (0%)]\tLosses F.softmax: 0.021502 log_softmax: 0.018484\n",
            "Train Epoch: 433 [200/1000 (20%)]\tLosses F.softmax: 0.002693 log_softmax: 0.002111\n",
            "Train Epoch: 433 [400/1000 (40%)]\tLosses F.softmax: 0.005259 log_softmax: 0.003475\n",
            "Train Epoch: 433 [600/1000 (60%)]\tLosses F.softmax: 0.419899 log_softmax: 0.938968\n",
            "Train Epoch: 433 [800/1000 (80%)]\tLosses F.softmax: 0.024401 log_softmax: 0.014320\n",
            "Train Epoch: 433 [1000/1000 (100%)]\tLosses F.softmax: 0.003608 log_softmax: 0.004600\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5572\tAccuracy: 8439.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5431\tAccuracy: 8464.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 434 [0/1000 (0%)]\tLosses F.softmax: 0.002025 log_softmax: 0.001782\n",
            "Train Epoch: 434 [200/1000 (20%)]\tLosses F.softmax: 0.000563 log_softmax: 0.000278\n",
            "Train Epoch: 434 [400/1000 (40%)]\tLosses F.softmax: 0.027857 log_softmax: 0.063416\n",
            "Train Epoch: 434 [600/1000 (60%)]\tLosses F.softmax: 0.005292 log_softmax: 0.007215\n",
            "Train Epoch: 434 [800/1000 (80%)]\tLosses F.softmax: 0.102178 log_softmax: 0.126435\n",
            "Train Epoch: 434 [1000/1000 (100%)]\tLosses F.softmax: 0.326959 log_softmax: 0.409854\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5572\tAccuracy: 8447.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5429\tAccuracy: 8464.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 435 [0/1000 (0%)]\tLosses F.softmax: 0.003871 log_softmax: 0.006996\n",
            "Train Epoch: 435 [200/1000 (20%)]\tLosses F.softmax: 0.234589 log_softmax: 0.300039\n",
            "Train Epoch: 435 [400/1000 (40%)]\tLosses F.softmax: 0.005329 log_softmax: 0.005561\n",
            "Train Epoch: 435 [600/1000 (60%)]\tLosses F.softmax: 0.034729 log_softmax: 0.040383\n",
            "Train Epoch: 435 [800/1000 (80%)]\tLosses F.softmax: 0.074862 log_softmax: 0.045919\n",
            "Train Epoch: 435 [1000/1000 (100%)]\tLosses F.softmax: 0.158704 log_softmax: 0.133956\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5582\tAccuracy: 8439.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5440\tAccuracy: 8466.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 436 [0/1000 (0%)]\tLosses F.softmax: 0.014567 log_softmax: 0.017929\n",
            "Train Epoch: 436 [200/1000 (20%)]\tLosses F.softmax: 0.095531 log_softmax: 0.070110\n",
            "Train Epoch: 436 [400/1000 (40%)]\tLosses F.softmax: 0.026330 log_softmax: 0.022333\n",
            "Train Epoch: 436 [600/1000 (60%)]\tLosses F.softmax: 0.004228 log_softmax: 0.008772\n",
            "Train Epoch: 436 [800/1000 (80%)]\tLosses F.softmax: 0.014515 log_softmax: 0.033371\n",
            "Train Epoch: 436 [1000/1000 (100%)]\tLosses F.softmax: 0.039528 log_softmax: 0.010521\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5589\tAccuracy: 8442.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5450\tAccuracy: 8458.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 437 [0/1000 (0%)]\tLosses F.softmax: 0.005385 log_softmax: 0.006182\n",
            "Train Epoch: 437 [200/1000 (20%)]\tLosses F.softmax: 0.023385 log_softmax: 0.012822\n",
            "Train Epoch: 437 [400/1000 (40%)]\tLosses F.softmax: 0.047756 log_softmax: 0.049754\n",
            "Train Epoch: 437 [600/1000 (60%)]\tLosses F.softmax: 0.018123 log_softmax: 0.013730\n",
            "Train Epoch: 437 [800/1000 (80%)]\tLosses F.softmax: 0.007424 log_softmax: 0.005284\n",
            "Train Epoch: 437 [1000/1000 (100%)]\tLosses F.softmax: 0.268854 log_softmax: 0.219775\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5604\tAccuracy: 8437.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5457\tAccuracy: 8461.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 438 [0/1000 (0%)]\tLosses F.softmax: 0.002894 log_softmax: 0.002269\n",
            "Train Epoch: 438 [200/1000 (20%)]\tLosses F.softmax: 0.004895 log_softmax: 0.003199\n",
            "Train Epoch: 438 [400/1000 (40%)]\tLosses F.softmax: 0.002656 log_softmax: 0.005481\n",
            "Train Epoch: 438 [600/1000 (60%)]\tLosses F.softmax: 0.005771 log_softmax: 0.008544\n",
            "Train Epoch: 438 [800/1000 (80%)]\tLosses F.softmax: 0.004627 log_softmax: 0.002628\n",
            "Train Epoch: 438 [1000/1000 (100%)]\tLosses F.softmax: 0.075875 log_softmax: 0.056134\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5599\tAccuracy: 8448.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5453\tAccuracy: 8462.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 439 [0/1000 (0%)]\tLosses F.softmax: 0.001465 log_softmax: 0.002812\n",
            "Train Epoch: 439 [200/1000 (20%)]\tLosses F.softmax: 0.006113 log_softmax: 0.011258\n",
            "Train Epoch: 439 [400/1000 (40%)]\tLosses F.softmax: 0.023110 log_softmax: 0.014855\n",
            "Train Epoch: 439 [600/1000 (60%)]\tLosses F.softmax: 0.118507 log_softmax: 0.142860\n",
            "Train Epoch: 439 [800/1000 (80%)]\tLosses F.softmax: 0.036815 log_softmax: 0.021953\n",
            "Train Epoch: 439 [1000/1000 (100%)]\tLosses F.softmax: 0.006336 log_softmax: 0.008578\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5613\tAccuracy: 8444.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5466\tAccuracy: 8457.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 440 [0/1000 (0%)]\tLosses F.softmax: 0.051855 log_softmax: 0.041925\n",
            "Train Epoch: 440 [200/1000 (20%)]\tLosses F.softmax: 0.031715 log_softmax: 0.033303\n",
            "Train Epoch: 440 [400/1000 (40%)]\tLosses F.softmax: 0.003855 log_softmax: 0.008833\n",
            "Train Epoch: 440 [600/1000 (60%)]\tLosses F.softmax: 0.376990 log_softmax: 0.335346\n",
            "Train Epoch: 440 [800/1000 (80%)]\tLosses F.softmax: 0.063693 log_softmax: 0.060742\n",
            "Train Epoch: 440 [1000/1000 (100%)]\tLosses F.softmax: 0.230707 log_softmax: 0.134165\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5618\tAccuracy: 8440.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5474\tAccuracy: 8457.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 441 [0/1000 (0%)]\tLosses F.softmax: 0.061237 log_softmax: 0.045014\n",
            "Train Epoch: 441 [200/1000 (20%)]\tLosses F.softmax: 0.468023 log_softmax: 0.478230\n",
            "Train Epoch: 441 [400/1000 (40%)]\tLosses F.softmax: 0.027193 log_softmax: 0.013459\n",
            "Train Epoch: 441 [600/1000 (60%)]\tLosses F.softmax: 0.002214 log_softmax: 0.003127\n",
            "Train Epoch: 441 [800/1000 (80%)]\tLosses F.softmax: 0.024713 log_softmax: 0.043539\n",
            "Train Epoch: 441 [1000/1000 (100%)]\tLosses F.softmax: 0.294368 log_softmax: 0.407491\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5608\tAccuracy: 8442.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5469\tAccuracy: 8455.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 442 [0/1000 (0%)]\tLosses F.softmax: 0.035061 log_softmax: 0.016291\n",
            "Train Epoch: 442 [200/1000 (20%)]\tLosses F.softmax: 0.011406 log_softmax: 0.015742\n",
            "Train Epoch: 442 [400/1000 (40%)]\tLosses F.softmax: 0.094902 log_softmax: 0.065789\n",
            "Train Epoch: 442 [600/1000 (60%)]\tLosses F.softmax: 0.009496 log_softmax: 0.008674\n",
            "Train Epoch: 442 [800/1000 (80%)]\tLosses F.softmax: 0.004526 log_softmax: 0.003190\n",
            "Train Epoch: 442 [1000/1000 (100%)]\tLosses F.softmax: 0.003150 log_softmax: 0.003958\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5617\tAccuracy: 8452.0/10000 (85%)\n",
            "log_softmax: Loss: 0.5471\tAccuracy: 8468.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 443 [0/1000 (0%)]\tLosses F.softmax: 0.001447 log_softmax: 0.003054\n",
            "Train Epoch: 443 [200/1000 (20%)]\tLosses F.softmax: 0.566030 log_softmax: 0.709822\n",
            "Train Epoch: 443 [400/1000 (40%)]\tLosses F.softmax: 0.406252 log_softmax: 0.616749\n",
            "Train Epoch: 443 [600/1000 (60%)]\tLosses F.softmax: 0.886794 log_softmax: 0.604302\n",
            "Train Epoch: 443 [800/1000 (80%)]\tLosses F.softmax: 0.015383 log_softmax: 0.009325\n",
            "Train Epoch: 443 [1000/1000 (100%)]\tLosses F.softmax: 0.057109 log_softmax: 0.040568\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5611\tAccuracy: 8454.0/10000 (85%)\n",
            "log_softmax: Loss: 0.5466\tAccuracy: 8467.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 444 [0/1000 (0%)]\tLosses F.softmax: 0.347024 log_softmax: 0.328094\n",
            "Train Epoch: 444 [200/1000 (20%)]\tLosses F.softmax: 0.025216 log_softmax: 0.016941\n",
            "Train Epoch: 444 [400/1000 (40%)]\tLosses F.softmax: 0.067202 log_softmax: 0.058699\n",
            "Train Epoch: 444 [600/1000 (60%)]\tLosses F.softmax: 0.009925 log_softmax: 0.004993\n",
            "Train Epoch: 444 [800/1000 (80%)]\tLosses F.softmax: 0.667946 log_softmax: 0.474443\n",
            "Train Epoch: 444 [1000/1000 (100%)]\tLosses F.softmax: 0.023874 log_softmax: 0.015141\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5619\tAccuracy: 8446.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5476\tAccuracy: 8468.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 445 [0/1000 (0%)]\tLosses F.softmax: 0.453642 log_softmax: 0.238559\n",
            "Train Epoch: 445 [200/1000 (20%)]\tLosses F.softmax: 0.031344 log_softmax: 0.019389\n",
            "Train Epoch: 445 [400/1000 (40%)]\tLosses F.softmax: 0.005308 log_softmax: 0.002461\n",
            "Train Epoch: 445 [600/1000 (60%)]\tLosses F.softmax: 0.129197 log_softmax: 0.111339\n",
            "Train Epoch: 445 [800/1000 (80%)]\tLosses F.softmax: 0.004440 log_softmax: 0.003964\n",
            "Train Epoch: 445 [1000/1000 (100%)]\tLosses F.softmax: 0.027480 log_softmax: 0.049808\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5624\tAccuracy: 8451.0/10000 (85%)\n",
            "log_softmax: Loss: 0.5475\tAccuracy: 8466.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 446 [0/1000 (0%)]\tLosses F.softmax: 0.023836 log_softmax: 0.025079\n",
            "Train Epoch: 446 [200/1000 (20%)]\tLosses F.softmax: 0.009821 log_softmax: 0.007292\n",
            "Train Epoch: 446 [400/1000 (40%)]\tLosses F.softmax: 0.000970 log_softmax: 0.001797\n",
            "Train Epoch: 446 [600/1000 (60%)]\tLosses F.softmax: 0.002774 log_softmax: 0.003054\n",
            "Train Epoch: 446 [800/1000 (80%)]\tLosses F.softmax: 0.026024 log_softmax: 0.035296\n",
            "Train Epoch: 446 [1000/1000 (100%)]\tLosses F.softmax: 0.000165 log_softmax: 0.000613\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5629\tAccuracy: 8450.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5481\tAccuracy: 8466.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 447 [0/1000 (0%)]\tLosses F.softmax: 0.218483 log_softmax: 0.173382\n",
            "Train Epoch: 447 [200/1000 (20%)]\tLosses F.softmax: 0.012451 log_softmax: 0.024437\n",
            "Train Epoch: 447 [400/1000 (40%)]\tLosses F.softmax: 0.026224 log_softmax: 0.025383\n",
            "Train Epoch: 447 [600/1000 (60%)]\tLosses F.softmax: 0.023916 log_softmax: 0.047269\n",
            "Train Epoch: 447 [800/1000 (80%)]\tLosses F.softmax: 0.288547 log_softmax: 0.267053\n",
            "Train Epoch: 447 [1000/1000 (100%)]\tLosses F.softmax: 0.054051 log_softmax: 0.057320\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5628\tAccuracy: 8448.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5481\tAccuracy: 8468.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 448 [0/1000 (0%)]\tLosses F.softmax: 0.019767 log_softmax: 0.012031\n",
            "Train Epoch: 448 [200/1000 (20%)]\tLosses F.softmax: 0.005813 log_softmax: 0.020783\n",
            "Train Epoch: 448 [400/1000 (40%)]\tLosses F.softmax: 0.007176 log_softmax: 0.004711\n",
            "Train Epoch: 448 [600/1000 (60%)]\tLosses F.softmax: 0.073679 log_softmax: 0.099975\n",
            "Train Epoch: 448 [800/1000 (80%)]\tLosses F.softmax: 0.000146 log_softmax: 0.000234\n",
            "Train Epoch: 448 [1000/1000 (100%)]\tLosses F.softmax: 0.011672 log_softmax: 0.006845\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5632\tAccuracy: 8458.0/10000 (85%)\n",
            "log_softmax: Loss: 0.5484\tAccuracy: 8458.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 449 [0/1000 (0%)]\tLosses F.softmax: 0.001150 log_softmax: 0.000574\n",
            "Train Epoch: 449 [200/1000 (20%)]\tLosses F.softmax: 0.001530 log_softmax: 0.001835\n",
            "Train Epoch: 449 [400/1000 (40%)]\tLosses F.softmax: 0.001152 log_softmax: 0.000755\n",
            "Train Epoch: 449 [600/1000 (60%)]\tLosses F.softmax: 0.008241 log_softmax: 0.010732\n",
            "Train Epoch: 449 [800/1000 (80%)]\tLosses F.softmax: 0.166147 log_softmax: 0.111751\n",
            "Train Epoch: 449 [1000/1000 (100%)]\tLosses F.softmax: 0.047473 log_softmax: 0.083478\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5640\tAccuracy: 8448.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5489\tAccuracy: 8467.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 450 [0/1000 (0%)]\tLosses F.softmax: 0.011381 log_softmax: 0.008687\n",
            "Train Epoch: 450 [200/1000 (20%)]\tLosses F.softmax: 0.002810 log_softmax: 0.004235\n",
            "Train Epoch: 450 [400/1000 (40%)]\tLosses F.softmax: 0.023621 log_softmax: 0.016686\n",
            "Train Epoch: 450 [600/1000 (60%)]\tLosses F.softmax: 0.003959 log_softmax: 0.016781\n",
            "Train Epoch: 450 [800/1000 (80%)]\tLosses F.softmax: 0.000483 log_softmax: 0.000647\n",
            "Train Epoch: 450 [1000/1000 (100%)]\tLosses F.softmax: 0.014453 log_softmax: 0.026374\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5655\tAccuracy: 8450.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5502\tAccuracy: 8463.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 451 [0/1000 (0%)]\tLosses F.softmax: 0.091302 log_softmax: 0.099294\n",
            "Train Epoch: 451 [200/1000 (20%)]\tLosses F.softmax: 0.328978 log_softmax: 0.310844\n",
            "Train Epoch: 451 [400/1000 (40%)]\tLosses F.softmax: 0.064057 log_softmax: 0.039146\n",
            "Train Epoch: 451 [600/1000 (60%)]\tLosses F.softmax: 0.003177 log_softmax: 0.003293\n",
            "Train Epoch: 451 [800/1000 (80%)]\tLosses F.softmax: 0.062737 log_softmax: 0.079373\n",
            "Train Epoch: 451 [1000/1000 (100%)]\tLosses F.softmax: 0.006818 log_softmax: 0.005090\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5657\tAccuracy: 8453.0/10000 (85%)\n",
            "log_softmax: Loss: 0.5506\tAccuracy: 8464.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 452 [0/1000 (0%)]\tLosses F.softmax: 0.000987 log_softmax: 0.000531\n",
            "Train Epoch: 452 [200/1000 (20%)]\tLosses F.softmax: 0.021196 log_softmax: 0.013362\n",
            "Train Epoch: 452 [400/1000 (40%)]\tLosses F.softmax: 0.018787 log_softmax: 0.030523\n",
            "Train Epoch: 452 [600/1000 (60%)]\tLosses F.softmax: 0.000614 log_softmax: 0.001826\n",
            "Train Epoch: 452 [800/1000 (80%)]\tLosses F.softmax: 0.007853 log_softmax: 0.004445\n",
            "Train Epoch: 452 [1000/1000 (100%)]\tLosses F.softmax: 0.000219 log_softmax: 0.000159\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5661\tAccuracy: 8443.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5511\tAccuracy: 8459.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 453 [0/1000 (0%)]\tLosses F.softmax: 0.004503 log_softmax: 0.002225\n",
            "Train Epoch: 453 [200/1000 (20%)]\tLosses F.softmax: 0.003257 log_softmax: 0.005027\n",
            "Train Epoch: 453 [400/1000 (40%)]\tLosses F.softmax: 0.000235 log_softmax: 0.000309\n",
            "Train Epoch: 453 [600/1000 (60%)]\tLosses F.softmax: 0.001238 log_softmax: 0.001762\n",
            "Train Epoch: 453 [800/1000 (80%)]\tLosses F.softmax: 0.058690 log_softmax: 0.044743\n",
            "Train Epoch: 453 [1000/1000 (100%)]\tLosses F.softmax: 0.035291 log_softmax: 0.048938\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5662\tAccuracy: 8447.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5513\tAccuracy: 8466.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 454 [0/1000 (0%)]\tLosses F.softmax: 0.290301 log_softmax: 0.412905\n",
            "Train Epoch: 454 [200/1000 (20%)]\tLosses F.softmax: 0.002962 log_softmax: 0.003511\n",
            "Train Epoch: 454 [400/1000 (40%)]\tLosses F.softmax: 0.009033 log_softmax: 0.009154\n",
            "Train Epoch: 454 [600/1000 (60%)]\tLosses F.softmax: 0.011868 log_softmax: 0.017189\n",
            "Train Epoch: 454 [800/1000 (80%)]\tLosses F.softmax: 0.002834 log_softmax: 0.004388\n",
            "Train Epoch: 454 [1000/1000 (100%)]\tLosses F.softmax: 0.008760 log_softmax: 0.008088\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5676\tAccuracy: 8447.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5526\tAccuracy: 8457.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 455 [0/1000 (0%)]\tLosses F.softmax: 0.119715 log_softmax: 0.105673\n",
            "Train Epoch: 455 [200/1000 (20%)]\tLosses F.softmax: 0.000451 log_softmax: 0.000682\n",
            "Train Epoch: 455 [400/1000 (40%)]\tLosses F.softmax: 0.020716 log_softmax: 0.011620\n",
            "Train Epoch: 455 [600/1000 (60%)]\tLosses F.softmax: 0.001165 log_softmax: 0.000623\n",
            "Train Epoch: 455 [800/1000 (80%)]\tLosses F.softmax: 0.016630 log_softmax: 0.013271\n",
            "Train Epoch: 455 [1000/1000 (100%)]\tLosses F.softmax: 0.003725 log_softmax: 0.004230\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5679\tAccuracy: 8444.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5529\tAccuracy: 8463.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 456 [0/1000 (0%)]\tLosses F.softmax: 0.000225 log_softmax: 0.000257\n",
            "Train Epoch: 456 [200/1000 (20%)]\tLosses F.softmax: 0.005311 log_softmax: 0.022190\n",
            "Train Epoch: 456 [400/1000 (40%)]\tLosses F.softmax: 0.026886 log_softmax: 0.021772\n",
            "Train Epoch: 456 [600/1000 (60%)]\tLosses F.softmax: 0.106363 log_softmax: 0.142809\n",
            "Train Epoch: 456 [800/1000 (80%)]\tLosses F.softmax: 0.352009 log_softmax: 0.415397\n",
            "Train Epoch: 456 [1000/1000 (100%)]\tLosses F.softmax: 0.004111 log_softmax: 0.002108\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5668\tAccuracy: 8457.0/10000 (85%)\n",
            "log_softmax: Loss: 0.5521\tAccuracy: 8464.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 457 [0/1000 (0%)]\tLosses F.softmax: 0.114963 log_softmax: 0.065388\n",
            "Train Epoch: 457 [200/1000 (20%)]\tLosses F.softmax: 0.079533 log_softmax: 0.157919\n",
            "Train Epoch: 457 [400/1000 (40%)]\tLosses F.softmax: 0.134757 log_softmax: 0.117923\n",
            "Train Epoch: 457 [600/1000 (60%)]\tLosses F.softmax: 0.011913 log_softmax: 0.010401\n",
            "Train Epoch: 457 [800/1000 (80%)]\tLosses F.softmax: 0.001227 log_softmax: 0.001885\n",
            "Train Epoch: 457 [1000/1000 (100%)]\tLosses F.softmax: 0.004831 log_softmax: 0.003780\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5676\tAccuracy: 8449.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5526\tAccuracy: 8472.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 458 [0/1000 (0%)]\tLosses F.softmax: 0.000093 log_softmax: 0.000369\n",
            "Train Epoch: 458 [200/1000 (20%)]\tLosses F.softmax: 0.002377 log_softmax: 0.002097\n",
            "Train Epoch: 458 [400/1000 (40%)]\tLosses F.softmax: 0.001122 log_softmax: 0.001626\n",
            "Train Epoch: 458 [600/1000 (60%)]\tLosses F.softmax: 0.003383 log_softmax: 0.009625\n",
            "Train Epoch: 458 [800/1000 (80%)]\tLosses F.softmax: 0.000970 log_softmax: 0.001494\n",
            "Train Epoch: 458 [1000/1000 (100%)]\tLosses F.softmax: 0.200751 log_softmax: 0.272722\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5681\tAccuracy: 8454.0/10000 (85%)\n",
            "log_softmax: Loss: 0.5533\tAccuracy: 8465.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 459 [0/1000 (0%)]\tLosses F.softmax: 0.266308 log_softmax: 0.319460\n",
            "Train Epoch: 459 [200/1000 (20%)]\tLosses F.softmax: 0.021727 log_softmax: 0.028991\n",
            "Train Epoch: 459 [400/1000 (40%)]\tLosses F.softmax: 0.790039 log_softmax: 0.520566\n",
            "Train Epoch: 459 [600/1000 (60%)]\tLosses F.softmax: 0.006602 log_softmax: 0.003751\n",
            "Train Epoch: 459 [800/1000 (80%)]\tLosses F.softmax: 0.032009 log_softmax: 0.007963\n",
            "Train Epoch: 459 [1000/1000 (100%)]\tLosses F.softmax: 0.091779 log_softmax: 0.131028\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5690\tAccuracy: 8458.0/10000 (85%)\n",
            "log_softmax: Loss: 0.5539\tAccuracy: 8467.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 460 [0/1000 (0%)]\tLosses F.softmax: 0.009911 log_softmax: 0.013544\n",
            "Train Epoch: 460 [200/1000 (20%)]\tLosses F.softmax: 0.000992 log_softmax: 0.000530\n",
            "Train Epoch: 460 [400/1000 (40%)]\tLosses F.softmax: 0.004098 log_softmax: 0.004701\n",
            "Train Epoch: 460 [600/1000 (60%)]\tLosses F.softmax: 0.011878 log_softmax: 0.005851\n",
            "Train Epoch: 460 [800/1000 (80%)]\tLosses F.softmax: 0.002514 log_softmax: 0.003256\n",
            "Train Epoch: 460 [1000/1000 (100%)]\tLosses F.softmax: 0.137608 log_softmax: 0.053531\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5697\tAccuracy: 8446.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5542\tAccuracy: 8465.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 461 [0/1000 (0%)]\tLosses F.softmax: 0.023998 log_softmax: 0.019874\n",
            "Train Epoch: 461 [200/1000 (20%)]\tLosses F.softmax: 0.003070 log_softmax: 0.005902\n",
            "Train Epoch: 461 [400/1000 (40%)]\tLosses F.softmax: 0.050447 log_softmax: 0.014376\n",
            "Train Epoch: 461 [600/1000 (60%)]\tLosses F.softmax: 0.004224 log_softmax: 0.001196\n",
            "Train Epoch: 461 [800/1000 (80%)]\tLosses F.softmax: 0.004413 log_softmax: 0.004712\n",
            "Train Epoch: 461 [1000/1000 (100%)]\tLosses F.softmax: 0.001367 log_softmax: 0.001974\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5703\tAccuracy: 8445.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5551\tAccuracy: 8471.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 462 [0/1000 (0%)]\tLosses F.softmax: 0.096822 log_softmax: 0.058617\n",
            "Train Epoch: 462 [200/1000 (20%)]\tLosses F.softmax: 0.015243 log_softmax: 0.019359\n",
            "Train Epoch: 462 [400/1000 (40%)]\tLosses F.softmax: 0.010987 log_softmax: 0.002682\n",
            "Train Epoch: 462 [600/1000 (60%)]\tLosses F.softmax: 0.003898 log_softmax: 0.002775\n",
            "Train Epoch: 462 [800/1000 (80%)]\tLosses F.softmax: 0.001105 log_softmax: 0.002296\n",
            "Train Epoch: 462 [1000/1000 (100%)]\tLosses F.softmax: 0.068652 log_softmax: 0.097048\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5704\tAccuracy: 8441.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5551\tAccuracy: 8469.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 463 [0/1000 (0%)]\tLosses F.softmax: 0.059949 log_softmax: 0.024659\n",
            "Train Epoch: 463 [200/1000 (20%)]\tLosses F.softmax: 0.216056 log_softmax: 0.173568\n",
            "Train Epoch: 463 [400/1000 (40%)]\tLosses F.softmax: 0.102899 log_softmax: 0.064894\n",
            "Train Epoch: 463 [600/1000 (60%)]\tLosses F.softmax: 0.145488 log_softmax: 0.123238\n",
            "Train Epoch: 463 [800/1000 (80%)]\tLosses F.softmax: 0.096288 log_softmax: 0.076716\n",
            "Train Epoch: 463 [1000/1000 (100%)]\tLosses F.softmax: 0.002501 log_softmax: 0.004918\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5716\tAccuracy: 8444.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5560\tAccuracy: 8468.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 464 [0/1000 (0%)]\tLosses F.softmax: 0.216613 log_softmax: 0.130748\n",
            "Train Epoch: 464 [200/1000 (20%)]\tLosses F.softmax: 0.003692 log_softmax: 0.011363\n",
            "Train Epoch: 464 [400/1000 (40%)]\tLosses F.softmax: 0.048791 log_softmax: 0.014241\n",
            "Train Epoch: 464 [600/1000 (60%)]\tLosses F.softmax: 0.006354 log_softmax: 0.004819\n",
            "Train Epoch: 464 [800/1000 (80%)]\tLosses F.softmax: 0.000742 log_softmax: 0.000993\n",
            "Train Epoch: 464 [1000/1000 (100%)]\tLosses F.softmax: 0.029231 log_softmax: 0.011507\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5712\tAccuracy: 8450.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5558\tAccuracy: 8469.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 465 [0/1000 (0%)]\tLosses F.softmax: 0.209395 log_softmax: 0.256624\n",
            "Train Epoch: 465 [200/1000 (20%)]\tLosses F.softmax: 0.006068 log_softmax: 0.010116\n",
            "Train Epoch: 465 [400/1000 (40%)]\tLosses F.softmax: 0.020737 log_softmax: 0.020849\n",
            "Train Epoch: 465 [600/1000 (60%)]\tLosses F.softmax: 0.008612 log_softmax: 0.018934\n",
            "Train Epoch: 465 [800/1000 (80%)]\tLosses F.softmax: 0.012240 log_softmax: 0.030674\n",
            "Train Epoch: 465 [1000/1000 (100%)]\tLosses F.softmax: 1.402174 log_softmax: 1.556908\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5720\tAccuracy: 8445.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5568\tAccuracy: 8468.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 466 [0/1000 (0%)]\tLosses F.softmax: 0.210435 log_softmax: 0.291615\n",
            "Train Epoch: 466 [200/1000 (20%)]\tLosses F.softmax: 0.060965 log_softmax: 0.059501\n",
            "Train Epoch: 466 [400/1000 (40%)]\tLosses F.softmax: 0.006929 log_softmax: 0.015054\n",
            "Train Epoch: 466 [600/1000 (60%)]\tLosses F.softmax: 0.000165 log_softmax: 0.000416\n",
            "Train Epoch: 466 [800/1000 (80%)]\tLosses F.softmax: 0.063284 log_softmax: 0.046077\n",
            "Train Epoch: 466 [1000/1000 (100%)]\tLosses F.softmax: 0.001533 log_softmax: 0.001362\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5722\tAccuracy: 8442.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5571\tAccuracy: 8470.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 467 [0/1000 (0%)]\tLosses F.softmax: 0.020771 log_softmax: 0.018075\n",
            "Train Epoch: 467 [200/1000 (20%)]\tLosses F.softmax: 0.018305 log_softmax: 0.011758\n",
            "Train Epoch: 467 [400/1000 (40%)]\tLosses F.softmax: 0.004718 log_softmax: 0.002274\n",
            "Train Epoch: 467 [600/1000 (60%)]\tLosses F.softmax: 0.028695 log_softmax: 0.017671\n",
            "Train Epoch: 467 [800/1000 (80%)]\tLosses F.softmax: 0.310233 log_softmax: 0.323635\n",
            "Train Epoch: 467 [1000/1000 (100%)]\tLosses F.softmax: 0.029597 log_softmax: 0.020941\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5727\tAccuracy: 8450.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5575\tAccuracy: 8472.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 468 [0/1000 (0%)]\tLosses F.softmax: 0.000173 log_softmax: 0.000225\n",
            "Train Epoch: 468 [200/1000 (20%)]\tLosses F.softmax: 0.002126 log_softmax: 0.009934\n",
            "Train Epoch: 468 [400/1000 (40%)]\tLosses F.softmax: 0.000919 log_softmax: 0.000740\n",
            "Train Epoch: 468 [600/1000 (60%)]\tLosses F.softmax: 0.009150 log_softmax: 0.006055\n",
            "Train Epoch: 468 [800/1000 (80%)]\tLosses F.softmax: 0.001354 log_softmax: 0.003016\n",
            "Train Epoch: 468 [1000/1000 (100%)]\tLosses F.softmax: 0.003911 log_softmax: 0.011105\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5735\tAccuracy: 8441.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5582\tAccuracy: 8470.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 469 [0/1000 (0%)]\tLosses F.softmax: 0.344100 log_softmax: 0.287310\n",
            "Train Epoch: 469 [200/1000 (20%)]\tLosses F.softmax: 0.003735 log_softmax: 0.010636\n",
            "Train Epoch: 469 [400/1000 (40%)]\tLosses F.softmax: 0.002122 log_softmax: 0.001866\n",
            "Train Epoch: 469 [600/1000 (60%)]\tLosses F.softmax: 0.013094 log_softmax: 0.026583\n",
            "Train Epoch: 469 [800/1000 (80%)]\tLosses F.softmax: 0.000092 log_softmax: 0.000104\n",
            "Train Epoch: 469 [1000/1000 (100%)]\tLosses F.softmax: 0.025899 log_softmax: 0.014522\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5732\tAccuracy: 8445.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5579\tAccuracy: 8470.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 470 [0/1000 (0%)]\tLosses F.softmax: 0.084395 log_softmax: 0.117820\n",
            "Train Epoch: 470 [200/1000 (20%)]\tLosses F.softmax: 0.000650 log_softmax: 0.000757\n",
            "Train Epoch: 470 [400/1000 (40%)]\tLosses F.softmax: 0.027740 log_softmax: 0.066289\n",
            "Train Epoch: 470 [600/1000 (60%)]\tLosses F.softmax: 0.001990 log_softmax: 0.002991\n",
            "Train Epoch: 470 [800/1000 (80%)]\tLosses F.softmax: 0.066072 log_softmax: 0.058390\n",
            "Train Epoch: 470 [1000/1000 (100%)]\tLosses F.softmax: 0.054984 log_softmax: 0.071221\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5742\tAccuracy: 8445.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5588\tAccuracy: 8466.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 471 [0/1000 (0%)]\tLosses F.softmax: 0.029869 log_softmax: 0.031518\n",
            "Train Epoch: 471 [200/1000 (20%)]\tLosses F.softmax: 0.007260 log_softmax: 0.011684\n",
            "Train Epoch: 471 [400/1000 (40%)]\tLosses F.softmax: 0.001450 log_softmax: 0.001475\n",
            "Train Epoch: 471 [600/1000 (60%)]\tLosses F.softmax: 0.003502 log_softmax: 0.008080\n",
            "Train Epoch: 471 [800/1000 (80%)]\tLosses F.softmax: 0.001971 log_softmax: 0.003046\n",
            "Train Epoch: 471 [1000/1000 (100%)]\tLosses F.softmax: 0.098974 log_softmax: 0.108504\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5746\tAccuracy: 8443.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5593\tAccuracy: 8470.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 472 [0/1000 (0%)]\tLosses F.softmax: 0.001923 log_softmax: 0.002747\n",
            "Train Epoch: 472 [200/1000 (20%)]\tLosses F.softmax: 0.007287 log_softmax: 0.005633\n",
            "Train Epoch: 472 [400/1000 (40%)]\tLosses F.softmax: 0.002269 log_softmax: 0.007655\n",
            "Train Epoch: 472 [600/1000 (60%)]\tLosses F.softmax: 0.005663 log_softmax: 0.003446\n",
            "Train Epoch: 472 [800/1000 (80%)]\tLosses F.softmax: 0.002267 log_softmax: 0.003624\n",
            "Train Epoch: 472 [1000/1000 (100%)]\tLosses F.softmax: 0.001527 log_softmax: 0.004996\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5756\tAccuracy: 8447.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5602\tAccuracy: 8471.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 473 [0/1000 (0%)]\tLosses F.softmax: 0.000198 log_softmax: 0.000484\n",
            "Train Epoch: 473 [200/1000 (20%)]\tLosses F.softmax: 0.035725 log_softmax: 0.060462\n",
            "Train Epoch: 473 [400/1000 (40%)]\tLosses F.softmax: 0.000432 log_softmax: 0.000680\n",
            "Train Epoch: 473 [600/1000 (60%)]\tLosses F.softmax: 0.032418 log_softmax: 0.014987\n",
            "Train Epoch: 473 [800/1000 (80%)]\tLosses F.softmax: 0.017036 log_softmax: 0.022069\n",
            "Train Epoch: 473 [1000/1000 (100%)]\tLosses F.softmax: 0.006173 log_softmax: 0.005387\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5757\tAccuracy: 8440.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5606\tAccuracy: 8472.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 474 [0/1000 (0%)]\tLosses F.softmax: 0.031494 log_softmax: 0.043279\n",
            "Train Epoch: 474 [200/1000 (20%)]\tLosses F.softmax: 0.000899 log_softmax: 0.001208\n",
            "Train Epoch: 474 [400/1000 (40%)]\tLosses F.softmax: 0.093508 log_softmax: 0.085358\n",
            "Train Epoch: 474 [600/1000 (60%)]\tLosses F.softmax: 0.007802 log_softmax: 0.003979\n",
            "Train Epoch: 474 [800/1000 (80%)]\tLosses F.softmax: 0.004776 log_softmax: 0.001714\n",
            "Train Epoch: 474 [1000/1000 (100%)]\tLosses F.softmax: 0.022135 log_softmax: 0.043926\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5766\tAccuracy: 8457.0/10000 (85%)\n",
            "log_softmax: Loss: 0.5615\tAccuracy: 8470.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 475 [0/1000 (0%)]\tLosses F.softmax: 0.018741 log_softmax: 0.011939\n",
            "Train Epoch: 475 [200/1000 (20%)]\tLosses F.softmax: 0.000091 log_softmax: 0.000325\n",
            "Train Epoch: 475 [400/1000 (40%)]\tLosses F.softmax: 0.008293 log_softmax: 0.005593\n",
            "Train Epoch: 475 [600/1000 (60%)]\tLosses F.softmax: 0.006951 log_softmax: 0.005479\n",
            "Train Epoch: 475 [800/1000 (80%)]\tLosses F.softmax: 0.015856 log_softmax: 0.015197\n",
            "Train Epoch: 475 [1000/1000 (100%)]\tLosses F.softmax: 0.000248 log_softmax: 0.000881\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5775\tAccuracy: 8450.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5618\tAccuracy: 8466.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 476 [0/1000 (0%)]\tLosses F.softmax: 0.006737 log_softmax: 0.009499\n",
            "Train Epoch: 476 [200/1000 (20%)]\tLosses F.softmax: 0.002873 log_softmax: 0.004040\n",
            "Train Epoch: 476 [400/1000 (40%)]\tLosses F.softmax: 0.003690 log_softmax: 0.003669\n",
            "Train Epoch: 476 [600/1000 (60%)]\tLosses F.softmax: 0.016268 log_softmax: 0.013529\n",
            "Train Epoch: 476 [800/1000 (80%)]\tLosses F.softmax: 0.039088 log_softmax: 0.041568\n",
            "Train Epoch: 476 [1000/1000 (100%)]\tLosses F.softmax: 0.017505 log_softmax: 0.022599\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5774\tAccuracy: 8456.0/10000 (85%)\n",
            "log_softmax: Loss: 0.5620\tAccuracy: 8468.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 477 [0/1000 (0%)]\tLosses F.softmax: 0.004125 log_softmax: 0.001071\n",
            "Train Epoch: 477 [200/1000 (20%)]\tLosses F.softmax: 0.025415 log_softmax: 0.015970\n",
            "Train Epoch: 477 [400/1000 (40%)]\tLosses F.softmax: 0.003414 log_softmax: 0.008290\n",
            "Train Epoch: 477 [600/1000 (60%)]\tLosses F.softmax: 0.108422 log_softmax: 0.103080\n",
            "Train Epoch: 477 [800/1000 (80%)]\tLosses F.softmax: 0.018688 log_softmax: 0.018969\n",
            "Train Epoch: 477 [1000/1000 (100%)]\tLosses F.softmax: 0.096316 log_softmax: 0.096272\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5777\tAccuracy: 8451.0/10000 (85%)\n",
            "log_softmax: Loss: 0.5622\tAccuracy: 8471.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 478 [0/1000 (0%)]\tLosses F.softmax: 0.003204 log_softmax: 0.002130\n",
            "Train Epoch: 478 [200/1000 (20%)]\tLosses F.softmax: 0.045477 log_softmax: 0.024576\n",
            "Train Epoch: 478 [400/1000 (40%)]\tLosses F.softmax: 0.000121 log_softmax: 0.000305\n",
            "Train Epoch: 478 [600/1000 (60%)]\tLosses F.softmax: 0.095087 log_softmax: 0.053311\n",
            "Train Epoch: 478 [800/1000 (80%)]\tLosses F.softmax: 0.006598 log_softmax: 0.008913\n",
            "Train Epoch: 478 [1000/1000 (100%)]\tLosses F.softmax: 0.064390 log_softmax: 0.046884\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5783\tAccuracy: 8450.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5630\tAccuracy: 8473.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 479 [0/1000 (0%)]\tLosses F.softmax: 0.062012 log_softmax: 0.035803\n",
            "Train Epoch: 479 [200/1000 (20%)]\tLosses F.softmax: 0.010294 log_softmax: 0.007516\n",
            "Train Epoch: 479 [400/1000 (40%)]\tLosses F.softmax: 0.003666 log_softmax: 0.002748\n",
            "Train Epoch: 479 [600/1000 (60%)]\tLosses F.softmax: 0.001865 log_softmax: 0.001811\n",
            "Train Epoch: 479 [800/1000 (80%)]\tLosses F.softmax: 0.037038 log_softmax: 0.029780\n",
            "Train Epoch: 479 [1000/1000 (100%)]\tLosses F.softmax: 0.107077 log_softmax: 0.141441\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5790\tAccuracy: 8447.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5631\tAccuracy: 8467.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 480 [0/1000 (0%)]\tLosses F.softmax: 0.001950 log_softmax: 0.003971\n",
            "Train Epoch: 480 [200/1000 (20%)]\tLosses F.softmax: 0.035006 log_softmax: 0.009008\n",
            "Train Epoch: 480 [400/1000 (40%)]\tLosses F.softmax: 0.008617 log_softmax: 0.005865\n",
            "Train Epoch: 480 [600/1000 (60%)]\tLosses F.softmax: 0.000301 log_softmax: 0.000830\n",
            "Train Epoch: 480 [800/1000 (80%)]\tLosses F.softmax: 0.002949 log_softmax: 0.004301\n",
            "Train Epoch: 480 [1000/1000 (100%)]\tLosses F.softmax: 0.001563 log_softmax: 0.001421\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5790\tAccuracy: 8451.0/10000 (85%)\n",
            "log_softmax: Loss: 0.5634\tAccuracy: 8467.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 481 [0/1000 (0%)]\tLosses F.softmax: 0.059165 log_softmax: 0.033838\n",
            "Train Epoch: 481 [200/1000 (20%)]\tLosses F.softmax: 0.032438 log_softmax: 0.022842\n",
            "Train Epoch: 481 [400/1000 (40%)]\tLosses F.softmax: 0.008227 log_softmax: 0.011938\n",
            "Train Epoch: 481 [600/1000 (60%)]\tLosses F.softmax: 0.218123 log_softmax: 0.169652\n",
            "Train Epoch: 481 [800/1000 (80%)]\tLosses F.softmax: 0.085574 log_softmax: 0.047358\n",
            "Train Epoch: 481 [1000/1000 (100%)]\tLosses F.softmax: 0.449491 log_softmax: 0.767132\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5789\tAccuracy: 8465.0/10000 (85%)\n",
            "log_softmax: Loss: 0.5635\tAccuracy: 8475.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 482 [0/1000 (0%)]\tLosses F.softmax: 0.007088 log_softmax: 0.006646\n",
            "Train Epoch: 482 [200/1000 (20%)]\tLosses F.softmax: 0.325194 log_softmax: 0.262820\n",
            "Train Epoch: 482 [400/1000 (40%)]\tLosses F.softmax: 0.015191 log_softmax: 0.010528\n",
            "Train Epoch: 482 [600/1000 (60%)]\tLosses F.softmax: 0.000082 log_softmax: 0.000212\n",
            "Train Epoch: 482 [800/1000 (80%)]\tLosses F.softmax: 0.000670 log_softmax: 0.000468\n",
            "Train Epoch: 482 [1000/1000 (100%)]\tLosses F.softmax: 0.001255 log_softmax: 0.001127\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5797\tAccuracy: 8454.0/10000 (85%)\n",
            "log_softmax: Loss: 0.5640\tAccuracy: 8473.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 483 [0/1000 (0%)]\tLosses F.softmax: 0.009161 log_softmax: 0.002164\n",
            "Train Epoch: 483 [200/1000 (20%)]\tLosses F.softmax: 0.000553 log_softmax: 0.000301\n",
            "Train Epoch: 483 [400/1000 (40%)]\tLosses F.softmax: 0.009406 log_softmax: 0.006788\n",
            "Train Epoch: 483 [600/1000 (60%)]\tLosses F.softmax: 0.000076 log_softmax: 0.000382\n",
            "Train Epoch: 483 [800/1000 (80%)]\tLosses F.softmax: 0.000099 log_softmax: 0.000259\n",
            "Train Epoch: 483 [1000/1000 (100%)]\tLosses F.softmax: 0.001344 log_softmax: 0.003797\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5798\tAccuracy: 8457.0/10000 (85%)\n",
            "log_softmax: Loss: 0.5644\tAccuracy: 8475.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 484 [0/1000 (0%)]\tLosses F.softmax: 0.007039 log_softmax: 0.003106\n",
            "Train Epoch: 484 [200/1000 (20%)]\tLosses F.softmax: 0.000775 log_softmax: 0.000421\n",
            "Train Epoch: 484 [400/1000 (40%)]\tLosses F.softmax: 0.007014 log_softmax: 0.007519\n",
            "Train Epoch: 484 [600/1000 (60%)]\tLosses F.softmax: 0.011356 log_softmax: 0.007772\n",
            "Train Epoch: 484 [800/1000 (80%)]\tLosses F.softmax: 0.001179 log_softmax: 0.002487\n",
            "Train Epoch: 484 [1000/1000 (100%)]\tLosses F.softmax: 0.002212 log_softmax: 0.002246\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5805\tAccuracy: 8459.0/10000 (85%)\n",
            "log_softmax: Loss: 0.5650\tAccuracy: 8474.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 485 [0/1000 (0%)]\tLosses F.softmax: 0.017229 log_softmax: 0.008317\n",
            "Train Epoch: 485 [200/1000 (20%)]\tLosses F.softmax: 0.005323 log_softmax: 0.002747\n",
            "Train Epoch: 485 [400/1000 (40%)]\tLosses F.softmax: 0.048569 log_softmax: 0.032156\n",
            "Train Epoch: 485 [600/1000 (60%)]\tLosses F.softmax: 0.005037 log_softmax: 0.003989\n",
            "Train Epoch: 485 [800/1000 (80%)]\tLosses F.softmax: 0.003259 log_softmax: 0.004085\n",
            "Train Epoch: 485 [1000/1000 (100%)]\tLosses F.softmax: 0.174548 log_softmax: 0.154539\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5814\tAccuracy: 8464.0/10000 (85%)\n",
            "log_softmax: Loss: 0.5658\tAccuracy: 8464.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 486 [0/1000 (0%)]\tLosses F.softmax: 0.018128 log_softmax: 0.008678\n",
            "Train Epoch: 486 [200/1000 (20%)]\tLosses F.softmax: 0.001856 log_softmax: 0.003842\n",
            "Train Epoch: 486 [400/1000 (40%)]\tLosses F.softmax: 0.006827 log_softmax: 0.004849\n",
            "Train Epoch: 486 [600/1000 (60%)]\tLosses F.softmax: 0.003395 log_softmax: 0.002089\n",
            "Train Epoch: 486 [800/1000 (80%)]\tLosses F.softmax: 0.007051 log_softmax: 0.014071\n",
            "Train Epoch: 486 [1000/1000 (100%)]\tLosses F.softmax: 0.215656 log_softmax: 0.269659\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5814\tAccuracy: 8459.0/10000 (85%)\n",
            "log_softmax: Loss: 0.5658\tAccuracy: 8475.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 487 [0/1000 (0%)]\tLosses F.softmax: 0.015314 log_softmax: 0.010815\n",
            "Train Epoch: 487 [200/1000 (20%)]\tLosses F.softmax: 0.007976 log_softmax: 0.011227\n",
            "Train Epoch: 487 [400/1000 (40%)]\tLosses F.softmax: 0.000919 log_softmax: 0.002147\n",
            "Train Epoch: 487 [600/1000 (60%)]\tLosses F.softmax: 0.008617 log_softmax: 0.004656\n",
            "Train Epoch: 487 [800/1000 (80%)]\tLosses F.softmax: 0.001256 log_softmax: 0.001515\n",
            "Train Epoch: 487 [1000/1000 (100%)]\tLosses F.softmax: 0.097932 log_softmax: 0.092729\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5818\tAccuracy: 8463.0/10000 (85%)\n",
            "log_softmax: Loss: 0.5664\tAccuracy: 8468.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 488 [0/1000 (0%)]\tLosses F.softmax: 0.947956 log_softmax: 0.571907\n",
            "Train Epoch: 488 [200/1000 (20%)]\tLosses F.softmax: 0.001624 log_softmax: 0.001568\n",
            "Train Epoch: 488 [400/1000 (40%)]\tLosses F.softmax: 0.151194 log_softmax: 0.134247\n",
            "Train Epoch: 488 [600/1000 (60%)]\tLosses F.softmax: 0.001207 log_softmax: 0.001056\n",
            "Train Epoch: 488 [800/1000 (80%)]\tLosses F.softmax: 0.005376 log_softmax: 0.004828\n",
            "Train Epoch: 488 [1000/1000 (100%)]\tLosses F.softmax: 0.033129 log_softmax: 0.022415\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5822\tAccuracy: 8458.0/10000 (85%)\n",
            "log_softmax: Loss: 0.5665\tAccuracy: 8472.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 489 [0/1000 (0%)]\tLosses F.softmax: 0.476780 log_softmax: 0.737901\n",
            "Train Epoch: 489 [200/1000 (20%)]\tLosses F.softmax: 0.001188 log_softmax: 0.001137\n",
            "Train Epoch: 489 [400/1000 (40%)]\tLosses F.softmax: 0.001003 log_softmax: 0.002360\n",
            "Train Epoch: 489 [600/1000 (60%)]\tLosses F.softmax: 0.042286 log_softmax: 0.037401\n",
            "Train Epoch: 489 [800/1000 (80%)]\tLosses F.softmax: 0.036232 log_softmax: 0.039531\n",
            "Train Epoch: 489 [1000/1000 (100%)]\tLosses F.softmax: 0.001001 log_softmax: 0.002076\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5830\tAccuracy: 8452.0/10000 (85%)\n",
            "log_softmax: Loss: 0.5670\tAccuracy: 8475.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 490 [0/1000 (0%)]\tLosses F.softmax: 0.031213 log_softmax: 0.051896\n",
            "Train Epoch: 490 [200/1000 (20%)]\tLosses F.softmax: 0.032652 log_softmax: 0.024702\n",
            "Train Epoch: 490 [400/1000 (40%)]\tLosses F.softmax: 0.062949 log_softmax: 0.063304\n",
            "Train Epoch: 490 [600/1000 (60%)]\tLosses F.softmax: 0.001742 log_softmax: 0.001399\n",
            "Train Epoch: 490 [800/1000 (80%)]\tLosses F.softmax: 0.005426 log_softmax: 0.004184\n",
            "Train Epoch: 490 [1000/1000 (100%)]\tLosses F.softmax: 0.000156 log_softmax: 0.000801\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5830\tAccuracy: 8462.0/10000 (85%)\n",
            "log_softmax: Loss: 0.5672\tAccuracy: 8475.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 491 [0/1000 (0%)]\tLosses F.softmax: 0.003058 log_softmax: 0.007415\n",
            "Train Epoch: 491 [200/1000 (20%)]\tLosses F.softmax: 0.021125 log_softmax: 0.060665\n",
            "Train Epoch: 491 [400/1000 (40%)]\tLosses F.softmax: 0.042625 log_softmax: 0.077783\n",
            "Train Epoch: 491 [600/1000 (60%)]\tLosses F.softmax: 0.002200 log_softmax: 0.005403\n",
            "Train Epoch: 491 [800/1000 (80%)]\tLosses F.softmax: 0.002084 log_softmax: 0.001196\n",
            "Train Epoch: 491 [1000/1000 (100%)]\tLosses F.softmax: 0.015523 log_softmax: 0.010877\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5847\tAccuracy: 8454.0/10000 (85%)\n",
            "log_softmax: Loss: 0.5686\tAccuracy: 8467.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 492 [0/1000 (0%)]\tLosses F.softmax: 0.062558 log_softmax: 0.115168\n",
            "Train Epoch: 492 [200/1000 (20%)]\tLosses F.softmax: 0.007411 log_softmax: 0.015821\n",
            "Train Epoch: 492 [400/1000 (40%)]\tLosses F.softmax: 0.003518 log_softmax: 0.003256\n",
            "Train Epoch: 492 [600/1000 (60%)]\tLosses F.softmax: 0.031739 log_softmax: 0.033551\n",
            "Train Epoch: 492 [800/1000 (80%)]\tLosses F.softmax: 0.063339 log_softmax: 0.093144\n",
            "Train Epoch: 492 [1000/1000 (100%)]\tLosses F.softmax: 0.014743 log_softmax: 0.015148\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5850\tAccuracy: 8455.0/10000 (85%)\n",
            "log_softmax: Loss: 0.5692\tAccuracy: 8470.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 493 [0/1000 (0%)]\tLosses F.softmax: 0.009804 log_softmax: 0.003931\n",
            "Train Epoch: 493 [200/1000 (20%)]\tLosses F.softmax: 0.039627 log_softmax: 0.037172\n",
            "Train Epoch: 493 [400/1000 (40%)]\tLosses F.softmax: 0.173656 log_softmax: 0.133135\n",
            "Train Epoch: 493 [600/1000 (60%)]\tLosses F.softmax: 0.028650 log_softmax: 0.037515\n",
            "Train Epoch: 493 [800/1000 (80%)]\tLosses F.softmax: 0.162238 log_softmax: 0.176031\n",
            "Train Epoch: 493 [1000/1000 (100%)]\tLosses F.softmax: 0.000376 log_softmax: 0.000132\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5858\tAccuracy: 8456.0/10000 (85%)\n",
            "log_softmax: Loss: 0.5703\tAccuracy: 8459.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 494 [0/1000 (0%)]\tLosses F.softmax: 0.026133 log_softmax: 0.010524\n",
            "Train Epoch: 494 [200/1000 (20%)]\tLosses F.softmax: 0.006179 log_softmax: 0.012278\n",
            "Train Epoch: 494 [400/1000 (40%)]\tLosses F.softmax: 0.001782 log_softmax: 0.002637\n",
            "Train Epoch: 494 [600/1000 (60%)]\tLosses F.softmax: 0.005296 log_softmax: 0.003511\n",
            "Train Epoch: 494 [800/1000 (80%)]\tLosses F.softmax: 0.001014 log_softmax: 0.001543\n",
            "Train Epoch: 494 [1000/1000 (100%)]\tLosses F.softmax: 0.020173 log_softmax: 0.009300\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5848\tAccuracy: 8458.0/10000 (85%)\n",
            "log_softmax: Loss: 0.5693\tAccuracy: 8475.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 495 [0/1000 (0%)]\tLosses F.softmax: 0.027355 log_softmax: 0.004665\n",
            "Train Epoch: 495 [200/1000 (20%)]\tLosses F.softmax: 0.004664 log_softmax: 0.004182\n",
            "Train Epoch: 495 [400/1000 (40%)]\tLosses F.softmax: 0.000274 log_softmax: 0.000680\n",
            "Train Epoch: 495 [600/1000 (60%)]\tLosses F.softmax: 0.000633 log_softmax: 0.002280\n",
            "Train Epoch: 495 [800/1000 (80%)]\tLosses F.softmax: 0.063685 log_softmax: 0.074951\n",
            "Train Epoch: 495 [1000/1000 (100%)]\tLosses F.softmax: 0.004131 log_softmax: 0.005486\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5863\tAccuracy: 8449.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5706\tAccuracy: 8469.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 496 [0/1000 (0%)]\tLosses F.softmax: 0.012743 log_softmax: 0.025737\n",
            "Train Epoch: 496 [200/1000 (20%)]\tLosses F.softmax: 0.001272 log_softmax: 0.001768\n",
            "Train Epoch: 496 [400/1000 (40%)]\tLosses F.softmax: 0.093595 log_softmax: 0.040426\n",
            "Train Epoch: 496 [600/1000 (60%)]\tLosses F.softmax: 0.001435 log_softmax: 0.000712\n",
            "Train Epoch: 496 [800/1000 (80%)]\tLosses F.softmax: 0.385183 log_softmax: 0.654286\n",
            "Train Epoch: 496 [1000/1000 (100%)]\tLosses F.softmax: 0.000907 log_softmax: 0.001370\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5858\tAccuracy: 8456.0/10000 (85%)\n",
            "log_softmax: Loss: 0.5702\tAccuracy: 8478.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 497 [0/1000 (0%)]\tLosses F.softmax: 0.219708 log_softmax: 0.381950\n",
            "Train Epoch: 497 [200/1000 (20%)]\tLosses F.softmax: 0.000029 log_softmax: 0.000143\n",
            "Train Epoch: 497 [400/1000 (40%)]\tLosses F.softmax: 0.318200 log_softmax: 0.183472\n",
            "Train Epoch: 497 [600/1000 (60%)]\tLosses F.softmax: 0.022333 log_softmax: 0.022191\n",
            "Train Epoch: 497 [800/1000 (80%)]\tLosses F.softmax: 0.142863 log_softmax: 0.092735\n",
            "Train Epoch: 497 [1000/1000 (100%)]\tLosses F.softmax: 0.001024 log_softmax: 0.000555\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5866\tAccuracy: 8460.0/10000 (85%)\n",
            "log_softmax: Loss: 0.5708\tAccuracy: 8470.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 498 [0/1000 (0%)]\tLosses F.softmax: 0.007690 log_softmax: 0.011551\n",
            "Train Epoch: 498 [200/1000 (20%)]\tLosses F.softmax: 0.000348 log_softmax: 0.000308\n",
            "Train Epoch: 498 [400/1000 (40%)]\tLosses F.softmax: 0.000297 log_softmax: 0.000778\n",
            "Train Epoch: 498 [600/1000 (60%)]\tLosses F.softmax: 0.015302 log_softmax: 0.009256\n",
            "Train Epoch: 498 [800/1000 (80%)]\tLosses F.softmax: 0.018779 log_softmax: 0.015123\n",
            "Train Epoch: 498 [1000/1000 (100%)]\tLosses F.softmax: 0.009864 log_softmax: 0.008350\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5867\tAccuracy: 8449.0/10000 (84%)\n",
            "log_softmax: Loss: 0.5708\tAccuracy: 8474.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 499 [0/1000 (0%)]\tLosses F.softmax: 0.045152 log_softmax: 0.067720\n",
            "Train Epoch: 499 [200/1000 (20%)]\tLosses F.softmax: 0.021354 log_softmax: 0.007137\n",
            "Train Epoch: 499 [400/1000 (40%)]\tLosses F.softmax: 0.025419 log_softmax: 0.028087\n",
            "Train Epoch: 499 [600/1000 (60%)]\tLosses F.softmax: 0.032765 log_softmax: 0.027826\n",
            "Train Epoch: 499 [800/1000 (80%)]\tLosses F.softmax: 0.000299 log_softmax: 0.000314\n",
            "Train Epoch: 499 [1000/1000 (100%)]\tLosses F.softmax: 0.035973 log_softmax: 0.009748\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5877\tAccuracy: 8463.0/10000 (85%)\n",
            "log_softmax: Loss: 0.5714\tAccuracy: 8467.0/10000 (85%)\n",
            "\n",
            "Train Epoch: 500 [0/1000 (0%)]\tLosses F.softmax: 0.000386 log_softmax: 0.000302\n",
            "Train Epoch: 500 [200/1000 (20%)]\tLosses F.softmax: 0.025317 log_softmax: 0.041234\n",
            "Train Epoch: 500 [400/1000 (40%)]\tLosses F.softmax: 0.002042 log_softmax: 0.003254\n",
            "Train Epoch: 500 [600/1000 (60%)]\tLosses F.softmax: 0.008498 log_softmax: 0.010437\n",
            "Train Epoch: 500 [800/1000 (80%)]\tLosses F.softmax: 0.002553 log_softmax: 0.002199\n",
            "Train Epoch: 500 [1000/1000 (100%)]\tLosses F.softmax: 0.005456 log_softmax: 0.009898\n",
            "Test set:\n",
            "F.softmax: Loss: 0.5879\tAccuracy: 8457.0/10000 (85%)\n",
            "log_softmax: Loss: 0.5718\tAccuracy: 8472.0/10000 (85%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jv5dQ_scbLX4",
        "colab_type": "code",
        "outputId": "ea52bd99-2901-4687-a780-ae3a84d2defd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plot_graphs(test_log, 'loss')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZJYmqLhmuAf",
        "colab_type": "code",
        "outputId": "2c404572-8755-41f6-9874-4d5322dc6852",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plot_graphs(test_log, 'accuracy')"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYi8f4pRmw73",
        "colab_type": "code",
        "outputId": "5dc0de1d-ebc0-4b79-fc6a-0f9606fdd723",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "test_log"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'F.softmax': [(2.346628357028961, tensor(10.3200)),\n",
              "  (2.3340728853225707, tensor(10.3200)),\n",
              "  (2.3254540694236754, tensor(10.3200)),\n",
              "  (2.319427324485779, tensor(10.3200)),\n",
              "  (2.3151577219963073, tensor(10.3200)),\n",
              "  (2.311979053783417, tensor(10.3200)),\n",
              "  (2.309715236854553, tensor(10.3200)),\n",
              "  (2.3080407976150514, tensor(10.3200)),\n",
              "  (2.306786625480652, tensor(10.3200)),\n",
              "  (2.3058515606880188, tensor(12.7300)),\n",
              "  (2.3051628129959107, tensor(10.2800)),\n",
              "  (2.3046465989112854, tensor(10.2800)),\n",
              "  (2.304237362766266, tensor(10.2800)),\n",
              "  (2.303939515686035, tensor(10.2800)),\n",
              "  (2.3037228024482728, tensor(10.2800)),\n",
              "  (2.303542326831818, tensor(10.2800)),\n",
              "  (2.303411889839172, tensor(10.2800)),\n",
              "  (2.303296516895294, tensor(10.2800)),\n",
              "  (2.3031931060791018, tensor(10.2800)),\n",
              "  (2.3031310984611513, tensor(10.2800)),\n",
              "  (2.3030977776527406, tensor(10.2800)),\n",
              "  (2.3030432970046997, tensor(10.2800)),\n",
              "  (2.3030055475234987, tensor(10.2800)),\n",
              "  (2.3029924315452575, tensor(10.2800)),\n",
              "  (2.302953277206421, tensor(10.2800)),\n",
              "  (2.3029318323135377, tensor(10.2800)),\n",
              "  (2.302913365840912, tensor(10.2800)),\n",
              "  (2.302877439022064, tensor(10.2800)),\n",
              "  (2.3028534843444826, tensor(10.2800)),\n",
              "  (2.3028413380622865, tensor(10.2800)),\n",
              "  (2.3028224912643434, tensor(10.2800)),\n",
              "  (2.302809533691406, tensor(10.2800)),\n",
              "  (2.3027931621551514, tensor(10.2800)),\n",
              "  (2.3027928524971006, tensor(10.2800)),\n",
              "  (2.30279289932251, tensor(10.2800)),\n",
              "  (2.302780293083191, tensor(10.2800)),\n",
              "  (2.3027880185127256, tensor(10.2800)),\n",
              "  (2.3027893355369566, tensor(10.2800)),\n",
              "  (2.302776241016388, tensor(10.2800)),\n",
              "  (2.3027605120658876, tensor(10.2800)),\n",
              "  (2.30276510181427, tensor(10.2800)),\n",
              "  (2.3027514523506163, tensor(10.2800)),\n",
              "  (2.302734213924408, tensor(10.2800)),\n",
              "  (2.302716717529297, tensor(10.2800)),\n",
              "  (2.302695546245575, tensor(10.2800)),\n",
              "  (2.302674779319763, tensor(10.2800)),\n",
              "  (2.30268085899353, tensor(10.2800)),\n",
              "  (2.3026580753326416, tensor(10.2800)),\n",
              "  (2.302637250709534, tensor(10.2800)),\n",
              "  (2.302630645275116, tensor(10.2800)),\n",
              "  (2.302631795883179, tensor(10.2800)),\n",
              "  (2.3026230276107786, tensor(10.2800)),\n",
              "  (2.3026012687683104, tensor(10.2800)),\n",
              "  (2.3025970520973207, tensor(10.2800)),\n",
              "  (2.3026021550178526, tensor(10.2800)),\n",
              "  (2.3026058348655702, tensor(10.2800)),\n",
              "  (2.3025778854370116, tensor(10.2800)),\n",
              "  (2.302552628135681, tensor(10.2800)),\n",
              "  (2.302545721721649, tensor(10.2800)),\n",
              "  (2.302537987232208, tensor(10.2800)),\n",
              "  (2.3025372036933898, tensor(10.2800)),\n",
              "  (2.3025030965805056, tensor(10.2800)),\n",
              "  (2.3024855134010314, tensor(10.2800)),\n",
              "  (2.302489752197266, tensor(10.2800)),\n",
              "  (2.302461468601227, tensor(10.2800)),\n",
              "  (2.3024529863357546, tensor(10.2800)),\n",
              "  (2.3024368492126466, tensor(10.2800)),\n",
              "  (2.302419795131683, tensor(10.2800)),\n",
              "  (2.3024021465301514, tensor(10.2800)),\n",
              "  (2.302379137325287, tensor(10.2800)),\n",
              "  (2.3023568761825564, tensor(10.2800)),\n",
              "  (2.3023529116630552, tensor(10.2800)),\n",
              "  (2.302341299247742, tensor(10.2800)),\n",
              "  (2.30232941570282, tensor(10.2800)),\n",
              "  (2.3023097763061524, tensor(10.2800)),\n",
              "  (2.302293752193451, tensor(10.2800)),\n",
              "  (2.3022846709251406, tensor(10.2800)),\n",
              "  (2.302290000247955, tensor(10.2800)),\n",
              "  (2.302294696521759, tensor(10.2800)),\n",
              "  (2.302272016811371, tensor(10.2800)),\n",
              "  (2.3022696241378786, tensor(10.2800)),\n",
              "  (2.3022407322883605, tensor(10.2800)),\n",
              "  (2.3022410104751585, tensor(10.2800)),\n",
              "  (2.3022208590507507, tensor(10.2800)),\n",
              "  (2.3021950558662416, tensor(10.2800)),\n",
              "  (2.3021613856315613, tensor(10.2800)),\n",
              "  (2.3021648901939393, tensor(10.2800)),\n",
              "  (2.302173755264282, tensor(10.2800)),\n",
              "  (2.3021462602615355, tensor(10.2800)),\n",
              "  (2.3021388826370237, tensor(10.2800)),\n",
              "  (2.30212893409729, tensor(10.2800)),\n",
              "  (2.3021180196762083, tensor(10.2800)),\n",
              "  (2.302100211906433, tensor(10.2800)),\n",
              "  (2.302086992454529, tensor(10.2800)),\n",
              "  (2.3020902130126952, tensor(10.2800)),\n",
              "  (2.3020905672073364, tensor(10.2800)),\n",
              "  (2.302056153678894, tensor(10.2800)),\n",
              "  (2.3020508069992065, tensor(10.2800)),\n",
              "  (2.3020431350708006, tensor(10.2800)),\n",
              "  (2.302028837108612, tensor(10.2800)),\n",
              "  (2.302034618282318, tensor(10.2800)),\n",
              "  (2.302019291782379, tensor(10.2800)),\n",
              "  (2.302004037666321, tensor(10.2800)),\n",
              "  (2.301989622783661, tensor(10.2800)),\n",
              "  (2.3019917093276976, tensor(10.2800)),\n",
              "  (2.301980742263794, tensor(10.2800)),\n",
              "  (2.3019560700416566, tensor(10.2800)),\n",
              "  (2.301931645107269, tensor(10.2800)),\n",
              "  (2.301916482925415, tensor(10.2800)),\n",
              "  (2.301927149963379, tensor(10.2800)),\n",
              "  (2.301900257587433, tensor(10.2800)),\n",
              "  (2.3018865198135376, tensor(10.2800)),\n",
              "  (2.301869228744507, tensor(10.2800)),\n",
              "  (2.301839250564575, tensor(10.2900)),\n",
              "  (2.301833751773834, tensor(10.2900)),\n",
              "  (2.3018255703926087, tensor(10.3100)),\n",
              "  (2.30181601266861, tensor(10.2900)),\n",
              "  (2.3018116930007935, tensor(10.2900)),\n",
              "  (2.301801877498627, tensor(10.2900)),\n",
              "  (2.301792042541504, tensor(10.2800)),\n",
              "  (2.3017791887283323, tensor(10.2800)),\n",
              "  (2.301776731491089, tensor(10.2800)),\n",
              "  (2.301740959358215, tensor(10.2800)),\n",
              "  (2.3017241097450256, tensor(10.2800)),\n",
              "  (2.301711351585388, tensor(10.2800)),\n",
              "  (2.3016923988342284, tensor(10.2900)),\n",
              "  (2.301693192100525, tensor(10.2800)),\n",
              "  (2.3016943873405458, tensor(10.2800)),\n",
              "  (2.301670650100708, tensor(10.2900)),\n",
              "  (2.301658519744873, tensor(10.2800)),\n",
              "  (2.3016355073928834, tensor(10.2800)),\n",
              "  (2.3016373399734498, tensor(10.2800)),\n",
              "  (2.3016116276741028, tensor(10.2900)),\n",
              "  (2.301600584697723, tensor(10.2800)),\n",
              "  (2.3015914467811585, tensor(10.2900)),\n",
              "  (2.301576953125, tensor(10.2900)),\n",
              "  (2.301579301071167, tensor(10.2900)),\n",
              "  (2.301540786743164, tensor(10.2900)),\n",
              "  (2.301524492740631, tensor(10.3300)),\n",
              "  (2.3015217883110046, tensor(10.2900)),\n",
              "  (2.3015116371154787, tensor(10.2900)),\n",
              "  (2.301496421909332, tensor(10.2900)),\n",
              "  (2.301489987182617, tensor(10.2800)),\n",
              "  (2.3014691358566286, tensor(10.2800)),\n",
              "  (2.3014703607559204, tensor(10.2800)),\n",
              "  (2.3014932864189146, tensor(10.2800)),\n",
              "  (2.301484167766571, tensor(10.2800)),\n",
              "  (2.301480223655701, tensor(10.2900)),\n",
              "  (2.3014724339485166, tensor(10.2900)),\n",
              "  (2.3014571558952333, tensor(10.2800)),\n",
              "  (2.301449171257019, tensor(10.2800)),\n",
              "  (2.301437206172943, tensor(10.2800)),\n",
              "  (2.301427816295624, tensor(10.2800)),\n",
              "  (2.301418420886993, tensor(10.2800)),\n",
              "  (2.301388208389282, tensor(10.2800)),\n",
              "  (2.301375921058655, tensor(10.2800)),\n",
              "  (2.301368579006195, tensor(10.2800)),\n",
              "  (2.301354639053345, tensor(10.2800)),\n",
              "  (2.30134156703949, tensor(10.2800)),\n",
              "  (2.3013468616485597, tensor(10.2800)),\n",
              "  (2.3013343068122865, tensor(10.2800)),\n",
              "  (2.3013118521690368, tensor(10.2800)),\n",
              "  (2.30127159948349, tensor(10.2800)),\n",
              "  (2.3012645098686217, tensor(10.2900)),\n",
              "  (2.3012519011497496, tensor(10.2900)),\n",
              "  (2.301233620262146, tensor(10.2800)),\n",
              "  (2.3012098019599914, tensor(10.2900)),\n",
              "  (2.301197835159302, tensor(10.2900)),\n",
              "  (2.301187815761566, tensor(10.2900)),\n",
              "  (2.3011840757369995, tensor(10.2800)),\n",
              "  (2.3011793460845946, tensor(10.2800)),\n",
              "  (2.3011670409202574, tensor(10.2800)),\n",
              "  (2.3011466086387635, tensor(10.3100)),\n",
              "  (2.3011519733428956, tensor(10.2800)),\n",
              "  (2.3011526951789856, tensor(10.2800)),\n",
              "  (2.3011275598526, tensor(10.2800)),\n",
              "  (2.301106306648254, tensor(10.2800)),\n",
              "  (2.3010984498023985, tensor(10.2800)),\n",
              "  (2.3010525342941284, tensor(10.3100)),\n",
              "  (2.3010460084915163, tensor(10.3500)),\n",
              "  (2.3010440591812134, tensor(10.3300)),\n",
              "  (2.3010438955307007, tensor(10.3500)),\n",
              "  (2.3010248563766478, tensor(10.3300)),\n",
              "  (2.301025684738159, tensor(10.2900)),\n",
              "  (2.301015817451477, tensor(10.2900)),\n",
              "  (2.3009939233779906, tensor(10.3300)),\n",
              "  (2.3009807956695556, tensor(10.3500)),\n",
              "  (2.300971540737152, tensor(10.4200)),\n",
              "  (2.3009489802360537, tensor(10.7000)),\n",
              "  (2.300951649570465, tensor(10.3900)),\n",
              "  (2.3009439260482787, tensor(10.3500)),\n",
              "  (2.3009252306938173, tensor(10.2900)),\n",
              "  (2.3009060133934023, tensor(10.2800)),\n",
              "  (2.3008993726730345, tensor(10.2800)),\n",
              "  (2.300883086681366, tensor(10.2800)),\n",
              "  (2.3008658953666687, tensor(10.2900)),\n",
              "  (2.300847692680359, tensor(10.2900)),\n",
              "  (2.3008359835624694, tensor(10.2900)),\n",
              "  (2.3008302227020265, tensor(10.2800)),\n",
              "  (2.3008321895599364, tensor(10.2800)),\n",
              "  (2.3007995735168456, tensor(10.2800)),\n",
              "  (2.3007862340927123, tensor(10.2800)),\n",
              "  (2.3007711141586302, tensor(10.2800)),\n",
              "  (2.300744871330261, tensor(10.3300)),\n",
              "  (2.3007188548088076, tensor(10.3900)),\n",
              "  (2.3007075843811036, tensor(10.3700)),\n",
              "  (2.3006982957839965, tensor(10.6500)),\n",
              "  (2.3006842365264895, tensor(10.4200)),\n",
              "  (2.300668512535095, tensor(10.5000)),\n",
              "  (2.300650789928436, tensor(10.5900)),\n",
              "  (2.30062096452713, tensor(11.5600)),\n",
              "  (2.300619434261322, tensor(11.6000)),\n",
              "  (2.3006140309333802, tensor(10.5800)),\n",
              "  (2.300602766895294, tensor(10.5600)),\n",
              "  (2.300586334705353, tensor(10.7800)),\n",
              "  (2.300598812007904, tensor(10.5900)),\n",
              "  (2.300587202358246, tensor(10.6200)),\n",
              "  (2.3005905374526976, tensor(10.3300)),\n",
              "  (2.3005608132362365, tensor(10.3400)),\n",
              "  (2.300538015651703, tensor(10.4100)),\n",
              "  (2.3005264298439028, tensor(10.3000)),\n",
              "  (2.3005109882354735, tensor(10.3300)),\n",
              "  (2.3004917300224306, tensor(10.3600)),\n",
              "  (2.3004596530914307, tensor(10.4300)),\n",
              "  (2.300449754047394, tensor(10.3900)),\n",
              "  (2.3004505316734316, tensor(10.4100)),\n",
              "  (2.300442657661438, tensor(10.3300)),\n",
              "  (2.300439392375946, tensor(10.3300)),\n",
              "  (2.3004267488479613, tensor(10.4200)),\n",
              "  (2.300419239330292, tensor(10.5800)),\n",
              "  (2.3003808846473692, tensor(10.5500)),\n",
              "  (2.300365771865845, tensor(10.5400)),\n",
              "  (2.300333980464935, tensor(10.7600)),\n",
              "  (2.3003203165054322, tensor(11.1000)),\n",
              "  (2.300308499622345, tensor(10.4700)),\n",
              "  (2.300298836708069, tensor(10.6700)),\n",
              "  (2.3002786799430845, tensor(10.8700)),\n",
              "  (2.3002523463249207, tensor(10.6800)),\n",
              "  (2.300267863368988, tensor(10.4100)),\n",
              "  (2.30026885137558, tensor(10.3400)),\n",
              "  (2.300256317615509, tensor(10.2900)),\n",
              "  (2.300253217792511, tensor(10.2900)),\n",
              "  (2.3002502126693725, tensor(10.2800)),\n",
              "  (2.3002346991539, tensor(10.2800)),\n",
              "  (2.300206304073334, tensor(10.2800)),\n",
              "  (2.3001849933624268, tensor(10.4100)),\n",
              "  (2.300177532672882, tensor(10.3300)),\n",
              "  (2.3001710474014283, tensor(10.3700)),\n",
              "  (2.3001507397651673, tensor(10.3400)),\n",
              "  (2.3001255717277527, tensor(10.4100)),\n",
              "  (2.3001038024902343, tensor(10.4100)),\n",
              "  (2.3000897673606873, tensor(10.5700)),\n",
              "  (2.300081900024414, tensor(10.7100)),\n",
              "  (2.3000583030700685, tensor(10.7000)),\n",
              "  (2.3000416049003602, tensor(10.8200)),\n",
              "  (2.3000199992179873, tensor(12.0100)),\n",
              "  (2.2999994276046754, tensor(11.9800)),\n",
              "  (2.3000011084556578, tensor(10.7000)),\n",
              "  (2.2999766191482545, tensor(11.2900)),\n",
              "  (2.2999770087242126, tensor(10.8000)),\n",
              "  (2.299964858531952, tensor(10.8600)),\n",
              "  (2.299954207897186, tensor(11.3400)),\n",
              "  (2.2999517175674438, tensor(10.8500)),\n",
              "  (2.2999447491645815, tensor(10.7100)),\n",
              "  (2.2999229432106016, tensor(11.7000)),\n",
              "  (2.2999013289451598, tensor(11.6800)),\n",
              "  (2.299874389266968, tensor(11.7200)),\n",
              "  (2.2998506547927855, tensor(12.9600)),\n",
              "  (2.2998586913108827, tensor(11.1600)),\n",
              "  (2.2998618401527406, tensor(10.8700)),\n",
              "  (2.2998434366226195, tensor(12.7600)),\n",
              "  (2.299823612308502, tensor(13.3700)),\n",
              "  (2.2998048733711243, tensor(13.5000)),\n",
              "  (2.2997794143676757, tensor(12.6600)),\n",
              "  (2.299747445678711, tensor(12.9300)),\n",
              "  (2.299759537124634, tensor(11.3000)),\n",
              "  (2.2997109607696533, tensor(12.1900)),\n",
              "  (2.29970695438385, tensor(13.8800)),\n",
              "  (2.299713988304138, tensor(12.1400)),\n",
              "  (2.299707466316223, tensor(12.)),\n",
              "  (2.299693354511261, tensor(11.4300)),\n",
              "  (2.29967132396698, tensor(12.1400)),\n",
              "  (2.2996586679458617, tensor(11.9500)),\n",
              "  (2.299640939426422, tensor(11.6500)),\n",
              "  (2.299623692512512, tensor(11.7300)),\n",
              "  (2.299603494644165, tensor(11.9800)),\n",
              "  (2.2995819604873655, tensor(12.3900)),\n",
              "  (2.299577640914917, tensor(12.4300)),\n",
              "  (2.2995632928848266, tensor(10.8900)),\n",
              "  (2.2995464096069336, tensor(11.1100)),\n",
              "  (2.299529538536072, tensor(10.8500)),\n",
              "  (2.299525951766968, tensor(10.6800)),\n",
              "  (2.2995069858551025, tensor(10.5800)),\n",
              "  (2.2995038432121278, tensor(10.7500)),\n",
              "  (2.299501894569397, tensor(11.1700)),\n",
              "  (2.2994795649528506, tensor(11.2900)),\n",
              "  (2.299459456539154, tensor(11.5900)),\n",
              "  (2.2994486787796022, tensor(12.2000)),\n",
              "  (2.2994368052482606, tensor(12.9400)),\n",
              "  (2.299407255268097, tensor(11.2000)),\n",
              "  (2.299392489719391, tensor(10.9000)),\n",
              "  (2.2993832794189455, tensor(10.9600)),\n",
              "  (2.299384980201721, tensor(11.1400)),\n",
              "  (2.2993572304725647, tensor(11.6300)),\n",
              "  (2.299335374355316, tensor(11.5900)),\n",
              "  (2.2993032105445863, tensor(11.7500)),\n",
              "  (2.299283012390137, tensor(12.0500)),\n",
              "  (2.29926230134964, tensor(11.6700)),\n",
              "  (2.299245022583008, tensor(12.1900)),\n",
              "  (2.2992256016731263, tensor(13.6200)),\n",
              "  (2.299209524154663, tensor(14.6700)),\n",
              "  (2.299185973072052, tensor(14.9700)),\n",
              "  (2.299188976097107, tensor(15.9000)),\n",
              "  (2.2991633143424988, tensor(16.3200)),\n",
              "  (2.299136264419556, tensor(15.9900)),\n",
              "  (2.299125189399719, tensor(16.3400)),\n",
              "  (2.299116018295288, tensor(15.5600)),\n",
              "  (2.2991148404121398, tensor(15.6000)),\n",
              "  (2.2990970591545103, tensor(16.0200)),\n",
              "  (2.2990928964614867, tensor(14.5300)),\n",
              "  (2.299085582256317, tensor(15.6800)),\n",
              "  (2.299080606174469, tensor(14.2500)),\n",
              "  (2.299059590148926, tensor(14.0100)),\n",
              "  (2.2990637630462647, tensor(14.2000)),\n",
              "  (2.299045306682587, tensor(13.8700)),\n",
              "  (2.2990239523887634, tensor(13.2600)),\n",
              "  (2.2990129007339477, tensor(13.5000)),\n",
              "  (2.299000619125366, tensor(14.2900)),\n",
              "  (2.2989834759712218, tensor(15.3200)),\n",
              "  (2.2989816464424133, tensor(12.5000)),\n",
              "  (2.2989637340545652, tensor(12.9900)),\n",
              "  (2.2989562004089357, tensor(12.2900)),\n",
              "  (2.29893806848526, tensor(14.1500)),\n",
              "  (2.2989153239250184, tensor(13.9900)),\n",
              "  (2.2989037563323973, tensor(13.7100)),\n",
              "  (2.298885294818878, tensor(12.1800)),\n",
              "  (2.2988896615028382, tensor(11.1200)),\n",
              "  (2.2988538821220397, tensor(11.7800)),\n",
              "  (2.2988272362709044, tensor(12.9000)),\n",
              "  (2.2988232089042664, tensor(11.8600)),\n",
              "  (2.2988200871467592, tensor(11.6900)),\n",
              "  (2.298785059928894, tensor(11.5400)),\n",
              "  (2.298776282405853, tensor(11.0400)),\n",
              "  (2.298761453914642, tensor(10.8200)),\n",
              "  (2.2987390785217285, tensor(10.5400)),\n",
              "  (2.298721739387512, tensor(11.1100)),\n",
              "  (2.2987219268798826, tensor(10.5600)),\n",
              "  (2.298692339229584, tensor(10.7300)),\n",
              "  (2.2986548011779786, tensor(11.0400)),\n",
              "  (2.2986631901741026, tensor(10.8200)),\n",
              "  (2.29864140291214, tensor(11.1200)),\n",
              "  (2.2986257402420045, tensor(11.6500)),\n",
              "  (2.2986107619285585, tensor(12.2900)),\n",
              "  (2.298575537586212, tensor(12.3000)),\n",
              "  (2.2985604132652284, tensor(12.6700)),\n",
              "  (2.29856390914917, tensor(12.4200)),\n",
              "  (2.2985411402702334, tensor(12.5200)),\n",
              "  (2.2985193285942076, tensor(11.7400)),\n",
              "  (2.2985198826789857, tensor(12.0900)),\n",
              "  (2.2985156606674195, tensor(11.6400)),\n",
              "  (2.2984914454460146, tensor(12.8300)),\n",
              "  (2.2984675137519837, tensor(13.0200)),\n",
              "  (2.2984559565544127, tensor(12.0500)),\n",
              "  (2.298440166568756, tensor(11.7300)),\n",
              "  (2.2984071600914002, tensor(13.0200)),\n",
              "  (2.298421584701538, tensor(12.4500)),\n",
              "  (2.2984046606063844, tensor(12.0600)),\n",
              "  (2.2983894406318663, tensor(11.4700)),\n",
              "  (2.2983489645957946, tensor(11.9700)),\n",
              "  (2.2983462336540224, tensor(12.8000)),\n",
              "  (2.298314921760559, tensor(13.4100)),\n",
              "  (2.2983038286209108, tensor(12.9900)),\n",
              "  (2.2982800577163696, tensor(14.0700)),\n",
              "  (2.298268551635742, tensor(14.7900)),\n",
              "  (2.298238614273071, tensor(16.)),\n",
              "  (2.2982219264030457, tensor(14.4100)),\n",
              "  (2.298213072681427, tensor(13.8000)),\n",
              "  (2.298190302658081, tensor(14.2500)),\n",
              "  (2.298177272415161, tensor(15.1100)),\n",
              "  (2.2981666239738465, tensor(15.8900)),\n",
              "  (2.2981433973312377, tensor(14.7800)),\n",
              "  (2.298131061935425, tensor(15.7600)),\n",
              "  (2.2980904866218568, tensor(15.6800)),\n",
              "  (2.298086170959473, tensor(14.0500)),\n",
              "  (2.29806789522171, tensor(13.3400)),\n",
              "  (2.298036321926117, tensor(13.1900)),\n",
              "  (2.298023421764374, tensor(12.9800)),\n",
              "  (2.2980006979942322, tensor(14.1800)),\n",
              "  (2.297988493347168, tensor(13.6000)),\n",
              "  (2.2979763602256775, tensor(13.6500)),\n",
              "  (2.2979630776405333, tensor(12.3800)),\n",
              "  (2.297947830867767, tensor(13.4900)),\n",
              "  (2.2979374647140505, tensor(13.9300)),\n",
              "  (2.2979023427963257, tensor(15.4200)),\n",
              "  (2.297899158668518, tensor(14.3200)),\n",
              "  (2.297880277824402, tensor(15.7900)),\n",
              "  (2.2978652975082396, tensor(15.3200)),\n",
              "  (2.2978617899894713, tensor(15.4900)),\n",
              "  (2.297855412864685, tensor(15.7200)),\n",
              "  (2.2978174472808837, tensor(15.9900)),\n",
              "  (2.2978042722702026, tensor(15.1700)),\n",
              "  (2.2977814610481264, tensor(14.8300)),\n",
              "  (2.297770413780212, tensor(13.5400)),\n",
              "  (2.2977399483680725, tensor(15.7100)),\n",
              "  (2.29774246339798, tensor(14.4200)),\n",
              "  (2.2977185277938843, tensor(14.2500)),\n",
              "  (2.2977051403045654, tensor(14.2900)),\n",
              "  (2.297672857761383, tensor(15.2200)),\n",
              "  (2.297641838932037, tensor(16.2100)),\n",
              "  (2.2976337467193604, tensor(16.6900)),\n",
              "  (2.297645212841034, tensor(14.4600)),\n",
              "  (2.2976357767105102, tensor(13.8700)),\n",
              "  (2.297623822784424, tensor(14.1600)),\n",
              "  (2.2976054266929626, tensor(13.7300)),\n",
              "  (2.297583263587952, tensor(13.7600)),\n",
              "  (2.2975668966293337, tensor(14.1600)),\n",
              "  (2.297532745361328, tensor(15.1400)),\n",
              "  (2.297536302280426, tensor(14.6100)),\n",
              "  (2.297527037525177, tensor(14.9100)),\n",
              "  (2.2975287048339843, tensor(14.1800)),\n",
              "  (2.2974960310935972, tensor(14.4200)),\n",
              "  (2.297497898864746, tensor(13.2600)),\n",
              "  (2.29748554983139, tensor(12.6900)),\n",
              "  (2.2974478791236876, tensor(13.8600)),\n",
              "  (2.2974336587905886, tensor(12.5200)),\n",
              "  (2.297424744796753, tensor(12.9000)),\n",
              "  (2.297408410453796, tensor(13.5200)),\n",
              "  (2.29738209028244, tensor(14.4800)),\n",
              "  (2.2973748630523683, tensor(14.2900)),\n",
              "  (2.297347689437866, tensor(14.8300)),\n",
              "  (2.2973254873275755, tensor(16.1100)),\n",
              "  (2.2973016504287718, tensor(15.9400)),\n",
              "  (2.2972789825439452, tensor(16.1400)),\n",
              "  (2.2972398944854735, tensor(16.3900)),\n",
              "  (2.2972110023498535, tensor(16.3400)),\n",
              "  (2.2971960180282593, tensor(16.7600)),\n",
              "  (2.2971827558517455, tensor(15.9100)),\n",
              "  (2.297160619163513, tensor(14.9600)),\n",
              "  (2.297151116275787, tensor(14.2400)),\n",
              "  (2.2971222180366517, tensor(15.1900)),\n",
              "  (2.297101721096039, tensor(15.4100)),\n",
              "  (2.29706860704422, tensor(15.7300)),\n",
              "  (2.297051368331909, tensor(15.2100)),\n",
              "  (2.2970206939697264, tensor(15.4700)),\n",
              "  (2.2969916563034056, tensor(15.9400)),\n",
              "  (2.2969759031295776, tensor(14.7700)),\n",
              "  (2.296952993774414, tensor(14.6400)),\n",
              "  (2.2969435240745546, tensor(13.8300)),\n",
              "  (2.29694389629364, tensor(14.1200)),\n",
              "  (2.296927648448944, tensor(13.9000)),\n",
              "  (2.296910123157501, tensor(14.1500)),\n",
              "  (2.296894019126892, tensor(12.7700)),\n",
              "  (2.2968622612953187, tensor(13.9600)),\n",
              "  (2.2968367455482483, tensor(14.9800)),\n",
              "  (2.296837409591675, tensor(14.4400)),\n",
              "  (2.2968175422668455, tensor(14.9100)),\n",
              "  (2.296804199886322, tensor(15.7400)),\n",
              "  (2.2967849662780764, tensor(15.7500)),\n",
              "  (2.296745978832245, tensor(15.0500)),\n",
              "  (2.2967270524978636, tensor(15.7600)),\n",
              "  (2.296717081737518, tensor(14.8400)),\n",
              "  (2.2967055888175962, tensor(16.4500)),\n",
              "  (2.2966959000587464, tensor(15.9000)),\n",
              "  (2.296667236804962, tensor(16.1600)),\n",
              "  (2.2966345989227297, tensor(16.)),\n",
              "  (2.2965934056282045, tensor(16.8800)),\n",
              "  (2.2965931624412534, tensor(17.3600)),\n",
              "  (2.2965512598991396, tensor(16.6300)),\n",
              "  (2.2965294025421144, tensor(16.5700)),\n",
              "  (2.296507341003418, tensor(15.6600)),\n",
              "  (2.2964939264297484, tensor(16.8400)),\n",
              "  (2.2964870810508726, tensor(17.5000)),\n",
              "  (2.296469221019745, tensor(17.0700)),\n",
              "  (2.2964605115890504, tensor(16.0700)),\n",
              "  (2.296453782749176, tensor(16.5100)),\n",
              "  (2.2964515953063964, tensor(15.7700)),\n",
              "  (2.2964238889694215, tensor(14.7500)),\n",
              "  (2.2963883158683775, tensor(15.9900)),\n",
              "  (2.296374333000183, tensor(16.1700)),\n",
              "  (2.296338233089447, tensor(17.2600)),\n",
              "  (2.296308152580261, tensor(17.5400)),\n",
              "  (2.2962715505599975, tensor(17.9500)),\n",
              "  (2.2962520312309267, tensor(17.9400)),\n",
              "  (2.296246161556244, tensor(17.3800)),\n",
              "  (2.296229570674896, tensor(17.3800)),\n",
              "  (2.296207840633392, tensor(17.3600)),\n",
              "  (2.2961951355934143, tensor(16.9500)),\n",
              "  (2.2961804829597474, tensor(17.1700)),\n",
              "  (2.296149891757965, tensor(17.1000)),\n",
              "  (2.296144030761719, tensor(16.3500)),\n",
              "  (2.296098534297943, tensor(17.1000)),\n",
              "  (2.2960690762519835, tensor(17.3800)),\n",
              "  (2.296059536266327, tensor(17.3900)),\n",
              "  (2.296042657375336, tensor(17.1200)),\n",
              "  (2.296007532310486, tensor(17.3900)),\n",
              "  (2.295981716442108, tensor(17.3600)),\n",
              "  (2.2959691617012026, tensor(17.0500)),\n",
              "  (2.2959581124305726, tensor(17.1400)),\n",
              "  (2.2959502774238585, tensor(15.6500)),\n",
              "  (2.2959274689674376, tensor(15.3400))],\n",
              " 'log_softmax': [(2.314306327152252, tensor(11.3500)),\n",
              "  (2.3111071219444277, tensor(11.3500)),\n",
              "  (2.3087360458374024, tensor(11.3500)),\n",
              "  (2.3070159461021422, tensor(11.3500)),\n",
              "  (2.3057869119644163, tensor(11.3500)),\n",
              "  (2.3048982970237732, tensor(11.3500)),\n",
              "  (2.304258832740784, tensor(11.3500)),\n",
              "  (2.303796380805969, tensor(11.3500)),\n",
              "  (2.3034365085601807, tensor(11.3500)),\n",
              "  (2.3032027292251587, tensor(11.3500)),\n",
              "  (2.3030443954467774, tensor(11.3500)),\n",
              "  (2.3029292938232424, tensor(11.3500)),\n",
              "  (2.3028456554412844, tensor(11.3500)),\n",
              "  (2.3028070382118226, tensor(11.3500)),\n",
              "  (2.3027906465530394, tensor(11.3500)),\n",
              "  (2.302785775375366, tensor(11.3500)),\n",
              "  (2.302781760025024, tensor(11.3500)),\n",
              "  (2.302780673980713, tensor(11.3600)),\n",
              "  (2.3027748052597046, tensor(11.3700)),\n",
              "  (2.302786572551727, tensor(10.8600)),\n",
              "  (2.3028104421615603, tensor(6.9900)),\n",
              "  (2.3028179889678957, tensor(7.0200)),\n",
              "  (2.3028287476539613, tensor(8.3400)),\n",
              "  (2.302856859111786, tensor(8.8400)),\n",
              "  (2.302858866214752, tensor(8.3700)),\n",
              "  (2.302867481422424, tensor(8.1600)),\n",
              "  (2.302877156543732, tensor(8.9000)),\n",
              "  (2.302864134120941, tensor(9.1800)),\n",
              "  (2.302859018135071, tensor(9.8200)),\n",
              "  (2.302862475967407, tensor(10.1100)),\n",
              "  (2.3028603255271913, tensor(9.9700)),\n",
              "  (2.3028615436553954, tensor(10.2300)),\n",
              "  (2.3028580554008484, tensor(10.2400)),\n",
              "  (2.302868622779846, tensor(10.2400)),\n",
              "  (2.302879689693451, tensor(10.2800)),\n",
              "  (2.302876999568939, tensor(10.2600)),\n",
              "  (2.302893625926971, tensor(10.2800)),\n",
              "  (2.3029026926040648, tensor(10.2800)),\n",
              "  (2.302897361469269, tensor(10.2800)),\n",
              "  (2.3028878924369813, tensor(10.2800)),\n",
              "  (2.302898468208313, tensor(10.2800)),\n",
              "  (2.3028902300834657, tensor(10.2800)),\n",
              "  (2.3028778895378115, tensor(10.2800)),\n",
              "  (2.3028651579856874, tensor(10.2800)),\n",
              "  (2.3028485652923583, tensor(10.2800)),\n",
              "  (2.3028318205833433, tensor(10.2800)),\n",
              "  (2.3028419489860537, tensor(10.2800)),\n",
              "  (2.302822704792023, tensor(10.2800)),\n",
              "  (2.302805032444, tensor(10.2800)),\n",
              "  (2.3028015900611876, tensor(10.2800)),\n",
              "  (2.3028057737350465, tensor(10.2800)),\n",
              "  (2.302799634552002, tensor(10.2800)),\n",
              "  (2.302780226612091, tensor(10.2800)),\n",
              "  (2.302778517150879, tensor(10.2800)),\n",
              "  (2.3027861810684205, tensor(10.2800)),\n",
              "  (2.3027921486854552, tensor(10.2800)),\n",
              "  (2.302765997982025, tensor(10.2800)),\n",
              "  (2.3027425525665284, tensor(10.2800)),\n",
              "  (2.3027377046585085, tensor(10.2800)),\n",
              "  (2.302731876659393, tensor(10.2800)),\n",
              "  (2.302733072280884, tensor(10.2800)),\n",
              "  (2.30270049533844, tensor(10.2800)),\n",
              "  (2.3026844636917114, tensor(10.2800)),\n",
              "  (2.3026904829978943, tensor(10.2800)),\n",
              "  (2.302663593387604, tensor(10.2800)),\n",
              "  (2.3026567527770996, tensor(10.2800)),\n",
              "  (2.3026421184539796, tensor(10.2800)),\n",
              "  (2.3026265171051024, tensor(10.2800)),\n",
              "  (2.3026103964805604, tensor(10.2800)),\n",
              "  (2.3025887291908265, tensor(10.2800)),\n",
              "  (2.302567771434784, tensor(10.2800)),\n",
              "  (2.302565331840515, tensor(10.2800)),\n",
              "  (2.3025551410675047, tensor(10.2800)),\n",
              "  (2.302544701385498, tensor(10.2800)),\n",
              "  (2.302526330947876, tensor(10.2800)),\n",
              "  (2.3025115767478943, tensor(10.2800)),\n",
              "  (2.3025038737297057, tensor(10.2800)),\n",
              "  (2.302510745811462, tensor(10.2800)),\n",
              "  (2.302516931438446, tensor(10.2800)),\n",
              "  (2.302495484828949, tensor(10.2800)),\n",
              "  (2.3024944709777833, tensor(10.2800)),\n",
              "  (2.302466709518433, tensor(10.2800)),\n",
              "  (2.3024685346603393, tensor(10.2800)),\n",
              "  (2.302449595069885, tensor(10.2800)),\n",
              "  (2.3024249151229856, tensor(10.2800)),\n",
              "  (2.3023923175811767, tensor(10.2800)),\n",
              "  (2.3023973915100098, tensor(10.2800)),\n",
              "  (2.302407763385773, tensor(10.2800)),\n",
              "  (2.3023815448760985, tensor(10.2800)),\n",
              "  (2.302375553035736, tensor(10.2800)),\n",
              "  (2.3023669305801393, tensor(10.2800)),\n",
              "  (2.3023573128700257, tensor(10.2800)),\n",
              "  (2.3023407024383546, tensor(10.2800)),\n",
              "  (2.302328778934479, tensor(10.2800)),\n",
              "  (2.302333477115631, tensor(10.2800)),\n",
              "  (2.302335258293152, tensor(10.2800)),\n",
              "  (2.3023019220352174, tensor(10.2800)),\n",
              "  (2.3022979578971863, tensor(10.2800)),\n",
              "  (2.302291576957703, tensor(10.2800)),\n",
              "  (2.30227850894928, tensor(10.2800)),\n",
              "  (2.3022857804298402, tensor(10.2800)),\n",
              "  (2.3022716569900514, tensor(10.2800)),\n",
              "  (2.302257654762268, tensor(10.2800)),\n",
              "  (2.3022445444107054, tensor(10.2800)),\n",
              "  (2.302248208141327, tensor(10.2800)),\n",
              "  (2.3022385378837584, tensor(10.2800)),\n",
              "  (2.3022148566246035, tensor(10.2800)),\n",
              "  (2.302191671848297, tensor(10.2800)),\n",
              "  (2.302177728176117, tensor(10.2800)),\n",
              "  (2.3021899904251097, tensor(10.2800)),\n",
              "  (2.302164287185669, tensor(10.2800)),\n",
              "  (2.3021518666267395, tensor(10.2800)),\n",
              "  (2.302135868740082, tensor(10.2800)),\n",
              "  (2.302106966495514, tensor(10.2800)),\n",
              "  (2.3021029666900636, tensor(10.2800)),\n",
              "  (2.3020961634635926, tensor(10.3200)),\n",
              "  (2.3020879187583922, tensor(10.2900)),\n",
              "  (2.3020849435806277, tensor(10.2900)),\n",
              "  (2.3020765434265136, tensor(10.2800)),\n",
              "  (2.302068022441864, tensor(10.2800)),\n",
              "  (2.3020564570426942, tensor(10.2800)),\n",
              "  (2.302055466747284, tensor(10.2800)),\n",
              "  (2.3020207204818726, tensor(10.2800)),\n",
              "  (2.3020050703048707, tensor(10.2800)),\n",
              "  (2.3019937806129454, tensor(10.2800)),\n",
              "  (2.301976303577423, tensor(10.2800)),\n",
              "  (2.3019786299705505, tensor(10.2800)),\n",
              "  (2.301981346988678, tensor(10.2800)),\n",
              "  (2.301958735656738, tensor(10.2800)),\n",
              "  (2.301947917461395, tensor(10.2800)),\n",
              "  (2.3019260789871216, tensor(10.2800)),\n",
              "  (2.301929382324219, tensor(10.2800)),\n",
              "  (2.3019049437522887, tensor(10.2800)),\n",
              "  (2.3018952290534975, tensor(10.2800)),\n",
              "  (2.3018874753952026, tensor(10.2800)),\n",
              "  (2.3018742835998536, tensor(10.2800)),\n",
              "  (2.3018782742500306, tensor(10.2800)),\n",
              "  (2.3018408222198485, tensor(10.2800)),\n",
              "  (2.3018258522987365, tensor(10.3100)),\n",
              "  (2.301824666118622, tensor(10.2800)),\n",
              "  (2.301815901374817, tensor(10.2800)),\n",
              "  (2.301802112865448, tensor(10.2800)),\n",
              "  (2.301797096538544, tensor(10.2800)),\n",
              "  (2.3017775844573976, tensor(10.2800)),\n",
              "  (2.3017804591178894, tensor(10.2800)),\n",
              "  (2.3018052210807802, tensor(10.2800)),\n",
              "  (2.3017975878715515, tensor(10.2800)),\n",
              "  (2.30179520406723, tensor(10.2800)),\n",
              "  (2.301788819026947, tensor(10.2800)),\n",
              "  (2.301775005245209, tensor(10.2800)),\n",
              "  (2.3017685344696046, tensor(10.2800)),\n",
              "  (2.301757995223999, tensor(10.2800)),\n",
              "  (2.3017501185417175, tensor(10.2800)),\n",
              "  (2.3017422494888304, tensor(10.2800)),\n",
              "  (2.3017133245468138, tensor(10.2800)),\n",
              "  (2.3017024011611937, tensor(10.2800)),\n",
              "  (2.3016966733932493, tensor(10.2800)),\n",
              "  (2.301684266281128, tensor(10.2800)),\n",
              "  (2.3016727564811705, tensor(10.2800)),\n",
              "  (2.30167974023819, tensor(10.2800)),\n",
              "  (2.3016687314987183, tensor(10.2800)),\n",
              "  (2.3016477464675904, tensor(10.2800)),\n",
              "  (2.301608800315857, tensor(10.2800)),\n",
              "  (2.30160338640213, tensor(10.2800)),\n",
              "  (2.3015924145698547, tensor(10.2800)),\n",
              "  (2.3015756567001344, tensor(10.2800)),\n",
              "  (2.30155319480896, tensor(10.2800)),\n",
              "  (2.301542872810364, tensor(10.2800)),\n",
              "  (2.3015345074653624, tensor(10.2800)),\n",
              "  (2.3015325110435487, tensor(10.2800)),\n",
              "  (2.3015294240951536, tensor(10.2800)),\n",
              "  (2.3015185991287233, tensor(10.2800)),\n",
              "  (2.301499717712402, tensor(10.3000)),\n",
              "  (2.3015068923950195, tensor(10.2800)),\n",
              "  (2.3015094079017637, tensor(10.2800)),\n",
              "  (2.3014857385635374, tensor(10.2800)),\n",
              "  (2.301466004371643, tensor(10.2800)),\n",
              "  (2.3014598753929136, tensor(10.2800)),\n",
              "  (2.3014152325630186, tensor(10.3000)),\n",
              "  (2.3014103318214416, tensor(10.3300)),\n",
              "  (2.3014101105690004, tensor(10.3100)),\n",
              "  (2.3014116403579714, tensor(10.3100)),\n",
              "  (2.301394189453125, tensor(10.3000)),\n",
              "  (2.301396800994873, tensor(10.2800)),\n",
              "  (2.301388693332672, tensor(10.2800)),\n",
              "  (2.3013683693885802, tensor(10.3100)),\n",
              "  (2.3013569056510925, tensor(10.3100)),\n",
              "  (2.3013493378639223, tensor(10.3700)),\n",
              "  (2.301328360652924, tensor(10.5100)),\n",
              "  (2.3013328043937684, tensor(10.3400)),\n",
              "  (2.301326819896698, tensor(10.3100)),\n",
              "  (2.301309726524353, tensor(10.2800)),\n",
              "  (2.3012921822547914, tensor(10.2800)),\n",
              "  (2.3012873454093934, tensor(10.2800)),\n",
              "  (2.301272597408295, tensor(10.2800)),\n",
              "  (2.301257128238678, tensor(10.2800)),\n",
              "  (2.3012406143188477, tensor(10.2800)),\n",
              "  (2.3012307189941406, tensor(10.2800)),\n",
              "  (2.3012267426490784, tensor(10.2800)),\n",
              "  (2.301230662345886, tensor(10.2800)),\n",
              "  (2.3011995295524597, tensor(10.2800)),\n",
              "  (2.3011879163742064, tensor(10.2800)),\n",
              "  (2.3011745900154112, tensor(10.2800)),\n",
              "  (2.3011500255584716, tensor(10.2900)),\n",
              "  (2.3011256660461425, tensor(10.3400)),\n",
              "  (2.3011162539482117, tensor(10.3200)),\n",
              "  (2.301108916187286, tensor(10.4700)),\n",
              "  (2.3010967542648317, tensor(10.3700)),\n",
              "  (2.3010828439712525, tensor(10.4000)),\n",
              "  (2.301067017173767, tensor(10.4300)),\n",
              "  (2.3010389167785643, tensor(10.8900)),\n",
              "  (2.301039434719086, tensor(10.8900)),\n",
              "  (2.301035927772522, tensor(10.4100)),\n",
              "  (2.3010266921043394, tensor(10.4000)),\n",
              "  (2.3010121181488037, tensor(10.5400)),\n",
              "  (2.301026833152771, tensor(10.4100)),\n",
              "  (2.301017166900635, tensor(10.4500)),\n",
              "  (2.30102251663208, tensor(10.3000)),\n",
              "  (2.300994619846344, tensor(10.3100)),\n",
              "  (2.300973610019684, tensor(10.3400)),\n",
              "  (2.3009639209747315, tensor(10.2800)),\n",
              "  (2.3009504388809203, tensor(10.2800)),\n",
              "  (2.3009330137252806, tensor(10.3100)),\n",
              "  (2.30090267496109, tensor(10.3600)),\n",
              "  (2.300894762802124, tensor(10.3400)),\n",
              "  (2.300897751903534, tensor(10.3400)),\n",
              "  (2.300891913318634, tensor(10.2800)),\n",
              "  (2.30089074010849, tensor(10.2900)),\n",
              "  (2.3008801637649534, tensor(10.3600)),\n",
              "  (2.3008747282981874, tensor(10.4100)),\n",
              "  (2.3008380908966064, tensor(10.3900)),\n",
              "  (2.300824868583679, tensor(10.3900)),\n",
              "  (2.300794861602783, tensor(10.5000)),\n",
              "  (2.300783190155029, tensor(10.6600)),\n",
              "  (2.3007735760688783, tensor(10.3600)),\n",
              "  (2.300765944957733, tensor(10.4500)),\n",
              "  (2.3007477485656738, tensor(10.5500)),\n",
              "  (2.3007233946800234, tensor(10.4600)),\n",
              "  (2.3007413409233095, tensor(10.3400)),\n",
              "  (2.300744529247284, tensor(10.2900)),\n",
              "  (2.3007340792655944, tensor(10.2800)),\n",
              "  (2.300733117389679, tensor(10.2800)),\n",
              "  (2.300732334327698, tensor(10.2800)),\n",
              "  (2.300718870830536, tensor(10.2800)),\n",
              "  (2.300692574596405, tensor(10.2800)),\n",
              "  (2.3006733169555664, tensor(10.3600)),\n",
              "  (2.300668005466461, tensor(10.2800)),\n",
              "  (2.3006636894226076, tensor(10.3100)),\n",
              "  (2.300645405673981, tensor(10.3000)),\n",
              "  (2.30062228269577, tensor(10.3400)),\n",
              "  (2.300602606201172, tensor(10.3500)),\n",
              "  (2.3005907971382142, tensor(10.3700)),\n",
              "  (2.300585217761993, tensor(10.4600)),\n",
              "  (2.300563810825348, tensor(10.4400)),\n",
              "  (2.30054937210083, tensor(10.5100)),\n",
              "  (2.3005299194335938, tensor(11.2200)),\n",
              "  (2.3005115837097168, tensor(11.1500)),\n",
              "  (2.300515688800812, tensor(10.4300)),\n",
              "  (2.300493418216705, tensor(10.7200)),\n",
              "  (2.300496227169037, tensor(10.5100)),\n",
              "  (2.300486361312866, tensor(10.5100)),\n",
              "  (2.3004780535697935, tensor(10.7400)),\n",
              "  (2.300477879333496, tensor(10.5100)),\n",
              "  (2.3004733508110045, tensor(10.4400)),\n",
              "  (2.3004538040161133, tensor(10.9800)),\n",
              "  (2.300434405994415, tensor(10.9500)),\n",
              "  (2.300409624290466, tensor(10.9800)),\n",
              "  (2.3003880719184875, tensor(11.8900)),\n",
              "  (2.3003987303733826, tensor(10.6200)),\n",
              "  (2.300404470825195, tensor(10.5100)),\n",
              "  (2.300388514995575, tensor(11.6700)),\n",
              "  (2.300371024131775, tensor(12.1900)),\n",
              "  (2.300354707622528, tensor(12.4100)),\n",
              "  (2.300331531715393, tensor(11.6200)),\n",
              "  (2.300301673698425, tensor(11.8400)),\n",
              "  (2.3003164180755613, tensor(10.6900)),\n",
              "  (2.3002699271202087, tensor(11.3100)),\n",
              "  (2.3002685168266295, tensor(12.7300)),\n",
              "  (2.3002781963348387, tensor(11.2600)),\n",
              "  (2.3002742389678956, tensor(11.1400)),\n",
              "  (2.3002625400543213, tensor(10.7300)),\n",
              "  (2.300242937088013, tensor(11.2500)),\n",
              "  (2.3002327878952027, tensor(11.1300)),\n",
              "  (2.3002174965858457, tensor(10.8700)),\n",
              "  (2.3002027521133424, tensor(10.9500)),\n",
              "  (2.300185020160675, tensor(11.1400)),\n",
              "  (2.300165960597992, tensor(11.3800)),\n",
              "  (2.3001642740249633, tensor(11.3800)),\n",
              "  (2.3001524074554442, tensor(10.5000)),\n",
              "  (2.300138095188141, tensor(10.5600)),\n",
              "  (2.300123810386658, tensor(10.4900)),\n",
              "  (2.300122921562195, tensor(10.3800)),\n",
              "  (2.300106461429596, tensor(10.3600)),\n",
              "  (2.3001062012672424, tensor(10.4000)),\n",
              "  (2.3001069766044617, tensor(10.5800)),\n",
              "  (2.30008716173172, tensor(10.6200)),\n",
              "  (2.300069606208801, tensor(10.7900)),\n",
              "  (2.3000615839004515, tensor(11.2700)),\n",
              "  (2.300052414035797, tensor(11.7900)),\n",
              "  (2.300025212955475, tensor(10.5900)),\n",
              "  (2.300013125705719, tensor(10.4700)),\n",
              "  (2.3000066640853882, tensor(10.5100)),\n",
              "  (2.3000112340927124, tensor(10.5700)),\n",
              "  (2.299986111831665, tensor(10.8000)),\n",
              "  (2.2999668751716613, tensor(10.7600)),\n",
              "  (2.299937329864502, tensor(10.9000)),\n",
              "  (2.2999197722435, tensor(11.1300)),\n",
              "  (2.299901722049713, tensor(10.8200)),\n",
              "  (2.2998872352600097, tensor(11.2200)),\n",
              "  (2.2998706073760986, tensor(12.4100)),\n",
              "  (2.2998574337005615, tensor(13.5000)),\n",
              "  (2.29983656167984, tensor(13.9200)),\n",
              "  (2.2998426535606384, tensor(14.9700)),\n",
              "  (2.299819701385498, tensor(15.5300)),\n",
              "  (2.29979536819458, tensor(15.1400)),\n",
              "  (2.299787155246735, tensor(15.5400)),\n",
              "  (2.299780927371979, tensor(14.5300)),\n",
              "  (2.299782829093933, tensor(14.5900)),\n",
              "  (2.299767948913574, tensor(15.1700)),\n",
              "  (2.299766732120514, tensor(13.4300)),\n",
              "  (2.2997623693466185, tensor(14.6300)),\n",
              "  (2.29976047410965, tensor(13.0500)),\n",
              "  (2.2997423535346986, tensor(12.7000)),\n",
              "  (2.2997496914863587, tensor(12.9700)),\n",
              "  (2.299734159851074, tensor(12.6100)),\n",
              "  (2.299715597820282, tensor(11.9700)),\n",
              "  (2.299707633495331, tensor(12.2500)),\n",
              "  (2.2996983395576476, tensor(13.0600)),\n",
              "  (2.2996842741012573, tensor(14.3000)),\n",
              "  (2.2996856183052063, tensor(11.3200)),\n",
              "  (2.2996706856727602, tensor(11.6900)),\n",
              "  (2.2996661972999575, tensor(11.1900)),\n",
              "  (2.2996511317253114, tensor(12.8900)),\n",
              "  (2.2996313000679014, tensor(12.7000)),\n",
              "  (2.2996229139328004, tensor(12.4400)),\n",
              "  (2.299607443714142, tensor(11.0700)),\n",
              "  (2.299615130138397, tensor(10.4900)),\n",
              "  (2.299582343864441, tensor(10.7900)),\n",
              "  (2.2995586908340453, tensor(11.5800)),\n",
              "  (2.2995579250335694, tensor(10.8600)),\n",
              "  (2.299558077335358, tensor(10.7700)),\n",
              "  (2.2995260150909425, tensor(10.6600)),\n",
              "  (2.299520589637756, tensor(10.4400)),\n",
              "  (2.299508946990967, tensor(10.4000)),\n",
              "  (2.2994898016929626, tensor(10.3300)),\n",
              "  (2.299475756263733, tensor(10.4400)),\n",
              "  (2.2994793704032896, tensor(10.3300)),\n",
              "  (2.2994528467178346, tensor(10.3600)),\n",
              "  (2.29941843585968, tensor(10.4300)),\n",
              "  (2.2994303149223327, tensor(10.4000)),\n",
              "  (2.2994116893768313, tensor(10.4500)),\n",
              "  (2.299399351024628, tensor(10.7200)),\n",
              "  (2.2993876987457273, tensor(11.0800)),\n",
              "  (2.2993556280136107, tensor(11.1200)),\n",
              "  (2.299343865966797, tensor(11.3500)),\n",
              "  (2.299350963973999, tensor(11.1700)),\n",
              "  (2.2993314982414246, tensor(11.2800)),\n",
              "  (2.2993129643440247, tensor(10.7400)),\n",
              "  (2.2993172063827516, tensor(10.9000)),\n",
              "  (2.299316537475586, tensor(10.6700)),\n",
              "  (2.2992956824302673, tensor(11.4600)),\n",
              "  (2.299275204372406, tensor(11.5700)),\n",
              "  (2.2992672132492067, tensor(10.8700)),\n",
              "  (2.2992548318862913, tensor(10.7200)),\n",
              "  (2.29922513256073, tensor(11.5400)),\n",
              "  (2.29924335193634, tensor(11.1900)),\n",
              "  (2.299229995250702, tensor(10.8700)),\n",
              "  (2.299218206119537, tensor(10.5500)),\n",
              "  (2.2991810732841493, tensor(10.8300)),\n",
              "  (2.299182224082947, tensor(11.3700)),\n",
              "  (2.2991542951583863, tensor(11.8800)),\n",
              "  (2.299146908378601, tensor(11.4900)),\n",
              "  (2.299126655769348, tensor(12.6200)),\n",
              "  (2.2991188656806947, tensor(13.5800)),\n",
              "  (2.299092559051514, tensor(14.9800)),\n",
              "  (2.2990794463157656, tensor(12.9800)),\n",
              "  (2.299074317550659, tensor(12.3200)),\n",
              "  (2.2990551461219786, tensor(12.7700)),\n",
              "  (2.299045865726471, tensor(13.8500)),\n",
              "  (2.2990391107559205, tensor(14.8100)),\n",
              "  (2.2990194994926454, tensor(13.5300)),\n",
              "  (2.2990110200881957, tensor(14.4800)),\n",
              "  (2.2989738646507263, tensor(14.4000)),\n",
              "  (2.298973307800293, tensor(12.5400)),\n",
              "  (2.298958744907379, tensor(11.6900)),\n",
              "  (2.2989307302474975, tensor(11.5600)),\n",
              "  (2.2989216388702394, tensor(11.4200)),\n",
              "  (2.298902702331543, tensor(12.7100)),\n",
              "  (2.298894301700592, tensor(11.9400)),\n",
              "  (2.2988861535072327, tensor(12.0100)),\n",
              "  (2.2988766777038574, tensor(10.9700)),\n",
              "  (2.2988654306411744, tensor(11.8000)),\n",
              "  (2.2988590668678284, tensor(12.3700)),\n",
              "  (2.2988276514053343, tensor(14.1200)),\n",
              "  (2.298828514480591, tensor(12.7800)),\n",
              "  (2.2988135683059694, tensor(14.5400)),\n",
              "  (2.298802540397644, tensor(13.9400)),\n",
              "  (2.298803097820282, tensor(14.2500)),\n",
              "  (2.2988007613182067, tensor(14.4000)),\n",
              "  (2.2987665671348574, tensor(14.8300)),\n",
              "  (2.298757398033142, tensor(13.7900)),\n",
              "  (2.298738480949402, tensor(13.5000)),\n",
              "  (2.2987315177917482, tensor(11.7400)),\n",
              "  (2.2987049607276915, tensor(14.3400)),\n",
              "  (2.2987116654396056, tensor(12.8600)),\n",
              "  (2.298691727733612, tensor(12.6000)),\n",
              "  (2.298682458972931, tensor(12.6500)),\n",
              "  (2.2986541843414305, tensor(13.8100)),\n",
              "  (2.298627210330963, tensor(14.9800)),\n",
              "  (2.298623451423645, tensor(15.8100)),\n",
              "  (2.298639158344269, tensor(12.9800)),\n",
              "  (2.2986339853286744, tensor(12.1800)),\n",
              "  (2.298626174449921, tensor(12.4100)),\n",
              "  (2.2986118927001953, tensor(11.9200)),\n",
              "  (2.2985938214302064, tensor(11.9200)),\n",
              "  (2.2985815780639647, tensor(12.3900)),\n",
              "  (2.298551563453674, tensor(13.7100)),\n",
              "  (2.298559639072418, tensor(13.0800)),\n",
              "  (2.298554696750641, tensor(13.4900)),\n",
              "  (2.2985610767364504, tensor(12.4300)),\n",
              "  (2.2985325149536133, tensor(12.7500)),\n",
              "  (2.2985389016151427, tensor(11.4200)),\n",
              "  (2.2985308906555177, tensor(11.0500)),\n",
              "  (2.298497357940674, tensor(12.0400)),\n",
              "  (2.2984873428344725, tensor(10.9400)),\n",
              "  (2.298482968902588, tensor(11.1700)),\n",
              "  (2.298470956802368, tensor(11.6300)),\n",
              "  (2.298449073600769, tensor(12.8800)),\n",
              "  (2.298446329498291, tensor(12.5200)),\n",
              "  (2.298423602294922, tensor(13.2900)),\n",
              "  (2.2984058434486387, tensor(14.8200)),\n",
              "  (2.2983864453315737, tensor(14.6000)),\n",
              "  (2.2983682173728943, tensor(14.8400)),\n",
              "  (2.2983334672927858, tensor(15.2200)),\n",
              "  (2.2983088854789733, tensor(15.1000)),\n",
              "  (2.2982985873222352, tensor(15.7800)),\n",
              "  (2.298289880561829, tensor(14.4700)),\n",
              "  (2.298272254180908, tensor(13.4300)),\n",
              "  (2.298267387294769, tensor(12.3900)),\n",
              "  (2.29824309463501, tensor(13.6300)),\n",
              "  (2.298227210235596, tensor(13.8500)),\n",
              "  (2.298198640060425, tensor(14.2200)),\n",
              "  (2.2981860294342042, tensor(13.6500)),\n",
              "  (2.298159924507141, tensor(13.8800)),\n",
              "  (2.298135499191284, tensor(14.5000)),\n",
              "  (2.2981243633270263, tensor(13.1100)),\n",
              "  (2.2981060688972472, tensor(12.9400)),\n",
              "  (2.298101395988464, tensor(11.7900)),\n",
              "  (2.2981067519187928, tensor(12.0700)),\n",
              "  (2.298095390701294, tensor(11.8900)),\n",
              "  (2.298082712364197, tensor(12.1500)),\n",
              "  (2.2980713871955873, tensor(11.0200)),\n",
              "  (2.2980443995475768, tensor(11.9300)),\n",
              "  (2.2980237131118773, tensor(13.3100)),\n",
              "  (2.298029331970215, tensor(12.5600)),\n",
              "  (2.2980143411636353, tensor(13.2000)),\n",
              "  (2.2980059401512145, tensor(14.1800)),\n",
              "  (2.2979916133880613, tensor(14.1900)),\n",
              "  (2.297957216835022, tensor(13.4000)),\n",
              "  (2.297943338394165, tensor(14.1900)),\n",
              "  (2.297938449859619, tensor(13.1000)),\n",
              "  (2.2979321298599245, tensor(15.1300)),\n",
              "  (2.2979274495124815, tensor(14.3400)),\n",
              "  (2.2979036774635313, tensor(14.7600)),\n",
              "  (2.29787593832016, tensor(14.5400)),\n",
              "  (2.297839643573761, tensor(15.7400)),\n",
              "  (2.2978447403907776, tensor(16.4200)),\n",
              "  (2.297807747364044, tensor(15.2700)),\n",
              "  (2.297790982532501, tensor(15.1500)),\n",
              "  (2.2977740015029906, tensor(13.9600)),\n",
              "  (2.297765876865387, tensor(15.6400)),\n",
              "  (2.297764369392395, tensor(16.8500)),\n",
              "  (2.297751578712463, tensor(16.0400)),\n",
              "  (2.2977481956481935, tensor(14.5500)),\n",
              "  (2.297746876335144, tensor(15.0500)),\n",
              "  (2.297750102043152, tensor(14.0800)),\n",
              "  (2.2977275215148927, tensor(12.7800)),\n",
              "  (2.2976972619056704, tensor(14.3800)),\n",
              "  (2.297688676261902, tensor(14.6400)),\n",
              "  (2.2976579107284545, tensor(16.2300)),\n",
              "  (2.297633054256439, tensor(16.8600)),\n",
              "  (2.2976016282081604, tensor(17.4400)),\n",
              "  (2.2975874552726747, tensor(17.4200)),\n",
              "  (2.297587034225464, tensor(16.5300)),\n",
              "  (2.2975759516716003, tensor(16.4100)),\n",
              "  (2.2975595336914063, tensor(16.3400)),\n",
              "  (2.2975523074150086, tensor(15.6900)),\n",
              "  (2.297543105220795, tensor(16.1100)),\n",
              "  (2.2975178589820864, tensor(15.9600)),\n",
              "  (2.2975177147865296, tensor(14.8200)),\n",
              "  (2.297477626609802, tensor(15.9600)),\n",
              "  (2.2974536890029906, tensor(16.3500)),\n",
              "  (2.2974498439788817, tensor(16.4200)),\n",
              "  (2.2974385094642638, tensor(16.0700)),\n",
              "  (2.2974088311195375, tensor(16.4300)),\n",
              "  (2.297388625049591, tensor(16.3100)),\n",
              "  (2.2973818342208863, tensor(15.7600)),\n",
              "  (2.2973765891075133, tensor(16.0600)),\n",
              "  (2.2973744866371155, tensor(13.8200)),\n",
              "  (2.29735748462677, tensor(13.3700))]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    }
  ]
}